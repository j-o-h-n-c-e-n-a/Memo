## 1
### Q. 質問 1
ANSI SQL クエリ用の、スキーマが変更されることがある JSON ファイルの保存について、正しいものはどれですか。
1. BigQuery に保存する。データを読み込むためのフォーマット ファイルを指定し、必要に応じて更新する。
2. BigQuery に保存する。[スキーマ] で [自動検出] をオンにする。
3. Cloud Storage に保存する。BigQuery の永続テーブルとしてデータをリンクさせ、BigQuery の [スキーマ] セクションで [自動検出] オプションをオンにする。
4. Cloud Storage に保存する。BigQuery の一時テーブルとしてデータをリンクさせ、BigQuery の [スキーマ] で [自動検出] オプションをオンにする。
<details><div>
    答え：2
正解
正解です。スキーマが変更されることがある JSON ファイルと、集計 ANSI SQL クエリをサポートするには、BigQuery を使用する必要があります。さらに、スキーマの変更を検出する最も簡単な方法は [自動検出] を使用することです。
</div></details>

### Q. 質問 2
2 台の 100 TB ファイル サーバーを Google Cloud に低コストで一方向に一度だけ移行するには、どうすればよいですか（データへのアクセスは頻繁で、ドイツからのみ行われます）。
1. Storage Transfer Service を使用して、Cloud Storage Standard バケットに転送する。
2. Transfer Appliance を使用して、Cloud Storage Nearline バケットに転送する。
3. Transfer Appliance を使用して、Cloud Storage Standard バケットに転送する。
4. Storage Transfer Service を使用して、Cloud StorageColdlineバケットに転送する。
<details><div>
    答え：3
正解です。1 つのリージョン（ドイツ）のユーザー向けに、オンプレミスから Google Cloud に 1 回のみ（継続的ではなく）データ転送を行います。Standard Storage バケットの使用は、頻繁にアクセスされるデータに最適で、コストが削減され、規制要件にも適合します。
不正解
1. 不正解です。1 回限りの一方向の転送には Transfer Service のみを使用します。また、Storage Transfer Service はオンプレミスで保存されたデータでは機能しません。
2. 不正解です。頻繁にアクセスされるデータには Nearline Storage バケットを使用しません。
</div></details>

### Q. 質問 3
月 1 回の障害復旧訓練などのために、コスト効率の高い方法で別のクラウドからマルチ テラバイトのデータベースを Google Cloud にバックアップするには、どうすればよいですか。
1. Storage Transfer Service を使用して、Cloud Storage Coldline バケットに転送する。
2. Storage Transfer Service を使用して、Cloud Storage Nearline バケットに転送する。
3. Transfer Appliance を使用して、Cloud Storage Nearline バケットに転送する。
4. Transfer Appliance を使用して、Cloud Storage Coldline バケットに転送する。
<details><div>
    答え：2
正解です。障害復旧プロセスをテストするためにバックアップ データに毎月アクセスする場合は、Nearline バケットを使用する必要があります。また、通常のデータ転送も継続的に行うことになるので、Storage Transfer Service を使用する必要があります。
不正解
1. 不正解です。ファイルに毎月アクセスする場合は Coldline を使用するべきではありません。
3. 不正解です。繰り返されるデータ転送には Transfer Appliance を使用するべきではありません。
</div></details>

### Q. 質問 4
25 万台のデバイスが 10 秒ごとに JSON デバイス ステータスを生成します。外れ値の時系列分析のためにイベントデータをキャプチャするにはどうすればよいですか。 
1. Cloud Bigtable でデータをキャプチャする。Cloud Bigtable 用 HBase シェルをインストールし、これを使ってデバイスの外れ値データについてテーブルにクエリを実行する。
2. BigQuery でデータをキャプチャする。BigQuery API のカスタム アプリケーションを開発し、データセットにクエリを実行してデバイスの外れ値データを表示する。
3. Cloud Bigtable でデータをキャプチャする。Cloud Bigtable cbt ツールを使用してデバイスの外れ値データを表示する。
4. BigQuery でデータをキャプチャする。BigQuery コンソールを使用し、データセットにクエリを実行してデバイスの外れ値データを表示する。
<details><div>
    答え：3
正解です。データ型、ボリューム、クエリパターンから判断して Cloud Bigtable の機能が最も適しています。
不正解
2.不正解です。このシナリオのクエリパターンに BigQuery を使用する必要はありません。ここで重要なのは、インタラクティブ分析ではなく単一のアクション（外れ値の特定）です。また、データの速度には Cloud Bigtable のほうが適しています。
</div></details>

### Q. 質問 5
CSV 形式のイベントデータで個々の値を時間ウィンドウでクエリ処理する場合、クエリのコストを最小限に抑えられるストレージとスキーマはどれですか。
1. Cloud Bigtable を使用して、縦長のテーブルを設計し、イベントの各バージョンに新しい行を使用する。
2. Cloud Storage を使用して、未加工のファイルデータを BigQuery ログテーブルに結合する。
3. Cloud Storage を使用して、Dataprep ジョブを作成し、データをパーティション分割テーブルに分ける。
4. Cloud Bigtable を使用して、横長のテーブルを設計し、イベントの各バージョンに新しい列を使用する。
<details><div>
    答え：1
正解です。おすすめの方法として、このシナリオには Cloud Bigtable とこのスキーマを使用します。Cloud Storage は Cloud Bigtable よりもストレージ コストを低く抑えられますが、ここで必要なのはクエリのコストを最小限にすることです。
不正解
4.不正解です。クエリは各イベントの新しい列ではなく新しい行に基づいている必要があります。横長ではなく縦長のテーブルを設計する必要があります。
</div></details>

### Q. 質問 6
すでに使用している Apache Spark のコードデータ パイプラインへの投資を維持したいお客様に適しているのはどれですか。
1. Dataprep
2. Dataflow
3. Dataproc
4. BigQuery
<details><div>
    答え：3
正解
正解です。Hadoop のマネージド サービスである Dataproc は、Apache Spark アプリケーションを実行します。
</div></details>

### Q. 質問 7
Google Cloud でディープ ニューラル ネットワーク機械学習モデルをホストし、失敗することがあるジョブを実行してモニタリングするには、どうすればよいですか。
1. Google Kubernetes Engine クラスタを使用してモデルをホストし、ジョブ オブジェクトのステータスをモニタリングして、「失敗した」ジョブの状態を把握する。
2. Google Kubernetes Engine クラスタを使用してモデルをホストし、オペレーション オブジェクトのステータスをモニタリングして、「エラー」の結果を把握する。
3. VertexAIを使用してモデルをホストし、オペレーション オブジェクトのステータスをモニタリングして、「エラー」の結果を把握する。
4. VertexAIを使用してモデルをホストし、ジョブ オブジェクトのステータスをモニタリングして、「失敗した」ジョブの状態を把握する。
<details><div>
    答え：4
正解です。ディープ ニューラル ネットワーク機械学習モデルをホストするという要件を満たすことができます。TensorFlow 用 Vertex AI はディープ ニューラル ネットワークを処理できます。 オペレーションではなくジョブのモニタリングをおすすめします。
不正解
3. 不正解です。失敗のモニタリングにオペレーション オブジェクトを使用するべきではありません。
</div></details>

### Q. 質問 8
Dataproc で重要性の低い Apache Spark ジョブを実行する場合、コスト効率の高い方法はどれですか。
1. ハイメモリ マシンタイプのクラスタを標準モードで設定し、プリエンプティブル ワーカーノードを 10 個追加する。
2. デフォルトのマシンタイプのクラスタを高可用性モードで設定し、プリエンプティブル ワーカーノードを 10 個追加する。
3. デフォルトのマシンタイプのクラスタを標準モードで設定し、ローカル SSD を 10 個追加する。
4. ハイメモリ マシンタイプのクラスタを高可用性モードで設定し、ローカル SSD を 10 個追加する。
<details><div>
    答え：1
正解です。Spark とハイメモリ マシンには標準モードで十分です。また、費用を節約する必要があること、そしてミッション クリティカルなジョブではないことから、プリエンプティブル ノードを使用します。
不正解
2. 不正解です。このシナリオでは高可用性モードは要求されていません。
</div></details>

### Q. 質問 9
大量のデータを含む Cloud Bigtable ソリューションを開発環境から本番環境に展開し、パフォーマンスを最適化するにはどうすればよいですか。
1. Cloud Bigtable のインスタンスのタイプを開発から本番環境に変更し、ノード数を 3 以上に設定する。ストレージ タイプが SSD であることを確認する。
2. 現在の Cloud Bigtable インスタンスから Cloud Storage にデータをエクスポートする。3 つ以上のノードで構成された Cloud Bigtable の本番環境インスタンスのタイプを新たに作成して、SSD ストレージ タイプを選択する。Cloud Storage から新しいインスタンスにデータをインポートする。
3. 現在の Cloud Bigtable インスタンスから Cloud Storage にデータをエクスポートする。3 つ以上のノードで構成された Cloud Bigtable の本番環境インスタンスのタイプを新たに作成して、HDD ストレージ タイプを選択する。Cloud Storage から新しいインスタンスにデータをインポートする。
4. Cloud Bigtable のインスタンスのタイプを開発から本番環境に変更し、ノード数を 3 以上に設定する。ストレージ タイプが HDD であることを確認する。
<details><div>
    答え：1
正解です。Cloud Bigtable を使用すると「インプレース スケール」が可能になり、このシナリオの要件を満たします。
不正解
2. 不正解です。新しい Cloud Bigtable インスタンスの作成とデータのエクスポートは無関係かつ不要です。ノードのインプレース アップグレードは可能ですが、ストレージの種類は変更できません。
</div></details>

### Q. 質問 10
プリフェッチ キャッシュを使用して、BigQuery クエリで Google データポータル レポートを実行するコストを最小限に抑えるにはどうすればよいですか。
1. オーナーの認証情報を使用して BigQuery の基になるデータにアクセスするようにレポートを設定し、営業日（24 時間）ごとに 1 回だけレポートを表示するようユーザーに指示する。
2. 閲覧者の認証情報を使用して BigQuery の基になるデータにアクセスするようにレポートを設定し、さらに「表示専用」レポートとして設定する。
3. オーナーの認証情報を使用して BigQuery の基になるデータにアクセスするようにレポートを設定し、レポートの [キャッシュを有効化] チェックボックスがオンになっていることを確認する。
4. 閲覧者の認証情報を使用して BigQuery の基になるデータにアクセスするようにレポートを設定し、レポートの [キャッシュを有効化] チェックボックスがオフになっていることを確認する。
<details><div>
    答え：3
正解です。オーナーの認証情報を設定して、BigQuery で [キャッシュを有効化] オプションを使用する必要があります。また、プリフェッチ キャッシュを使用する必要があるビジネス シナリオでは、[キャッシュを有効化] オプションの使用が推奨されています。

1）レポートではオーナーの認証情報を使用する必要があります。2）レポートを使用しないようにユーザーに指示するのではなく、クエリ キャッシュとプリフェッチ キャッシュを使用するようにシステムを設定して、BigQuery ジョブを削減する必要があります。
不正解
1. 不正解です。キャッシュは 12 時間ごとに自動的に期限切れになります。プリフェッチ キャッシュは、オーナーの認証情報（閲覧者の認証情報ではなく）を使用するデータソースのみを対象とします。
4. 不正解です。キャッシュは 12 時間ごとに自動的に期限切れになります。プリフェッチ キャッシュは、オーナーの認証情報（閲覧者の認証情報ではなく）を使用するデータソースのみを対象とします。
</div></details>

### Q. 質問 11
データ アナリストが BigQuery クエリのコストが高くなる可能性を懸念している場合は、どう対処すればよいですか。
1. 課金される最大バイト数を設定して、処理されるバイト数を制限する。ただし、リクエストされたバイト数が上限を超えた場合でもクエリを実行する。
2. GROUP BY を使用して、結果を少数の出力値にグループ化する。
3. SELECT 句を使用してクエリ内のデータ量を制限する。データを日付別に分割してクエリの対象を絞る。
4. LIMIT 句を使用して、結果に出力される値の数を制限する。
<details><div>
    答え：3
正解です。SELECT は入力データを制限します。
不正解
1. 不正解です。クエリに含まれるバイト数が多すぎると、ジョブが失敗して実行されません。
4. 不正解です。LIMIT 句によって出力は制限されますが、データ処理は制限されません。
</div></details>

### Q. 質問 12
BigQuery データが Cloud Storage の外部 CSV ファイルに保存されています。データが増加するにつれてクエリのパフォーマンスが低下してきた場合はどうすればよいですか。
1. あらゆる処理が高速化される Cloud Bigtable に移行する。
2. データを BigQuery にインポートしてパフォーマンスを向上させる。
3. 日付に基づいてデータをパーティションに分割する。
4. より多くのスロットをリクエストして容量を増やし、パフォーマンスを向上させる。
<details><div>
    答え：2
正解です。パフォーマンスのQuestionは、データが外部ストレージ メディアに最適でない形式で格納されていることに起因しています。
不正解
1. 不正解です。BigQuery を使用するソリューションが利用可能です。また、データ パイプラインの再設計は、既存のソリューションを調整するよりも複雑です。
</div></details>

### Q. 質問 13
ソースデータがバーストでストリーミングされ、使用前に変換する必要がある場合はどうすればよいですか。
1. Cloud Storage にデータを取り込み、Dataproc を使用して ETL を行う。
2. Pub/Sub を使用してデータをバッファしてから、BigQuery を使用して ETL 処理を行う。
3. Cloud Bigtable を使用して高速で入力し、cbt を使用して ETL を行う。
4. Pub/Sub を使用してデータをバッファしてから、Dataflow を使用して ETL を行う。
<details><div>
    答え：4
正解
正解です。予測不能なデータにはバッファが必要です。
</div></details>

### Q. 質問 14
遅延して順不同で到着する可能性があるストリーミング データの移動平均を計算するにはどうすればよいですか。
1. スライディング タイム ウィンドウで Pub/Sub と Dataflow を使用する。
2. 適切なタイミングと順序で到着するように Pub/Sub を使用する。
3. 順序指定とフィルタリングに Dataflow の組み込みタイムスタンプを使用する。
4. Pub/Sub と Google データポータルを使用する。
<details><div>
    答え：1
正解
正解です。Pub/Sub と Dataflow を併用することでソリューションを提供できます。
</div></details>

### Q. 質問 15
クライアントは、アプリケーションで使用されるデータをホストする、変更頻度が低いルックアップ テーブルを提供するために、Cloud SQL データベースを使用しています。アプリケーションによってテーブルが変更されることはありません。他の地理的リージョンに進出するにあたって、クライアントが優れたパフォーマンスの確保を希望している場合は、どのような方法を提案しますか。
1. インスタンスの高可用性構成
2. 外部サーバーからのレプリケーション
3. Cloud Spanner への移行
4. リードレプリカ
<details><div>
    答え：4
正解
正解です。リードレプリカはサービスの可用性を向上させるほか、新しい地域のユーザーの近くに配置できます。
</div></details>

### Q. 質問 16
クライアントは、あるロケーションで保存したファイルを別のロケーションで取得したいと考えています。セキュリティ要件は、ファイルがクラウドでホストされている間はだれもファイルの内容にアクセスできないようにすることです。最適な方法はどれですか。
1. 顧客管理の暗号鍵（CMEK）
2. デフォルトの暗号化で構わない
3. クライアントサイド暗号化
4. 顧客指定の暗号鍵（CSEK）
<details><div>
    答え：3
不正解
1. 不正解です。クラウドでホストされている間もファイルを復号できます。
2. 不正解です。ファイルはプロジェクト内で読み取り可能になります。
4. 本ケースの要件は、クラウド内でファイルを復号できないようにすることです。この機能は復号の機密性と安全性を高めるだけで、Questionに記載されているビジネス要件を満たさないため、最適なソリューションではありません。
</div></details>

### Q. 質問 17
データ エンジニアリング ソリューションでよく一緒に使用される 3 つの Google Cloud サービスは何ですか（本コースで説明しています）。
1. Cloud Bigtable、Dataproc、Cloud Spanner
2. Pub/Sub、Dataflow、BigQuery
3. Pub/Sub、Google Kubernetes Engine、Cloud Spanner
4. Dataproc、Cloud SQL、BigQuery
<details><div>
    答え：2
正解
正解です。Pub/Sub はメッセージングを提供し、Dataflow は ETL とデータ変換に使用され、BigQuery はインタラクティブ クエリに使用されます。
</div></details>

### Q. 質問 18
AVRO は何に使用されますか。
1. データのシリアル化と非シリアル化（オブジェクト構造を維持したままデータを転送して保存できるようにする）。
2. AVRO は 38 桁の値を 9 桁の 10 進表現で格納する SQL の数値型であり、財務計算における丸め誤差の発生を防ぐ。
3. AVRO は通常 *.avr で指定される、スプレッドシートの一般的なファイル形式である。
4. AVRO は暗号化方式であり、AVRO-256 は 256 ビットキーの標準である。
<details><div>
    答え：1
正解です。AVRO はシリアル化と非シリアル化の標準です。
不正解
3. 不正解です。AVRO はファイル形式ではなく、シリアル化方式です。
</div></details>

### Q. 質問 19
ある企業が新しい IoT パイプラインを採用しました。次の設計で有効なサービスはどれですか。
図のアイコン「1」と「2」で使用する必要があるサービスを選択してください。
1. IoT Core、Pub/Sub
2. Pub/Sub、Cloud Storage
3. IoT Core、Datastore
4. App Engine、IoT Core
<details><div>
    答え：1
正解
正解です。Cloud IoT Core によって取り込まれたデバイスのデータは Pub/Sub にパブリッシュされます。
</div></details>

### Q. 質問 20
ある企業が自社のデータセンター内にある Oracle データベースにクラウド アプリケーションを接続する場合、最大 9 Gbps のデータと 99% のサービスレベル契約（SLA）という要件を満たすにはどうすればよいですか。
1. VPN 対応の Cloud Router
2. 高スループットの Cloud VPN 接続の実装
3. Partner Interconnect
4. Dedicated Interconnect
<details><div>
    答え：3
正解
正解です。Partner Interconnect は 10 Gbps までのデータに適しており、SLA を導入している ISP によって提供されます。
</div></details>

### Q. 質問 21
ローカル プログラミングの手法を使用して PCollection に基づいたパイプラインを開発しているクライアントが、本番環境にスケールアップしようとしています。どうすればよいですか。
1. Dataflow のクラウド ランナーを使用する。
2. パイプラインを BigQuery にインポートする。
3. ローカル バージョンのランナーを使用する。
4. パイプラインを Dataproc にアップロードする。
<details><div>
    答え：1
正解
正解です。PCollection は、これが Dataflow のパイプラインであることを示しており、クラウド ランナーによってパイプラインが本番環境レベルにスケールできるようになります。
</div></details>

### Q. 質問 22
Hadoop クラスタをクラウドに移行した企業が、データセンターの場合と同じ設定、同じ方法で Dataproc を使用しています。クラウド環境の有効活用のためには、どのようにアドバイスしますか。
1. Dataproc から Compute Engine でホストされているオープンソースの Hadoop クラスタに移行する。これが、効率化に必要なすべての Hadoop カスタマイズを行う唯一の方法である。
2. クラスタ使用率がコストに見合うように、実行可能なその他のジョブを探す。
3. HDFS の最新バージョンにアップグレードし、各種作業に合わせて最適化されるように Hadoop コンポーネントの設定を変更する。
4. 永続データをクラスタ外に保存する。1 種類の作業のためにクラスタを起動して、データを処理していないときはシャットダウンする。
<details><div>
    答え：4
不正解
1. 不正解です。クラウドが提供する追加機能の使用よりもデータセンターに着目したシミュレーションになっており、逆行しています。
2. 不正解です。これは、データセンターの Hadoop インストールに関するQuestionで、クラウドベースの Hadoop によって解決されます。
3. 不正解です。これは、データセンターのオープンソース Hadoop のインストールに関するアドバイスです。クラウドに適したアドバイスではありません。
</div></details>

### Q. 質問 23
アプリケーションには次のデータ要件があります。
* 強整合性を備えたトランザクションが必要である。
* データの総量は 500 GB 未満である。
* データはストリーミングする必要もリアルタイムである必要もない。
これらの要件に適したデータ技術はどれですか。
1. BigQuery
2. Cloud SQL
3. Cloud Bigtable
4. Memorystore
<details><div>
    答え：2
正解
正解です。Cloud SQL は強整合性を備えたトランザクションをサポートし
ます。サイズ要件も Cloud SQL インスタンスに適合します。
</div></details>

## 2
### Q. 
あなたは、単一のテーブルで繰り返し実行されるクエリについて、BigQuery の最適化に取り組んでいます。クエリされるデータは約 1 GB で、一部の行は 1 時間に約 10 回変更されることが予想されます。SQL ステートメントは可能な限りの最適化が済んでおり、クエリのパフォーマンスをさらに最適化したいと考えています。どうすればよいですか。
1. テーブルに基づくマテリアライズド ビューを作成して、そのビューをクエリします。
2. クエリデータのキャッシュ保存を有効にして、後続のクエリを高速化します。
3. スケジュール設定済みクエリを作成して、レポート作成の数分前にそのクエリを実行します。
4. 多数のスロットを事前に予約して、クエリの実行のためのコンピューティング能力を最大化します。
<details><div>
    答え：１
正解
1. テーブルに基づくマテリアライズド ビューを作成して、そのビューをクエリします。
フィードバック
A: 選択肢 A は正解です。マテリアライズド ビューは、パフォーマンスを向上させるためにクエリの結果を定期的にキャッシュ保存します。マテリアライズド ビューは、頻繁にクエリされる小さなデータセットに適しています。基盤となるテーブルデータが変更されると、影響を受ける部分をマテリアライズド ビューが無効化して再度読み込みます。
B: 選択肢 B は不正解です。キャッシュ保存は自動的に有効化されますが、基盤となるデータが変更されるとパフォーマンスを発揮できません。
C: 選択肢 C は不正解です。スケジュール設定済みクエリにより定期的なクエリをスケジュールできますが、パフォーマンスは特に最適化されません。また、クエリを早く実行しすぎると古いデータが使用される場合があります。
D: 選択肢 D は不正解です。多くのスロットを予約すると、BigQuery スロットの可用性は確保されますがパフォーマンスは向上しません。
 
https://cloud.google.com/bigquery/docs/materialized-views-intro
 
https://cloud.google.com/bigquery/docs/materialized-views-best-practices
 
https://cloud.google.com/bigquery/docs/materialized-views
</div></details>

### Q. 
あなたは数年前に e コマース企業用の機械学習モデルを構築しました。このモデルは高い精度で予測をしていましたが、世界的なパンデミックが発生してロックダウンが課され、多くの人々が在宅勤務を始めたため、モデルの品質が低下してしまいました。モデルの品質を向上させ、今後のパフォーマンス低下を防ぐには、どうすればよいですか。
1. ロックダウンの最初の 30 日間のデータを使用してモデルを再トレーニングします。
2. 使用パターンが正常化するまでデータをモニタリングしてから、モデルを再トレーニングします。
3. 過去 30 日間のデータを使用してモデルを再トレーニングします。1 年経過したら、古いモデルに戻します。
4. 過去 30 日間のデータを使用してモデルを再トレーニングします。モデルの入力データに変更がないか継続的にモニタリングするステップを追加してから、モデルを再トレーニングします。
<details><div>
    答え：４
フィードバック
A: 選択肢 A は不正解です。ロックダウンの最初の 30 日間のデータに基づく再トレーニングは同様のロックダウン中の予測にのみ有効で、通常の期間には有効ではありません。
B: 選択肢 B は不正解です。使用パターンの変化は恒久的である可能性があり、今後も変化し続けることが考えられます。
C: 選択肢 C は不正解です。古いモデルは 1 年後のユーザーの行動の指標にならない可能性があります。
D: 選択肢 D は正解です。元のモデルの構築に使用されたデータは関連性を失っています。過去 30 日間の最新データを使用してモデルを再トレーニングすることで、予測精度が向上します。今後のデータの動きを監視するため、受信データをモニタリングします。
 
https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai
</div></details>

### Q. 
開発チームの新しいメンバーはリモートで勤務しています。このデベロッパーはノートパソコンを使用してローカルでコードを記述する予定です。ノートパソコンは Cloud SQL の MySQL インスタンスに接続します。インスタンスには外部（公開）IP アドレスが割り当てられています。Google の推奨手法に従って新しいチームメンバーに Cloud SQL へのアクセスを許可するには、どうすればよいですか。
1. デベロッパーにノートパソコンの IP アドレスを尋ねて、承認済みネットワーク リストに追加します。
2. 外部 IP アドレスを削除して、内部 IP アドレスに置き換えます。承認済みのリストには、リモートで作業するデベロッパーのノートパソコンの IP アドレスのみを追加します。
3. Identity and Access Management（IAM）のインスタンスのアクセス権限を付与し、デベロッパーが Cloud SQL Auth Proxy を実行して MySQL インスタンスに接続するようにします。
4. Identity and Access Management（IAM）のインスタンスのアクセス権限を付与し、セキュリティのためにアクセスを「プライベート サービス アクセス」に変更して、デベロッパーがノートパソコンから Cloud SQL にアクセスできるようにします。
<details><div>
    答え：３
フィードバック
A: 選択肢 A は不正解です。承認済みネットワーク リストへの追加は可能ですが、追跡作業が増え、安全性も低下します。
B: 選択肢 B は不正解です。外部 IP アドレスを削除すると、リモートで作業するユーザーにとってアクセスがさらに複雑になります。これは、リモートのユーザーが限定公開の RFC 1918 アドレス空間内にも存在する必要があるためです。
C: 選択肢 C は正解です。推奨手法は、Cloud SQL Auth Proxy を使用することです。 権限は IAM によって制御可能で、ユーザーの IP アドレスの変更について、承認済みのリストを追跡する必要はありません。
D: 選択肢 D は不正解です。プライベート サービス アクセスは限定公開の RFC 1918 アドレス空間を必要としますが、このアドレス空間はリモートで作業するデベロッパーには使用できない場合があります。
 
https://cloud.google.com/sql/docs/mysql/sql-proxy
 
https://cloud.google.com/sql/docs/mysql/connect-admin-proxy
 
https://cloud.google.com/sql/docs/mysql/connect-overview
 
https://cloud.google.com/sql/docs/postgres/configure-ip
 
https://codelabs.developers.google.com/codelabs/cloud-sql-connectivity-gce-private
</div></details>

### Q. 
Cloud Spanner データベースには、マーケティング チームが頻繁にアクセスするお客様のアドレス情報が保存されています。お客様が居住する国と州を入力すると、この情報は、外部キーで接続されている別のテーブルに保存されます。現在のアーキテクチャにはパフォーマンスの問題があり、あなたは Google の推奨手法に従ってパフォーマンスを改善したいと考えています。どうすればよいですか。
1. インターリーブされたテーブルを作成して、国の下に州を保存します。
2. データを非正規化して、対応する国とともに各州ごとの行を作成します。
3. 既存のアーキテクチャを維持しながら、国と州に短い 2 文字のコードを使用します。
4. 国を 1 つのセルのテキストに結合します。たとえば、「country:state1,state2, …」などです。必要な場合はデータを分割します。
<details><div>
    答え：１
正解
1. インターリーブされたテーブルを作成して、国の下に州を保存します。
フィードバック
A: 選択肢 A は正解です。Cloud Spanner はインターリーブをサポートしており、同じスプリットにデータが保存されるようにします。これにより、強いデータ局所性関係が必要なときのパフォーマンスが向上します。
B: 選択肢 B は不正解です。リレーショナル データベースでは、非正規化は好ましい手法ではありません。繰り返しデータにより複数の行が発生するためです。
C: 選択肢 C は不正解です。フィールドの大きさを略称で小さくしても、あまり違いは生じません。データのアクセスと結合がパフォーマンスのさらに大きな問題となるためです。
D: 選択肢 D は不正解です。複数の種類のデータを同じセルに詰め込むことは、リレーショナル データベースでは推奨されません。
 
https://cloud.google.com/spanner/docs/schema-and-data-model#creating-interleaved-tables
</div></details>

### Q. 
あなたの会社はビジネス クリティカルなシステムを PostgreSQL で実行しています。このシステムは、数百万ものユーザーに対応しており、世界中の多くの場所から同時にアクセスされます。データベース管理チームは冗長性とスケーリングを手動で管理しており、あなたは、データベースを Google Cloud に移行したいと考えています。グローバル スケールと可用性を提供する一方でメンテンナンスが最少のソリューションが必要です。どうすればよいですか。
1. BigQuery に移行します。
2. Cloud Spanner に移行します。
3. Cloud SQL for PostgreSQL インスタンスに移行します。
4. PostgreSQL がインストールされているベアメタル マシンに移行します。
<details><div>
    答え：２
正解
2. Cloud Spanner に移行します。
フィードバック
A: 選択肢 A は不正解です。BigQuery はグローバル スケールをサポートしていません。また、BigQuery は、PostgreSQL のようなトランザクション データベースの移行にはあまり適していません。分析に重点が置かれているためです。
B: 選択肢 B は正解です。Cloud Spanner は、リレーショナル データをサポートする、グローバル スケールで高可用性のデータベースを提供します。
C: 選択肢 C は不正解です。Cloud SQL のオプションはリージョナルで、Cloud Spanner に比べてスケーラビリティが小さくなります。
D: 選択肢 D は不正解です。ベアメタル マシンで PostgreSQL を実行すると、必要なメンテナンスが増えます。
 
https://cloud.google.com/spanner/docs/migrating-postgres-spanner
</div></details>

### Q. 
あなたの会社は、お客様に関するデータを収集してお客様の健康状態を定期的にチェックしています。お客様は世界中に数百万人います。データは、ユーザーあたり 10 秒ごとに 2 つのイベントという平均レートで取り込まれますが、データをユーザーごとに Bigtable で可視化できるようにする必要があります。オペレーションのパフォーマンスが高くなるように Bigtable キーを作成する必要がありますが、どうすればよいですか。
1. キーを user-id#device-id#activity-id#timestamp として作成します。
2. キーを timestamp#user-id#device-id#activity-id として作成します。
3. キーを timestamp#device-id#activity-id#user-id として作成します。
4. キーを user-id#timestamp#device-id#activity-id として作成します。
<details><div>
    答え：１
フィードバック
A: 選択肢 A は正解です。設計が単調に増加しないため、ホットスポットを回避します。
B: 選択肢 B は不正解です。設計が単調に増加するため、ホットスポットが生じます。
C: 選択肢 C は不正解です。設計が単調に増加するため、ホットスポットが生じます。
D: 選択肢 D は不正解です。設計が単調に増加するため、ホットスポットが生じます。
 
https://cloud.google.com/bigtable/docs/schema-design
</div></details>

### Q. 
あなたの会社は、BigQuery の知識がないビジネス アナリストを数人雇用していますが、今後、彼らは BigQuery を使用して大量のデータを分析する予定です。あなたは、BigQuery でのコスト管理を行い、クエリ結果の品質を維持しつつ予算が超過しないようにする必要があります。どうすればよいですか。
1. プロジェクト レベルまたはユーザーレベルでカスタマイズした 1 日の割り当てを許容できる値に設定します。
2. BigQuery テーブルのデータを減らしてアナリストがクエリするデータ量を減らしてから、残りのデータをアーカイブします。
3. クエリ検証ツールまたは --dry_run を使用して費用を見積もれるようにアナリストをトレーニングし、アナリストが使用量を自身で制御できるようにします。
4. 各アナリストに対し BigQuery の 1 日あたりの費用をエクスポートして Looker でデータを可視化し、アナリストが使用量を自身で制御できるようにします。
<details><div>
    答え：１
正解
1. プロジェクト レベルまたはユーザーレベルでカスタマイズした 1 日の割り当てを許容できる値に設定します。
フィードバック
A: 選択肢 A は正解です。BigQuery プロジェクトとユーザーが複数ある場合は、1 日あたりに処理されるクエリデータの量に上限を指定するカスタム割り当てをリクエストして、コストを管理できます。
B: 選択肢 B は不正解です。部分的なデータのみをアナリストに渡しても、正確なクエリ結果が得られません。
C: 選択肢 C は不正解です。費用が予算を超過する可能性が残ります。この手法は、アナリストがガイドラインを常に遵守することが前提となります。
D: 選択肢 D は不正解です。費用が予算を超過する可能性が残ります。また、この手法は、アナリストが毎日チャートを確認してそれに応じて行動を調整することが前提となります。
 
https://cloud.google.com/bigquery/docs/custom-quotas
</div></details>

### Q. 
最近、Bigtable データベースが本番環境にデプロイされ、取り込まれて分析されるデータ規模が大幅に増大しましたが、パフォーマンスが低下しました。パフォーマンスの問題を特定するにはどうすればよいですか。
1. Key Visualizer を使用してパフォーマンスを分析します。
2. Cloud Trace を使用してパフォーマンスの問題を特定します。
3. ログ ステートメントをコードに追加して、どの挿入が遅延の原因か調べます。
4. クラスタにノードをさらに追加して、パフォーマンスの問題が解消されるか確認します。
<details><div>
    答え：１
フィードバック
A: 選択肢 A は正解です。Bigtable 用の Key Visualizer により、テーブルに関する視覚的なレポートが生成されます。このレポートは、アクセスする行キーに基づき使用を詳細に説明し、どのように Bigtable が動作しているかを示し、パフォーマンス問題のトラブルシューティングに役立ちます。
B: 選択肢 B は不正解です。Cloud Trace は、アプリケーションのレイテンシのデバッグに使用されます。
C: 選択肢 C は不正解です。ログ ステートメントを追加しても、Bigtable 内のパフォーマンスの問題の把握には役立ちません。
D: 選択肢 D は不正解です。ノードをさらに追加するとパフォーマンスが向上する可能性はありますが、キーが適切に設計されていないと、データベースで引き続きパフォーマンスの問題が生じます。
 
https://cloud.google.com/bigtable/docs/keyvis-overview
</div></details>

### Q. 
あなたの会社はデータ分析を BigQuery に移行しています。他のオペレーションはオンプレミスのままとなる予定です。このため、800 TB の過去のデータを転送する必要があります。また、翌日に分析するために、毎日 30 Gbps のデータ転送をアペンドする計画をしなくてはなりません。Google 推奨手法に従ってデータを転送するにはどうすればよいですか。
1. Cloud VPN を使用して、毎日できるだけ早く既存データをインターネットで転送します。
2. Transfer Appliance を使用して既存データを Google Cloud に移動します。Cloud VPN を使用して、データを毎日転送します。
3. Transfer Appliance を使用して既存データを Google Cloud に移動します。VPC ネットワーク ピアリングを使用して、データを毎日転送します。
4. Transfer Appliance を使用して既存データを Google Cloud に移動します。毎日の転送用に、Dedicated Interconnect または Partner Interconnect を設定します。
<details><div>
    答え：４
フィードバック
A: 選択肢 A は不正解です。一般的にインターネットは安定性と速度が低く、大量のデータの転送には向きません。
B: 選択肢 B は不正解です。Cloud VPN は数 Gbps（1.5～3 Gbps）のレートでのデータ転送に有効です。
C: 選択肢 C は不正解です。VPC ネットワーク ピアリングは Google Cloud 組織内のデータ転送に使用されます。
D: 選択肢 D は正解です。数百テラバイトのデータの転送には Transfer Appliance の使用が推奨されます。 定期的に大量のデータが転送される場合、専用のハイブリッド ネットワーク接続が推奨されます。
 
https://cloud.google.com/hybrid-connectivity
 
https://cloud.google.com/transfer-appliance/docs/4.0
 
https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview
</div></details>

### Q. 
あなたのチームは Dataproc ワークロードを実行しており、ワーカーノードは処理に約 45 分かかります。費用の面から、ワーカーノードを積極的にシャットダウンすることも含めてシステムを最適化するさまざまな選択肢を検討してきましたが、指標ではジョブ全体がさらに長くなってしまいます。費用を抑えながらジョブ完了までの時間を延長せずにシステムを最適化するには、どうすればよいですか。
1. 正常なデコミッションのタイムアウトを 45 分より大きく設定します。
2. Cloud Data Fusion での処理を書き換えて、ジョブを自動で実行します。
3. Dataflow での処理を書き換えて、同一データのストリーム処理を使用します。
4. 各ワーカーノードの vCPU の数を増やして処理完了までの時間を短縮します。
<details><div>
    答え：１
正解
1. 正常なデコミッションのタイムアウトを 45 分より大きく設定します。
フィードバック
A: 選択肢 A は正解です。正常なデコミッションにより、ワーカーノードで進行中の作業が Dataproc クラスタから削除される前に終了します。
B: 選択肢 B は不正解です。Cloud Data Fusion でのデータ パイプラインの再構築は、労力、費用、時間が増大します。
C: 選択肢 C は不正解です。Dataflow のコードの書き換えは、労力、費用、時間が増大します。
D: 選択肢 D は不正解です。vCPU の数の増加により費用が大幅に増大します。
 
https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters
 
https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#choosing_a_graceful_decommissioning_timeout
</div></details>

### Q. 
お客様の SQL Server データベースには、別のパブリック クラウドにある約 5 TB のデータが含まれます。データは最大 25 TB まで増大することが予測されます。このデータベースは、週に 1 度使用される社内レポート アプリケーションのバックエンドです。このアプリケーションを Google Cloud に移行して、費用を同レベルに維持するか削減しつつ管理作業を軽減するには、どうすればよいですか。
1. データベースを Bigtable に移行します。
2. データベースを Cloud Spanner に移行します。
3. SQL Server を Compute Engine VM にインストールします。
4. データベースを Cloud SQL の SQL Server に移行します。
<details><div>
    答え：４
フィードバック
A: 選択肢 A は不正解です。Bigtable は NoSQL データベースであるため、SQL Server ソースには適切ではありません。
B: 選択肢 B は不正解です。Cloud Spanner は Cloud SQL より費用がかかります。Spanner はグローバルな可用性がありますが、このアプリケーション要件には不要です。
C: 選択肢 C は不正解です。カスタムの SQL Server インスタンスを Compute Engine VM にインストールすると、管理作業が増加します。
D: 選択肢 D は正解です。Cloud SQL はマネージド MySQL、PostgreSQL、SQL Server データベースを提供し、管理作業を軽減します。Cloud SQL は 25 TB に効率的に対応できます。
 
https://cloud.google.com/sql/docs/sqlserver/quickstart
 
https://cloud.google.com/sql/docs/sqlserver/import-export/import-export-sql
 
https://cloud.google.com/products/databases
 
https://cloud.google.com/sql
</div></details>

### Q. 
IT チームは構造化データの保存に BigQuery を使用しています。財務チームは、最近、スタンドアロンのデスクトップ版スプレッドシート プロセッサから Google Workspace Enterprise エディションに移行しました。財務チームがデータ分析情報を必要なとき、IT チームは BigQuery でクエリを実行してデータを CSV ファイルにエクスポートし、メールの添付ファイルとして財務チームメンバーに送信します。財務チームが慣れているデータ分析方法を変えずにこのプロセスを改善するには、どうすればよいですか。
1. BigQuery でクエリを実行して、分析可能な結果ビューへのアクセスを財務チームに許可します。
2. BigQuery でクエリを実行して、Google データポータルのデータ ビジュアリゼーションへのアクセスを財務チームに許可します。
3. BigQuery でクエリを実行して、データを CSV にエクスポートし、そのファイルを Cloud Storage バケットにアップロードして財務チームと共有します。
4. BigQuery でクエリを実行して、財務チームがアクセスして分析できる Google スプレッドシートの共有スプレッドシートに結果を保存します。
<details><div>
    答え：４
正解
4. BigQuery でクエリを実行して、財務チームがアクセスして分析できる Google スプレッドシートの共有スプレッドシートに結果を保存します。
フィードバック
A: 選択肢 A は不正解です。財務チームは Google Cloud へのアクセス権が必要なことに加えて、BigQuery の使用に関するトレーニングも必要となります。財務チームにとって BigQuery はなじみのある方法ではありません。
B: 選択肢 B は不正解です。データポータルでビジュアリゼーションへのアクセスを許可するだけでは、財務チームはデータを分析できません。
C: 選択肢 C は不正解です。財務チームは Google Cloud へのアクセス権が必要なことに加え、Cloud Storage の使用に関するトレーニングも必要となります。財務チームにとって Google Cloud はなじみのある方法ではありません。
D: 選択肢 D は正解です。コネクテッド シートにより、Google スプレッドシートを通じて BigQuery データを簡単に共有できます。
 
https://cloud.google.com/bigquery/docs/connected-sheets
 
https://cloud.google.com/bigquery/docs/writing-results#saving-query-results-to-sheets
 
https://www.youtube.com/watch?v=rkimIhnLKGI
</div></details>

### Q. 
あなたのスクーター シェアリング会社は、位置、バッテリー残量、速度など、保有するスクーターに関する情報を収集しています。このデータはリアルタイムで可視化されます。断続的な接続を防ぐため、各スクーターは短い間隔で特定のメッセージを繰り返し送信します。時折、データエラーも見受けられます。メッセージは、Pub/Sub で受信されて BigQuery に保存されます。データに重複がなく、空白フィールドのある異常なデータが拒否されるようにするには、どうすればよいですか。
1. データを BigQuery に保存して、異常なデータと重複データのクエリ削除を実行します。
2. Dataflow を使用して Pub/Sub にサブスクライブし、データを処理して BigQuery に保存します。
3. Kubernetes を使用して、重複データと異常データを削除できるマイクロサービス アプリケーションを作成します。その後に、データを BigQuery に挿入します。
4. マネージド インスタンス グループで Compute Engine に重複データと異常データを削除できるアプリケーションを作成します。その後に、データを BigQuery に挿入します。
<details><div>
    答え：２
フィードバック
A: 選択肢 A は不正解です。BigQuery にデータを直接保存するとデータが上書きされる可能性があり、異常データが削除されるまで存在することになります。このような懸念を防ぐ回避策を BigQuery で実現しようとすると労力、時間、費用が増大します。
B: 選択肢 B は正解です。Dataflow は、ストリーミング データのための推奨データ処理プロダクトです。Dataflow をプログラミングして重複を削除し、空白フィールドを削除して、他のカスタムデータ処理を実行できます。
C: 選択肢 C は不正解です。Kubernetes でのストリーミング処理用カスタム アプリケーションの作成は非常に手間がかかり、推奨されません。
D: 選択肢 D は不正解です。Compute Engine でのストリーミング処理用カスタム アプリケーションの作成は非常に手間がかかり、推奨されません。
 
https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-planning
</div></details>

### Q. 
あなたの暗号通貨取引会社では、価格を可視化してお客様の取引の意思決定をサポートしています。さまざまな取引がリアルタイムで行われるため、価格データは、処理に Dataflow を使用するデータ パイプラインに提供されます。移動平均を計算するにはどうすればよいですか。
1. Dataflow でホッピング ウィンドウを使用します。
2. Dataflow でセッション ウィンドウを使用します。
3. Dataflow でタンブリング ウィンドウを使用します。
4. Dataflow SQL を使用して、時間でグループ化された平均を計算します。
<details><div>
    答え：１
正解
1. Dataflow でホッピング ウィンドウを使用します。
フィードバック
A: 選択肢 A は正解です。ホッピング ウィンドウを使用して移動平均を計算できます。
B: 選択肢 B は不正解です。セッション ウィンドウは移動平均の計算には使用されません。
C: 選択肢 C は不正解です。タンブリング ウィンドウは移動平均の計算には使用されません。
D: 選択肢 D は不正解です。時間によるグループ化だけでは移動平均を計算できません。
 
https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines
</div></details>

### Q. 
あなたは、数百万人のトレーダーがいる証券取引所用の取引プラットフォームを構築しています。取引データは迅速に書き込まれます。あなたは、特定の株式の経時的な価格変動などのデータを迅速に取得して、可視化データをトレーダーに表示する必要があります。そのために Google Cloud のストレージ ソリューションを選択しなくてはなりません。どうすればよいですか。
1. Bigtable を使用します。
2. Firestore を使用します。
3. Cloud SQL を使用します。
4. Memorystore を使用します。
<details><div>
    答え：１
正解
1. Bigtable を使用します。
フィードバック
A: 選択肢 A は正解です。Bigtable は、高スループットの読み取りと書き込みを必要とする時系列データに推奨されるデータベースです。
B: 選択肢 B は不正解です。Firestore には、時系列データに最適な高スループット機能がありません。
C: 選択肢 C は不正解です。Cloud SQL には、時系列データに最適な高スループット機能がありません。
D: 選択肢 D は不正解です。Memorystore は高速インメモリ データベースで、大量のデータの永続的な保存には向いていません。
 
https://cloud.google.com/bigtable/docs/overview#what-its-good-for
</div></details>

### Q. 
お客様は、Hadoop と Spark を使用してデータ分析をオンプレミスで実行しています。主要データはハードディスクに保存され、アクセスは一元的です。お客様は、スケーラビリティを考慮しつつ、ワークロードを Google Cloud に効率的に移行する必要があります。作業が最小限のアーキテクチャを選択するには、どうすればよいですか。
1. Dataproc を使用して Hadoop と Spark のジョブを実行します。データを Cloud Storage に移動します。
2. Dataflow を使用して、サーバーレス アプローチでジョブを再作成します。データを Cloud Storage に移動します。
3. Dataproc を使用して Hadoop と Spark のジョブを実行します。永続ディスクが接続された Compute Engine VM でデータを保持します。
4. Dataflow を使用して、サーバーレス アプローチでジョブを再作成します。永続ディスクが接続された Compute Engine VM でデータを保持します。
<details><div>
    答え：１
フィードバック
A: 選択肢 A は正解です。Dataproc は、Apache Spark、Presto、Apache Flink、Apache Hadoop など、オープンソースの分散処理プラットフォームを Google Cloud でホストするためのフルマネージド サービスです。Google Storage は、永続ストレージのあらゆるニーズに対応できる、おすすめのストレージ オプションです。
B: 選択肢 B は不正解です。Dataflow を使用するには、すべてのジョブを書き換える必要があります。
C: 選択肢 C は不正解です。一元的にアクセスされるデータを永続ディスクに保存することは推奨されません。
D: 選択肢 D は不正解です。Dataflow を使用するには、すべてのジョブを書き換える必要があります。一元的にアクセスされるデータを永続ディスクに保存することは推奨されません。
 
https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs
 
https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage
 
https://cloud.google.com/blog/topics/developers-practitioners/dataproc-best-practices-guide
</div></details>

### Q. 
あなたは BigQuery で作業するアナリスト チームのメンバーで、すでに SQL に精通しています。チームは、BigQuery のデータを使用する、マルチラベルの機械学習分類モデルを構築する必要があります。トレーニング データセットには 6,000 行のデータがあります。推論は 200 のラベルの可能性の内の一つとなると考えられます。高精度のモデルを作成するにはどうすればよいですか。
1. BigQuery ML を使用してモデルを作成します。
2. データを CSV ファイルにエクスポートします。TensorFlow を使用してモデルを構築します。
3. BigQuery のデータを AutoML に接続し、AutoML でモデルを構築します。
4. AI Notebooks を使用してデータに接続して、モデルをインタラクティブに構築します。
<details><div>
    答え：３
正解
3. BigQuery のデータを AutoML に接続し、AutoML でモデルを構築します。
フィードバック
A: 選択肢 A は不正解です。高精度のモデルを構築するには BigQuery ML は多くのデータを必要としますが、手元にあるデータは多くありません。
B: 選択肢 B は不正解です。TensorFlow でカスタムモデルを構築するには十分なデータがありません。
C: 選択肢 C は正解です。データ量が比較的少なく多様なため、このデータのみでは構築されたモデルの精度は低くなります。AutoML は他の類似データに基づく転移学習を使用するため、適切です。
D: 選択肢 D は不正解です。AI Notebooks でカスタムモデルを構築するには十分なデータがありません。
 
https://cloud.google.com/vertex-ai/docs/start/automl-model-types#tabular
</div></details>

### Q. 
少量のデータを使用して、テスト時に適切な推論を提示する機械学習モデルを作成しましたが、結果は、実世界データを使用してモデルを実行するとエラーが多くなることを示しています。テストのために追加データを収集することはできません。モデルの能力をもっと正確に把握するにはどうすればよいですか。
1. データ量を減らしてモデルを改善します。
2. データを交差検証して、モデル構築プロセスを再度実行します。
3. 新しい列を追加する特徴クロスを作成して、データ量を増やします。
4. データを 2 回複製してデータを増やし、モデル構築プロセスを再度実行します。
<details><div>
    答え：２
フィードバック
A: 選択肢 A は不正解です。このモデルは学習不足ではありません。
B: 選択肢 B は正解です。このモデルは過学習と思われます。交差検証によりデータを複数回使用して検証が実行され、過学習が軽減されます。
C: 選択肢 C は不正解です。新しい列を追加しても、過学習は軽減されません。
D: 選択肢 D は不正解です。データを複製しても、過学習は軽減されません。
 
https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting
</div></details>

### Q. 
あなたの組織は、住所やクレジット カード詳細など、お客様に関する情報を多年にわたり収集しており、この顧客データを使用して Google Cloud に機械学習モデルを構築する予定です。あなたは機械学習モデルへの個人データの漏洩を心配しています。経営陣も、個人データの直接的な漏洩により同社の評判が損なわれることを懸念しています。データ セキュリティに関するこのような懸念に対処するには、どうすればよいですか。
1. センシティブ データを含むすべてのテーブルを削除します。
2. SciPy などのライブラリを使用してローカル PC に ML モデルを構築します。
3. Cloud Data Loss Prevention（DLP）API を使用してセンシティブ データを削除します。
4. センシティブ データを含む行を特定し、SQL クエリを使用してこれらの行だけを削除します。
<details><div>
    答え：３
フィードバック
A: 選択肢 A は不正解です。テーブル全体などデータを削除すると、作成したモデルの効果が低くなる可能性があります。
B: 選択肢 B は不正解です。機械学習モデルを単体のパソコンに構築することは、大量のデータを扱う場合、現実的な方法ではありません。
C: 選択肢 C は正解です。Cloud DLP は、データ プライバシーの保護に役立つテキストと画像の削除、マスク、トークン化、変換の推奨手法です。
D: 選択肢 D は不正解です。行全体などのデータを削除すると、作成したモデルの効果が低くなる可能性があります。
 
https://cloud.google.com/dlp/docs/concepts-de-identification
</div></details>

### Q. 
あなたの会社のヘルスケアのアプリケーションには、IoT デバイスからイベントデータを直接受け取るバックエンド システムがあります。最近、アプリケーションのユーザーとデバイスが増加しており、システムに負担のかかる突然のデータ流入が発生しています。あなたは、データ パイプラインを再設計して、すべてのデータが処理されイベントが失われないようにする必要があります。Google が推奨する方法に沿って対応する場合、どうすればよいですか。
1. Kafka を pull モードで使用します。
2. Pub/Sub を pull モードで使用します
3. Pub/Sub を push モードで使用します。
4. Cloud Scheduler を一定の間隔で実行します。
<details><div>
    答え：２
フィードバック
A: 選択肢 A は不正解です。Kafka は Google Cloud のマネージド ソリューションではありません。Google 推奨のオプションは、フルマネージドのサーバーレス ソリューションである Pub/Sub です。
B: 選択肢 B は正解です。pull モードにより、前のデータの処理時に、新規イベントデータをオンデマンドでの処理に pull できます。その間、Pub/Sub は新規イベントを失うことなく吸収して保持します。
C: 選択肢 C は不正解です。push モードの Pub/Sub は、システムに引き続き負担をかける可能性があります。
D: 選択肢 D は不正解です。新規イベントデータは、前の処理の完了時に処理のために pull される必要があります。これが一定の間隔で行われることは予期されません。
 
https://cloud.google.com/pubsub/docs/pull
 
https://cloud.google.com/pubsub/docs/push
</div></details>

### Q. 
レポート生成アプリケーションを毎晩 11:30 PM に実行する Cloud Scheduler のジョブがあります。他のアプリケーションは、レポート情報を Pub/Sub トピックに一日中送信し、レポート生成アプリケーションは同じトピックにサブスクライブされます。すべてのレポート生成リクエストを受信するようにレポート生成アプリケーションを設定するには、どうすればよいですか。
1. pull メカニズムを使用してリクエストされたレポートを収集します。
2. push メカニズムを使用してリクエストされたレポートを収集します。
3. 未処理のメッセージ キューを使用して、リクエストされたレポートを収集します。
4. 毎晩スナップショットを作成して、最後のスナップショット以降に作成されたすべてのレポート リクエストを取得します。
<details><div>
    答え：１
フィードバック
A: 選択肢 A は正解です。レポート アプリケーションがオンラインでないときもあるため、pull メカニズムを使用する必要があります。
B: 選択肢 B は不正解です。データを受信するエンドポイントは、受信できるオンライン状態にない可能性があります。
C: 選択肢 C は不正解です。未処理のメッセージ キューは、送信と承認ができなかったメッセージの保持に使用されます。
D: 選択肢 D は不正解です。スナップショットの使用は、推奨される手法ではありません。
 
https://cloud.google.com/pubsub/docs/subscriber#push_pull
</div></details>

### Q. 
あなたの会社は、オンプレミスのデータセンターを Google Cloud に移動しています。あなたは、MongoDB Atlas をビジネスクリティカルなシステムの一部として実行してきました。今後、マルチクラウド アプローチに移行する予定です。Google Cloud の MongoDB Atlas に最小限の労力で移行するには、どうすればよいですか。
1. Google Cloud Marketplace で、おすすめのバージョンの MongoDB Atlas を選択してインストールします。
2. MongoDB サポートに連絡し、連携して MongoDB Atlas をインストールします。
3. VM を Compute Engine にプロビジョニングし、オンプレミスのデータセンターと同様に MongoDB Atlas をインストールします。
4. オンプレミスの MongoDB を最初に Firestore に移行し、Firestore アプリケーションが安定したら Firestore を MongoDB Atlas に移行します。
<details><div>
    答え：１
フィードバック
A: 選択肢 A は正解です。Marketplace には、Google Cloud によって精査された、簡単にデプロイできる統合ソリューションがあります。
B: 選択肢 B は不正解です。MongoDB Atlas のインストールには多大な労力が必要です。
C: 選択肢 C は不正解です。この手順では、自身ですべて手動インストールするという労力が発生します。
D: 選択肢 D は不正解です。Firestore の中間データストアは不要な逸脱で、追加作業が必要です。
 
https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro
</div></details>

### Q. 
あなたは、財務チームが費用請求をもっと迅速に処理できるように支援するツールを構築しています。従業員は、レシートの画像が添付された、説明が最小限の費用請求を提出します。監査と分析のために、レシートの詳細をキャプチャするにはどうすればよいですか。
1. Document AI をワークフローに統合し、レシート画像の情報をキャプチャしてデータベースに追加します。
2. AutoML Vision を使用してレシート画像の情報をキャプチャし、データベースに追加します。
3. Pandas と SciPy を使用してレシート画像から情報を抽出できるモデルを構築して、情報をデータベースに追加します。
4. Cloud Natural Language API を使用してレシート画像からテキストを抽出し、データベースに追加します。
<details><div>
    答え：１
正解
1. Document AI をワークフローに統合し、レシート画像の情報をキャプチャしてデータベースに追加します。
フィードバック
A: 選択肢 A は正解です。Document AI では、請求書の画像を読み込んでテキスト情報を抽出する事前パッケージ ソリューションが提供されます。
B: 選択肢 B は不正解です。AutoML Vision には、レシートと請求書の画像から情報を抽出する既製のモデルがありません。
C: 選択肢 C は不正解です。適切なデータの収集と高品質の ML モデルの構築には、時間と労力がさらにかかります。
D: 選択肢 D は不正解です。Cloud Natural Language API には、レシートと請求書の画像から情報を抽出する既製のモデルがありません。Cloud Natural Language API は、非構造化テキストの分析情報を引き出す際に使用されます。
 
https://cloud.google.com/document-ai
 
https://cloud.google.com/document-ai/docs/processors-list#processor_expense-parser
</div></details>

### Q. 
機械学習モデルの構築に使用するデータセットがあります。そのデータセットはあなたにはなじみのないものですが、社内の他のユーザーはそれで作業しておりよく知っています。あなたは、ML モデルを反復的に構築しつつ、データを自身で確認したいと考えています。また、インタラクティブに協力してチーム環境で ML モデルを構築することも考えています。  どうすればよいですか。
1. Vertex AI Workbench ノートブックを使用して、ノートブックを同僚と共有します。
2. Google ドキュメントを使用して、ドキュメントを同僚と共有します。
3. BigQuery テーブルを使用して同僚にビューを提供します。
4. BigQuery を使用して、データを Looker で可視化します。ダッシュボードを同僚と共有します。
<details><div>
    答え：１
正解
1. Vertex AI Workbench ノートブックを使用して、ノートブックを同僚と共有します。
フィードバック
A: 選択肢 A は正解です。Vertex AI Workbench ノートブックでは、他のユーザーと協力して、インタラクティブかつ反復的に機械学習モデルを構築できます。
B: 選択肢 B は不正解です。Google ドキュメントはコラボレーション機能を提供しますが、他の ML モデル構築ツールと直接統合されていません。
C: 選択肢 C は不正解です。BigQuery ビューの共有ではデータが共有されるだけで、モデル構築作業の残りの部分はサイロ化された状態となります。
D: 選択肢 D は不正解です。データのビジュアリゼーションのみを共有するだけでは、目的の達成には不十分です。他の ML モデル構築機能も必要です。
 
https://cloud.google.com/vertex-ai-workbench
 
https://cloud.google.com/vertex-ai/docs/workbench/introduction
</div></details>

### Q. 
あなたの会社は Cloud SQL を 2 つのリージョンで実行しています。1 つ目のリージョンである us-central1 はエンドユーザーに近く、本番環境での使用頻度は多く、予測可能です。もう一つのリージョンである europe-west1 は開発チームに近く、使用は断続的です。労力、レイテンシ、パフォーマンス面で妥協することなく費用を削減するには、どうすればよいですか。
1. 米国リージョンの確約利用割引（CUD）を利用します。開発チームの割り当て方法をそのまま保持します
2. 米国リージョンの確約利用割引（CUD）を利用します。開発チームの割り当てを米国リージョンに移動し、低い費用のメリットを利用します。
3. 希望する費用の削減の程度に応じて、VM に割り当てられている仮想 CPU の数を 1% 単位で減らします。
4. カスタムのより低コストの VM を Compute Engine にプロビジョニングして、必要に応じてデータベースをインストールします。
その他:
<details><div>
    答え：１
正解
1. 米国リージョンの確約利用割引（CUD）を利用します。開発チームの割り当て方法をそのまま保持します
フィードバック
A: 選択肢 A は正解です。使用が予測可能な場合、Cloud SQL の確約利用割引を利用できます。
B: 選択肢 B は不正解です。すべてのシステムを米国に移動して確約利用割引を利用すると費用の面でメリットがありますが、レイテンシの面でデメリットがあります。
C: 選択肢 C は不正解です。vCPU を減らすとパフォーマンスが低下します。
D: 選択肢 D は不正解です。データベース管理のローリングは、労力が余計にかかります。
 
https://cloud.google.com/sql/cud
 
https://cloud.google.com/blog/products/databases/reduce-cloud-sql-costs-with-optimizations-by-active-assist/
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>
