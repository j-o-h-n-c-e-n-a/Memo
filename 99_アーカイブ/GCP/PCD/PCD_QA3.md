
## Q. 
Cloud Buildを使ってDockerイメージをビルドしています。単体テストと統合テストを実行するためにビルドを変更する必要があります。障害が発生した場合、ビルド履歴にビルドが失敗した段階を明確に表示させたいとします。
どうすればいいでしょうか？
1. ユニットテストと統合テストを実行するために、DockerfileにRUNコマンドを追加します。
2. ユニットテストと統合テストをコンパイルする単一のビルドステップを持つCloud Buildビルド設定ファイルを作成する。
3. 単体テストと統合テスト用に別々のクラウドビルドパイプラインを生成するクラウドビルドビルド設定ファイルを作成します。
4. ユニットテストと統合テストをコンパイルして実行するために、個別のクラウドビルダーステップを持つクラウドビルドビルド構成ファイルを作成します。
<details><div>
    答え：
説明
不正解
A. DockerfileにRUNコマンドを追加すると、イメージのビルドプロセス中にテストが実行されますが、ビルド履歴にビルドが失敗した段階が明確に表示されません。
B. 単体テストと統合テストの両方に対して単一のビルドステップを作成しても、ビルドがどの段階で失敗したかを理解するのに必要なきめ細かさは得られません。
C. C. 単体テストと統合テスト用に別々のクラウドビルドパイプラインを生成すると、不必要に複雑になり、必要な明確なビルド履歴が得られない可能性があります。
正解
D. Dockerイメージのビルドと単体テストおよび統合テストの実行を同時に処理し、障害が発生した段階を明確に表示する最善の方法は、各タスクに個別のステップを持つCloud Buildビルド設定ファイルを作成することです。
コンパイル、単体テストの実行、統合テストの実行を別々のステップにすることで、Cloud Buildはビルド履歴にビルドが失敗した段階を明確に表示します。
リンク
https://cloud.google.com/build/docs/build-config-file-schema
</div></details>

## Q. 
Compute Engineインスタンス上で起動するアプリケーションを、複数のプロジェクトにまたがって開発しています。
各プロジェクトのインスタンスは、同じアプリケーションコードを持ちますが、構成は異なります。デプロイ時に、各インスタンスは、提供する環境に基づいてアプリケーションの構成を受け取る必要があります。このフローを構成するためのステップ数を最小限に抑えたい。
どうすればよいでしょうか？
1. インスタンスを作成するときに、gcloudコマンドを使用して起動スクリプトを構成し、正しい環境を示すプロジェクト名を決定します。
2. 各プロジェクトで、提供する環境に対応する値を持つ「environment」という名前のメタデータキーを構成します。デプロイツールを使用してインスタンスメタデータを照会し、環境の値に基づいてアプリケーションを構成します。
3. 選択したデプロイツールを各プロジェクトのインスタンスにデプロイします。デプロイジョブを使用してバージョン管理システムから適切な設定ファイルを取得し、各インスタンスにアプリケーションをデプロイするときに設定を適用します。
4. 各インスタンスの起動時に、インスタンスが提供する環境に対応する値を持つ「environment」という名前のインスタンスカスタムメタデータキーを構成します。デプロイメント ツールを使用してインスタンス メタデータをクエリし、環境値に基づいてアプリケーションを構成します。
<details><div>
    答え：
説明
不正解
A. 
この方法は機能する可能性がありますが、プロジェクト名から環境を決定するスタートアップスクリプトを記述し、維持することは、特にプロジェクトの命名規則が将来変更された場合に、より複雑でエラーが発生しやすくなる可能性があります。
C. 
この方法では、各プロジェクトのインスタンスにデプロイツールをデプロイし、デプロイ ジョブを設定する必要があるため、より複雑で時間がかかる可能性があります。
D. 
このアプローチでは、インスタンスごとに手動で構成する必要があり、プロジェクト全体の設定を利用しないため、不整合や管理オーバーヘッドの増加につながる可能性があります。
正解
B. 
この方法はシンプルで柔軟だ。環境にプロジェクト全体のメタデータキーを設定することで、環境を識別するための明確で一貫性のある方法が作成されます。デプロイツールはこのメタデータを簡単に照会し、カスタムスクリプトや手動介入を必要とせずに、適切な設定を適用できます。
リンク
https://cloud.google.com/compute/docs/metadata/querying-metadata
</div></details>

## Q. 
あなたはイベント駆動型のアプリケーションを開発しています。Pub/Sub に送信されたメッセージを受信するトピックを作成しました。これらのメッセージはリアルタイムで処理する必要があります。アプリケーションは他のシステムから独立しており、新しいメッセージが到着したときにのみコストが発生する必要があります。
どのようにアーキテクチャを構成しますか？
1. Compute Engineにアプリケーションをデプロイします。Pub/Subプッシュサブスクリプションを使用して、トピック内の新しいメッセージを処理します。
2. コードをCloud Functionsにデプロイする。Pub/Sub トリガーを使用してクラウド関数を呼び出します。Pub/Sub API を使用して、Pub/Sub トピックへのプル・サブスクリプションを作成し、そこからメッセージを読み取ります。
3. Google Kubernetes Engineにアプリケーションをデプロイする。Pub/Sub APIを使用して、Pub/SubトピックへのPullサブスクリプションを作成し、そこからメッセージを読み込む。
4. コードをCloud Functionsにデプロイする。Pub/Sub トリガーを使用して、トピック内の新しいメッセージを処理します。
<details><div>
    答え：
説明
不正解
A. 
これは、新しいメッセージが到着したときにのみコストを発生させるという要件に合致しません。
B. 
Cloud Functions上にコードをデプロイするのは良い選択ですが、Pub/Subトリガーとプル・サブスクリプションの両方を使用するのは冗長です。Pub/Sub トリガーだけでメッセージを処理できます。
C. 
Compute Engineのオプションと同様に、これはリソースの継続的な実行を伴い、メッセージがない場合でもコストが発生します。
正解
D. 
クラウドファンクションでは、クラウドイベントによってトリガーされる単一目的の関数を書くことができます。Pub/Sub トリガーと一緒に使用すると、指定したトピックにメッセージが送信されるたびに関数が呼び出されます。関数の実行時間に対してのみ支払いが発生するため、新しいメッセージが到着したときにのみコストが発生するという要件を満たすことができます。
Cloud Functions で Pub/Sub トリガーを使用する場合、手動でサブスクリプションを作成する必要はありません。代わりに、関数をデプロイすると、指定されたトピックに対してサブスクリプションが自動的に作成されます。そのトピックにメッセージがパブリッシュされるたびに関数がトリガーされ、そのメッセージが関数の引数として渡されます。
したがって、オプション D は有効であり、記述されたシナリオに適しています。これはPub/Subメッセージのリアルタイム処理を可能にし、メッセージがトピックにパブリッシュされたときにのみコストが発生します。
リンク
https://cloud.google.com/solutions/event-driven-architecture-pubsub
</div></details>

## Q. 
Compute Engineのインスタンスグループが、全体のCPU使用率に応じて自動的にスケールするように設定しました。
しかし、クラスタがインスタンスの追加を完了する前に、アプリケーションの応答レイテンシが急激に増加します。インスタンスグループのオートスケーラの構成を変更することで、エンドユーザーにより一貫したレイテンシ体験を提供したいとします。
どの2つの構成を変更する必要がありますか？(2つの選択肢を選んでください)
1. インスタンスグループテンプレートにAUTOSCALEというラベルを追加します。
2. グループに追加されたインスタンスのクールダウン期間を短くする。
3. インスタンスグループのオートスケーラーのターゲットCPU使用率を増やす。
4. インスタンスグループのオートスケーラーのターゲットCPU使用率を減らす。
5. インスタンスグループ内の個々のVMのヘルスチェックを削除する。
<details><div>
    答え：
説明
不正解
A. 
ラベルは通常、整理のために使用され、オートスケールの動作を本質的に変更するものではありません。これは望ましい効果をもたらさないでしょう。
C. 
ターゲットCPU使用率を増やすと、CPU使用率の増加に対するオートスケーラーの感度が低下する。
E. 
ヘルスチェックは、グループ内のインスタンスの健全性を判断するために使用される。ヘルスチェックを削除しても、CPU使用量の増加に対してオートスケーリングがより迅速に反応するようになるとは限りません。
正解
B. 
クールダウン期間は、インスタンスから使用情報を収集する前に、オートスケーラがインスタンスの起動後に待機すべき時間です。この期間を短くすることは、インスタンスが追加された後、オートスケーラがスケーリングの決定をより迅速に行うことができることを意味し、状況の変化により迅速に対応できる可能性があります。
D. 
ターゲットCPU使用率を下げると、オートスケーラーはCPU使用率により敏感になり、CPU使用率がターゲットに近づくと、より迅速にインスタンスを追加するようになります。これにより、システムはレイテンシーの増加により迅速に対応できるようになるはずだ。
これら2つの設定変更は、インスタンス・グループ・オートスケーラーの設定を変更することで、より一貫したレイテンシー・エクスペリエンスを提供するのに役立つはずだ。
リンク
https://cloud.google.com/compute/docs/autoscaler#cool_down_period
https://cloud.google.com/compute/docs/autoscaler/scaling-cpu#scaling_based_on_cpu_utilization
</div></details>

## Q. 
マネージド・インスタンス・グループによって管理されているアプリケーションがあります。アプリケーションの新バージョンをデプロイする場合、コストを最小限に抑え、インスタンス数を増やしてはなりません。新しいインスタンスが作成されるたびに、新しいインスタンスが健全である場合にのみデプロイが継続されるようにしたい。
どうすればよいでしょうか？
1. maxSurgeを1、maxUnavailableを0に設定してローリングアクションを実行します。
2. maxSurgeを0、maxUnavailableを1に設定してローリングアクションを実行します。
3. maxHealthyを1、maxUnhealthyを0に設定してローリングアクションを実行する。
4. maxHealthyを0、maxUnhealthyを1にしてローリングアクションを実行する。
<details><div>
    答え：
説明
不正解
A. 
maxSurgeを1に設定すると、更新中に追加のインスタンスが許可され、コストが増加する可能性があります。
C. 
D. 
CとD。maxHealthyとmaxUnhealthyは管理インスタンスグループのローリングアップデートのパラメータとして認識されていません。
正解
B. 
maxSurgeを0に設定することで、更新中にインスタンス数が目的の数を超えないようにします。一時的にインスタンスが追加作成されることはないため、コストを最小限に抑えるという要件に一致します。
maxUnavailableを1に設定すると、更新のために一度に1つのインスタンスを停止できますが、新しいインスタンスが健全であるとマークされた場合にのみプロセスが続行されます。
リンク
MIGでVMの設定更新を自動的に適用する｜Compute Engine Documentation｜Google Cloud
</div></details>

## Q. 
あなたはCloud Run上にデプロイされ、Cloud Functionsを使用する新しいアプリケーションに取り組んでいます。新しい機能が追加されるたびに、新しい Cloud Functions と Cloud Run サービスがデプロイされます。ENV変数を使用してサービスを追跡し、サービス間通信を可能にしていますが、ENV変数を維持するのが難しくなっています。動的ディスカバリーをスケーラブルに実装したい。
どうすればいいでしょうか？
1. Google CloudプロジェクトにデプロイされたCloud RunサービスとCloud Functionsをクエリするために、Cloud Run AdminとCloud Functions APIを使用するようにマイクロサービスを構成します。
2. Service Directoryネームスペースを作成する。デプロイ時にAPIコールを使用してサービスを登録し、実行時にクエリを実行します。
3. C. きちんと文書化された命名規則を使用して、Cloud FunctionsとCloud Runサービスのエンドポイントの名前を変更する。
4. 単一のCompute EngineインスタンスにHashicorp Consulをデプロイする。デプロイ時にConsulにサービスを登録し、実行時にクエリを実行する。
<details><div>
    答え：
説明
不正解
A. 
Cloud Run Admin APIとCloud Functions APIを直接クエリしても動作しますが、複雑さが増し、レートが制限される可能性があります。
C. 
十分に文書化された命名規則に依存することは、真の動的発見メカニズムを提供しない。命名規則が変更されたり、命名に間違いがあったりすると、問題につながる可能性がある。
D. 
単一のCompute EngineインスタンスにHashicorp Consulをデプロイすると、運用がさらに複雑になり、単一障害点となる可能性がある。また、Service DirectoryのようなGoogleが管理するサービスほどシームレスではありません。
正解
B. 
Service Directoryはサービスエンドポイントのリアルタイム検索を提供し、保守とサービスの動的発見を容易にします。
サービスを名前空間に整理し、デプロイ時にサービスを登録し、実行時にクエリを実行できる。
Google Cloudによるマネージド・サービスなので、既存のアーキテクチャとうまく統合でき、必要に応じて拡張できる。
そのため、オプションBはスケーラブルでダイナミックなディスカバリーの要件に最も合致している。
リンク
https://medium.com/google-cloud/fine-grained-cloud-dns-iam-via-service-directory-446058b4362e
https://cloud.google.com/service-directory/docs/overview
</div></details>

## Q. 
アプリケーションがカスタム・マシン・イメージとして構築されている。マシンイメージの複数の固有のデプロイメントがあります。各デプロイは、独自のテンプレートを持つ個別の管理対象インスタンスグループです。各デプロイメントには、固有の設定値のセットが必要です。これらの固有の値を各配備に提供しますが、すべての配備で同じカスタ ムマシンイメージを使用します。Compute Engineのすぐに使える機能を使用したい。
どうすればよいですか?
1. 一意の構成値を永続ディスクに配置します。
2. 固有の構成値をCloud Bigtableテーブルに置く。
3. インスタンステンプレートのスタートアップスクリプトに固有の構成値を配置する。
4. インスタンス・テンプレートのインスタンス・メタデータに一意の構成値を配置します。
<details><div>
    答え：
説明
不正解
A. 
構成値を永続ディスクに配置すると、構成とストレージが緊密に結合されるため、柔軟性が低下し、異なるインスタンス間で管理するのが難しくなります。
B. 
Cloud Bigtableを使用すると、この特定のユースケースに明確なメリットをもたらすことなく、さらなる複雑さとコストが発生する。
C. 
起動スクリプトを使用してインスタンスを構成することはできますが、起動スクリプトに一意の構成値を直接配置することは、インスタンスメタデータを使用する場合、特に複数の一意のデプロイがある場合、柔軟性と保守性が劣る可能性があります。
正解です：
D. 
インスタンスメタデータは、より柔軟でエレガントなソリューションを提供し、異なるデプロイメント間で同じイメージを使用しながら、各デプロイメントに固有の構成を提供できます。したがって、オプションDが正しい選択です。
リンク
インスタンスグループ｜Compute Engineドキュメント｜Google Cloud
https://cloud.google.com/compute/docs/storing-retrieving-metadata#custom
</div></details>

## Q. 
アプリケーション用に `fully baked` または `golden` Compute Engine イメージを作成したい。
アプリケーションの実行環境(test, staging, production)に応じて、適切なデータベースに接続するようにアプリケーションをブートストラップする必要があります。
何をすべきでしょうか？
1. 適切なデータベース接続文字列をイメージに埋め込みます。環境ごとに異なるイメージを作成する。
2. Compute Engineのインスタンスを作成する際に、接続するデータベース名のタグを追加する。アプリケーションでは、Compute Engine APIに問い合わせて現在のインスタンスのタグを取得し、タグを使用して適切なデータベース接続文字列を構築します。
3. C. Compute Engineインスタンスを作成するときに、DATABASEのキーと適切なデータベース接続文字列の値を持つメタデータ項目を作成します。アプリケーションでは、DATABASE環境変数を読み取り、その値を使用して適切なデータベースに接続します。
4. Compute Engineインスタンスを作成するときに、DATABASEのキーと適切なデータベース接続文字列の値を持つメタデータ項目を作成します。アプリケーションで、メタデータ・サーバーにDATABASE値を問い合わせ、その値を使用して適切なデータベースに接続します。
<details><div>
    答え：
説明
不正解
A. 
データベース接続文字列をイメージに埋め込み、環境ごとに異なるイメージを作成することは、複数のイメージを管理することになり面倒です。柔軟性がなく、接続文字列を変更すると新しいイメージが必要になる。
B. 
タグを使用すると、データベース接続文字列のような機密情報がインスタンスの詳細を閲覧できる人に公開され、潜在的なセキュリティリスクにつながる可能性があります。
C. 
環境変数の読み取りは、メタデータ・サーバーへの問い合わせほど簡単ではないかもしれません。さらに、このアプローチでは、インスタンスの作成時または起動時に環境変数が何らかの方法で設定されることを前提としていますが、これがどのように発生するかは明確になっていません。
正解
D. 
これにより、柔軟性とセキュリティが向上します。接続文字列をインスタンスのメタデータに格納することで、メタデータに基づいて異なる環境に適応する単一のイメージを持つことができます。メタデータ・サーバーへの問い合わせはGoogle Cloudで一般的にサポートされている方法であり、このオプションは説明したシナリオに最適です。インスタンスメタデータの保存と取得に関するドキュメントに、この方法の詳細が記載されています。
リンク
https://cloud.google.com/compute/docs/metadata/querying-metadata
https://cloud.google.com/compute/docs/metadata/setting-custom-metadata
</div></details>

## Q. 
貴社はオンプレミスのHadoop環境をクラウドに移行する計画を立てています。HDFSに保存されているデータのストレージコストとメンテナンスの増加は、貴社にとって大きな懸念事項です。また、既存のデータ分析ジョブや既存のアーキテクチャへの変更は最小限に抑えたいと考えています。
どのように移行を進めるべきでしょうか？
1. Hadoopに保存されているデータをBigQueryに移行します。オンプレミスのHadoop環境ではなく、BigQueryから情報を取得するようにジョブを変更します。
2. コスト削減のため、SSDの代わりにHDDを搭載したCompute Engineインスタンスを作成する。そして、既存の環境をCompute Engineインスタンスで新しい環境に完全移行する。
3. Google Cloud Platform上にCloud Dataprocクラスタを作成し、Hadoop環境を新しいCloud Dataprocクラスタに移行する。HDFSデータをより大きなHDDディスクに移動し、ストレージコストを節約する。
4. Google Cloud Platform上にCloud Dataprocクラスタを作成し、Hadoopコードオブジェクトを新しいクラスタに移行する。データをCloud Storageに移動し、Cloud Dataprocコネクタを活用してそのデータ上でジョブを実行する。
<details><div>
    答え：
説明
不正解
A. 
データをBigQueryに移行し、BigQueryから情報を取得するようにジョブを変更するには、既存のデータ分析ジョブや既存のアーキテクチャを大幅に変更する必要があります。これでは、既存のジョブに最小限の変更を加えるという要件を満たせません。
B. 
単にHDDを搭載したCompute Engineインスタンスを作成し、既存のHadoop環境を移行するだけでは、ストレージコストとメンテナンスの増加という懸念には対処できない。また、Cloud Dataprocのようなマネージドサービスのメリットも活用できない。
C. 
HDFSのデータをより大きなHDDディスクに移動しても、必ずしもストレージコストが削減できるとは限らないし、メンテナンスの負担も軽減されないかもしれない。Cloud DataprocはGoogle Cloud Storageとシームレスに動作するように設計されており、このオプションはその機能を活用しません。
正解
D. 
オプションDは、フルマネージドApache SparkおよびApache HadoopサービスであるCloud DataprocとGoogle Cloud Storageを活用する。データをCloud Storageに移行することで、HDFSに比べてストレージコストとメンテナンスオーバーヘッドを削減できる。既存のHadoopジョブは最小限の変更でCloud Dataprocクラスタに移行でき、Cloud Dataprocコネクタを使ってCloud Storageに保存されたデータに対してジョブを実行できる。このアプローチは既存のジョブへの変更を最小限に抑え、ストレージコストとメンテナンスを削減するという目標に合致している。
リンク
https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-overview
</div></details>

## Q. 
アプリケーションをGoogle Cloud Platformに移行し、既存の監視プラットフォームはそのまま使用しています。現在、タイムクリティカルな問題に対して通知システムが遅すぎることに気づきました。
どうしたらよいでしょうか？
1. 監視プラットフォーム全体をStackdriverに置き換える。
2. Compute EngineインスタンスにStackdriverエージェントをインストールする。
3. Stackdriverを使用してログをキャプチャし、アラートを出し、それを既存のプラットフォームに送信する。
4. 一部のトラフィックを旧プラットフォームに戻し、2つのプラットフォームで同時にA/Bテストを実施する。
<details><div>
    答え：
説明
不正解
A. 
Stackdriverは強力な監視とロギングのソリューションですが、既存の監視プラットフォーム全体を置き換えるのは大変な作業です。このオプションは、通知が遅いという特定の問題を解決するには、やり過ぎかもしれません。
B. 
エージェントをインストールすることで、メトリクスとログを収集できるようになりますが、通知システムが遅いという問題に対処できるとは限りません。エージェントのインストールに加えて、アラートを設定する必要があります。
D. 
このオプションは、通知が遅いという問題に直接対処しておらず、核心的な問題を解決せずに不必要な複雑さをもたらす可能性が高い。
正解
C. 
Stackdriver（現在はCloud Monitoring and Cloud Loggingとして知られている）を利用することで、タイムクリティカルな問題に対してGoogle Cloudのアラート機能を利用することができます。こうすることで、既存の監視プラットフォームと統合しながら、迅速な通知というメリットを得ることができます。Stackdriverで特定のアラート条件を設定し、必要に応じてログとメトリクスを既存のシステムに転送できます。オプションCでは、Google Cloudのアラート機能を活用しながら、既存の監視プラットフォームの部分を残すことができます。
リンク
https://cloud.google.com/monitoring/alerts/concepts-indepth#notification-latency
</div></details>

## Q. 
BigQuery APIを使用して、数分ごとにBigQuery上で数百のクエリを実行する分析アプリケーションがあります。これらのクエリの実行にかかる時間を調べたいとします。
どうすればよいでしょうか？
1. Stackdriver Monitoringを使用して、スロットの使用状況をプロットします。
2. API 実行時間をプロットするには、Stackdriver Trace を使用します。
3. Stackdriver Traceを使用して、クエリ実行時間をプロットします。
4. Stackdriver Monitoringを使用して、クエリ実行時間をプロットします。
<details><div>
    答え：
説明
不正解
A. 
スロットの使用状況をプロットすると、クエリの具体的な実行時間ではなく、使用されている計算リソースに関する情報が得られます。
B. 
Stackdriver Traceは、アプリケーションリクエストの待ち時間の分析に使用され、BigQueryクエリの実行時間に特化したものではありません。
C. 
オプションBと同様に、Stackdriver TraceはBigQueryクエリの実行時間を直接プロットするために使用されません。
正しい答え
D. 
これにより、クエリの実行にかかる時間を把握することができ、記載されている要件を満たすことができます。
リンク
https://cloud.google.com/bigquery/docs/monitoring
</div></details>

## Q. 
貴社は、人気のアプリケーションのユーザーを米国外に拡大したいと考えています。同社は、アプリケーション用のデータベースの99.999%の可用性を確保し、世界中のユーザーの読み取り待ち時間を最小化したいと考えています。
どの2つのアクションを取るべきでしょうか？(2つの選択肢を選んでください)
1. nam-asia-eur1」構成でマルチリージョンのCloud Spannerインスタンスを作成します。
2. nam3」構成でマルチリージョンのCloud Spannerインスタンスを作成します。
3. 少なくとも3つのSpannerノードでクラスタを作成します。
4. 少なくとも1つのSpannerノードでクラスタを作成します。
5. 少なくとも1つのノードで、別々のリージョンに最低2つのCloud Spannerインスタンスを作成する。
6. F. 異なるデータベース間でデータをレプリケートするためにCloud Dataflowパイプラインを作成する。
<details><div>
    答え：
説明
不正解
B. 
nam3」構成は北米にしか及ばないため、グローバルユーザーの読み取り待ち時間を最小限に抑えることはできません。
D. 
Spannerノードが1つしかないと、予想される負荷を処理し、望ましい可用性を達成するのに十分でない可能性があります。
E.
別々のインスタンスを2つ作成すると、レプリケーションを手動で管理する必要がありますが、Cloud Spannerは複数リージョン構成ですでに処理しています。
F. 
Cloud Dataflowは、このシナリオでは異なるデータベース間でデータを複製するようには設計されていない。それは、データベースの複製ではなく、データの変換と処理タスクに使用されます。
正解
A. 
nam-asia-eur1 "のようなマルチリージョン構成は、北米、アジア、およびヨーロッパにまたがり、グローバルな配布と、これらの地域のユーザーのための低い読み取りレイテンシを保証します。
C. 
Spannerクラスタに少なくとも3つのノードを持つことで、より高い可用性を提供し、より多くのクエリと読み取り/書き込み操作を処理できます。
リンク
https://cloud.google.com/spanner/docs/instances
https://cloud.google.com/spanner/docs/latency
</div></details>

## Q. 
あなたは金融機関の開発者です。Cloud Shell を使って Google Cloud サービスとやりとりしています。ユーザーデータは現在エフェメラルディスクに保存されていますが、最近可決された規制により、機密情報をエフェメラルディスクに保存することはできなくなりました。ユーザーデータ用に新しいストレージソリューションを実装する必要があります。コードの変更は最小限に抑えたい。
ユーザーデータはどこに保存すべきでしょうか？
1. ユーザーデータをCloud Shellのホームディスクに保存し、削除を防ぐために少なくとも120日ごとにログインします。
2. Compute Engineインスタンスの永続ディスクにユーザーデータを保存します。
3. ユーザーデータをクラウドストレージのバケットに保存する。
4. ユーザーデータをBigQueryのテーブルに格納する。
<details><div>
    答え：
説明
不正解
A. 
Cloud Shellのホームディスクは機密データや永続的なデータを保存するためのものではありません。長期間(120日間)アクセスされないと削除される可能性があり、機密性の高いユーザー情報を保存するには信頼できません。
C. 
クラウドストレージのバケットを使用してデータを安全に保管することができるが、このシナリオではコードの変更を最小限に抑える必要がある。現在のアーキテクチャにもよるが、データをクラウドストレージに移すにはコードに大幅な変更を加える必要があり、コードの変更を最小限に抑えるという要件と矛盾する可能性がある。
D. 
BigQueryは、サーバーレスで、拡張性が高く、費用対効果の高いマルチクラウド・データウェアハウスです。BigQueryは分析用に設計されており、一般的に機密性の高いユーザーデータの保存には使用されません。
正解
B. 
永続ディスクは、より安定した信頼性の高いストレージソリューションを提供します。データは機密性の高いものであるため、制御されたCompute Engineインスタンス内の永続ディスクに格納することで、適切なアクセス制御と管理が可能になり、規制コンプライアンスに沿うことができます。
新しい規制を遵守し、コードの変更を最小限に抑えるという要件を考慮すると、このコンテキストにおける正しい選択肢は、Compute Engineインスタンスで永続ディスクを使用することでしょう。
リンク
https://cloud.google.com/shell/docs/how-cloud-shell-works#persistent_disk_storage
</div></details>

## Q. 1-1
同じVPC（仮想プライベートクラウド）内の複数のクライアントから呼び出される必要がある、Compute Engine仮想マシンインスタンス上でホストされるHTTP APIを開発しています。クライアントがサービスのIPアドレスを取得できるようにしたい。どうすればよいでしょうか？
1. 静的な外部IPアドレスを予約し、HTTP(S)負荷分散サービスの転送ルールに割り当てる。クライアントはこのIPアドレスを使ってサービスに接続する。
2. 静的な外部IPアドレスを予約し、HTTP(S)ロードバランシングサービスの転送ルールに割り当てる。次に、クラウドDNSでAレコードを定義する。クライアントはAレコードの名前を使用してサービスに接続する。
3. クライアントが、https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/.というURLでインスタンス名に接続し、Compute Engineの内部DNSを使用するようにします。
4. クライアントが、https://[API_NAME]/[API_VERSION]/のURLでインスタンス名に接続して、Compute Engineの内部DNSを使用するようにします。
<details><div>
    答え：3
A. 
1．静的外部IPアドレスは、クライアントがサービスに接続するために使用できる固定IPアドレスを提供します。インスタンスが再起動されたり、インスタンスがオートスケーリンググループの一部である場合、インスタンスのIPアドレスが変更される可能性があるため、これは重要です。
2．HTTP(S)ロードバランシング・サービスは、トラフィックを複数のインスタンスに分散させることができ、サービスのパフォーマンスと可用性を向上させることができます。また、利用可能なインスタンスのプールから不健康なインスタンスを自動的に削除するヘルスチェックも処理できる。
3．転送ルールを使用することで、URLまたはIPアドレスに基づいて、トラフィックを適切なインスタンスに誘導することができる。これにより、複数のサービスを単一のIPアドレスとポートの組み合わせでホストすることができる。
4．クライアントは、転送ルールに割り当てられたIPアドレスを使用してサービスに接続できる。
結論として、この複雑なソリューションは "非最適 "と評価できる。
B. 説明
このオプションはオプションAと似ていますが、サービスに接続するためにIPアドレスを使用する代わりに、クライアントはクラウドDNSで定義されたAレコードの名前を使用します。これは、サービスによりユーザーフレンドリーな名前を提供できますが、セットアップにさらなる複雑さが加わります。
さらに、クラウドDNSを使用すると、構成および管理する必要がある別のサービスが追加されるため、問題やダウンタイムが発生する可能性が高まります。
D. 説明
このオプションはオプションCと似ていますが、URLでインスタンス名を使用する代わりに、API名とバージョンを使用します。これは、より使いやすいURLを提供できますが、Compute Engineが提供する内部DNSサービスに依存します。
また、カスタムのAPI名とバージョンを使用すると、セットアップがさらに複雑になり、追加の設定と管理が必要になる場合があります。
正解
C. このオプションは、Compute Engineが提供する内部DNSサービスを使用して、サービスをホストするインスタンスのIPアドレスを解決します。これは、すべてのクライアントが同じVPC内にあり、内部DNSサービスにアクセスできる場合に機能します。
まとめると、同じVPC内のクライアントがCompute Engineの仮想マシンインスタンス上でホストされているHTTP APIのIPアドレスを取得できるようにするための最良の選択肢は、クライアントが、https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/というURLでインスタンス名に接続してCompute Engineの内部DNSを使用するようにすることです。
Links: 

https://cloud.google.com/compute/docs/internal-dns
</div></details>

## Q. 1-2
Webアプリケーションは企業のイントラネットにデプロイされています。このWebアプリケーションをGoogle Cloudに移行する必要があります。ウェブアプリケーションは、会社の従業員だけが利用でき、従業員が移動中にアクセスできる必要があります。アプリケーションの変更を最小限に抑えながら、Webアプリケーションのセキュリティとアクセシビリティを確保する必要があります。

どのような対応が必要ですか？
1. アプリケーションへの HTTP(S)リクエストごとに認証情報をチェックするようにアプリケーションを構成する。
2. 従業員がパブリックIPアドレス経由でアプリケーションにアクセスできるように、Identity-Aware Proxyを構成する。
3. ユーザーに企業アカウントへのログインを要求するCompute Engineインスタンスを構成します。WebアプリケーションのDNSをプロキシのCompute Engineインスタンスを指すように変更します。認証後、Compute EngineインスタンスはWebアプリケーションとの間でリクエストを転送します。
4. ユーザーに企業アカウントへのログインを要求するCompute Engineインスタンスを構成する。WebアプリケーションのDNSをプロキシのCompute Engineインスタンスを指すように変更します。認証後、Compute Engineは、WebアプリケーションをホストするパブリックIPアドレスにHTTPリダイレクトを発行します。
<details><div>
    答え：3
A. このオプションでは、HTTP(S)リクエストごとに認証チェックを実装するために、アプリケーションに大幅な変更が必要になる可能性が高く、アプリケーションの変更を最小限に抑えるという要件に反する。
B. IAP（Identity-Aware Proxy）は、Google Cloud Platformの機能であり、IDおよびコンテキストベースのアクセス制御を使用してリソースへのアクセスを保護することができます。IAPを使用すると、リソース（ウェブ・アプリケーションなど）へのアクセスを、認証され許可されたユーザーまたはサービス・アカウントのみに制限することができます。
しかし、このシナリオでは、ウェブ・アプリケーションは企業のイントラネット上でホストされているため、パブリックIPアドレスを持たず、インターネットからアクセスすることはできません。また、IAP を使用して、イントラネットでホストされているアプリケーションへのアクセスを、その IP アドレスによって制限することはできません。
D. これらのオプションには、プロキシとして機能するCompute Engineインスタンスを設定し、企業アカウントを通じてユーザーを認証することが含まれます。また、WebアプリケーションをホストするパブリックIPアドレスへのHTTPリダイレクトが含まれるため、従業員のみにアクセスを制限する必要性に合致しない可能性があります。
正解
C. このアプローチでは、Google Cloudのインフラストラクチャを利用して、Webアプリケーションへのアクセスを許可する前に、企業のイントラネットを通じてユーザーを認証することができます。プロキシとして動作するCompute Engineインスタンスを設定し、WebアプリケーションのDNSをこのプロキシを指すように変更することで、Webアプリケーションへのアクセスは、企業イントラネットで認証された従業員のみに制限されます。さらに、この方法では、従業員がインターネットにアクセスできる環境であれば、出張中でもWebアプリケーションにアクセスすることができます。
Links:
https://cloud.google.com/compute/docs

https://cloud.google.com/iam
</div></details>

## Q. 1-13
本番環境にデプロイされたアプリケーションがあります。新しいバージョンがデプロイされたとき、いくつかの問題は、アプリケーションが本番環境のユーザからトラフィックを受けるまで発生しません。影響と影響を受けるユーザ数の両方を減らしたい。

どのデプロイメント戦略を使用すべきですか?
1. ブルー／グリーン・デプロイメント
2. カナリア展開
3. ローリングデプロイメント
4. デプロイメントの再作成
<details><div>
    答え：2
A. このアプローチでは、2つの別々の環境（実行中のバージョンはブルー、新しいバージョンはグリーン）を切り替えることができます。全員にロールアウトする前に、問題を検出するために一部のユーザーでテストすることは特にできません。
C. ローリング デプロイメントでは、インスタンスを次々に段階的に更新します。段階的なロールアウトが可能ですが、Canaryデプロイメントのように特定のユーザ サブセットを対象としていないため、ユーザ固有の問題を検出するのには適していません。
D. この方法では、古いバージョンを削除し、新しいバージョンをデプロイする。すべてのインスタンスが同時に置き換えられるため、影響を軽減し、ユーザーのサブセットでテストするという要件には適合しません。
正解
B. カナリアデプロイメントでは、新バージョンを少人数のユーザに徐々にリリースしてから、すべてのユーザに使用できるようにします。これにより、本番環境の実際のユーザーで新バージョンの動作をテストできますが、対象者が限定されるため、潜在的な問題の影響と影響を受けるユーザー数の両方を減らすことができます。これは、説明した状況に適しています。
Links:

https://cloud.google.com/architecture/application-deployment-and-testing-strategies#canary_test_pattern
</div></details>

## Q. 1-15
あなたの会社には「Master」という名前のBigQueryデータセットがあり、そこには従業員の部署別に整理された、従業員の出張と経費に関する情報が含まれています。従業員は各部門の情報しか閲覧できないようにする必要があるため、セキュリティフレームワークを適用して、最小限のステップ数でこの要件を実施したいとします。

どうすればよいでしょうか。
1. 部門ごとに個別のデータセットを作成する。適切なWHERE句を指定してビューを作成し、特定の部門の特定のデータセットからレコードを選択する。このビューに、マスターデータセットからレコードにアクセスする権限を与える。従業員にこの部門別データセットへのアクセス権限を与える。
2. 部門ごとに個別のデータセットを作成する。部門ごとにデータパイプラインを作成し、マスターデータセットから部門固有のデータセットに適切な情報をコピーする。従業員にこの部門別データセットへのアクセス権を与える。
3. マスター」データセットという名前のデータセットを作成する。マスターデータセットの中に、部署ごとに個別のビューを作成する。従業員に、所属する部署に特化したビューへのアクセス権を与える。
4. マスター・データセットという名前のデータセットを作成する。マスター・データセットの中に、部門ごとに個別のテーブルを作成する。従業員には、所属する部門のテーブルにアクセスできるようにする。
<details><div>
    答え：3
A.この方法では、データセットとビューを別々に作成し、適切なアクセス制御を行います。柔軟なアプローチですが、特に基礎となるデータ構造が変更された場合、管理が複雑になる可能性があります。
B.オプションAと同様に、部署ごとに個別のデータセットを作成する。データパイプラインの使用は複雑さを増し、データが重複することで整合性に問題が生じる可能性がある。
D.この方法では、「マスター」データセット内に個別のテーブルを作成する。オプションCと同様に、すべてを単一のデータセット内に保持しますが、テーブルごとのアクセス制御をより慎重に管理する必要があります。
正解
C.この方法では、既存の「マスター」データセットの中にビューを作成し、部門ごとのニーズに合わせてアクセスできるようにします。実装と管理が最も簡単で、セキュリティと使いやすさのバランスがよく、ステップ数も最小限で済む。
Links:

https://cloud.google.com/bigquery/docs/share-access-views
</div></details>

## Q. 1-18
貴社は新しいAPIをCompute Engineインスタンスにデプロイしました。テスト中、APIが期待通りに動作しません。アプリケーションを再デプロイすることなく、アプリケーションコード内の問題を診断するために、12時間にわたってアプリケーションを監視したい。

どのツールを使用すべきでしょうか？
1. クラウドトレース
2. Cloud Monitoring
3. クラウドDebuggerのログポイント
4. クラウド・デバッガ・スナップショット
<details><div>
    答え：3
A. レイテンシの分析に重点を置いており、再デプロイせずにアプリケーションコード内の問題を診断することに特化して設計されていないため、質問の要件を満たしていない。
B. システムのパフォーマンスを監視するのに便利ですが、シナリオが求めている、ログステートメントを挿入したり、再デプロイせずにアプリケーションコードの動作を分析したりすることはできません。
D. スナップショットは、12時間にわたって監視するよりも、特定の時点でのアプリケーションの状態を分析するのに適しています。このシナリオにはあまり適していません。
正解
C. アプリケーションをデプロイまたは起動した後、Google Cloud コンソールで Cloud Debugger を開くことができます。Cloud Debugger Logpoints を使用すると、サービスの通常の機能を再起動または妨害することなく、実行中のサービスにロギングを注入できます。これは、ログステートメントを追加して再デプロイすることなく、運用上の問題をデバッグするのに便利です。
Link: https://cloud.google.com/debugger/docs/using/logpoints
</div></details>

## Q. 1-26
500 MB のファイル サイズ制限がある内部ファイル アップロード API を App Engine に移行する必要があります。

どうすればよいでしょうか。
1. FTPを使用してファイルをアップロードします。
2. CPanelを使用してファイルをアップロードします。
3. 署名付きURLを使用してファイルをアップロードする。
4. APIをマルチパートのファイルアップロードAPIに変更する。
<details><div>
    答え：3
不正解
オプションA（FTP）は、App Engineのようなクラウド環境における一般的なプラクティスに合致しません。
オプションB（CPanel）は、プログラムでファイルアップロードを処理することとは関係ありません。
オプションD（マルチパートファイルアップロードAPI）は可能な解決策かもしれませんが、特に指定されたファイルサイズ制限やApp Engine環境に対応していません。
正解
署名付きURLを使用してファイルをアップロードするオプションCは、これらの選択肢の中で最良のアプローチです。署名付きURLは、特に大きなファイルを扱う場合に、ファイルアップロードを安全かつ効率的に処理する方法を提供します。これにより、特定のクラウドリソース（この場合はファイルをアップロードする機能）への一時的なアクセスをユーザーに与えることができます。Google Cloud Storageは署名付きURLをサポートしており、App Engineと組み合わせてファイルアップロードを処理できます。署名付きURLを作成することで、App Engineサーバーでファイルを処理することなく、クライアントがCloud Storageのバケットに直接ファイルをアップロードすることを許可できます。

したがって、正解は C
Links:

https://cloud.google.com/storage/docs/access-control/signed-urls

https://cloud.google.com/appengine/docs/standard/php/googlestorage/user_upload
</div></details>

## Q. 1-29
アプリケーションを、Stackdriver Monitoring AgentがインストールされたCompute Engine仮想マシンインスタンスにデプロイしています。アプリケーションはインスタンス上のunixプロセスです。unixプロセスが少なくとも5分間実行されなかった場合、アラートが必要です。メトリクスやログを生成するようにアプリケーションを変更することはできません。

どのアラート条件を構成しますか?
1. アップタイム・チェック
2. プロセスの健全性（Process health）
3. メトリックの不在（Metric absence）
4. メトリックしきい値（Metric threshold）
<details><div>
    答え：2
不正解
A. これは、マシン上の特定のプロセスではなく、Webサーバなどのネットワーク・エンドポイントの可用性を監視するために使用されます。
C. これは、プロセスの不在ではなく、特定のメトリックのデータの不在を指す。これはより複雑なシナリオで使用されるかもしれないが、プロセスの健全性を監視するようには直接設計されていない。
D. これは、特定のメトリック値が特定のしきい値を超えたことに基づいてアラートを設定することができますが、特定のプロセスが実行されているかどうかをチェックするようには設計されていません。
正解
B. 説明するシナリオでは、特定のUnixプロセスを監視し、そのプロセスが少なくとも5分間実行されていない場合にアラートを出したいとします。プロセスの有無を監視しているので、適切なアラート条件はプロセスの健全性です。

Stackdriverモニタリング・エージェントは、システムとプロセスのメトリクスを監視することができ、このタイプのアラートは、特にUnixプロセスの健全性を追跡します。

Links:

Behavior of metric-based alerting policies | Cloud Monitoring
</div></details>

## Q. 1-30
貴社は新しい API を App Engine Standard 環境にデプロイしました。テスト中、API が期待どおりに動作しません。アプリケーションを再デプロイすることなく、アプリケーション コード内の問題を診断するために、アプリケーションを長期にわたって監視したいとします。

どのツールを使用すべきでしょうか?
1. スタックドライバートレース
2. スタックドライバ・モニタリング
3. Stackdriver デバッグ・スナップショット
4. スタックドライバ・デバッグ・ログポイント
<details><div>
    答え：4
不正解
A. Stackdriver Trace は、リクエストがアプリケーションをどのように伝搬するかを分析し、これらのリクエストの待ち時間を測定するために使用されます。アプリケーションコード内の問題の診断に直接焦点を当てるものではありません。
B. システムの健全性、パフォーマンス、カスタムメトリクスの監視には最適ですが、Stackdriver Monitoringでは、再デプロイせずにアプリケーションコード内の特定の問題をピンポイントで診断することはできません。
C. スナップショットは、コード内の特定の場所でローカル変数とコールスタックをキャプチャします。このツールは、実行中のアプリケーションを停止させたり速度を落としたりすることなく、プログラムの状態を調べるために使用される。役に立ちますが、ログポイントほど直接シナリオに合わせたものではありません。
正解
D. Logpoints を使えば、実行中のアプリケーションを停止したり再デプロイしたりすることなく、リアルタイムでログ文を追加することができるので、これは正しい選択です。これを使用して、問題が発生していると思われる特定のポイントのログを検査することで、アプリケーションコード内の問題を診断することができます。オプション D は、アプリケーションを再デプロイすることなく、アプリケーションコード内の問題を診断するた めに、アプリケーションを長期にわたって監視するための最良の選択です。

Links:

https://cloud.google.com/debugger/docs/using/logpoints
</div></details>

## Q. 1-31
あなたは、XMLHttpRequestを使用してサードパーティAPIとコンテンツ通信を行う、ユーザーインターフェースを持つ単一ページのWebアプリケーションを書いています。APIの結果によってUIに表示されるデータは、同じWebページに表示される他のデータよりも重要度が低いため、リクエストによってはAPIのデータがUIに表示されなくても構いません。しかし、APIへの呼び出しによって、ユーザーインターフェースの他の部分のレンダリングが遅れてはならない。APIレスポンスがエラーまたはタイムアウトの場合、アプリケーションのパフォーマンスを向上させたい。
どうすればよいでしょうか？
1. APIへのリクエストの非同期オプションをfalseに設定し、タイムアウトまたはエラーが発生したときにAPI結果を表示するウィジェットを省略します。
2. APIへのリクエストの非同期オプションをtrueに設定し、タイムアウトまたはエラーが発生したときにAPI結果を表示するウィジェットを省略する。
3. APIコールからのタイムアウトまたはエラー例外をキャッチし、API応答が成功するまで指数関数バックオフで試行を続ける。
4. APIコールのタイムアウトまたはエラー例外をキャッチし、UIウィジェットにエラー・レスポンスを表示する。
<details><div>
    答え：2
不正解
A. この場合、リクエストは同期的に処理され、リクエストが完了するまでUIの他の部分のレンダリングがブロックされます。
C. これは不必要な遅延を引き起こし、APIが失敗し続ければ無限にトライし続ける可能性がある。データはそれほど重要ではないので、繰り返しフェッチしようとするのは要件に合致しない。
D. UIにエラー・レスポンスを表示することは、特にデータの重要度が低い場合、望ましくないかもしれない。また、これは呼び出しを非同期にするという要件には対応していません。
正解
B. このシナリオでは、サードパーティAPIへの呼び出しがUIの他の部分のレンダリングを遅らせないようにすることに重点を置いています。

したがって、ここでの最良の選択は、UI の他の部分のレンダリングをブロックしないようにリクエストを非同期（オプション B）にすることです。

https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest
</div></details>

## Q. 1-32
App Engineでアプリケーションを実行しています。

アプリケーションは Stackdriver Trace でインスツルメンテーションされています。product-detailsリクエストは、以下のように/sku-detailsにある4つの既知のユニークな商品に関する詳細をレポートします。リクエストが完了するまでの時間を短縮したい。

どうすればよいでしょうか？
1. インスタンスクラスのサイズを大きくする。
2. 永続ディスクのタイプをSSDに変更する。
3. リクエストを並行して実行するように/product-detailsを変更する。
4. sku-details情報をデータベースに保存し、Webサービスコールをデータベースクエリに置き換える。
<details><div>
    答え：3
A. インスタンスクラスのサイズを大きくすることで、より多くのリソースを提供できるかもしれませんが、シーケンシャルなリクエスト処理という核心的な問題には必ずしも対処できないでしょう。
B. 永続ディスク・タイプをSSDに変更すると、ディスクI/Oパフォーマンスが向上するかもしれないが、ここで説明する問題はネットワーク・リクエストに関連するものであり、ディスク操作に関連するものではない。
D. データベースに/sku-details情報を格納することで、シナリオによってはパフォーマンスが向上する可能性がありますが、説明した問題とは必ずしも一致しません。sku-detailsデータが頻繁に変更され、サードパーティによって管理されている場合、データベースの保存は適切ではないかもしれません。
正解
C. リクエストを並列に実行することで、/product-detailsリクエスト全体が完了するまでの時間を短縮できます。

Links:

https://cloud.google.com/appengine/docs/standard/java/datastore/queries
</div></details>

## Q. 1-33
App Engineの標準設定は以下のとおりです：

- サービス: production

- インスタンスクラス B1

アプリケーションを5インスタンスに制限したい。

どのコードスニペットを構成に含める必要がありますか？
1. manual_scaling: インスタンス： 5 min_pending_latency: 30ms
2. manual_scaling: max_instances： 5 idle_timeout： 10m
3. basic_scaling: インスタンス数： 5 min_pending_latency: 30ms
4. basic_scaling: max_instances： 5 idle_timeout： 10m
<details><div>
    答え：4
不正解
manual_scalingではインスタンスの最大数を設定できない（固定数である）ため、これらの他の選択肢は正しくありません。また、選択肢Cはbasic_scalingに対して誤った構文を使用しています。
正解です：
App Engineでインスタンス数を制限したい場合、特定の最大インスタンス数でbasic scalingを使用できます。
この設定の正しいコードスニペットは以下の通り：
basic_scaling: max_instances： 5 idle_timeout： 10m
この設定により、App Engineは最大5つのインスタンスを実行し続け、10分以上アイドル状態のインスタンスを自動的にシャットダウンします。
Links:

https://cloud.google.com/appengine/docs/legacy/standard/python/how-instances-are-managed#scaling_types
</div></details>

## Q. 1-35
データはCloud Storageのバケットに保存されます。

他の開発者から、Cloud StorageからダウンロードしたデータによってAPIのパフォーマンスが低下しているという報告を受けています。Google Cloudのサポートチームに詳細を報告するために、この問題を調査したいと思います。

どのコマンドを実行すべきですか？
1. gsutil test -o output.json gs://my-bucket
2. gsutil perfdiag -o output.json gs://my-bucket
3. gcloud compute scp example-instance:~/test-data -o output.json gs://my-bucket
4. gcloud services test -o output.json gs://my-bucket
<details><div>
    答え：2
説明
不正解
A. このコマンドはgsutilコマンドラインツールに存在せず、オプション "test "が認識されないため、正しく実行されません。
C. gcloud compute scpコマンドは、ローカルマシンと仮想マシン間、または2つの仮想マシン間でファイルをコピーするために使用されます。クラウドストレージのパフォーマンス問題の診断とは関係ないので、今回のタスクには当てはまらない。
D. このコマンドはgcloudコマンドラインツールには存在しません。gcloud servicesの下に「test」コマンドはないので、この行はエラーになります。
正しい答え
B. gsutil perfdiagコマンドは、Google Cloud Storageのパフォーマンスの問題を診断するために使用できます。一連の診断テストを実行し、パフォーマンス問題の原因を特定するのに役立つ情報を収集します。
Links:

https://cloud.google.com/storage/docs/gsutil/commands/perfdiag#providing-diagnostic-output-to-cloud-storage-team
</div></details>

## Q. 1-43
コンテナ化したアプリケーションの新バージョンのテストが完了し、Google Kubernetes Engine上で本番環境にデプロイする準備が整いました。
本番前の環境では新バージョンの負荷テストを十分に行うことができなかったため、デプロイ後のパフォーマンスに問題がないことを確認する必要があります。デプロイは自動化する必要があります。
あなたは何をすべきでしょうか？
1. クラウドロードバランシングを使用して、バージョン間のトラフィックを徐々に増加させます。クラウドモニタリングを使用してパフォーマンスの問題を探します。
2. カナリアデプロイメントを使用して、継続的デリバリーパイプライン経由でアプリケーションをデプロイする。クラウドモニタリングを使用してパフォーマンスの問題を調べ、メトリクスがサポートするようにトラフィックを増加させる。
3. ブルー／グリーン・デプロイメントを使用して、継続的デリバリー・パイプラインを介してアプリケーションをデプロイする。Cloud Monitoringを使用してパフォーマンスの問題を探し、メトリクスがそれをサポートするときに完全に起動します。
4. kubectlを使用してアプリケーションをデプロイし、spec.updateStrategv.typeをRollingUpdateに設定します。Cloud Monitoringを使用してパフォーマンスの問題を探し、問題があればkubectl rollbackコマンドを実行します。
<details><div>
    答え：4
説明
不正解
A. クラウドロードバランシングは、Kubernetes環境における新しいアプリケーションバージョンの制御されたロールアウトに適したツールではありません。
B. カナリア・デプロイは、少数のサブセット・ユーザーで新バージョンをテストする良い方法ですが、この選択肢は選択された正解には一致しません。
C. ブルー／グリーン・デプロイメントは、バージョン間の迅速な切り替えを可能にしますが、新しいバージョンへの露出を徐々に増やすことには適していません。
正解
D. KubernetesのRollingUpdateでは、アプリケーションを徐々に更新することができます。Cloud Monitoringを通じてパフォーマンスの問題が検出された場合、kubectl rollbackを使用してデプロイメントをロールバックできます。これにより、問題を検出するために必要な段階的な露出と、デプロイを効率的に管理するための自動化の両方が提供される。
Links:

https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/

https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#overview
</div></details>

## Q. 1-49
Google Kubernetes Engine（GKE）にデプロイされたアプリケーションがあります。Google Cloudのマネージドサービスに認可されたリクエストを行うために、アプリケーションをアップデートする必要があります。これは一度だけのセットアップであり、セキュリティキーの自動ローテーションと暗号化されたストアへの保存というセキュリティのベストプラクティスに従う必要があります。Google Cloud サービスへの適切なアクセス権を持つサービスアカウントは作成済みです。

次に何をすべきでしょうか？
1. Workload Identityを使用して、GKEポッドにGoogleクラウドサービスアカウントを割り当てます。
2. Google Cloudサービスアカウントをエクスポートし、KubernetesシークレットとしてPodと共有します。
3. Google Cloudサービスアカウントをエクスポートし、アプリケーションのソースコードに埋め込みます。
4. Google Cloudのサービスアカウントをエクスポートし、HashiCorp Vaultにアップロードして、アプリケーション用の動的なサービスアカウントを生成します。
<details><div>
    答え：1
説明
不正解
B. サービスアカウントキーの保存にKubernetesシークレットを使用すると、自動ローテーションが提供されず、正しく処理されないとセキュリティリスクを引き起こす可能性があります。
C. ソースコードにサービスアカウントを埋め込むことは悪い習慣であり、重大なセキュリティリスクをもたらす。
D. HashiCorp Vaultはシークレット管理のための強力なツールですが、Google Cloudサービスと相互作用するGKEワークロードのIDを処理するには、ワークロードアイデンティティを使用する方がより簡単で推奨される方法です。
正解
A. Workload Identityは、GKE内からGoogle Cloudサービスにアクセスするための推奨方法です。KubernetesサービスアカウントをGoogleサービスアカウントにバインドすることができ、このバインドによってKubernetesサービスアカウントがGoogleサービスアカウントとして機能します。Workload Identityを使用すると、サービスアカウントのキーを管理する必要がなく、ベストプラクティスに沿った自動ローテーションが行われます。
Links:

https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity
</div></details>

## Q. 2-7
あなたのチームはCloud Run上でサーバーレスWebアプリケーションを作成しています。このアプリケーションは、プライベートクラウドストレージバケットに保存された画像にアクセスする必要があります。アプリケーションにバケット内の画像にアクセスするIAM（Identity and Access Management）権限を与えると同時に、Googleが推奨するベストプラクティスを使ってサービスを保護したい。

どうすればよいでしょうか？
1. 目的のバケットに対して署名付きURLを強制します。Compute Engineのデフォルトのサービスアカウントに、バケットのStorage Object Viewer IAMロールを付与します。
2. 目的のバケットに対して、パブリックアクセス防止を強制します。Compute Engineのデフォルトのサービスアカウントに、バケットのStorage Object Viewer IAMロールを付与する。
3. 目的のバケットに対して署名付きURLを強制します。ユーザー管理サービスアカウントを使用するようにCloud Runサービスを作成し、更新します。サービスアカウントに、バケット上のStorage Object Viewer IAMロールを付与する。
4. 目的のバケットに対してパブリックアクセス防止を実施します。ユーザー管理サービスアカウントを使用するようにCloud Runサービスを作成し、更新します。バケット上のStorage Object Viewer IAMロールをサービスアカウントに付与します。
<details><div>
    答え：4
説明
不正解
オプションAとBは、Compute Engineのデフォルトのサービスアカウントに関係しており、Cloud Runサービスに特有ではないため、最小特権の原則に合致していません。
C. オプションCは署名付きURLを利用し、バケット内のオブジェクトへの一時的なアクセスを提供する。これはアクセスを許可する有効な方法ですが、選択肢Dほど説明したシナリオには適合しません。
正解
D. パブリックアクセス防止を実施することで、バケットが誤って公開されないようにし、プライバシーを維持します。

ユーザが管理するサービスアカウントを使用するようにCloud Runサービスを作成・更新することで、特定のサービスに必要な権限のみを付与する最小権限の原則に従うことになります。

サービスアカウントにStorage Object Viewer IAMロールを付与することで、Cloud Runサービスはより広範な権限を与えることなく、指定されたバケットからオブジェクトを読み取ることができます。
Links:
https://cloud.google.com/run/docs/securing/service-identity#user-managed_service_account
https://cloud.google.com/storage/docs/public-access-prevention
https://cloud.google.com/storage/docs/access-control/using-iam-permissions
</div></details>

## Q. 2-9
あなたは大企業の開発者です。Google Cloud上で3つのGoogle Kubernetes Engineクラスタを管理しています。あなたのチームの開発者は、好みの開発ツールへのアクセスを失うことなく、クラスタを定期的に切り替える必要があります。あなたは、Googleが推奨するベストプラクティスに従いながら、これらの複数のクラスタへのアクセスを設定したいと考えています。

どうすればよいでしょうか？
1. 開発者にCloud Shellを使用するように依頼し、gcloud container clusters get-credentialを実行して別のクラスタに切り替えます。
2. 設定ファイルで、クラスタ、ユーザー、コンテキストを定義します。このファイルを開発者と共有し、kubectl configを使用してクラスタ、ユーザー、コンテキストの詳細を追加するように依頼します。
3. 開発者にワークステーションにgcloud CLIをインストールしてもらい、gcloud container clusters get-credentialsを実行して別のクラスターに切り替える。
4. 開発者にワークステーション上で3つのターミナルを開いてもらい、kubectl configを使用して各クラスタへのアクセスを設定する。
<details><div>
    答え：2
説明
不正解
A. C. 
オプションCも有効な方法ですが、共有設定ファイルに定義済みのコンテキストがあるのとは対照的に、開発者は切り替えが必要なたびにコマンドを実行する必要があるかもしれません。
D. 選択肢AとDは、Bに比べて利便性と柔軟性に劣ります。
正解
B. 設定ファイルでクラスタ、ユーザー、コンテキストを定義することで、開発者は適切なパーミッションが設定されていることを前提に、kubectl config use-contextを使用して異なる環境をすばやく切り替えることができます。

適切な詳細を含む設定ファイルを共有することで、開発者は複数のクラスタへの接続をより簡単に管理できるようになります。

異なるクラスタにコンテキストを定義することで、開発者は単純なコマンドでクラスタ間を素早く切り替えることができ、クラスタを定期的に切り替える必要があるという要件に沿う。
Links:

https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/
</div></details>

## Q. 2-11
最近、OpenTelemetry で新しいアプリケーションをインスツルメンテーションし、アプリケーションのリクエストのレイテンシを Trace でチェックしたいとします。特定のリクエストが常にトレースされるようにしたい。

どうすればいいでしょうか？
1. リクエストにX-Cloud-Trace-Contextヘッダを適切なパラメータとともに追加します。
2. 開発プロジェクトからこのタイプのリクエストを繰り返し送信するカスタムスクリプトを記述します。
3. Trace API を使用して、カスタム属性をトレースに適用する。
4. 10 分待ってから、Trace がそれらのタイプのリクエストを自動的に捕捉することを確認してください。
<details><div>
    答え：1
説明
不正解です：
B. カスタムスクリプトを書いてリクエストを繰り返し送信しても、 オプション A のように特定のトレースヘッダを組み込まない限り、 特定のリクエストが常にトレースされるわけではありません。
C. Trace API を使用してカスタム属性をトレースに適用すると、 トレースに注釈をつけることができますが、 特定のリクエストをトレースするかどうかを直接制御することはできません。
D. トレースはサンプリングやその他の設定に依存している可能性があるため、一定時間待っても特定のリクエストが常にトレースされるとは限りません。
正解
A. このヘッダーは、HTTPリクエストのトレースの動作を制御することができます。このヘッダーに適切なパラメーターを含めることで、特定のリクエストを確実にトレースすることができます。
Links:

https://cloud.google.com/trace/docs/setup#force-trace
</div></details>

## Q. 2-21
あなたのチームはGoogle Kubernetes Engine上で動作するサービスを開発しており、Googleが推奨するプラクティスを使用してログデータを標準化する必要があります。最も効率的な方法でデータをより有用にするために取るべき2つのステップは何ですか？(2つの選択肢を選んでください)
1. アプリケーション・ログをBigQueryに集約してエクスポートし、ログ分析を容易にします。
2. アプリケーション・ログの集約エクスポートをCloud Storageに作成し、ログ分析を容易にします。
3. ログ出力を標準出力（stdout）に単一行JSONとして書き込み、構造化ログとしてCloud Loggingに取り込む。
4. Cloud Loggingに構造化ログを書き込むために、アプリケーションコードでLogging APIを使用することを義務付ける。
5. Pub/Sub APIを使用して構造化データをPub/Subに書き込むことを義務付け、Dataflowストリーミングパイプラインを作成してログを正規化し、分析のためにBigQueryに書き込む。
<details><div>
    答え：1,3
説明
Reference(1)に記載のベストプラクティスから、A, Cが正解となります。
Loggingから自動的に必要なログをフィルターしてBigQueryに取り込むことができます。

誤り説明
不正解
A. B. E. 
他の選択肢（A,B,E）は、特定の文脈では有効であるが、ログデータの効率的な標準化に直接関係しないか、不必要な複雑さをもたらす可能性がある（例えば、選択肢EはPub/SubやDataflowのような追加のコンポーネントを導入する）。選択肢 A と B は、より広範なログ戦略の一部かもしれませんが、システム内でログデータを標準化し利用する最も効率的な方法であるとは限りません。
正解
C. D. 
KubernetesとGoogle Cloudで作業するためのベストプラクティスに基づくと、オプションCとDは、通常、ログデータの効率的な標準化と構造化に沿ったステップになります：
C. ログ出力を単一行のJSONとして標準出力（stdout）に書き込むことで、KubernetesとCloud Loggingはログを構造化されたデータとして認識し、扱うことができます。この標準化により、ログの取り込みプロセスが簡素化され、下流の分析やクエリが容易になります。
D. Logging APIを直接使用して構造化ログを記述することで、ログエントリが確実にフォーマットされ、Cloud Loggingのプラクティスと一致する方法で処理されます。構造化されたログは、ログのクエリーと分析を容易にします。
リンク
https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#best_practices
</div></details>

## Q. 2-29
あなたの開発チームは、既存のモノリシックなアプリケーションをコンポーザブルなマイクロサービス群にリファクタリングするよう求められています。新しいアプリケーションには、どのような設計面を実装すべきでしょうか？(2つの選択肢を選んでください)
1. マイクロサービスの呼び出し元が使用するのと同じプログラミング言語でマイクロサービスのコードを開発する。
2. マイクロサービス実装とマイクロサービス呼び出し元との間でAPI契約書を作成する。
3. すべてのマイクロサービス実装とマイクロサービス呼び出し元との間で非同期通信を要求する。
4. パフォーマンス要件に対応するために、マイクロサービスの十分なインスタンスが実行されていることを確認する。
5. 現在のインターフェイスと互換性がない可能性のある将来の変更を許可するために、バージョニングスキームを実装する。
<details><div>
    答え：2,5
説明
不正解
A. 柔軟性を制限します。マイクロサービスは異なる言語で開発できます。
C. 通信は、一律の要件ではなく、特定のニーズに基づいて選択されるべきである。
D. これは基本的な設計の側面ではなく、運用上の懸念事項です。
正解
B. マイクロサービスがどのように相互作用するかを定義し、明確性を確保し、統合の問題を減らします。
E. 既存のクライアントを壊すことなく、マイクロサービスが独立して進化することを可能にする。
リンク
https://cloud.google.com/appengine/docs/standard/java/designing-microservice-api#using_strong_contracts
</div></details>

## Q. 2-38
目的は、App Engineにウェブサイトをデプロイし、URL http://www.altostrat.com/ からアクセスできるようにすることです。

どのようなアクションが必要だと思いますか？
1. ウェブマスターセントラルでドメインの所有権を確認する。App Engineの正規名ghs.googlehosted.comを指すDNS CNAMEレコードを作成します。
2. Webmaster Centralでドメインの所有権を確認します。単一のグローバルApp Engine IPアドレスを指すようにAレコードを定義します。
3. dispatch.yamlでマッピングを定義して、ドメインwww.altostrat.com をApp Engineサービスに指定します。App Engineの正規名ghs.googlehosted.comを指すDNS CNAMEレコードを作成します。
4. dispatch.yamlでマッピングを定義し、ドメインwww.altostrat.com をApp Engineサービスに向けます。単一のグローバルApp Engine IPアドレスを指すように、Aレコードを定義します。
<details><div>
    答え：1
説明
不正解
B. このオプションにはドメインの所有権の確認が含まれますが、単一のグローバルIPアドレスにAレコードを定義することは、IPアドレスが変更される可能性があるため、App Engineでは推奨されません。
C. dispatch.yamlでマッピングを定義することは関連性があるように見えるかもしれませんが、単にカスタムドメインをApp Engineにマッピングする場合には必須のステップではありません。DNS CNAMEレコードの作成は正しいですが、ドメインの所有権の確認ができないため、このオプションは不完全です。
D. オプションBと同様に、これには単一のIPアドレスを指すAレコードの定義が含まれますが、これは推奨されません。また、dispatch.yamlでマッピングを定義することは、この目的には必要な手順ではありません。ドメイン所有権の確認がないため、この選択肢は正しくありません。
正解
A. このオプションは、カスタムドメインをApp Engineにマッピングする標準的な手順の概要です：
Webmaster Centralでドメインの所有権を確認します。App Engineの正式名称ghs.googlehosted.comを指すDNS CNAMEレコードを作成し、カスタムドメインの解決先をApp Engineアプリケーションにします。
リンク
https://cloud.google.com/appengine/docs/flexible/mapping-custom-domains
</div></details>

## Q. 2-50
あなたの e コマース・アプリケーションは外部からのリクエストを受け取り、図のようにクレジットカード処理、配送、在庫管理のためのサードパーティ API サービスに転送します。


顧客から、アプリケーションの動作が予測不可能な時間帯に遅くなるという報告を受けています。アプリケーションはメトリクスを報告しません。一貫性のないパフォーマンスの原因を特定する必要があります。

あなたは何をすべきでしょうか？
1. 各言語用の OpenTelemetry ライブラリをインストールし、アプリケーションを計測してください。
2. コンテナ内にOps Agentをインストールし、アプリケーション・メトリクスを収集するように構成します。
3. ダウンストリーム・サービスを呼び出すときに、X-Cloud-Trace-Context ヘッダーを読み取り、転送するようにアプリケーションを修正する。
4. アプリケーションメトリクスを収集するために、Google Kubernetes EngineクラスタでPrometheusのマネージドサービスを有効にする。
<details><div>
    答え：1
説明
不正解
B. このオプションは、システムとコンテナからメトリクスを収集するのに役立ちますが、サードパーティ・サービスとの相互作用を理解するのに必要な詳細なトレース情報が得られるとは限りません。
C. このオプションは Google Cloud トレースに特有であり、相互作用している外部のサードパーティ サービスには適用されない場合があります。スローダウンが発生している場所の全体像を把握できない可能性があります。
D. Managed Service for Prometheusは、GKEクラスタ上のメトリクスを収集するのに便利ですが、サードパーティのサービスを通じて要求がどのように処理されているかの詳細なトレースを提供するとは限りません。
正解
A. OpenTelemetry は、分散トレースやメトリクス収集を含む、観測可能性のための API とインスツルメンテーションを提供する複合ライブラリです。アプリケーションにOpenTelemetryを追加することで、様々なコンポーネントやサービスを流れるリクエストをトレースすることができ、どこでスローダウンが発生しているのかをピンポイントで特定することができます。

リンク
https://cloud.google.com/trace/docs/trace-app-latency
</div></details>

## Q. 3-19
アプリケーションでは、ホストとなるCompute Engine仮想マシンインスタンスに保存された認証情報を使用して、サービスアカウントをGCP製品に認証する必要があります。これらの認証情報をできるだけ安全にホストインスタンスに配布したいとします。
どうすればよいでしょうか。
1. HTTP署名付きURLを使用して、必要なリソースへのアクセスを安全に提供します。
2. インスタンスのサービスアカウントのアプリケーションデフォルト認証情報を使用して、必要なリソースを認証します。
3. インスタンスのデプロイ後にGCPコンソールからP12ファイルを生成し、アプリケーションを開始する前に認証情報をホストインスタンスにコピーします。
4. クレデンシャル JSON ファイルをアプリケーションのソースリポジトリにコミットし、CI/CD プロセスに、インスタンスにデプロイされるソフトウェアと一緒にパッケージ化させます。
<details><div>
    答え：2
説明
不正解
A. 署名付きURLは、リソース（Googleクラウドストレージのファイルなど）への一時的なアクセスに使用され、サービスアカウントの認証には使用されません。
C. インスタンスのデプロイ後に GCP コンソールから P12 ファイルを生成し、アプリケーションを起動する前にホスト・インスタンスに資格情報をコピーする。また、P12 ファイルは安全性が低く、Google Cloud のサービス・アカウント管理では非推奨とされています。
D. ソースコードリポジトリにクレデンシャルをコミットすることは、セキュリティ上悪い習慣です。リポジトリにアクセスできる人なら誰でも認証情報にアクセスできるので、セキュリティリスクにつながる可能性があります。
正しい答え
B. Compute EngineインスタンスがGoogle Cloudサービスとやり取りする必要がある場合、作成時にインスタンスにサービスアカウントを割り当てるのがベストプラクティスです。Googleが提供するApplication Default Credentials (ADC)ライブラリは、インスタンスに関連付けられたサービスアカウントの認証情報を自動的に取得し、Google Cloudサービスへの認証されたリクエストを可能にします。このアプローチは安全であり、手動でのクレデンシャル管理を必要としない。
リンク
https://cloud.google.com/compute/docs/api/how-tos/authorization
</div></details>

## Q. 3-39
Google Kubernetes Engineにコンテナをデプロイしています。コンテナの起動に時間がかかることがあるため、ライブネス・プローブを実装しました。あなたは、起動時にライブネスプローブが時々失敗することに気づきました。

どうすればよいでしょうか？
1. 起動プローブを追加します。
2. ライブネス・プローブの初期遅延を増やします。
3. コンテナの CPU リミットを増やす。
4. readinessProbeを追加する。
<details><div>
    答え：1
説明
AかBかで非常に悩む問題です。
Reference (2) の資料に「デッドロックに対する迅速な反応を損なうことなくLiveness Probeのパラメーターを設定することは難しい場合があります。」 とあるので、initialDelaySeconds での設定が難しいケースがあるものと思います。
そのような場合に、Startupプローブの設定が有効と記述されていますので、A を正解とします。
誤り説明
不正解
A. 起動プローブは、コンテナ内のアプリケーションが正常に起動したかどうかを確認するために使用されます。しかし、質問で説明されている問題は、コンテナの起動が遅いために起動時にライブネス・プローブが失敗することに関連しており、アプリケーションが起動に失敗することに起因しているわけではありません。したがって、起動プローブでは根本的な問題に対処できません。
C. 遅さが CPU の制約に関連している場合、このオプションはコンテナの起動を高速化する可能性がある。しかし、問題には、コンテナが CPU 制約のために遅いという証拠が示されていないため、この解決策では記述されている特定の問題に対処できない可能性があります。
D. 準備完了プローブは、コンテナがトラフィックの受け入れを開始する準備ができているかどうかを判断するために使用されます。この問題は、コンテナがトラフィックを処理する準備ができていないのではなく、コンテナの起動が遅いことに関連しているため、readinessProbeを追加しても、起動時にreadinessProbeが失敗する問題は解決しません。
正解
B. ライブネス・プローブは、コンテナを再起動するタイミングを知るために使用されます。コンテナの起動が遅い場合、コンテナが起動するのに十分な時間がかかる前に、有効性プローブが失敗する可能性があります。初期遅延を大きくすることで、コンテナの起動時間を増やすことができ、コンテナの起動が遅いという理由だけでライブネス・プローブが失敗することを防ぐことができる。これが、説明した問題に対する正しい解決策である。
リンク
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
</div></details>

## Q. 3-49
あなたは、Google Kubernetes Engine（GKE）上でホストされるJPEG画像リサイズAPIを開発しています。サービスの呼び出し元は同じGKEクラスタ内に存在します。クライアントがサービスのIPアドレスを取得できるようにしたい。

どうすればいいでしょうか？
1. GKEサービスを定義します。クライアントは、サービスのクラスタIPアドレスを見つけるために、Cloud DNSのAレコードの名前を使用する必要があります。
2. GKEサービスを定義する。クライアントは URL でサービス名を使用してサービスに接続する必要があります。
3. GKEエンドポイントを定義します。クライアントは、クライアントコンテナ内の適切な環境変数からエンドポイント名を取得する必要があります。
4. GKEエンドポイントを定義します。クライアントはCloud DNSからエンドポイント名を取得する必要があります。
<details><div>
    答え：2
説明
不正解
A. GKEサービスを定義することは正しいですが、通常Kubernetesクラスタ内では、Cloud DNSのAレコードに依存する必要はありません。クラスタ内のPodはサービス名を直接使用できます。
C. D. 
Kubernetesには "GKEエンドポイント "という概念はありません。Kubernetesのエンドポイントはサービスの一部であり、サービスがプロキシすべきポッドのIPを追跡するために内部的に使用されます。これらはクライアントが直接やりとりするものではありません。
正しい答え
B. これはクラスタ内の内部通信を許可する正しい方法です。サービス名はDNS名として機能し、サービスのクラスタIPはクラスタ内で自動的に解決されます。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/service-discovery
</div></details>

## Q. 4-6
あなたのコードはプロジェクトAのCloud Functions上で実行されています。プロジェクトBが所有するCloud Storageバケットにオブジェクトを書き込むことになっています。
この問題を解決するにはどうすればいいですか？
1. ユーザーアカウントに、Cloud Storageバケットのroles/storage.objectCreatorロールを付与します。
2. あなたのユーザアカウントに、service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com サービスアカウントの roles/iam.serviceAccountUser ロールを付与します。
3. service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com サービスアカウントに、Cloud Storage バケットの roles/storage.objectCreator ロールを付与します。
4. プロジェクトBでCloud Storage APIを有効にする。
<details><div>
    答え：3
説明
不正解
A. あなたのユーザーアカウントに権限を与えても問題は解決しません。なぜなら、コードはあなたのユーザーアカウントではなく、サービスアカウントを使って実行されるからです。
B. roles/iam.serviceAccountUserロールは、Cloud Storageの権限付与とは関係ないため、サービス・アカウントのユーザー・アカウントにこのロールを付与しても問題は解決しません。
D. 
プロジェクトBでCloud Storage APIを有効にしても、"403 Forbidden "のようなパーミッションエラーが修正されるとは限りません。バケットを使用しているプロジェクトでは、APIはすでに有効になっているはずであり、エラーメッセージはAPIの有効化ではなく、パーミッションに関連しています。
正解
C. クラウドファンクションでコードを実行する場合、コードはファンクションに関連付けられたサービスアカウントの権限で実行されます。この場合、"403 Forbidden "というエラーは、通常、サービスアカウントがCloud Storageバケットに書き込もうとしていることに関連したパーミッションの問題を示しています。
コードはプロジェクトAのCloud Functions上で実行され、プロジェクトBが所有するCloud Storageバケットにオブジェクトを書き込むことになっているので、プロジェクトBのCloud StorageバケットのCloud Functionsインスタンス(service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com)に関連付けられたサービスアカウントに適切なパーミッションを与える必要があります。
リンク
https://cloud.google.com/functions/docs/concepts/iam#troubleshooting_permission_errors
</div></details>

## Q. 4-13
開発時間を最小限に抑えながら、本番稼動中のサービス低下をオンコールエンジニアに通知したい。

どうすればよいでしょうか？
1. Cloud Functionを使用してリソースを監視し、アラートを発生させます。
2. Cloud Pub/Subを使用してリソースを監視し、アラートを発生させる。
3. Stackdriver Error Reportingを使用してエラーをキャプチャし、アラートを発生させます。
4. リソースの監視とアラートの発生にはStackdriver Monitoringを使用します。
<details><div>
    答え：4
説明
不正解
A. リソースを監視するCloud Functionを記述することは可能ですが、これにはかなりのカスタム開発が必要になり、この目的のために特別に設計された既存のMonitoring toolsを活用することはできません。
B. Cloud Pub/Subは主にメッセージング・サービスであり、リソースの監視やアラートの発生に特化したものではない。
C. Stackdriver Error Reporting（現在のCloud Error Reporting）は、アプリケーションのエラーをキャプチャして追跡するために使用されますが、Cloud Monitoringのような広範なリソースおよびパフォーマンス監視機能を提供しません。
正解
D. ここでの最も適切な選択です。様々なメトリクスを追跡し、アラートをトリガーする条件を設定し、必要な担当者に通知するための組み込み機能を提供します。
リンク
https://cloud.google.com/error-reporting/docs/notifications
https://cloud.google.com/blog/products/gcp/drilling-down-into-stackdriver-service-monitoring
</div></details>

## Q. 4-15
ユーザが管理するキーを持つユーザが管理するサービスアカウントを使用してCloud Storage APIを認証するオンプレミス・アプリケーションがあります。このアプリケーションは、Dedicated Interconnectリンクを介してPrivate Google Accessを使用してCloud Storageに接続します。アプリケーションからのCloud Storageバケット内のオブジェクトへのアクセス要求が、403 Permission Deniedエラー・コードで失敗していることがわかりました。

この問題の原因として何が考えられますか？
1. バケット内のフォルダ構造とオブジェクトのパスが変更された。
2. サービスアカウントの定義済みロールのパーミッションが変更された。
3. サービスアカウントキーはローテーションされましたが、アプリケーションサーバー上で更新されていません。
4. オンプレミスのデータセンターから Google Cloud へのインターコネクトリンクに一時的な障害が発生しています。
<details><div>
    答え：3
説明
不正解
A. 
バケツ内のフォルダ構造やオブジェクトパスが変更された場合、通常 "403 Permission Denied" エラーではなく、"404 Not Found" エラーが発生します。
B. 
サービスアカウントの事前定義されたロールのパーミッションが変更された場合、それは異常で意図的な行動である。しかし、サービスアカウントが必要なリソースにアクセスできなくなるような方法でパーミッションが減らされた場合、"403 Permission Denied "エラーになる可能性があります。
D. 
インターコネクトリンクの停止は、接続性の問題やネットワークのタイムアウトや障害に関連するエラーにつながる可能性が高く、特にアクセス許可に関連する "403 Permission Denied" エラーにはつながりません。
正解
C. 
エラーコード "403 Permission Denied "は、通常、接続性の問題ではなく、認証または認可の問題を示します。
提供された選択肢の中で、このエラーの最も可能性の高い原因は、サービスアカウントのキーがローテートされたにもかかわらず、アプリケーションサーバ上で更新されておらず、認証の試みが失敗したことでしょう。
つまり、選択肢 C は、"403 Permission Denied "エラーの最も典型的なシナリオであり、記述されている問題の最も可能性の高い原因です。
リンク
403エラーメッセージのトラブルシューティング
</div></details>

## Q. 4-28
あなたは大企業の開発者です。あなたはGoogle Kubernetes Engine（GKE）にWebアプリケーションをデプロイしています。DevOpsチームは、Cloud Deployを使用してアプリケーションをGKEのDev、Test、ProdクラスタにデプロイするCI/CDパイプラインを構築しました。Cloud DeployがDevクラスタへのアプリケーションのデプロイに成功した後、それを自動的にTestクラスタに昇格させたいとします。

Googleが推奨するベストプラクティスに従って、このプロセスをどのように構成すればよいでしょうか？
1. 
- clouddeploy-operations トピックからの SUCCEEDED Pub/Sub メッセージをリッスンする Cloud Build トリガーを作成します。
- アプリケーションをTestクラスタにプロモートするステップを含むようにCloud Buildを構成します。
2. 
- Google Cloud Deploy API を呼び出してアプリケーションを Test クラスタにプロモートする Cloud Function を作成します。
- この関数は、cloud-builds トピックからの SUCCEEDED Pub/Sub メッセージによってトリガーされるように構成します。
3. 
- Google Cloud Deploy APIを呼び出してアプリケーションをTestクラスタに昇格させるCloud Functionを作成します。
- clouddeploy-operationsトピックからのSUCCEEDED Pub/Subメッセージによってトリガーされるようにこの関数を構成する。
4. 
- gke-deploy ビルダーを使用する Cloud Build パイプラインを作成します。
- cloud-builds トピックからの SUCCEEDED Pub/Sub メッセージをリッスンする Cloud Build トリガーを作成します。
- このパイプラインを構成して、Test クラスタへのデプロイメントステップを実行します。
<details><div>
    答え：3
説明
不正解
A.
アプリケーションをプロモートするためのCloud Deploy APIではなくCloud Buildが関係するため、正しくありません。Cloud Deployは環境間のデプロイを管理するように設計されています。
B.
クラウド・デプロイではなく、クラウド・ビルドに関連する cloud-builds トピックを参照しているため、正しくありません。
D.
Cloud Buildパイプラインに依存し、Cloud Deploy操作ではなくcloud-buildsトピックからのメッセージをリッスンし、Cloud Deploy APIではなくgke-deployビルダーを使用するため、不正解です。
正解です：
C.
このオプションには、Google Cloud Deploy APIを呼び出すCloud Functionの作成が含まれ、CI/CDパイプラインの環境間でアプリケーションをプロモートするのに適しています。また、Cloud Deploy操作に関連するclouddeploy-operationsトピックからのSUCCEEDED Pub/Subメッセージを正しくリッスンします。
リンク
https://cloud.google.com/functions/docs/calling/pubsub
</div></details>

## Q. 4-29
あなたの会社には、アプリケーション情報をBigQueryで保持するデータウェアハウスがあります。BigQueryデータウェアハウスには2PBのユーザーデータが保存されています。最近、貴社はEUユーザを含むユーザベースを拡大しました：

ユーザーの要求に応じて、すべてのユーザーアカウント情報を削除できる必要があります。

すべてのEUユーザーデータは、EUユーザー専用の単一リージョンに保存する必要があります。

あなたはどの2つのアクションを取るべきですか？(2つの選択肢を選んでください)
1. BigQuery連携クエリを使用して、クラウドストレージからデータをクエリします。
2. EU ユーザーの情報のみを保持するデータセットを EU 地域に作成する。
3. EUリージョンにCloud Storageバケットを作成し、EUユーザーの情報のみを保存します。
4. クラウドデータフローパイプラインを使用して、ユーザーレコードをフィルタリングしてデータを再アップロードします。
5. BigQueryのDMLステートメントを使用して、リクエストに基づいてユーザーレコードを更新/削除する。
<details><div>
    答え：2,5
説明
不正解
A. C. D. 
オプション A. 連携クエリにより、Google Cloud サービス全体でデータを分析できる。このオプションは、地域ストレージやユーザー情報の削除機能という特定の要件には対応していません。
オプション C. EUリージョンにCloud Storageバケットを作成しても、データウェアハウスが存在するBigQueryには直接関係しません。このオプションは所定の要件を満たしていません。
オプション D. ユーザーレコードをフィルタリングしてデータを再アップロードすることは、EU ユーザーデータを特定のリージョンに保存したり、要求に応じてユーザーデータを削除したりする要件に直接関係しません。
正解
B. E. 
オプション B. EU 地域に別のデータセットを作成することで、必要に応じて特定の地域の EU ユーザーの情報を保存できます。これにより、すべてのEUユーザーデータを単一のリージョンに保存するというコンプライアンス要件を満たすことができます。
オプションE. BigQueryのDML（Data Manipulation Language）ステートメントを利用することで、ユーザーの要求に基づいて特定のユーザーレコードを更新または削除することができます。これにより、ユーザーの要求に応じてすべてのユーザーアカウント情報を削除するという要件に準拠できます。
リンク
https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language
https://cloud.google.com/architecture/bigquery-data-warehouse
</div></details>

## Q. 4-39
新しいアプリケーションコードを本番環境に移行する前に、様々な異なるユーザに対してテストを実施したい とします。この計画はリスクを伴いますが、本番ユーザでアプリケーションの新バージョンをテストし、オペレー ティングシステムに基づいて、どのユーザがアプリケーションの新バージョンに転送されるかを制御したいでしょう。新バージョンにバグが発見された場合、新しくデプロイされたバージョンのアプリケーションをできるだけ早くロールバックしたい。

どうすればよいでしょうか？
1. アプリケーションをCloud Runにデプロイします。トラフィック分割を使用して、リビジョンタグに基づいて、ユーザートラフィックのサブセットを新しいバージョンに誘導します。
2. Anthosサービスメッシュを使用してGoogle Kubernetes Engineにアプリケーションをデプロイします。トラフィックの分割を使用して、ユーザーエージェントヘッダーに基づく新しいバージョンにユーザートラフィックのサブセットを誘導する。
3. アプリケーションを App Engine にデプロイします。トラフィックの分割を使用して、IP アドレスに基づいてユーザー トラフィックのサブセットを新しいバージョンに誘導します。
4. Compute Engineにアプリケーションをデプロイします。Traffic Directorを使用して、事前に定義された重みに基づいて、ユーザートラフィックのサブセットを新しいバージョンに誘導します。
<details><div>
    答え：2
説明
不正解
A. 
オプションA（Cloud Run）はリビジョンタグに基づくトラフィック分割を可能にしますが、オペレーティングシステム（ユーザーエージェントヘッダー）に基づくユーザートラフィックの制御を提供しません。
C. 
オプションC（App Engine）ではトラフィックの分割が可能ですが、IPアドレスに基づく分割は、オペレーティングシステムに基づいてユーザーをルーティングする必要性に合致しません。
D. 
オプションD（Traffic Directorを備えたCompute Engine）は、事前に定義された重みに基づくトラフィック制御を可能にしますが、オペレーティングシステムに基づいてユーザーをルーティングするために必要なきめ細かさを提供しません。
正解
B. 
オプションBでは、ユーザーエージェントなどのHTTPヘッダに基づいてトラフィックを分割するなど、高度なルーティングとトラフィック制御が可能です。これにより、ユーザーをオペレーティングシステムに基づいて異なるバージョンのアプリケーションにルーティングすることが可能になり、記載されている要件を満たすことができます。
リンク
https://cloud.google.com/traffic-director/docs/ingress-traffic#sending-traffic
</div></details>

## Q. 4-46
あなたはプロジェクトAとプロジェクトBという2つのGoogle Cloudプロジェクトを持っています。あなたはプロジェクトBのクラウドストレージバケットに出力を保存するクラウド関数をプロジェクトAに作成する必要があります。あなたは何をすべきですか？
1. 
- プロジェクトBでGoogleサービスアカウントを作成する。
- プロジェクトAのサービスアカウントでCloud Function をデプロイします。
- このサービスアカウントに、プロジェクトBにあるストレージバケットのroles/storage.objectCreatorロールを割り当てます。
2. 
- プロジェクトAにGoogleサービスアカウントを作成する。
- プロジェクトAのサービスアカウントでCloud Function をデプロイする。
- このサービスアカウントに、プロジェクトBにあるストレージバケットのroles/storage.objectCreatorロールを割り当てる。
3. 
- プロジェクト A のデフォルトの App Engine サービスアカウント (PROJECT_ID@appspot.gserviceaccount.com) を決定します。
- プロジェクトAのデフォルトのApp Engineサービスアカウントを使用して、Cloud Function をデプロイします。
- デフォルトのApp Engineサービスアカウントに、プロジェクトBに存在するストレージバケットのroles/storage.objectCreatorロールを割り当てます。
4. 
- プロジェクト B のデフォルトの App Engine サービスアカウント (PROJECT_ID@appspot.gserviceaccount.com) を決定します。
- プロジェクトAのデフォルトApp EngineサービスアカウントでCloud Functionをデプロイする。
- デフォルトのApp Engineサービスアカウントに、プロジェクトBに存在するストレージバケットのroles/storage.objectCreatorロールを割り当てます。
<details><div>
    答え：2
説明
不正解：
A.C.D.
プロジェクトBのサービスアカウントは、プロジェクトAのCloud Function のデプロイに使用できないため、このオプションは正しくありません。
技術的にはうまくいくかもしれませんが、デフォルトの App Engine サービス アカウントには必要以上に広い権限が設定されている可能性があるため、最小権限の原則には従っていません。
選択肢 D. この選択肢は、プロジェクト B のデフォルトの App Engine サービス アカウントを使用してプロジェクト A にCloud Function をデプロイすることに言及しているため、正しくありません。
これは実行不可能です：
B.
オプションBは、プロジェクトBのターゲットバケット上に、必要最小限の権限（roles/storage.objectCreator）を持つ特定のサービスアカウントをプロジェクトAに作成することで、最小権限の原則に正しく沿います。
リンク
https://cloud.google.com/docs/authentication/production#providing_credentials_to_your_application
</div></details>

## Q. 4-48
Cloud Build を使用して、Cloud Source Repositories に保存されているアプリケーションのソース コードをビルドおよびテストしています。ビルドプロセスには、Cloud Build 環境で利用できないビルドツールが必要です。
どうすればよいですか？
1. ビルド・プロセス中にインターネットからバイナリをダウンロードします。
2. カスタムクラウドビルダーイメージを作成し、ビルド手順でそのイメージを参照する。
3. バイナリを Cloud Source Repositories リポジトリにインクルードし、ビルドスクリプトで参照します。
4. Cloud BuildパブリックIssue Trackerに対して機能要求を提出することで、バイナリをCloud Build環境に追加してもらう。
<details><div>
    答え：2
説明
不正解です：
A. 
ビルドプロセス中にインターネットからバイナリをダウンロードすることは、特にソースが信頼できない場合、安全でない可能性があります。
C. 
バイナリを Cloud Source Repositories リポジトリに含めると、リポジトリのサイズが大きくなり、特にバイナリを頻繁に更新する必要がある場合、バージョン管理の問題につながる可能性があります。
D. 
Cloud Build public Issue Trackerに対して機能要求を提出しても、必要なバイナリが標準のCloud Build環境に追加されるとは限りませんし、追加されたとしてもプロセスに時間がかかる可能性があります。そのため、緊急のニーズに対する実用的なソリューションではありません。
正解
B. 
カスタムクラウドビルダーイメージを作成することで、必要なビルドツールやビルドプロセスが必要とするその他の依存関係を含めることができます。その後、ビルド構成でこのカスタムイメージを参照し、ビルドプロセス中に必要なツールが利用できるようにします。
リンク
https://cloud.google.com/cloud-build/docs/configuring-builds/use-community-and-custom-builders#creating_a_custom_builder
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>
