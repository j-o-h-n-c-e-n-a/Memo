
## Q. 1-1
同じVPC（仮想プライベートクラウド）内の複数のクライアントから呼び出される必要がある、Compute Engine仮想マシンインスタンス上でホストされるHTTP APIを開発しています。クライアントがサービスのIPアドレスを取得できるようにしたい。どうすればよいでしょうか？
1. 静的な外部IPアドレスを予約し、HTTP(S)負荷分散サービスの転送ルールに割り当てる。クライアントはこのIPアドレスを使ってサービスに接続する。
2. 静的な外部IPアドレスを予約し、HTTP(S)ロードバランシングサービスの転送ルールに割り当てる。次に、クラウドDNSでAレコードを定義する。クライアントはAレコードの名前を使用してサービスに接続する。
3. クライアントが、https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/.というURLでインスタンス名に接続し、Compute Engineの内部DNSを使用するようにします。
4. クライアントが、https://[API_NAME]/[API_VERSION]/のURLでインスタンス名に接続して、Compute Engineの内部DNSを使用するようにします。
<details><div>
    答え：3
A. 
1．静的外部IPアドレスは、クライアントがサービスに接続するために使用できる固定IPアドレスを提供します。インスタンスが再起動されたり、インスタンスがオートスケーリンググループの一部である場合、インスタンスのIPアドレスが変更される可能性があるため、これは重要です。
2．HTTP(S)ロードバランシング・サービスは、トラフィックを複数のインスタンスに分散させることができ、サービスのパフォーマンスと可用性を向上させることができます。また、利用可能なインスタンスのプールから不健康なインスタンスを自動的に削除するヘルスチェックも処理できる。
3．転送ルールを使用することで、URLまたはIPアドレスに基づいて、トラフィックを適切なインスタンスに誘導することができる。これにより、複数のサービスを単一のIPアドレスとポートの組み合わせでホストすることができる。
4．クライアントは、転送ルールに割り当てられたIPアドレスを使用してサービスに接続できる。
結論として、この複雑なソリューションは "非最適 "と評価できる。
B. 説明
このオプションはオプションAと似ていますが、サービスに接続するためにIPアドレスを使用する代わりに、クライアントはクラウドDNSで定義されたAレコードの名前を使用します。これは、サービスによりユーザーフレンドリーな名前を提供できますが、セットアップにさらなる複雑さが加わります。
さらに、クラウドDNSを使用すると、構成および管理する必要がある別のサービスが追加されるため、問題やダウンタイムが発生する可能性が高まります。
D. 説明
このオプションはオプションCと似ていますが、URLでインスタンス名を使用する代わりに、API名とバージョンを使用します。これは、より使いやすいURLを提供できますが、Compute Engineが提供する内部DNSサービスに依存します。
また、カスタムのAPI名とバージョンを使用すると、セットアップがさらに複雑になり、追加の設定と管理が必要になる場合があります。
正解
C. このオプションは、Compute Engineが提供する内部DNSサービスを使用して、サービスをホストするインスタンスのIPアドレスを解決します。これは、すべてのクライアントが同じVPC内にあり、内部DNSサービスにアクセスできる場合に機能します。
まとめると、同じVPC内のクライアントがCompute Engineの仮想マシンインスタンス上でホストされているHTTP APIのIPアドレスを取得できるようにするための最良の選択肢は、クライアントが、https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/というURLでインスタンス名に接続してCompute Engineの内部DNSを使用するようにすることです。
Links: 

https://cloud.google.com/compute/docs/internal-dns
</div></details>

## Q. 1-2
Webアプリケーションは企業のイントラネットにデプロイされています。このWebアプリケーションをGoogle Cloudに移行する必要があります。ウェブアプリケーションは、会社の従業員だけが利用でき、従業員が移動中にアクセスできる必要があります。アプリケーションの変更を最小限に抑えながら、Webアプリケーションのセキュリティとアクセシビリティを確保する必要があります。

どのような対応が必要ですか？
1. アプリケーションへの HTTP(S)リクエストごとに認証情報をチェックするようにアプリケーションを構成する。
2. 従業員がパブリックIPアドレス経由でアプリケーションにアクセスできるように、Identity-Aware Proxyを構成する。
3. ユーザーに企業アカウントへのログインを要求するCompute Engineインスタンスを構成します。WebアプリケーションのDNSをプロキシのCompute Engineインスタンスを指すように変更します。認証後、Compute EngineインスタンスはWebアプリケーションとの間でリクエストを転送します。
4. ユーザーに企業アカウントへのログインを要求するCompute Engineインスタンスを構成する。WebアプリケーションのDNSをプロキシのCompute Engineインスタンスを指すように変更します。認証後、Compute Engineは、WebアプリケーションをホストするパブリックIPアドレスにHTTPリダイレクトを発行します。
<details><div>
    答え：3
A. このオプションでは、HTTP(S)リクエストごとに認証チェックを実装するために、アプリケーションに大幅な変更が必要になる可能性が高く、アプリケーションの変更を最小限に抑えるという要件に反する。
B. IAP（Identity-Aware Proxy）は、Google Cloud Platformの機能であり、IDおよびコンテキストベースのアクセス制御を使用してリソースへのアクセスを保護することができます。IAPを使用すると、リソース（ウェブ・アプリケーションなど）へのアクセスを、認証され許可されたユーザーまたはサービス・アカウントのみに制限することができます。
しかし、このシナリオでは、ウェブ・アプリケーションは企業のイントラネット上でホストされているため、パブリックIPアドレスを持たず、インターネットからアクセスすることはできません。また、IAP を使用して、イントラネットでホストされているアプリケーションへのアクセスを、その IP アドレスによって制限することはできません。
D. これらのオプションには、プロキシとして機能するCompute Engineインスタンスを設定し、企業アカウントを通じてユーザーを認証することが含まれます。また、WebアプリケーションをホストするパブリックIPアドレスへのHTTPリダイレクトが含まれるため、従業員のみにアクセスを制限する必要性に合致しない可能性があります。
正解
C. このアプローチでは、Google Cloudのインフラストラクチャを利用して、Webアプリケーションへのアクセスを許可する前に、企業のイントラネットを通じてユーザーを認証することができます。プロキシとして動作するCompute Engineインスタンスを設定し、WebアプリケーションのDNSをこのプロキシを指すように変更することで、Webアプリケーションへのアクセスは、企業イントラネットで認証された従業員のみに制限されます。さらに、この方法では、従業員がインターネットにアクセスできる環境であれば、出張中でもWebアプリケーションにアクセスすることができます。
Links:
https://cloud.google.com/compute/docs

https://cloud.google.com/iam
</div></details>

## Q. 1-3
あなたは、Google Cloud上で実行されるあなたの会社のeコマースプラットフォームの決済システムを管理しています。貴社は、内部監査目的で1年間、コンプライアンス要件を満たすために3年間、ユーザーログを保持する必要があります。オンプレミスのストレージ使用量を削減し、ログを簡単に検索できるようにするために、新しいユーザーログをGoogle Cloudに保存する必要があります。

ログが正しく保存されていることを確認しながら、労力を最小限に抑えるために、どのような行動を取るべきでしょうか？
1. バケットロックをオンにして、ログをクラウドストレージのバケットに保存する。
2. ログをCloud Storageバケットに保存し、保存期間を3年にする。
3. Cloud Loggingに、カスタム保存期間を持つカスタムログとしてログを保存します。
4. 保存期間1年のCloud Storageバケットにログを保存する。1年後、ログを保存期間2年の別のバケットに移動する。
<details><div>
    答え：3
要件では、ログを簡単に検索できるようにする必要があります。これはクラウドストレージでは容易に実現できないため、選択肢A、B、Dは除外されます。
正解
Cloud Loggingは、ログが保存されているログバケットタイプに適用される保持ルールに従ってログを保持します。
Cloud Logging は、ログを 1 日から 365 日の範囲で保持するように構成できます。カスタム保持ルールは、ログタイプやログが別の場所からコピーされたかどうかに関係なく、バケット内のすべてのログに適用されます。
Links:
https://cloud.google.com/logging/docs/buckets#custom-retention

https://cloud.google.com/logging/docs/routing/overview#logs-retention

https://cloud.google.com/logging/docs/audit/best-practices#custom-retention

https://cloud.google.com/logging/docs/central-log-storage
</div></details>

## Q. 1-4
アプリケーションは、Compute Engine上で実行されるコードによってオーケストレーションされた、疎結合のサービス群で構成されています。アプリケーションは、サービスの特定のバージョンを見つけて使用する新しいCompute Engineインスタンスを簡単に起動できるようにしたい。
これはどのように設定すべきでしょうか？
1. 実行時に取得され、目的のサービスに接続するために使用されるメタデータとして、サービス・エンドポイント情報を定義します。
2. 実行時に取得され、目的のサービスに接続するために使用されるラベル・データとして、サービス・エンドポイント情報を定義する。
3. 実行時に環境変数から取得し、目的のサービスに接続するために使用するサービス・エンドポイント情報を定義する。
4. 固定ホスト名とポートを使用して目的のサービスに接続するようにサービスを定義する。エンドポイントのサービスを新しいバージョンに置き換えます。
<details><div>
    答え：1
B. ラベルは通常、実行時の設定ではなく、リソースの整理に使用されます。
C. 環境変数に依存すると、結合が密になり、サービスのバージョンを管理するための柔軟性が得られない可能性がある。
D. 固定のホスト名とポートを使用することは、サービスの異なるバージョンで動作するために必要な柔軟性を提供しない可能性があります。
答え
A. オプションAでは、エンドポイント情報をメタデータとして保存し、実行時にCompute Engineインスタンスから取得することができます。この方法では、インスタンスを変更することなくエンドポイント情報を変更することができるため、疎結合サービスをサポートし、異なるバージョンのサービスを管理するプロセスが容易になります。
Links:

https://cloud.google.com/apis/design/glossary#api_service_endpoint

https://cloud.google.com/compute/docs/metadata/overview

https://cloud.google.com/service-infrastructure/docs/service-metadata/reference/rest#service-endpoint

</div></details>

## Q. 1-5
以下のgcloudコマンドを使用してHTTP(s) Load Balancerをデプロイしました。


Compute Engine仮想マシンインスタンスのポート80へのヘルスチェックが失敗し、インスタンスにトラフィックが送信されません。この問題を解決することが目的です。
1. gcloud compute instances add-access-config ${NAME}-backend-instance-1
2. gcloud compute instances add-tags ${NAME}-backend-instance-1 --tags http-server
3. gcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --source-ranges 130.211.0.0/22,35.191.0.0/16 --direction INGRESS。
4. gcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --destination-ranges 130.211.0.0/22,35.191.0.0/16 --direction EGRESS
<details><div>
    答え：3
A. オプションAは、インスタンスに外部IPアドレスを追加するために使用されるため、役に立ちません。
B. Bはインスタンスにタグを追加することに関連するが、対応するファイアウォールルールがなければ、問題は解決しない。このコマンドはインスタンスにメタデータを適用するために使われ、ロードバランサーとは関係ありません。
D. Dは発信(EGRESS)トラフィック用のファイアウォールルールを作成していますが、ここでの問題は着信(INGRESS)のヘルスチェックです。
答え:
このコマンドは、指定されたソース範囲（Googleのロードバランサーがヘルスチェックに使用するIPアドレス範囲）からのポート80の着信TCPトラフィックを許可するファイアウォールルールを作成する。
正しいコマンドは、上記の修正されたオプションに示されているように、TCPポート80を指定する必要があることに注意してください。

Link: https://cloud.google.com/vpc/docs/special-configurations
</div></details>

## Q. 1-6
あなたは他のGoogle Cloudリソースにアクセスするクラウド関数を書きました。あなたは最小特権の原則を使用して環境を保護したいと思います。

どのようなアクションが必要ですか？
1. リソースにアクセスするエディタ権限を持つ新しいサービスアカウントを作成します。デプロイヤーにアクセストークンを取得する権限を与えます。
2. リソースにアクセスするためのカスタムIAMロールを持つ新しいサービスアカウントを作成します。デプロイ先にはアクセストークンを取得する権限が与えられる。
3. リソースにアクセスする編集権限を持つ新しいサービスアカウントを作成する。デプロイ担当者には、新しいサービスアカウントとして行動する権限が与えられます。
4. リソースにアクセスするためのカスタムIAMロールを持つ新しいサービスアカウントを作成します。デプロイ担当者には、新しいサービスアカウントとして行動する権限が与えられます。
<details><div>
    答え：4
A. エディター権限は一般的に、プロジェクト内の多くのリソースやアクションへの広範なアクセスを提供します。Editor権限を与えることで、Cloud Function が実行する特定のタスクに実際に必要な以上の権限を与えている可能性があります。これは最小特権の原則に違反します。
B. このオプションではカスタムIAMロールを作成するため、最小特権の原則に沿う可能性がありますが、デプロイ者に与えられる権限は「アクセストークンを取得する」ことです。これは、デプロイ者がサービスアカウントを "act as "することを許可するよりも安全性が低く、あまり一般的ではありません。慎重に扱わなければ、トークンの誤用につながる可能性がある。
C. 選択肢Aと同様に、この選択肢ではリソースへの広範なアクセスを提供するEditor権限を付与します。この場合も、必要以上の権限を与えることになり、最小権限の原則と矛盾する可能性があります。
正解
D. このオプションは、必要なリソースへのアクセスに必要な権限のみを含むように正確に定義できるカスタムIAMロールを作成することで、最小特権の原則に従います。こうすることで、Cloud Function はセキュリティリスクにつながる不必要なパーミッションを持つことがなくなる。デプロイヤーに新しいサービスアカウントとして動作する権限を与えることで、デプロイ時にCloud Function がこの役割を引き受けることができます。
他のオプションはより広い権限（Editor権限）を与えるか、役割のカスタム性を強調しない。
Links:

https://cloud.google.com/functions/docs/securing/function-identity

https://cloud.google.com/blog/products/application-development/least-privilege-for-cloud-functions-using-cloud-iam

https://cloud.google.com/functions/docs/securing/function-identity#per-function_identity
</div></details>

## Q. 1-7
あなたは、顧客、注文、在庫データをCloud Spanner内のリレーショナル・テーブルとして格納するeコマース・アプリケーションを開発しています。最近の負荷テスト中に、Spanner のパフォーマンスが期待どおりに線形にスケーリングされていないことがわかりました。

次のうちどれが原因ですか？
1. 32ビットの数値に64ビットの数値型を使用すること。
2. 単調に増加する主キーとしてバージョン1のUUIDを使用すること。
3. STRINGデータ型の任意精度値への使用。
4. パラメータ化されたSQL問い合わせで、STARTS_WITHキーワードの代わりにLIKEを使用すること。
<details><div>
    答え：2
A. ストレージの効率は悪くなるかもしれないが、Spannerのリニアスケーリング機能に大きな影響を与えることはないだろう。
C. 任意精度の値にSTRINGデータ型を使用することは最適な選択ではないかもしれませんが、リニアスケーリングに大きな影響を与えることはないでしょう。
D. これはクエリ・パフォーマンスに影響する可能性がありますが、通常、システム全体の線形スケーリングには影響しません。
正解
B. Cloud Spannerでは、単調に増加するVersion 1 UUIDを主キーとして使用すると、均等に分散されないためパフォーマンスの問題が発生する可能性があります。このため、特定のノードまたはノードの範囲に不釣り合いな数のリクエストが送信されるホットリージョンが発生し、そのノードが過負荷になり、パフォーマンスが低下する可能性があります。パフォーマンスを改善するには、ハッシュベースのキーやランダムな整数など、より均等に分散されたプライマリ・キーの使用を検討すべきである。

Links:

https://cloud.google.com/spanner/docs/schema-and-data-model#choosing_a_primary_key

https://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots
</div></details>

## Q. 1-8
あなたのチームは、Cloud Identityによって管理されるユーザーIDで実行されるGoogle Cloudアプリケーションを開発しています。アプリケーションの各ユーザは、メッセージが発行される関連する Pub/Sub トピックと、同じユーザが発行されたメッセージを取得する Pub/Sub サブスクリプションを持ちます。

許可されたユーザのみが、特定のPub/Subトピックとサブスクリプションにパブリッシュおよびサブスクライブできるようにする必要があります。
1. ユーザ ID に、pubsub.topics.create および pubsub.subscriptions.create 許可を含むカスタム・ロールを付与する。
2. pubsub.publisherおよびpubsub.subscriberロールを持つサービスアカウントとしてアプリケーションを実行するように構成する。
3. リソース・レベルでユーザ ID を pubsub.publisher および pubsub.subscriber ロールにバインドします。
4. プロジェクトレベルで、ユーザ ID に pubsub.publisher および pubsub.subscriber ロールを付与します。
<details><div>
    答え：3
A. pubsub.topics.create および pubsub.subscriptions.create パーミッションを含むカスタム・ロールをユーザ ID に付与すると、ユーザはトピックおよびサブスクリプションを作成できますが、特定のトピックまたはサブスクリプションへのアクセス権は付与されません。
B. pubsub.publisherロールとpubsub.subscriberロールを持つサービスアカウントとしてアプリケーションを実行するように構成すると、ユーザのきめ細かな権限管理ができません。
D. プロジェクト・レベルでユーザ ID に pubsub.publisher および pubsub.subscriber ロールを付与すると、ユーザはプロジェクト内のすべてのトピックおよびサブスクリプションにアクセスできるようになります。
正解
C. リソースレベルでユーザ ID を pubsub.publisher および pubsub.subscriber ロールにバインドすることで、各ユーザが特定の Pub/Sub トピックおよびサブスクリプションに対してのみパブリッシュおよびサブスクライブできるようにできます。このアプローチにより、きめ細かな権限管理が可能になり、各ユーザが許可されたリソースのみにアクセスできるようになります。
Link:

https://cloud.google.com/pubsub/docs/access-control
</div></details>

## Q. 1-9
最近、アプリケーションに影響する政府規制が可決されました。コンプライアンス目的のために、アプリケーションのプロジェクトからセキュリティチームに限定されたプロジェクトに、特定のアプリケーションログの複製を送信することが要求されるようになりました。

あなたは何をすべきでしょうか？
1. セキュリティチームのプロジェクトにユーザー定義のログバケットを作成する。Cloud Loggingシンクを構成して、アプリケーションのログをセキュリティチームのプロジェクト内のログバケットにルーティングする。
2. 必要なログバケツからセキュリティチームのプロジェクトのログバケツにログをコピーするジョブを作成する。
3. デフォルトのログバケツのシンクルールを変更し、ログをセキュリティチームのログバケツに再ルーティングする。
4. 必要なログバケツのシステムイベントログを、セキュリティチームのプロジェクトのログバケツにコピーするジョブを作成する。
<details><div>
    答え：1
オプションB、C、およびDは、最も効果的な解決策ではありません。エラーが発生しやすい手動ジョブ（BとD）を含むか、デフォルトのバケツ（C）を変更するため、この規制の遵守に関連しない他のログに影響を与える可能性があります。
正解
このソリューションは、特定のアプリケーションのログを複製し、セキュリティチームのプロジェクトに送信するための直接的かつ自動化されたソリューションを提供します。この方法は、ログをログバケットや Pub/Sub トピックなどの他の宛先にルーティングするための強力なツールである Cloud Logging のシンク機能を使用します。シンクを使用することで、ログの複製がリアルタイムかつ自動的に実行され、手作業による介入とエラーのリスクを最小限に抑えることができます。

Links:

https://cloud.google.com/architecture/security-log-analytics
</div></details>

## Q. 1-10
あなたのチームはあなたの会社のためにeコマースプラットフォームを開発しています。ユーザーはウェブサイトにログインし、ショッピングカートに商品を追加します。ユーザーは30分間操作しないと自動的にログアウトします。ユーザーが再びログインすると、ショッピングカートが保存されます。

Googleが推奨するベストプラクティスに従い、ユーザーのセッション情報とショッピングカート情報をどのように保存すべきでしょうか？
1. Pub/Subにセッション情報を格納し、Cloud SQLにショッピングカート情報を格納する。
2. クラウドストレージ上のファイルにショッピングカート情報を保存する。
3. セッションとショッピングカートの情報を、複数のCompute Engineインスタンス上で動作するMySQLデータベースに保存します。
4. セッション情報をMemorystore for RedisまたはMemcachedに保存し、ショッピングカート情報をFirestoreに保存します。

<details><div>
    答え：4
A. Pub/Subはイベント・ストリーミングとメッセージング用に設計されており、セッション情報を管理するためのものではありません。Pub/Subにセッションデータを格納することは、効率的でない、あるいは従来のサービスの使い方ではないでしょう。Cloud SQLは完全に管理されたリレーショナルデータベースサービスであり、ショッピングカート情報を格納するために使用できますが、オプションDで述べたようにFirestoreを使用する方が適しています。
B. クラウドストレージのバケットにアクセスすると、セッション情報に対して時間がかかり、コストもかかります。これはGoogle Cloudのベストプラクティスではありません。
C. C. Compute Engineインスタンス間でMySQLデータベースを管理することはできますが、Memorystoreのようなスケーラビリティとセッション情報の低レイテンシアクセスを提供することはできません。これはGoogle Cloudのベストプラクティスではありません。
正解
D. eコマースプラットフォームのセッション情報とショッピングカート情報を保存する場合、スケーラビリティ、信頼性、セキュリティを考慮することが重要です。Googleが推奨するベストプラクティスに従ったソリューションの1つは、セッション情報の保存にMemorystore for RedisまたはMemcachedを使用し、ショッピングカート情報の保存にFirestoreを使用することです。

Memorystoreはセッション情報を保存し、大量の同時接続を簡単に処理することができます。これは、ユーザーが頻繁にログインし、ショッピングカートに商品を追加するeコマースプラットフォームにとって非常に重要です。

Firestoreは、ショッピングカート内のアイテムのような大量の半構造化データを容易に扱うことができます。また、Firestoreはスケーラブルで信頼性の高いソリューションであり、自動スケーリングとレプリケーションをサポートしています。

セッション情報とショッピングカート情報を異なるサービスに分離することで、セキュリティを高め、潜在的なデータ侵害を回避することができます。また、異なるサービスを使用することで、それらを独立して拡張することができます。

このことから、回答Dが最良の選択肢となります。

Links:

https://cloud.google.com/memorystore/docs/redis/redis-overview
</div></details>

## Q. 1-11
POSTで呼び出されるHTTPクラウド関数があります。各サブミッションのリクエストボディには、数値とテキストデータを含むフラットでネストされていない JSON 構造があります。クラウド関数が完了した後、収集されたデータは、多くのユーザーが並行して継続的かつ複雑な分析を行うためにすぐに利用できる必要があります。

どのようにサブミッションを永続化しますか？
1. 各POSTリクエストのJSONデータをDatastoreに直接永続化する。
2. POSTリクエストのJSONデータを変換し、BigQueryにストリームします。
3. POSTリクエストのJSONデータを変換し、地域のCloud SQLクラスタに格納する。
4. 各POSTリクエストのJSONデータを、リクエスト識別子を含むファイル名で、Cloud Storage内に個別のファイルとして永続化する
<details><div>
    答え：2
A. 各POSTリクエストのJSONデータをDatastoreに直接永続化する。DatastoreはNoSQLのドキュメントデータベースであり、構造化されたデータを格納するために使用できますが、ほぼリアルタイムで分析する必要がある大量のデータを処理するようには設計されていません。また、分析に使えるようにするには、追加の処理が必要になる。
C. Cloud SQLはデータの永続性を処理できるが、多数の並列ユーザーを持つ複雑な分析にはBigQueryほど効率的ではないかもしれない。
D. 各 POST リクエストの JSON データを個別のファイルとして Cloud Storage 内に保存することは、即時かつ複雑な分析には非効率的です。
正解
B. BigQueryは拡張性の高いデータウェアハウスであり、大量のデータや複雑な分析をほぼリアルタイムで処理するのに適しています。Cloud Function からのJSONデータを直接BigQueryにストリーミングすることで、収集したデータを即座に多くのユーザーが並行して分析できるようになります。BigQueryはJSONを含む様々なデータ型をサポートしているため、リクエストボディを変換することなく保存できます。
Links:

https://cloud.google.com/bigquery/docs/streaming-data-into-bigquery
</div></details>

## Q. 1-12
あなたは複数のルームをホストし、各ルームのメッセージ履歴を保持するチャットルームアプリケーションを設計しています。あなたはデータベースとしてFirestoreを選択しました。

Firestoreのデータをどのように表現する必要がありますか？



A. ルーム用のコレクションを作成します。各ルームについて、メッセージの内容をリストするドキュメントを作成します。
1. 部屋のコレクションを作成する。各部屋に対して、メッセージの内容をリストしたドキュメントを作成する。
2. 部屋ごとにコレクションを作成する。部屋ごとに、各メッセージのドキュメントを含むコレクションを作成する。
3. 部屋用のコレクションを作成する。各部屋に対して、ドキュメント用のコレクションを含むドキュメントを作成し、各ドキュメントにはメッセージが含まれます。
4. 部屋用のコレクションを作成し、各部屋用のドキュメントを作成する。メッセージ用に別のコレクションを作成し、メッセージごとに1つのドキュメントを作成します。各部屋のドキュメントには、メッセージへの参照のリストが含まれます。
<details><div>
    答え：3
A. この方法は、各部屋のメッセージ数が多い場合、全てのメッセージを一つのドキュメントに収めようとするため、うまく拡張できません。
B. 部屋用のコレクションは、それぞれメッセージのサブコレクションを持ち、多対多のリレーションシップを作成します。
D. このアプローチでは、部屋とメッセージを異なるコレクションに分離しますが、それらの間の参照を維持する必要があります。
正解
C. このシナリオでメッセージを格納する最善の方法は、サブコレクションを使用することです。サブコレクションは、特定のドキュメントに関連付けられたコレクションです。
Links:

https://firebase.google.com/docs/firestore/data-model#hierarchical-data

https://firebase.google.com/docs/firestore/data-model#subcollections
</div></details>

## Q. 1-13
本番環境にデプロイされたアプリケーションがあります。新しいバージョンがデプロイされたとき、いくつかの問題は、アプリケーションが本番環境のユーザからトラフィックを受けるまで発生しません。影響と影響を受けるユーザ数の両方を減らしたい。

どのデプロイメント戦略を使用すべきですか?
1. ブルー／グリーン・デプロイメント
2. カナリア展開
3. ローリングデプロイメント
4. デプロイメントの再作成
<details><div>
    答え：2
A. このアプローチでは、2つの別々の環境（実行中のバージョンはブルー、新しいバージョンはグリーン）を切り替えることができます。全員にロールアウトする前に、問題を検出するために一部のユーザーでテストすることは特にできません。
C. ローリング デプロイメントでは、インスタンスを次々に段階的に更新します。段階的なロールアウトが可能ですが、Canaryデプロイメントのように特定のユーザ サブセットを対象としていないため、ユーザ固有の問題を検出するのには適していません。
D. この方法では、古いバージョンを削除し、新しいバージョンをデプロイする。すべてのインスタンスが同時に置き換えられるため、影響を軽減し、ユーザーのサブセットでテストするという要件には適合しません。
正解
B. カナリア配置では、新バージョンを少人数のユーザに徐々にリリースしてから、すべてのユーザに使用できるようにします。これにより、本番環境の実際のユーザーで新バージョンの動作をテストできますが、対象者が限定されるため、潜在的な問題の影響と影響を受けるユーザー数の両方を減らすことができます。これは、説明した状況に適しています。
Links:

https://cloud.google.com/architecture/application-deployment-and-testing-strategies#canary_test_pattern
</div></details>

## Q. 1-14
Stackdriver Logging Agentを使用して、アプリケーションのログファイルをCompute Engine仮想マシンインスタンスからStackdriverに送信したいとします。
Stackdriver Logging Agentをインストールした後、最初に何をすべきですか？
1. プロジェクトのエラー報告APIを有効にする。
2. インスタンスにすべてのクラウドAPIへのフルアクセスを許可する。
3. アプリケーションログファイルをカスタムソースとして設定します。
4. アプリケーションのログエントリに一致するフィルタを使用して、Stackdriver ログエクスポートシンクを作成します。
<details><div>
    答え：3
A. これはコンテキストによっては便利ですが、Stackdriver にログを送信するためのロギングエージェントをインストールした直後のステップではありません。
B. すべてのクラウドAPIへのフルアクセスを許可するのは過剰であり、Stackdriverにログを送信するために必要ではない。ロギングに関連する特定のパーミッションが必要ですが、通常これらはインスタンスに関連するサービスアカウントに割り当てられたロールとパーミッションによって管理されます。
D. シンクを使用してログをエクスポートすることは、ログを異なる宛先（BigQuery、Pub/Sub など）にルーティングする方法であり、特定のアプリケーションログファイルからログを収集するためにエージェントを設定するタスクとは関係ありません。
正解
C. これは正しい次のステップです。ログファイルをカスタムソースとして構成することで、エージェントはログを探す場所と処理方法を知ることができます。したがって、オプション C は、Stackdriver Logging Agent をインストールした後に実行する最も適切なステップです。
Links:

https://cloud.google.com/logging/docs/agent/configuration

https://cloud.google.com/logging/docs/agent/configuration#streaming_logs_from_additional_inputs
</div></details>

## Q. 1-15
あなたの会社には「Master」という名前のBigQueryデータセットがあり、そこには従業員の部署別に整理された、従業員の出張と経費に関する情報が含まれています。従業員は各部門の情報しか閲覧できないようにする必要があるため、セキュリティフレームワークを適用して、最小限のステップ数でこの要件を実施したいとします。

どうすればよいでしょうか。
1. 部門ごとに個別のデータセットを作成する。適切なWHERE句を指定してビューを作成し、特定の部門の特定のデータセットからレコードを選択する。このビューに、マスターデータセットからレコードにアクセスする権限を与える。従業員にこの部門別データセットへのアクセス権限を与える。
2. 部門ごとに個別のデータセットを作成する。部門ごとにデータパイプラインを作成し、マスターデータセットから部門固有のデータセットに適切な情報をコピーする。従業員にこの部門別データセットへのアクセス権を与える。
3. マスター」データセットという名前のデータセットを作成する。マスターデータセットの中に、部署ごとに個別のビューを作成する。従業員に、所属する部署に特化したビューへのアクセス権を与える。
4. マスター・データセットという名前のデータセットを作成する。マスター・データセットの中に、部門ごとに個別のテーブルを作成する。従業員には、所属する部門のテーブルにアクセスできるようにする。
<details><div>
    答え：3
A.この方法では、データセットとビューを別々に作成し、適切なアクセス制御を行います。柔軟なアプローチですが、特に基礎となるデータ構造が変更された場合、管理が複雑になる可能性があります。
B.オプションAと同様に、部署ごとに個別のデータセットを作成する。データパイプラインの使用は複雑さを増し、データが重複することで整合性に問題が生じる可能性がある。
D.この方法では、「マスター」データセット内に個別のテーブルを作成する。オプションCと同様に、すべてを単一のデータセット内に保持しますが、テーブルごとのアクセス制御をより慎重に管理する必要があります。
正解
C.この方法では、既存の「マスター」データセットの中にビューを作成し、部門ごとのニーズに合わせてアクセスできるようにします。実装と管理が最も簡単で、セキュリティと使いやすさのバランスがよく、ステップ数も最小限で済む。
Links:

https://cloud.google.com/bigquery/docs/share-access-views
</div></details>

## Q. 1-16
あなたのチームは、クラウドストレージイベントをトリガーとするCloud Function を開発しています。Googleが推奨するベストプラクティスに従いつつ、Cloud Function のテストと開発を加速したい。

どうすればよいでしょうか？
1. クラウド監査ログが元のクラウド関数のcloudfunctions.functions.sourceCodeSet操作を検出したときにトリガされる新しいクラウド関数を作成します。新しい関数にモック要求を送信して、機能を評価します。
2. クラウド関数のコピーを作成し、HTTPトリガーになるようにコードを書き換えます。新しいバージョンを編集し、HTTPエンドポイントをトリガーしてテストする。新しい関数にモックリクエストを送り、機能を評価する。
3. Functions Frameworksライブラリをインストールし、localhost上でCloud Functionを構成します。関数のコピーを作成し、新しいバージョンに編集します。curlを使用して新しいバージョンをテストします。
4. Google CloudコンソールでCloud Functionのコピーを作成する。クラウド コンソールのインライン エディタを使用して、新しい関数にソース コードを変更します。新しい関数を呼び出すように Web アプリケーションを変更し、新しいバージョンを実運用環境でテストします。
<details><div>
    答え：3
不正解
A. 特定の操作でトリガーされる新しいクラウド関数を作成することは、元の関数をテストする間接的な方法であり、この目的には効率的でないかもしれません。
B. クラウド関数のコピーを作成し、HTTPトリガーに書き換えることで、テストにHTTPリクエストを使用できます。ただし、この方法では元のクラウド ストレージ イベント動作を複製するための追加作業が必要になる場合があります。
D. Google Cloud コンソールでクラウド関数のコピーを作成し、新しいバージョンを本番環境でテストすることは、テストと開発のベストプラクティスに合致しません。本番環境にエラーをもたらすリスクがあります。
正解
C. Functions Frameworks ライブラリを使用すると、Cloud Function をローカルで実行してテストすることができ、実際の Cloud Function ランタイムに似た環境を提供できます。関数に変更を加え、curl などのツールを使用してテストできるので、開発サイクルを短縮できます。
Links:

https://cloud.google.com/functions/docs/running/calling#cloudevent_functions

https://cloud.google.com/functions/docs/running/overview#choosing_an_abstraction_layer
</div></details>

## Q. 1-17
あなたは、Cloud RunとFirestoreのDatastoreモードで動作する新しい小売システムの開発リーダーです。Web UI の要件は、ユーザーがシステムにアクセスしたときに利用可能な商品のリストを表示し、ユーザーがすべての商品を閲覧できることです。この要件は、最小実行可能製品（MVP）の段階で、Firestoreに格納されているすべての利用可能な製品のリストを返すことによって実装されました。



本稼働から数カ月後、Cloud Run インスタンスが HTTP 500 で終了していることに気づきました： Container instances are exceeding memory limits エラーで終了することに気づきました。このエラーは、データストア・エンティティの読み取り数の急増と一致します。Cloud Runのクラッシュを防ぎ、Datastoreエンティティの読み取り回数を減らす必要があります。システムパフォーマンスを最適化するソリューションを使用したい。

どうすればよいでしょうか？
1. 整数オフセットを使用して商品リストを返すクエリを修正してください。
2. 制限を使用して、商品リストを返すクエリを変更します。
3. Cloud Runの設定を変更してメモリ制限を増やす。
4. カーソルを使用して商品リストを返すクエリを修正します。
<details><div>
    答え：4
不正解
A. データ・ストア・モードのデータベースは整数オフセットをサポートしていますが、オフセットの使用は避けて、代わりにカーソルを使用してください。オフセットを使用すると、アプリケーションにスキップされたエンティティを返さずに済みますが、これらのエンティティは内部的に取得されます。これらのスキップされたエンティティはクエリの待ち時間に影響し、アプリケーションはそれらを取得するために必要な読み取り操作に対して課金されます。オフセットの代わりにカーソルを使用することで、これらのコストを全て回避することができます。
B. クエリで制限を使用すると、1回の応答で返される結果の数が制限されますが、複数の要求間の状態は管理されません。そのため、多くの商品があり、ユーザーがそれらすべてを閲覧できるようにしたい場合、単に結果を制限するだけでは、リスト全体を効率的にページ分割する方法は提供されません。それぞれの新しいリクエストは最初から始まり、最後のリクエストの続きから続ける効率的な方法はありません。
C. クラウド・ラン・インスタンスのメモリ制限を増やすことで、この問題を一時的に軽減することはできますが、問題の根本的な原因（ビジー時のデータストア・エンティティの読み取り数の多さ）には対処できません。時間が経つにつれて、より多くの製品がシステムに追加されるにつれて、この問題はより深刻になり、Cloud Runがクラッシュするのを防ぐためにメモリ制限を継続的に増やす必要があります。
正解
D. カーソルを使用して結果をページ分割し、一度に限られた数の製品を取得することは、より持続可能なソリューションを提供します。データストアから読み込む必要があるデータ量を減らし、Cloud Runインスタンスのメモリ使用量を減らします。このようにして、システムのパフォーマンスを維持し、時間の経過とともに製品が追加されてもクラッシュするのを防ぐことができます。
Links:

https://cloud.google.com/datastore/docs/concepts/queries#cursors
</div></details>

## Q. 1-18
貴社は新しいAPIをCompute Engineインスタンスにデプロイしました。テスト中、APIが期待通りに動作しません。アプリケーションを再デプロイすることなく、アプリケーションコード内の問題を診断するために、12時間にわたってアプリケーションを監視したい。

どのツールを使用すべきでしょうか？
1. クラウドトレース
2. Cloud Monitoring
3. クラウドDebuggerのログポイント
4. クラウド・デバッガ・スナップショット
<details><div>
    答え：3
A. レイテンシの分析に重点を置いており、再デプロイせずにアプリケーションコード内の問題を診断することに特化して設計されていないため、質問の要件を満たしていない。
B. システムのパフォーマンスを監視するのに便利ですが、シナリオが求めている、ログステートメントを挿入したり、再デプロイせずにアプリケーションコードの動作を分析したりすることはできません。
D. スナップショットは、12時間にわたって監視するよりも、特定の時点でのアプリケーションの状態を分析するのに適しています。このシナリオにはあまり適していません。
正解
C. アプリケーションをデプロイまたは起動した後、Google Cloud コンソールで Cloud Debugger を開くことができます。Cloud Debugger Logpoints を使用すると、サービスの通常の機能を再起動または妨害することなく、実行中のサービスにロギングを注入できます。これは、ログステートメントを追加して再デプロイすることなく、運用上の問題をデバッグするのに便利です。
Link: https://cloud.google.com/debugger/docs/using/logpoints
</div></details>

## Q. 1-19
御社の開発チームは、プロジェクトでCloud Buildを使用してDockerイメージをビルドし、Container Registryにプッシュしたいと考えています。運用チームは、すべてのDockerイメージを、運用チームが管理する一元化された安全に管理されたDockerレジストリに公開する必要があります。

どうすればいいでしょうか？
1. Container Registryを使用して、各開発チームのプロジェクトにレジストリを作成します。プロジェクトのレジストリにDockerイメージをプッシュするようにCloud Buildビルドを構成します。運用チームに各開発チームのレジストリへのアクセス権を付与します。
2. Container Registryを設定した運用チーム用の別のプロジェクトを作成します。各開発チームのプロジェクトのCloud Buildサービスアカウントに適切な権限を割り当て、運用チームのレジストリへのアクセスを許可する。
3. Container Registryを設定した運用チーム用の別のプロジェクトを作成します。各開発チームにサービスアカウントを作成し、運用チームのレジストリへのアクセスを許可する適切な権限を割り当てます。サービスアカウントのキーファイルをソースコードリポジトリに格納し、運用チームのレジストリに対する認証に使用します。
4. Compute Engineの仮想マシンインスタンス上にオープンソースのDockerレジストリをデプロイした、運用チーム用の別のプロジェクトを作成します。開発チームごとにユーザー名とパスワードを作成します。ユーザー名とパスワードをソースコードリポジトリに保存し、運用チームのDockerレジストリに対する認証に使用する。

<details><div>
    答え：2
不正解
オプションA：運用チームに各開発チームのレジストリへのアクセスを許可する必要があり、安全でない可能性があるため、理想的ではありません。
オプションC：サービスアカウントのキーファイルをソースコードリポジトリに保存する必要があり、安全でない可能性があるため、最適な選択ではない。
オプションD：オープンソースのDockerレジストリを使用し、各開発チームのユーザー名とパスワードを作成する必要があるため、これは最良の選択ではありません。
正解
Bは、運用チームが集中管理され安全に管理されたDockerレジストリを制御できる一方で、開発チームがプロジェクトでCloud Buildを使用できるようになるため、最良の選択です。このオプションでは、運用チームはContainer Registryが設定された別のプロジェクトを作成し、各開発チームのプロジェクトでCloud Buildサービスアカウントに適切な権限を付与して、運用チームのレジストリへのアクセスを許可することができます。このアプローチにより、開発チームは運用チームの要件を順守しながら、Dockerイメージをビルドして集中レジストリにプッシュすることができます。
Links:

https://cloud.google.com/container-registry/

https://stackoverflow.com/questions/48602546/google-cloud-functions-how-to-securely-store-service-account-private-key-when
</div></details>

## Q. 1-20
アプリケーションは複数のGoogle Kubernetes Engineクラスタで実行されている。各クラスタの Deployment によって管理されています。Deploymentは各クラスタにPodの複数のレプリカを作成しています。あなたは、すべてのクラスタのあなたのDeployment内のすべてのレプリカの標準出力に送信されたログを表示したいと思います。

どのコマンドを使用する必要がありますか?
1. kubectl logs [PARAM].
2. gcloud logging read [PARAM].
3. kubectl exec -it [PARAM] journalctl
4. gcloud compute ssh [PARAM] --command='sudo journalctl' 
<details><div>
    答え：2
不正解
A. すべてのクラスタのすべてのレプリカのログを表示する機能はありません。
C. は特定のPod内でコマンドを実行するために使用され、Deployment内のすべてのレプリカからログを取得するために使用されるわけではありません。
D. は、Compute EngineインスタンスにSSH接続してコマンドを実行するために使用され、GKEデプロイメントからログを取得するために使用されません。
正解
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine: "gcloud コマンドラインツール - gcloud logging read コマンドを使用して、適切なクラスタ、ノード、ポッド、およびコンテナのログを選択します。"
Links:

https://cloud.google.com/logging/docs/reference/tools/gcloud-logging#examples_2

https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine

https://stackoverflow.com/questions/62007471/how-to-view-container-logs-via-stackdriver-on-gke
</div></details>

## Q. 1-21
あなたは最近新しいアプリケーションを開発し、Dockerfileを使わずにCloud Run上にデプロイしたいと考えています。

すべてのコンテナイメージは一元管理されたコンテナリポジトリにプッシュされなければならないという組織の要件を考慮すると、Google Cloudサービスを使ってどのようにコンテナを構築すべきでしょうか？(2つの選択肢を選んでください)
1. ソースコードをArtifact Registryにプッシュします。
2. イメージをプッシュするためにCloud Buildジョブを送信します。
3. pack CLIでpack buildコマンドを使用します。
4. gcloud run deploy CLIコマンドに-sourceフラグを含める。
5. gcloud run deploy CLIコマンドに--platform=kubernetesフラグを含める。
<details><div>
    答え：3,4
不正解
選択肢A：Artifact Registryはソースコードではなくコンテナイメージ用に設計されているため、不正解です。
選択肢B：ビルドされたイメージのみをCloud Runにデプロイする必要があるため、不正解です。一元管理されたコンテナリポジトリ」はGoogleの外部にある可能性があるため、ビルドツールは必ずしもCloud Buildとは限らない。
オプション E：Kubernetes（K8S）は設問に関係ないので、この場合は関係ありません。
正解
オプション C：Google Cloud は buildpacks をサポートしています。これは、Dockerfile を必要とせず、ソース コードから安全で本番環境に適したコンテナ イメージを迅速かつ容易に作成するオープン ソース テクノロジーです。https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks

オプションD：ソースコードからのデプロイはCloud Runで可能です。単一のgcloud CLIコマンド、gcloud run deploy、-sourceフラグを使用して、ソースコードから新しいサービスや新しいリビジョンを直接デプロイできます。

Links:

https://cloud.google.com/run/docs/deploying-source-code

https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks
</div></details>

## Q. 1-22
あなたは最近Cloud Runで新しいサービスを開発しました。新しいサービスはカスタムサービスを使用して認証し、トランザクション情報をCloud Spannerデータベースに書き込みます。発生する可能性のあるボトルネックを特定しながら、アプリケーションが1秒あたり最大5,000の読み取りトランザクションと1,000の書き込みトランザクションをサポートできることを検証する必要があります。また、テストインフラはオートスケールできなければなりません。

どうすればよいでしょうか？
1. リクエストを生成するテストハーネスを構築し、Cloud Runにデプロイします。Cloud Loggingを使用してVPCフローログを分析します。
2. 負荷テストを動的に生成するために、LocustまたはJMeterイメージを実行しているGoogle Kubernetes Engineクラスタを作成します。Cloud Traceを使用して結果を分析します。
3. テスト負荷を生成するためにクラウドタスクを作成します。Cloud Schedulerを使用して、毎分60,000のCloud Taskトランザクションを10分間実行します。Cloud Monitoringを使用して結果を分析します。
4. マーケットプレイスからLAMPスタックイメージを使用するCompute Engineインスタンスを作成し、Apache Benchを使用してサービスに対する負荷テストを生成する。Cloud Traceを使って結果を分析する。
<details><div>
    答え：2
不正解
A. テストハーネスをCloud Runにデプロイすることは、負荷テストにとって理想的なアプローチではありません。また、VPCフローログは、アプリケーションのパフォーマンス分析よりもむしろネットワークのモニタリングに適しています。
C. Cloud TaskとCloud Schedulerを使用すると、特に異なるシナリオをテストするために負荷を動的に変化させる必要がある場合、1秒間に必要な読み取りと書き込みのトランザクション数をシミュレートするには柔軟性に欠ける可能性があります。
D. Apache Benchを使用する単一のCompute Engineインスタンスでは、必要な1秒あたりのトランザクション数に達するのに十分な負荷を生成できない可能性があります。さらに、マーケットプレイスからのLAMPスタックイメージは、Cloud Runサービスのテストの要件とは無関係です。
正解
B. アプリケーションが1秒あたり最大5,000の読み取りトランザクションと1,000の書き込みトランザクションをサポートできることを確認し、発生する可能性のあるボトルネックを特定するには、LocustやJMeterなどの負荷テストツールを使用してCloud Runサービスの負荷テストを生成します。これらのツールを使用すると、多数の同時リクエストをシミュレートすることができ、サービスが処理できる最大リクエスト数を決定するのに役立ちます。

負荷テストツールをGoogle Kubernetes Engine (GKE)クラスタ上で実行することで、オートスケール機能を提供することができます。こうすることで、大量のリクエストを管理し、Cloud Traceを使って結果を分析することができる。この分析により、パフォーマンスに関する洞察が得られ、ボトルネックの特定に役立つ。
Links:

https://cloud.google.com/architecture/distributed-load-testing-using-gke
</div></details>

## Q. 1-23
あなたは、ユーザーに静的コンテンツを提供する、高可用性でグローバルにアクセス可能なアプリケーションを構築しています。ストレージとサービング・コンポーネントを構成する必要があります。管理オーバーヘッドとレイテンシを最小限に抑えつつ、ユーザーの信頼性を最大化したい。

どうすればよいでしょうか。
1. 
2. 
3. 
4. ① Standardストレージクラスで、マルチリージョンのCloud Storageバケットを作成します。静的コンテンツをバケットに入れます。② 外部IPアドレスを予約し、外部HTTP(S)ロードバランサーを作成する。③ Cloud CDNを有効にし、バックエンドバケットにトラフィックを送る。
<details><div>
    答え：4
正解
オプションAおよびオプションB：インスタンスグループ（管理対象か非管理対象かにかかわらず）に依存すると、VM間で静的コンテンツを複製して維持するために、より多くの管理オーバーヘッドが必要になります。また、提供された問題ではスケーリングは必要ありません。
選択肢C：リージョナルバケットを使用すると、データがリージョン内の2つの特定の場所に制限されるため、世界中のユーザーに対するグローバルなアクセシビリティと低レイテンシが保証されません。
正解
オプション D.
1. Standardストレージクラスで、マルチリージョンのCloud Storageバケットを作成します。静的コンテンツをバケットに入れます。
2. 外部IPアドレスを予約し、外部HTTP(S)ロードバランサーを作成する。
3. Cloud CDNを有効にし、バックエンドバケットにトラフィックを送る。
静的コンテンツをグローバルに、レイテンシと管理オーバーヘッドを最小限に抑えて配信するには、マルチリージョンのCloud StorageバケットのようにGoogle Cloudのグローバル分散システムを活用し、Cloud CDNでキャッシュするのが最適なソリューションです。オプションDが最良の選択である理由は以下の通りです：
* マルチリージョンのバケットは、コンテンツが複数のリージョンに冗長的に保存されることを保証し、高可用性と低レイテンシーでのグローバルアクセスを提供します。
* 外部のHTTP(S)ロードバランサーは、ユーザーのリクエストを最も近いグローバルロケーションに自動的にルーティングする。
* Cloud CDNは、Googleの高度に分散されたエッジキャッシングを活用し、エンドユーザーの待ち時間を最小限に抑えます。
Links:

https://cloud.google.com/storage/docs/hosting-static-website

https://cloud.google.com/load-balancing/docs/https
</div></details>

## Q. 1-24
オンプレミスのLinux仮想マシン（VM）で稼働しているスタンドアロンJavaアプリケーションを、費用対効果の高い方法でGoogle Cloudに移行する必要があります。リフト・アンド・シフトのアプローチはとらず、コンテナに変換してアプリケーションを最新化することにしました。

このタスクをどのように達成すべきでしょうか。
1. Migrate for Anthosを使用して、VMをコンテナとしてGoogle Kubernetes Engine（GKE）クラスタに移行します。
2. VM を raw ディスクとしてエクスポートし、イメージとしてインポートします。インポートしたイメージからCompute Engineインスタンスを作成します。
3. Migrate for Compute Engineを使用してVMをCompute Engineインスタンスにマイグレートし、Cloud Buildを使用してコンテナに変換する。
4. Jibを使用してソースコードからDockerイメージを構築し、Artifact Registryにアップロードします。アプリケーションをGKEクラスタにデプロイし、アプリケーションをテストします。
<details><div>
    答え：4
不正解
Migrate for Anthosを使用するオプションAは、VMをGKEに移行しますが、これはリフト・アンド・シフトのアプローチであり、必ずしもアプリケーションの近代化を伴うものではありません。

オプションBは、VMをRAWディスクとしてエクスポートし、イメージとしてインポートするもので、コンテナ化を伴わず、VMをクラウドに複製することに関連します。

Migrate for Compute Engineを使用してVMをCompute Engineに移行し、Cloud Buildを使用するオプションCは、コンテナ化されたソリューションにつながるかもしれませんが、指定されたタスクにとって必要以上に複雑です。
正解
Jibを使用してソースコードからDockerイメージを構築するオプションDは、ソースコードから直接コンテナ化されたイメージを構築することで、より現代的なアプローチを可能にします。また、GKEでのデプロイも含まれており、コンテナベースのアーキテクチャに移行するという目標に合致しています。Jibは、Javaコンテナ・イメージを構築するために特別に設計されたMaven/Gradleプラグインであり、このJavaアプリケーションの移行に適した選択肢となっている。

指定されたシナリオでは、VMを単に持ち上げて移行するのではなく、コンテナに変換することでアプリケーションを最新化することが目標であるため、オプションDが最も適切なアプローチとなります。
Links:

https://cloud.google.com/blog/products/application-development/introducing-jib-build-java-docker-images-better
</div></details>

## Q. 1-25
Compute Engineにアプリケーションをデプロイしています。Compute Engineインスタンスの1つが起動に失敗しました。

あなたは何をすべきですか？(2つのオプションを選択してください)
1. ファイルシステムが壊れているかどうかを判断します。
2. 別のSSHユーザーとしてCompute Engineにアクセスします。
3. インスタンスのファイアウォールルールまたはルートをトラブルシューティングします。
4. インスタンスのブートディスクが完全に一杯になっていないか確認してください。
5. インスタンスへの、またはインスタンスからのネットワークトラフィックがドロップされていないかチェックする。
<details><div>
    答え：1,4
不正解：
オプションAは文脈によっては関連するかもしれませんが、インスタンスの起動に失敗する原因である可能性は低いです。
オプションBは、SSHユーザーが起動プロセスに影響を与えることはないため、インスタンスの起動失敗を解決する可能性は低い。
オプションEは、起動そのものというよりも、起動後のネットワーク通信に関連しています。
正解です：

Links:

https://cloud.google.com/compute/docs/troubleshooting/vm-startup
</div></details>

## Q. 1-26
500 MB のファイル サイズ制限がある内部ファイル アップロード API を App Engine に移行する必要があります。

どうすればよいでしょうか。
1. FTPを使用してファイルをアップロードします。
2. CPanelを使用してファイルをアップロードします。
3. 署名付きURLを使用してファイルをアップロードする。
4. APIをマルチパートのファイルアップロードAPIに変更する。
<details><div>
    答え：3
不正解
オプションA（FTP）は、App Engineのようなクラウド環境における一般的なプラクティスに合致しません。
オプションB（CPanel）は、プログラムでファイルアップロードを処理することとは関係ありません。
オプションD（マルチパートファイルアップロードAPI）は可能な解決策かもしれませんが、特に指定されたファイルサイズ制限やApp Engine環境に対応していません。
正解
署名付きURLを使用してファイルをアップロードするオプションCは、これらの選択肢の中で最良のアプローチです。署名付きURLは、特に大きなファイルを扱う場合に、ファイルアップロードを安全かつ効率的に処理する方法を提供します。これにより、特定のクラウドリソース（この場合はファイルをアップロードする機能）への一時的なアクセスをユーザーに与えることができます。Google Cloud Storageは署名付きURLをサポートしており、App Engineと組み合わせてファイルアップロードを処理できます。署名付きURLを作成することで、App Engineサーバーでファイルを処理することなく、クライアントがCloud Storageのバケットに直接ファイルをアップロードすることを許可できます。

したがって、正解は C
Links:

https://cloud.google.com/storage/docs/access-control/signed-urls

https://cloud.google.com/appengine/docs/standard/php/googlestorage/user_upload
</div></details>

## Q. 1-27
アプリケーションは Stackdriver にログを記録している。すべての /api/alpha/* エンドポイント上のすべてのリクエストのカウントを取得したい。

どうすればいいでしょうか？
1. path:/api/alpha/のStackdriverカウンタ・メトリックを追加します。
2. endpoint:/api/alpha/*のStackdriverカウンタ・メトリックを追加します。
3. ログをクラウドストレージにエクスポートし、/api/alphaに一致する行を数えます。
4. ログをCloud Pub/Subにエクスポートし、/api/alphaに一致する行をカウントする。
<details><div>
    答え：2
不正解
A. このオプションは、ワイルドカード文字を使用せずに特定のパスを参照するため、意図したとおりに動作しない可能性があります。
C. ログをクラウドストレージにエクスポートして行数をカウントすることは、これを達成するための非効率的な方法であり、オペレーションスイートのリアルタイムの監視およびアラート機能を活用することはできない。
D. ログをCloud Pub/Subにエクスポートして行数をカウントすることは、同様に非効率的であり、オペレーション・スイート内で利用可能なツールをフルに活用できません。
正解
B. オプションBは、指定されたエンドポイントに対する要求のカウントを取得するためにGoogle Cloud Loggingでカウンターメトリックを作成するための正しい答えです。

Links:

https://cloud.google.com/logging/docs/logs-based-metrics/counter-metrics#console
</div></details>

## Q. 1-28
Google Kubernetes Engine（GKE）にデプロイメントを設定する必要がある。コンテナがデータベースに接続できることを確認するチェックを含めたい。Podが接続に失敗した場合、コンテナ上でスクリプトを実行し、グレースフル・シャットダウンを完了させたい。

デプロイはどのように構成すればよいですか？
1. 1つはコンテナがデータベースに接続できるかどうかをチェックするジョブ、もう1つはPodが失敗している場合にシャットダウンスクリプトを実行するジョブです。
2. コンテナがデータベースに接続できない場合に失敗する、コンテナ用の livenessProbe を含む配置を作成します。コンテナが失敗している場合にシャットダウンスクリプトを実行する Prestop ライフサイクルハンドラを構成します。
3. サービスの可用性をチェックするPostStartライフサイクル・ハンドラを使用してデプロイメントを作成します。コンテナに障害が発生した場合にシャットダウンスクリプトを実行するPreStopライフサイクルハンドラを構成する。
4. サービスの可用性をチェックする initContainer を使用して配置を作成します。Pod に障害が発生した場合にシャットダウン スクリプトを実行する Prestop ライフサイクル ハンドラを構成します。
<details><div>
    答え：2
不正解
A. ジョブは通常バッチ処理に使用され、継続的な監視や実行中のコンテナのグレースフル・シャットダウンの処理には適していません。
C. PostStart ハンドラーは、コンテナの起動直後に 1 回だけ実行され、データベースへの接続を継続的に監視することはない。また、条件（たとえば、ライブネス・プローブの失敗）なしで PreStop ハンドラを構成することは、コンテナが失敗している場合にのみシャットダウン・スクリプトを実行する必要性に合致しない。
D. そのため、initContainer を使用すると、コンテナの実行後にデータベース接続を継続的に監視できなくなります。また、Podが初期化された後に失敗している場合にスクリプトを実行するという要件にも合致しません。
正解
B. コンテナがデータベースに接続できない場合に失敗するコンテナ用の livenessProbe を使用して、配置を作成します。liveness probeを利用することで、Kubernetesはデータベースへの接続を継続的にチェックできる。コンテナがデータベースへの接続に失敗すると、livenessプローブが失敗し、Kubernetesがコンテナを再起動するトリガーとなる。

コンテナが失敗している場合にシャットダウンスクリプトを実行するPreStopライフサイクルハンドラを設定する。Kubernetesのドキュメントによると、PreStopフックは、APIリクエスト、livenessプローブの失敗、その他の管理イベントなど、さまざまな理由でコンテナが終了する直前に呼び出される。PreStopフックは、Google Cloudのベストプラクティスで言及されているように、アプリケーションを修正せずにグレースフル・シャットダウンをトリガーするための良い選択肢です。 このフックは、コンテナを停止するTERMシグナルを送信する前に完了する必要があります。Podの終了猶予期間のカウントダウンは、PreStopフックが実行される前に始まるため、ハンドラの結果にかかわらず、コンテナは最終的にPodの終了猶予期間内に終了します。

これら2つの機能を組み合わせることで、コンテナがデータベースへの接続性を継続的に監視し、コンテナに障害が発生した場合にグレースフル・シャットダウンスクリプトが実行されるようにすることができます。
Links:

https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations



https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details
</div></details>

## Q. 1-29
アプリケーションを、Stackdriver Monitoring AgentがインストールされたCompute Engine仮想マシンインスタンスにデプロイしています。アプリケーションはインスタンス上のunixプロセスです。unixプロセスが少なくとも5分間実行されなかった場合、アラートが必要です。メトリクスやログを生成するようにアプリケーションを変更することはできません。

どのアラート条件を構成しますか?
1. アップタイム・チェック
2. プロセスの健全性（Process health）
3. メトリックの不在（Metric absence）
4. メトリックしきい値（Metric threshold）
<details><div>
    答え：2
不正解
A. これは、マシン上の特定のプロセスではなく、Webサーバなどのネットワーク・エンドポイントの可用性を監視するために使用されます。
C. これは、プロセスの不在ではなく、特定のメトリックのデータの不在を指す。これはより複雑なシナリオで使用されるかもしれないが、プロセスの健全性を監視するようには直接設計されていない。
D. これは、特定のメトリック値が特定のしきい値を超えたことに基づいてアラートを設定することができますが、特定のプロセスが実行されているかどうかをチェックするようには設計されていません。
正解
B. 説明するシナリオでは、特定のUnixプロセスを監視し、そのプロセスが少なくとも5分間実行されていない場合にアラートを出したいとします。プロセスの有無を監視しているので、適切なアラート条件はプロセスの健全性です。

Stackdriverモニタリング・エージェントは、システムとプロセスのメトリクスを監視することができ、このタイプのアラートは、特にUnixプロセスの健全性を追跡します。

Links:

Behavior of metric-based alerting policies | Cloud Monitoring
</div></details>

## Q. 1-30
貴社は新しい API を App Engine Standard 環境にデプロイしました。テスト中、API が期待どおりに動作しません。アプリケーションを再デプロイすることなく、アプリケーション コード内の問題を診断するために、アプリケーションを長期にわたって監視したいとします。

どのツールを使用すべきでしょうか?
1. スタックドライバートレース
2. スタックドライバ・モニタリング
3. Stackdriver デバッグ・スナップショット
4. スタックドライバ・デバッグ・ログポイント
<details><div>
    答え：4
不正解
A. Stackdriver Trace は、リクエストがアプリケーションをどのように伝搬するかを分析し、これらのリクエストの待ち時間を測定するために使用されます。アプリケーションコード内の問題の診断に直接焦点を当てるものではありません。
B. システムの健全性、パフォーマンス、カスタムメトリクスの監視には最適ですが、Stackdriver Monitoringでは、再デプロイせずにアプリケーションコード内の特定の問題をピンポイントで診断することはできません。
C. スナップショットは、コード内の特定の場所でローカル変数とコールスタックをキャプチャします。このツールは、実行中のアプリケーションを停止させたり速度を落としたりすることなく、プログラムの状態を調べるために使用される。役に立ちますが、ログポイントほど直接シナリオに合わせたものではありません。
正解
D. Logpoints を使えば、実行中のアプリケーションを停止したり再デプロイしたりすることなく、リアルタイムでログ文を追加することができるので、これは正しい選択です。これを使用して、問題が発生していると思われる特定のポイントのログを検査することで、アプリケーションコード内の問題を診断することができます。オプション D は、アプリケーションを再デプロイすることなく、アプリケーションコード内の問題を診断するた めに、アプリケーションを長期にわたって監視するための最良の選択です。

Links:

https://cloud.google.com/debugger/docs/using/logpoints
</div></details>

## Q. 1-31
あなたは、XMLHttpRequestを使用してサードパーティAPIとコンテンツ通信を行う、ユーザーインターフェースを持つ単一ページのWebアプリケーションを書いています。APIの結果によってUIに表示されるデータは、同じWebページに表示される他のデータよりも重要度が低いため、リクエストによってはAPIのデータがUIに表示されなくても構いません。しかし、APIへの呼び出しによって、ユーザーインターフェースの他の部分のレンダリングが遅れてはならない。APIレスポンスがエラーまたはタイムアウトの場合、アプリケーションのパフォーマンスを向上させたい。
どうすればよいでしょうか？
1. APIへのリクエストの非同期オプションをfalseに設定し、タイムアウトまたはエラーが発生したときにAPI結果を表示するウィジェットを省略します。
2. APIへのリクエストの非同期オプションをtrueに設定し、タイムアウトまたはエラーが発生したときにAPI結果を表示するウィジェットを省略する。
3. APIコールからのタイムアウトまたはエラー例外をキャッチし、API応答が成功するまで指数関数バックオフで試行を続ける。
4. APIコールのタイムアウトまたはエラー例外をキャッチし、UIウィジェットにエラー・レスポンスを表示する。
<details><div>
    答え：2
不正解
A. この場合、リクエストは同期的に処理され、リクエストが完了するまでUIの他の部分のレンダリングがブロックされます。
C. これは不必要な遅延を引き起こし、APIが失敗し続ければ無限にトライし続ける可能性がある。データはそれほど重要ではないので、繰り返しフェッチしようとするのは要件に合致しない。
D. UIにエラー・レスポンスを表示することは、特にデータの重要度が低い場合、望ましくないかもしれない。また、これは呼び出しを非同期にするという要件には対応していません。
正解
B. このシナリオでは、サードパーティAPIへの呼び出しがUIの他の部分のレンダリングを遅らせないようにすることに重点を置いています。

したがって、ここでの最良の選択は、UI の他の部分のレンダリングをブロックしないようにリクエストを非同期（オプション B）にすることです。

https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest
</div></details>

## Q. 1-32
App Engineでアプリケーションを実行しています。

アプリケーションは Stackdriver Trace でインスツルメンテーションされています。product-detailsリクエストは、以下のように/sku-detailsにある4つの既知のユニークな商品に関する詳細をレポートします。リクエストが完了するまでの時間を短縮したい。

どうすればよいでしょうか？
1. インスタンスクラスのサイズを大きくする。
2. 永続ディスクのタイプをSSDに変更する。
3. リクエストを並行して実行するように/product-detailsを変更する。
4. sku-details情報をデータベースに保存し、Webサービスコールをデータベースクエリに置き換える。
<details><div>
    答え：3
A. インスタンスクラスのサイズを大きくすることで、より多くのリソースを提供できるかもしれませんが、シーケンシャルなリクエスト処理という核心的な問題には必ずしも対処できないでしょう。
B. 永続ディスク・タイプをSSDに変更すると、ディスクI/Oパフォーマンスが向上するかもしれないが、ここで説明する問題はネットワーク・リクエストに関連するものであり、ディスク操作に関連するものではない。
D. データベースに/sku-details情報を格納することで、シナリオによってはパフォーマンスが向上する可能性がありますが、説明した問題とは必ずしも一致しません。sku-detailsデータが頻繁に変更され、サードパーティによって管理されている場合、データベースの保存は適切ではないかもしれません。
正解
C. リクエストを並列に実行することで、/product-detailsリクエスト全体が完了するまでの時間を短縮できます。

Links:

https://cloud.google.com/appengine/docs/standard/java/datastore/queries
</div></details>

## Q. 1-33
App Engineの標準設定は以下のとおりです：

- サービス: production

- インスタンスクラス B1

アプリケーションを5インスタンスに制限したい。

どのコードスニペットを構成に含める必要がありますか？
1. manual_scaling: インスタンス： 5 min_pending_latency: 30ms
2. manual_scaling: max_instances： 5 idle_timeout： 10m
3. basic_scaling: インスタンス数： 5 min_pending_latency: 30ms
4. basic_scaling: max_instances： 5 idle_timeout： 10m
<details><div>
    答え：4
不正解
manual_scalingではインスタンスの最大数を設定できない（固定数である）ため、これらの他の選択肢は正しくありません。また、選択肢Cはbasic_scalingに対して誤った構文を使用しています。
正解です：
App Engineでインスタンス数を制限したい場合、特定の最大インスタンス数でbasic scalingを使用できます。
この設定の正しいコードスニペットは以下の通り：
basic_scaling: max_instances： 5 idle_timeout： 10m
この設定により、App Engineは最大5つのインスタンスを実行し続け、10分以上アイドル状態のインスタンスを自動的にシャットダウンします。
Links:

https://cloud.google.com/appengine/docs/legacy/standard/python/how-instances-are-managed#scaling_types
</div></details>

## Q. 1-34
アプリケーションはCompute Engine上で実行されており、少数のリクエストで持続的な障害が発生しています。原因を1つのCompute Engineインスタンスに絞り込みましたが、そのインスタンスはSSHに応答しません。

次に何をすべきでしょうか?
1. マシンを再起動します。
2. シリアルポート出力を有効にして確認してください。
3. マシンを削除し、新しいマシンを作成する。
4. ディスクのスナップショットを取り、新しいマシンに添付する。
<details><div>
    答え：2
不正解
A. 一時的に問題は解決するかもしれませんが、問題の原因を知ることができないので、再発する可能性があります。
C. 一時的に問題は解決するかもしれないが、根本的な原因を調査しない限り、再発を防ぐことはできない。
D. より抜本的な対策を後で検討することになるかもしれませんが、シリアルポートの出力をチェックすることは、問題を診断するのにより簡単で直接的な方法です。
正解
B. 次のステップは正しいです。これにより、ブート・ログやシステム・ログを見ることができ、何が問題だったのかを知る手がかりになります。
Links:

https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#debug_with_serial_console
</div></details>

## Q. 1-35
データはCloud Storageのバケットに保存されます。

他の開発者から、Cloud StorageからダウンロードしたデータによってAPIのパフォーマンスが低下しているという報告を受けています。Google Cloudのサポートチームに詳細を報告するために、この問題を調査したいと思います。

どのコマンドを実行すべきですか？
1. gsutil test -o output.json gs://my-bucket
2. gsutil perfdiag -o output.json gs://my-bucket
3. gcloud compute scp example-instance:~/test-data -o output.json gs://my-bucket
4. gcloud services test -o output.json gs://my-bucket
<details><div>
    答え：2
説明
不正解
A. このコマンドはgsutilコマンドラインツールに存在せず、オプション "test "が認識されないため、正しく実行されません。
C. gcloud compute scpコマンドは、ローカルマシンと仮想マシン間、または2つの仮想マシン間でファイルをコピーするために使用されます。クラウドストレージのパフォーマンス問題の診断とは関係ないので、今回のタスクには当てはまらない。
D. このコマンドはgcloudコマンドラインツールには存在しません。gcloud servicesの下に「test」コマンドはないので、この行はエラーになります。
正しい答え
B. gsutil perfdiagコマンドは、Google Cloud Storageのパフォーマンスの問題を診断するために使用できます。一連の診断テストを実行し、パフォーマンス問題の原因を特定するのに役立つ情報を収集します。
Links:

https://cloud.google.com/storage/docs/gsutil/commands/perfdiag#providing-diagnostic-output-to-cloud-storage-team
</div></details>

## Q. 1-36
あなたは、Compute Engine上で財務部門向けの企業ツールを開発しています。このツールでは、ユーザーを認証し、財務部門に所属していることを確認する必要があります。全社員がG Suiteを使用しています。

あなたは何をすべきですか?
1. HTTPロードバランサーでCloud Identity-Aware Proxyを有効にし、財務部門のユーザーを含むGoogleグループへのアクセスを制限します。提供された JSON Web トークンをアプリケーション内で確認します。
2. HTTP(s)ロードバランサーでCloud Identity-Aware Proxyを有効にし、財務部門のユーザーを含むGoogleグループへのアクセスを制限する。財務チームの全員にクライアント側証明書を発行し、アプリケーションで証明書を検証します。
3. Cloud Armor Security Policies を構成して、企業 IP アドレス範囲のみにアクセスを制限する。提供された JSON Web トークンをアプリケーション内で検証する。
4. Cloud Armor Security Policiesを構成して、企業IPアドレス範囲のみにアクセスを制限する。財務チームの全員にクライアント側証明書を発行し、アプリケーションで証明書を検証する。
<details><div>
    答え：1
説明
不正解
B. このケースでは、明確な要件もメリットもないのにクライアント側証明書を発行することで、不必要な複雑さを追加する。
C. D. 
オプション C と D は、IP アドレスに基づいてアクセスを制限する Cloud Armor Security Policies に依存しています。また、会社の全従業員がG Suiteを使用しているという事実を利用しておらず、財務部門のみにアクセスを制限するにはIPベースの制限では不十分かもしれません。
正解
A. HTTP(s)ロードバランサーでCloud Identity-Aware Proxy (IAP)を有効にする： IAP は Google Cloud 上で動作するクラウドアプリケーションへのアクセスを制御します。ID とコンテキストを検証してアプリケーションへのアクセスを許可するかどうかを判断することで、適切な人だけがアクセスできるようにします。
財務部門のユーザーを含むGoogleグループへのアクセスを制限する： これにより、財務部門の特定のグループのメンバーだけがアクセスできるようになります。IAP は、Google Workspace グループに基づいてアクセスを許可するように設定できます。
提供された JSON Web Token をアプリケーション内で確認します： IAP は JSON Web Token（JWT）を設定し、アプリケーションはこの JWT を検証して、リクエストが IAP によって承認されたことを確認できます。
Links:

https://cloud.google.com/iap/docs/signed-headers-howto#securing_iap_headers
</div></details>

## Q. 1-37
既存のApache/MySQL/PHPアプリケーションスタックをシングルマシンからGoogle

Kubernetes Engineに移植しようとしています。アプリケーションをコンテナ化する方法を決定する必要があります。あなたのアプローチは、Googleが推奨する可用性のベストプラクティスに従う必要があります。

あなたは何をすべきでしょうか？
1. 各コンポーネントを個別のコンテナにパッケージする。readinessProbeとライブネス・プローブを実装する。
2. アプリケーションを単一のコンテナにパッケージ化する。プロセス管理ツールを使用して、各コンポーネントを管理する。
3. 各コンポーネントを個別のコンテナにパッケージする。スクリプトを使用して、コンポーネントの起動をオーケストレーションする。
4. アプリケーションを1つのコンテナにパッケージする。コンテナへのエントリポイントとしてbashスクリプトを使用し、各コンポーネントをバックグラウンドジョブとしてスポーンする。
<details><div>
    答え：1
説明
不正解
B. このアプローチは、コンテナごとに1プロセスという原則に反するため、システムの管理、拡張、トラブルシューティングが困難になります。1つのプロセスに障害が発生すると、コンテナ全体に影響する可能性があるため、ダウンタイムが長くなり、問題の切り分けが複雑になる。
C. 各コンポーネントを別のコンテナに分離することはベストプラクティスに合致するが、オーケストレーションにカスタムスクリプトを使用すると、コンテナを管理するためのKubernetesのネイティブ機能が活用されない。このため、Kubernetesの組み込みオーケストレーション機能を使用する場合と比較して、メンテナンスコストが高くなり、効率的な管理ができなくなる可能性がある。
D. 繰り返しますが、このオプションは1コンテナ1プロセスの原則に従いません。1つのコンテナ内で複数のバックグラウンドジョブを管理すると、依存関係がもつれ、問題の診断が困難になる可能性がある。1つのバックグラウンド・ジョブに障害が発生すると、コンテナ全体に影響が及ぶ可能性がある。また、管理にbashスクリプトを使用すると、Kubernetesのオーケストレーション機能を活用できず、より脆弱でスケーラビリティの低いシステムになってしまいます。
正解
A. 各コンポーネントを個別のコンテナにパッケージ化し、readinessProbeとライブネス・プローブを実装することで、コンテナ化のベストプラクティスに沿い、Kubernetesの機能を活用してコンテナを効率的かつ効果的に管理できます。
Links:

https://cloud.google.com/architecture/best-practices-for-building-containers#package_a_single_app_per_container
</div></details>

## Q. 1-38
あなたはCloud Storage APIを使用するアプリケーションをサポートしています。ログを確認し、APIからの複数のHTTP 503 Service Unavailableエラー応答を発見しました。アプリケーションはエラーをログに記録し、それ以上のアクションは取りません。Googleが推奨する再試行ロジックを実装し、成功率を向上させたいと考えています。

どのアプローチを取るべきでしょうか？
1. 設定された数の失敗が記録された後、バッチで失敗を再試行する。
2. 設定された時間間隔で、最大回数まで各失敗を再試行する。
3. 失敗するたびに、最大トライ回数まで時間間隔を空けてリトライします。
4. 最大トライ回数まで、時間間隔を狭めて各障害をリトライする。
<details><div>
    答え：3
説明
不正解
A. このアプローチでは、503エラーの一過性の性質を考慮していないため、すべての再試行が一度に行われ、問題が悪化する可能性があります。
B. この方法は状況に適応せず、すでに苦戦しているサービスに負荷をかけ続ける可能性がある。
D. リトライの間隔を短くすることは、苦戦しているサービスの負荷を増加させ、状況を改善するどころか悪化させる可能性があります。
正解
C. すでに問題が発生しているシステムに負荷をかけるリスクを軽減し、各再試行の間に回復する時間を確保できるため、このシナリオではベストプラクティスです。
Links:

https://cloud.google.com/storage/docs/retry-strategy
</div></details>

## Q. 1-39
トラフィックの大幅な増加に対応できるように、認証サービスからの監査イベントの取り込みを再設計する必要があります。現在、監査サービスと認証システムは、同じCompute Engine仮想マシンで実行されています。新しいアーキテクチャでは、以下のGoogle Cloudツールを使用する予定です：

複数のCompute Engineマシンで、それぞれが認証サービスのインスタンスを実行している。

複数のCompute Engineマシンで、それぞれ監査サービスのインスタンスを実行する。

認証サービスからイベントを送信するためのPub/Sub。

システムが大量のメッセージを処理し、効率的に拡張できるようにするには、トピックとサブスクリプションをどのように設定すればよいですか？
1. 1つのPub/Subトピックを作成する。監査サービスがメッセージを共有できるように、1つのプル・サブスクリプションを作成します。
2. 1つのPub/Subトピックを作成します。監査サービスインスタンスごとに1つのプル・サブスクリプションを作成し、サービスがメッセージを共有できるようにします。
3. 1つのPub/Subトピックを作成する。監査サービスの前にロードバランサーを指すエンドポイントを持つプッシュサブスクリプションを1つ作成する。
4. 認証サービスごとに1つのPub/Subトピックを作成する。1つの監査サービスによって使用されるために、1つのトピックごとに1つのプルサブスクリプションを作成する。
<details><div>
    答え：1
説明
不正解
B. これは、監査サービスごとに個別のサブスクリプションを作成することになります。これらはすべて同じトピックから消費されるため、同じメッセージが複数のサブスクリプションに送信され、重複処理が発生する可能性があります。
C. このアプローチには利点がありますが、ロードバランサーとプッシュサブスクリプションを使用することは、より複雑になる可能性があり、オプションAほどこの特定のユースケースに適していないかもしれません。
D. このセットアップは、各認証サービスと監査サービスの間の緊密な結合をもたらし、潜在的なボトルネックとリソースの非効率的な使用をもたらします。
正解
A. この設定では、すべての監査サービスが同じサブスクリプションからメッセージをプルできます。これは、各メッセージが利用可能な監査サービスの 1 つによって 1 回処理されることを保証し、メッセージを重複させることなくスケーラブルなソリューションを提供します。
Links:

https://cloud.google.com/pubsub/docs/subscriber
</div></details>

## Q. 1-40
あなたは、Google Kubernetes Engine (GKE)クラスタ内の顧客に専用のブログソフトウェアをデプロイするSaaSプロバイダーです。各顧客が自分のブログのみにアクセスでき、他の顧客のワークロードに影響を与えないように、セキュアなマルチテナント・プラットフォームを構成したいと考えています。

あなたは何をすべきでしょうか？
1. クラスタを保護するために、GKEクラスタでアプリケーションレイヤーシークレットを有効にします。
2. テナントごとにネームスペースをデプロイし、各ブログのデプロイメントでネットワークポリシーを使用します。
3. GKE監査ロギングを使用して、悪意のあるコンテナを特定し、発見時に削除する。
4. ブログ・ソフトウェアのカスタム・イメージを構築し、Binary Authorizationを使用して信頼できないイメージのデプロイを防止する。
<details><div>
    答え：2
説明
不正解
A. これは、テナントを分離するというよりも、シークレットを保護することを目的としているため、マルチテナントの要件には対応していません。
C. 監査ロギングは疑わしい活動や不正な活動の特定に役立ちますが、テナント間の隔離や制限を直接的に提供するものではありません。
D. バイナリ認証は、信頼できるコンテナイメージのみがデプロイされることを保証しますが、異なるテナント間の分離や個々のブログへのアクセスの制御は提供しません。
正解
B. テナントごとに個別のネームスペースを作成することで、Kubernetesレベルでリソースを分離し、各テナントのワークロードを他のテナントから確実に分離できます。
Network Policiesを実装すると、異なるテナント間の通信がさらに制限され、ネットワークレベルでの分離が強制され、あるテナントのワークロードが他のテナントのワークロードと相互作用できないようになります。
Links:

https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview#network_policies
</div></details>

## Q. 1-41
あなたは、従業員が社内でコミュニティイベントを開催するための社内アプリケーションを開発しています。アプリケーションは1つのCompute Engineインスタンスにデプロイしました。あなたの会社はGoogle Workspace（旧G Suite）を使用しており、会社の従業員がどこからでもアプリケーションに認証できるようにする必要があります。

どうすればいいでしょうか？
1. インスタンスにパブリックIPアドレスを追加し、ファイアウォールルールを使用してインスタンスへのアクセスを制限します。会社のプロキシを唯一のソースIPアドレスとして許可します。
2. インスタンスの前にHTTP(S)ロードバランサーを追加し、Identity-Aware Proxy(IAP)を設定する。会社のドメインがウェブサイトにアクセスできるようにIAP設定を構成する。
3. 社内ネットワークとGoogle Cloud上のインスタンスのVPCロケーションの間にVPNトンネルを設定します。オンプレミスとGoogle Cloudの両方のネットワークに対して、必要なファイアウォールルールとルーティング情報を設定する。
4. インスタンスにパブリックIPアドレスを追加し、インターネットからのトラフィックを許可する。ランダムなハッシュを生成し、このハッシュを含み、インスタンスを指すサブドメインを作成します。このDNSアドレスを会社の従業員に配布する。
<details><div>
    答え：2
説明
不正解
A. この方法では、会社のドメインに基づく認証がないため、従業員が会社のネットワーク外からアプリケーションにアクセスしようとすると問題が発生する可能性があります。
C. この方法では、社内ネットワークまたはVPN接続からのみアプリケーションにアクセスできるため、従業員が他の場所からアプリケーションにアクセスできない可能性がある。
D. この方法では、ユーザーの身元に基づく認証は行われない。共有秘密（サブドメインのハッシュ）に依存し、ユーザーが従業員であることを検証しません。
正解
B. Identity-Aware Proxy（IAP）：Googleクラウド上で動作するアプリケーションへのアクセスを制御し、ユーザー（企業ドメインなど）のIDやグループメンバーシップに基づいてアクセスレベルを設定できます。

HTTP(S)ロードバランサー： HTTP(S)トラフィックを様々なパラメータに基づいて異なるインスタンスにルーティングすることができ、スケーラビリティと制御性を高めることができます。

したがって、ユーザーのアイデンティティに基づいたアクセスを可能にし、会社ドメインの従業員だけがアプリケーションを認証できるようにするオプションBが最良のアプローチです。
Links:

https://cloud.google.com/iap/docs/concepts-overview#how_iap_works

https://cloud.google.com/blog/topics/developers-practitioners/control-access-your-web-sites-identity-aware-proxy
</div></details>

## Q. 1-42
最近Google Kubernetes Engineにアプリケーションをデプロイし、新しいバージョンのアプリケーションをリリースする必要があります。新しいバージョンで問題が発生した場合に備えて、以前のバージョンに即座にロールバックする機能が必要です。

どのデプロイモデルを使うべきでしょうか？
1. ローリングデプロイメントを実行し、デプロイメント完了後に新しいアプリケーションをテストします。
2. A/Bテストを実施し、新しいテストが実施された後、定期的にアプリケーションをテストする。
3. ブルー/グリーンデプロイメントを実行し、デプロイメント完了後に新しいアプリケーションをテストします。
4. カナリアデプロイメントを実行し、新しいバージョンがデプロイされた後、新しいアプリケーションを定期的にテストしてください。
<details><div>
    答え：3
説明
不正解
A. ローリングデプロイメントでは、古いバージョンのインスタンスを新しいバージョンに徐々に置き換えます。これによってダウンタイムをゼロにすることができますが、システムが段階的な変更を元に戻さなければならないため、即座にロールバックすることはより複雑になる可能性があります。
B. A/Bテストは、ユーザーを異なるグループに分け、アプリケーションの異なるバージョンをテストすることを含みます。これは、即座にロールバックする仕組みを提供するというよりも、パフォーマンスやユーザビリティを比較するためのものです。
D. カナリアデプロイメントでは、完全なロールアウトの前に、新バージョンを一部のユーザーにリリースします。問題を早期に検出するのに役立ちますが、すべてのユーザーに対して以前のバージョンに即座にロールバックするメカニズムが提供されない可能性があります。
正解
C. ブルー/グリーンのデプロイメントでは、2つの別々の環境を持つことになります。1つは古いバージョン（ブルー）を実行し、もう1つは新しいバージョン（グリーン）を実行します。トラフィックをリダイレクトするだけで、これらのバージョンを切り替えることができます。グリーン環境で何か問題があれば、即座にブルー環境に戻すことができる。このように、ブルー／グリーンのデプロイメント・モデルは、新しいバージョンに問題があれば、即座に以前のバージョンにロールバックするという要件に最も適している。
Links:

https://cloud.google.com/architecture/application-deployment-and-testing-strategies#choosing_the_right_strategy
</div></details>

## Q. 1-43
コンテナ化したアプリケーションの新バージョンのテストが完了し、Google Kubernetes Engine上で本番環境にデプロイする準備が整いました。
本番前の環境では新バージョンの負荷テストを十分に行うことができなかったため、デプロイ後のパフォーマンスに問題がないことを確認する必要があります。デプロイは自動化する必要があります。
あなたは何をすべきでしょうか？
1. クラウドロードバランシングを使用して、バージョン間のトラフィックを徐々に増加させます。クラウドモニタリングを使用してパフォーマンスの問題を探します。
2. カナリアデプロイメントを使用して、継続的デリバリーパイプライン経由でアプリケーションをデプロイする。クラウドモニタリングを使用してパフォーマンスの問題を調べ、メトリクスがサポートするようにトラフィックを増加させる。
3. ブルー／グリーン・デプロイメントを使用して、継続的デリバリー・パイプラインを介してアプリケーションをデプロイする。Cloud Monitoringを使用してパフォーマンスの問題を探し、メトリクスがそれをサポートするときに完全に起動します。
4. kubectlを使用してアプリケーションをデプロイし、spec.updateStrategv.typeをRollingUpdateに設定します。Cloud Monitoringを使用してパフォーマンスの問題を探し、問題があればkubectl rollbackコマンドを実行します。
<details><div>
    答え：4
説明
不正解
A. クラウドロードバランシングは、Kubernetes環境における新しいアプリケーションバージョンの制御されたロールアウトに適したツールではありません。
B. カナリア・デプロイは、少数のサブセット・ユーザーで新バージョンをテストする良い方法ですが、この選択肢は選択された正解には一致しません。
C. ブルー／グリーン・デプロイメントは、バージョン間の迅速な切り替えを可能にしますが、新しいバージョンへの露出を徐々に増やすことには適していません。
正解
D. KubernetesのRollingUpdateでは、アプリケーションを徐々に更新することができます。Cloud Monitoringを通じてパフォーマンスの問題が検出された場合、kubectl rollbackを使用してデプロイメントをロールバックできます。これにより、問題を検出するために必要な段階的な露出と、デプロイを効率的に管理するための自動化の両方が提供される。
Links:

https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/

https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#overview
</div></details>

## Q. 1-44
クラウドランでホストしているウェブサイトがトラフィック急増時に反応が遅すぎるとユーザーから苦情が来ています。
トラフィックのピーク時に、より良いユーザーエクスペリエンスを提供したいと考えています。
どうすればよいでしょうか？
1. アプリケーションの起動時にデータベースからアプリケーション構成と静的データを読み込みます。
2. ビルド時にアプリケーション構成と静的データをアプリケーション・イメージにパッケージする。
3. レスポンスがユーザーに返された後、できるだけ多くの作業をバックグラウンドで実行する。
4. タイムアウト例外やエラーによってCloud Runインスタンスが迅速に終了し、代替インスタンスが開始できるようにする。
<details><div>
    答え：2
説明
不正解
A. 特に、新しいインスタンスが頻繁に起動される可能性が高いトラフィック急増時には、待ち時間が増加する可能性があります。
C. バックグラウンド・タスクをオフロードすることで、ユーザーが認識するパフォーマンスは向上しますが、トラフィック急増時の応答時間の遅さという問題には特に対処できません。バックグラウンド・タスクは、入ってくるリクエストの処理に使えるリソースをさらに消費する可能性さえある。
D. 障害が発生したインスタンスを迅速に交換することは一般的に良いプラクティスですが、これはトラフィックが多いときの応答時間の低下の問題には特に対処していません。さらに、常にインスタンスを交換することは、追加のオーバーヘッドにつながり、応答時間をさらに遅くする可能性があります。
正解
B. 構成データと静的データをアプリケーション・イメージにバンドルすることで、実行時に外部ソースからデータを取得する必要性を減らすことができます。これにより、特にシステムの負荷が増大するトラフィック・ピーク時に、応答時間を短縮することができます。
Links:

https://cloud.google.com/blog/topics/developers-practitioners/3-ways-optimize-cloud-run-response-times
</div></details>

## Q. 1-45
アプリケーションは、複数のゾーンにある管理インスタンスグループ（MIG）内の数百のCompute Engineインスタンスにデプロイされています。重要な脆弱性を直ちに修正するために新しいインスタンステンプレートをデプロイする必要がありますが、サービスへの影響は避けなければなりません。

インスタンステンプレートを更新した後、MIGにどのような設定を行う必要がありますか？
1. Max Surgeを100%に設定します。
2. 更新モードをオポチュニスティックに設定する。
3. 最大利用不可を100%に設定する。
4. 最小待機時間を0秒に設定します。
<details><div>
    答え：4
説明
不正解
A. Max Surgeは、アップデート中に作成できる追加インスタンス数を定義します。100%に設定すると、インスタンス数が一時的に2倍になります。更新プロセスを高速化できますが、コストの増加など別の結果も発生する可能性があり、個々のインスタンスの更新間隔を制御できません。
B. 前述したように、このモードは、他のアクティビティによってインスタンスが再作成された場合にのみインスタンスを更新します。このモードでは、重大な脆弱性を修正するためにインスタンスを直ちに更新する方法は提供されないため、このシナリオで必要とされる緊急の更新には適していません。
C. この設定では、アップデート中にグループ内のすべてのインスタンスが同時に利用できなくなり、サービスへの影響を避けるという要件に直接反してしまいます。このオプションはダウンタイムにつながるため、この状況には適していません。
正解
D. この設定により、個々のインスタンスの更新の間に待機することなく、更新プロセスを可能な限り迅速に行うことができ、サービスを中断することなく重要な脆弱性を緊急に修正する必要性に沿う。
Links:

https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#minimum_wait_time
</div></details>

## Q. 1-46
Google Kubernetes Engine（GKE）にデプロイされたアプリケーションの1つに、断続的なパフォーマンスの問題が発生しています。あなたのチームはサードパーティのロギングソリューションを使用しています。このソリューションをGKEクラスタの各ノードにインストールして、ログを表示できるようにしたいと思います。

あなたは何をすべきですか？
1. サードパーティのソリューションをDaemonSetとしてデプロイする
2. コンテナイメージを変更し、監視ソフトウェアを含める
3. SSH を使用して GKE ノードに接続し、ソフトウェアを手動でインストールする。
4. Terraformを使用してサードパーティソリューションをデプロイし、KubernetesデプロイメントとしてロギングPodをデプロイする。
<details><div>
    答え：1
説明
不正解
B. これは、アプリケーションコンテナ自体にロギングソリューションを組み込むことになります。特定のアプリケーションを実行していないノードも含め、クラスタ内のすべてのノードにロギング・ソリューションをデプロイする必要がある場合、この方法は機能しません。
C. 手動インストールは、特にKubernetesでは、インフラストラクチャを維持するためのベストプラクティスに従わない。ノード間の一貫性の維持や、ノードの障害からの復旧に困難が生じます。また、GKEノードは管理されていることが多く、このような変更のための直接アクセスを許可しない場合があることも注目に値する。
D. 標準的なデプロイメントとしてロギングソリューションをデプロイしても、ロギングソリューションのインスタンスがすべてのノードで実行されていることは保証されません。デプロイは、アプリケーションの指定された数のレプリカが実行されていることを保証するように設計されており、アプリケーションがクラスタ内のすべてのノードで実行されていることを強制するものではありません。
正解
A. DaemonSetは、すべてまたは一部のワーカーノードがPodのコピーを実行するようにします。クラスタにノードが追加されると、Podがノードに追加されます。クラスタ内のすべてのノードに監視またはロギングエージェントをデプロイする場合は、DaemonSetが正しい方法です。
Links:

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset

https://cloud.google.com/kubernetes-engine/docs/concepts/daemonset#usage_patterns
</div></details>

## Q. 1-47
あなたは最近、オンプレミスのモノリシック・アプリケーションをGoogle Kubernetes Engine（GKE）上のマイクロサービス・アプリケーションに移行しました。このアプリケーションは、CRMシステムや個人を特定できる情報（PII）を含むMySQLデータベースなど、オンプレミスのバックエンドサービスに依存しています。バックエンド・サービスは、規制要件を満たすためにオンプレミスのままでなければなりません。

オンプレミスのデータセンターとGoogle Cloudの間にクラウドVPN接続を確立しました。GKE上のマイクロサービス・アプリケーションからバックエンド・サービスへのリクエストの一部が、帯域幅の変動によるレイテンシの問題で失敗し、アプリケーションのクラッシュを引き起こしていることに気づきました。

レイテンシーの問題にどのように対処すべきでしょうか？
1. Memorystoreを使用して、オンプレミスのMySQLデータベースから頻繁にアクセスされるPIIデータをキャッシュします。
2. Istio を使用して、GKE 上のマイクロサービスとオンプレミスのサービスを含むサービスメッシュを作成する。
3. Google Cloudとオンプレミスのサービス間の接続にクラウドVPNトンネルの数を増やす
4. クラウドVPNのMTU（Maximum Transmission Unit）値をデフォルト値から下げることで、ネットワーク層のパケットサイズを小さくする。
<details><div>
    答え：3
説明
不正解
A. キャッシュは一部のデータアクセスではレイテンシーを削減できますが、PIIデータには規制要件により適用できない場合があります。また、帯域幅の変動が問題を引き起こしているという根本的な問題には対処できない。
B. Istioのようなサービスメッシュは、より良い制御、観測可能性、ルーティングを提供できる。しかし、帯域幅が変動するという問題を本質的に解決しているわけではないので、レイテンシーの問題は続く可能性が高い。
D. MTUを小さくすることで、より多くのパケットを送信することができますが、必ずしも帯域幅の変動の問題を解決することはできません。
正解
C. VPNトンネルの数を増やすことで、より多くの帯域幅を提供でき、遅延の問題を軽減できる可能性があります。このアプローチは、問題の根本原因に直接対処します。
Links:

https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#more-bandwidth
</div></details>

## Q. 1-48
GoアプリケーションからCloud Spannerデータベースに書き込んでいます。Google が推奨するベストプラクティスを使用して、アプリケーションのパフォーマンスを最適化したいと考えています。

どうすればよいでしょうか？
1. Cloud Client Librariesを使用してCloud Spannerに書き込みます。
2. Google API クライアント ライブラリを使用して Cloud Spanner に書き込む。
3. カスタムgRPCクライアント・ライブラリを使用してCloud Spannerに書き込む。
4. サードパーティのHTTPクライアント・ライブラリを使用してCloud Spannerに書き込む。
<details><div>
    答え：1
説明
不正解です：
B. 技術的には可能ですが、Cloud Client Librariesはサービスに特化しているため、開発が簡単で効率的です。
C. カスタム gRPC クライアント ライブラリを作成することは、公式にサポートされているライブラリを使用することに比べて、エラーが発生しやすく、時間がかかる可能性があります。
D. サードパーティのライブラリに依存すると、互換性とサポートのリスクが発生し、Cloud Spanner用に最適化されていない可能性があります。
正解
A. GoアプリケーションからCloud Spannerを使用する場合、GoogleはCloud Client Librariesの使用を推奨します。これらのライブラリは、Google API Client Libraries よりも高レベルで便利な抽象化を提供し、Google Cloud サービスとの統合を容易にします。

GoからCloud Spannerを操作する慣用的で最適化された方法を提供するため、オプションAが推奨されるアプローチです。
Links:

https://cloud.google.com/apis/docs/client-libraries-explained

https://cloud.google.com/go/docs/reference
</div></details>

## Q. 1-49
Google Kubernetes Engine（GKE）にデプロイされたアプリケーションがあります。Google Cloudのマネージドサービスに認可されたリクエストを行うために、アプリケーションをアップデートする必要があります。これは一度だけのセットアップであり、セキュリティキーの自動ローテーションと暗号化されたストアへの保存というセキュリティのベストプラクティスに従う必要があります。Google Cloud サービスへの適切なアクセス権を持つサービスアカウントは作成済みです。

次に何をすべきでしょうか？
1. Workload Identityを使用して、GKEポッドにGoogleクラウドサービスアカウントを割り当てます。
2. Google Cloudサービスアカウントをエクスポートし、KubernetesシークレットとしてPodと共有します。
3. Google Cloudサービスアカウントをエクスポートし、アプリケーションのソースコードに埋め込みます。
4. Google Cloudのサービスアカウントをエクスポートし、HashiCorp Vaultにアップロードして、アプリケーション用の動的なサービスアカウントを生成します。
<details><div>
    答え：1
説明
不正解
B. サービスアカウントキーの保存にKubernetesシークレットを使用すると、自動ローテーションが提供されず、正しく処理されないとセキュリティリスクを引き起こす可能性があります。
C. ソースコードにサービスアカウントを埋め込むことは悪い習慣であり、重大なセキュリティリスクをもたらす。
D. HashiCorp Vaultはシークレット管理のための強力なツールですが、Google Cloudサービスと相互作用するGKEワークロードのIDを処理するには、ワークロードアイデンティティを使用する方がより簡単で推奨される方法です。
正解
A. Workload Identityは、GKE内からGoogle Cloudサービスにアクセスするための推奨方法です。KubernetesサービスアカウントをGoogleサービスアカウントにバインドすることができ、このバインドによってKubernetesサービスアカウントがGoogleサービスアカウントとして機能します。Workload Identityを使用すると、サービスアカウントのキーを管理する必要がなく、ベストプラクティスに沿った自動ローテーションが行われます。
Links:

https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity
</div></details>

## Q. 1-50
アプリケーションにユニット・テストを追加することを計画しています。発行された Pub/Sub メッセージがサブスクライバによって順番に処理されることを保証できるようにする必要があります。あなたは、ユニット・テストを費用対効果が高く、信頼できるものにしたいと考えています。

あなたは何をすべきでしょうか？
1. モッキングフレームワークを実装する。
2. テスターごとにトピックとサブスクリプションを作成する。
3. サブスクリプションにテスターによるフィルタを追加する。
4. Pub/Subエミュレータを使用する。
<details><div>
    答え：4
説明
不正解
A. モッキング・フレームワークを実装することは有効な選択肢ですが、Pub/Subエミュレータを使用する方がよりシンプルで正確です。
B. テスターごとにトピックとサブスクリプションを作成すると、実際の Pub/Sub サービスとやり取りすることになり、テストにコストや複雑さ、信頼性の問題が生じる可能性があります。
C. サブスクリプションにテスターによるフィルタを追加することも、実際の Pub/Sub サービスとのやりとりを伴います。
正解です：
D. オプションDの「Pub/Subエミュレータを使用する」は、ここでの最も適切なアプローチです。Pub/Subエミュレータを使用すると、実際のPub/Subサービスを実際に使用することなく、Pub/Subと相互作用するコードをユニットテストすることができます。これはローカルで実行され、Pub/Sub の動作をシミュレートするため、テストが高速になり、コストもかかりません。
Links:

https://cloud.google.com/pubsub/docs/emulator
</div></details>

## Q. 2-1
Google Kubernetes Engine（GKE）に、ライブストリームを配信するマイクロサービス・アプリケーションをデプロイしようとしています。予測不可能なトラフィックパターンと同時ユーザー数の大きな変動が予想されます。アプリケーションは以下の要件を満たす必要があります：

- 人気のあるイベント時に自動的にスケールし、高可用性を維持する。

- ハードウェア障害が発生した場合の回復力

デプロイメントパラメータはどのように構成しますか? (2つのオプションを選択してください)
1. マルチゾーンノードプールを使用してワークロードを均等に分散します。
2. 複数のゾーンノードプールを使用してワークロードを均等に分散します。
3. クラスターオートスケーラーを使用してノードプールのノード数をリサイズし、ホリゾンタルポッドオートスケーラーを使用してワークロードをスケールします。
4. クラスタノードでCompute Engine用のマネージドインスタンスグループを作成します。マネージドインスタンスグループのオートスケーリングルールを設定します。
5. GKEのCPUとメモリの使用率に基づいて、Cloud Monitoringでアラートポリシーを作成する。CPUとメモリの使用率が事前に定義したしきい値を超えたら、スクリプトを実行してワークロードをスケールするよう、当番のエンジニアに依頼する。
<details><div>
    答え：1,3
説明
不正解
B. 複数のゾーンノードプールを使用すると、管理が複雑になる可能性があります。一般的に、マルチゾーナルノードプールは、より簡単な構成で同じ利点を提供します。
D. マネージドインスタンスグループは一般的にKubernetesの外部で使用され、GKEと直接統合されないため、このオプションは説明したシナリオには適していません。
E. アラートポリシーの作成とワークロードの手動スケーリングは自動プロセスではなく、手動介入に依存するとエラーが発生しやすく、スケーリングが遅れる可能性があります。
正解
A. これにより、ノードがリージョン内の複数のゾーンに分散され、ゾーンの1つに障害が発生した場合のフォールトトレランスが確保されます。また、ゾーン間のリソースの利用率も向上します。
C. クラスタオートスケーラは、リソース要件に基づいてノードプールのサイズを自動的に調整し、必要なときにノードが追加され、不要なときにノードが削除されるようにします。Horizontal Pod Autoscalerは、観測されたCPUまたはメモリの使用量に基づいてデプロイメント内のPodの数を自動的に調整し、アプリケーションがさまざまな負荷に対応できるようにします。
Links:

https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available

https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#multi-zonal_clusters
</div></details>

## Q. 2-2
Cloud Shellからkubectlを使ってGoogle Kubernetes Engine (GKE)クラスタに接続しようとしています。
GKEクラスターをパブリックエンドポイントでデプロイしています。Cloud Shellから以下のコマンドを実行します：

gcloud container clusters get-credentials <cluster-name> ￤ -zone <none
---zone <none> --project <プロジェクト名> \
kubectl コマンドがエラーメッセージを返さずにタイムアウトしていることに気付きます。

この問題の最も可能性の高い原因は何ですか。
1. ユーザーアカウントには、kubectlを使用してクラスタと対話する権限がありません。
2. Cloud Shellの外部IPアドレスはクラスタの許可されたネットワークの一部ではありません。
3. クラウドシェルが GKE クラスターと同じ VPC に属していない。
4. VPCファイアウォールがクラスタのエンドポイントへのアクセスをブロックしている。
<details><div>
    答え：2
説明
不正解
A. ユーザーアカウントに権限がない場合、通常はタイムアウトではなく、権限に関連するエラーメッセージが表示されます。
C. パブリックエンドポイントに接続しているため、クラウドシェルが同じ VPC に属していなくてもこの問題は発生しません。
D. VPC ファイアウォールは、Cloud Shell が VPC の外側にあるため、Cloud Shell からクラスタのパブリックエンドポイントへの接続には影響しません。
正解
B. パブリックエンドポイントを持つGKEクラスタを設定するとき、Kubernetes APIサーバーへの接続が許可される許可されたネットワークを設定することができます。Cloud Shellの外部IPアドレスがこれらの許可されたネットワークの一部でない場合、クラスタに接続しようとするとブロックされ、説明されているようなタイムアウトの問題が発生します。
Links:

https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#cloud_shell
</div></details>

## Q. 2-3
あなたのチームはCloud Functionsコードのユニットテストを開発しています。

コードは Cloud Source Repositories リポジトリに保存されています。あなたはテストを実装する責任があります。特定のサービスアカウントだけが、コードを Cloud Functions にデプロイするのに必要な権限を持っています。最初にテストに合格しないとコードがデプロイできないようにしたい。

ユニットテストプロセスをどのように構成しますか？
1. Cloud Build を構成してクラウド関数をデプロイします。コードがテストに合格すると、デプロイ承認が送信されます。
2. 特定のサービス アカウントをビルド エージェントとして使用して、Cloud Functions をデプロイするように Cloud Build を構成します。デプロイ成功後にユニットテストを実行する。
3. ユニット テストを実行するように Cloud Build を構成します。コードがテストに合格すると、開発者は Cloud Functions をデプロイします。
4. 特定のサービスアカウントをビルドエージェントとして使用して、ユニットテストを実行するようにCloud Buildを構成します。コードがテストに合格すると、Cloud BuildはCloud Functionsをデプロイします。
<details><div>
    答え：4
説明
不正解
A. このオプションは、デプロイの前に単体テストを実行することについては特定されていません。
B. このオプションはデプロイ後にユニットテストを実行しますが、これは必要なことではありません。テストはデプロイ前にパスする必要があります。
C. このオプションは、開発者がCloud Function をデプロイすることを可能にしますが、特定のサービスアカウントがデプロイ権限を持つ必要があります。
正解
D. 目標は、まずテストに合格しなければコードをデプロイできないようにすることであり、特定のサービスアカウントだけがCloud Functionsにコードをデプロイするのに必要な権限を持っています。

このセットアップにより、デプロイの前にユニットテストが実行され、コードがテストに合格した場合のみ、特定のサービスアカウントを使用して Cloud Build によってデプロイが実行されることが保証されます。これにより、テストの前提条件とデプロイの正しい権限処理の両方が保証されます。
Links:

https://firebase.google.com/docs/functions/unit-testing
</div></details>

## Q. 2-4
あなたは、MySQLリレーショナル・データベース・スキーマを使用するGoogle Cloud上でホストされるアプリケーションを開発しています。このアプリケーションでは、データベースへの大量の読み取りと書き込みが発生するため、バックアップと継続的なキャパシティプランニングが必要になります。あなたのチームにはデータベースを完全に管理する時間はありませんが、小さな管理タスクを引き受けることはできます。

どのようにデータベースをホストすべきでしょうか？
1. データベースをホストするためにCloud SQLを構成し、スキーマをCloud SQLにインポートする。
2. クライアントを使用してGoogle Cloud MarketplaceからデータベースにMySQLをデプロイし、スキーマをインポートします。
3. データベースをホストするためにBigtableを設定し、データをBigtableにインポートする。
4. データベースをホストするためにCloud Spannerを構成し、スキーマをCloud Spannerにインポートする。
<details><div>
    答え：1
説明
不正解
B. Google Cloud MarketplaceからMySQLをデプロイする必要があり、管理オーバーヘッドが増えるため、管理タスクを最小限に抑えたいチームには適していません。
C. Bigtableを使うことを提案するが、BigtableはNoSQLデータベースであり、MySQLのリレーショナルデータベーススキーマには適していない。
D. Cloud Spannerを提案するが、Spannerはリージョン間の水平スケーリング用に設計されたグローバル分散データベースであり、従来のMySQLデータベースのホスティングには適していない。
正解
A. Google Cloud SQLは、MySQLをサポートするフルマネージドデータベースサービスを提供し、スケーリング、バックアップ、メンテナンスを容易にします。
Links:

https://cloud.google.com/spanner/docs/migrating-mysql-to-spanner#migration-process

https://cloud.google.com/sql/docs/mysql
</div></details>

## Q. 2-5
新しい Go アプリケーションを Cloud Run にデプロイする予定です。ソース コードはクラウド ソース リポジトリに格納されています。ソース コードのコミットが行われたときに実行される、完全に管理された自動継続デプロイ パイプラインを構成する必要があります。最も単純なデプロイメントソリューションを使用したい。

どうすればいいでしょうか？
1. ワークステーション上でcronジョブを構成して、作業ディレクトリでgcloud run deploy --sourceを定期的に実行します。
2. Jenkins トリガーを構成して、クラウド・ソース・リポジトリへのソース・コードのコミットごとに、コンテナのビルドとデプロイ処理を実行する。
3. ビルドパックを使用して、Cloud Run用ソースリポジトリからの新しいリビジョンの継続的デプロイを構成する。
4. Cloud Source Repositoriesへのソースコードのコミットごとにコンテナビルドとデプロイプロセスを実行するように構成されたトリガーでCloud Buildを使用する。
<details><div>
    答え：4
説明
不正解
A. オプションAは、ワークステーション上でcronジョブを構成する必要があるため、完全に管理または自動化されません。
B. オプションBは、Jenkinsのセットアップを伴うため、Google独自のマネージドサービスを使用する場合と比較すると、最も単純なソリューションではない。
C. 
正解
D. GoogleによるフルマネージドサービスであるCloud Buildを使用したエンドツーエンドのソリューションを提供し、Cloud Source RepositoriesおよびCloud Runとの容易な統合を可能にする。Cloud Buildでトリガーを設定し、ソースコードリポジトリに新しいコミットが行われたときにコンテナを自動的にビルドおよびデプロイし、質問の要件を満たすことができる。
Links:

https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build
</div></details>

## Q. 2-6
ベスト・プラクティスに準拠するために、Cloud Build 手順を見直し、更新しています。現在、ビルド手順には以下が含まれています：

1. ソース リポジトリからソース コードをプルします。

2. コンテナ・イメージをビルドする。

3. ビルドしたイメージを Artifact Registry にアップロードする。

ビルドしたコンテナイメージの脆弱性スキャンを実行するステップを追加する必要があり、スキャンの結果を Google Cloud で実行中のデプロイパイプラインで利用できるようにしたい。他のチームのプロセスを混乱させる可能性のある変更は最小限に抑えたい。どうすればいいでしょうか？
1. バイナリ認証を有効にし、コンテナイメージに脆弱性が存在しないことを証明するように構成します。
2. ビルドしたコンテナイメージをDocker Hubインスタンスにアップロードし、脆弱性をスキャンする。
3. ArtifactレジストリでコンテナスキャンAPIを有効にし、構築されたコンテナイメージの脆弱性をスキャンします。
4. Aqua SecurityインスタンスにArtifact Registryを追加し、ビルドされたコンテナイメージの脆弱性をスキャンする。
<details><div>
    答え：3
説明
不正解
A. Binary Authorizationとは、コンテナのデプロイ時に署名検証を実施するために使用されるものであり、脆弱性のスキャンに使用されるものではありません。
B. 外部サービス（Docker Hub）を使用することになり、既存のプロセスに大幅な変更が必要になる可能性があります。
D. サードパーティツール（Aqua Security）との統合が必要で、既存のプロセスを混乱させる可能性があります。
正解
C. Google Cloud 内、特に Artifact Registry 内で脆弱性スキャンを実行でき、コンテナ イメージのビルド方法や保存方法を変更する必要がありません。Container Analysis API は、脆弱性スキャンを含むコンテナイメージの分析とメタデータの生成に使用され、Artifact Registry で有効にすることができます。
Links:

https://cloud.google.com/container-analysis/docs/automated-scanning-howto#view_the_image_vulnerabilities
</div></details>

## Q. 2-7
あなたのチームはCloud Run上でサーバーレスWebアプリケーションを作成しています。このアプリケーションは、プライベートクラウドストレージバケットに保存された画像にアクセスする必要があります。アプリケーションにバケット内の画像にアクセスするIAM（Identity and Access Management）権限を与えると同時に、Googleが推奨するベストプラクティスを使ってサービスを保護したい。

どうすればよいでしょうか？
1. 目的のバケットに対して署名付きURLを強制します。Compute Engineのデフォルトのサービスアカウントに、バケットのStorage Object Viewer IAMロールを付与します。
2. 目的のバケットに対して、パブリックアクセス防止を強制します。Compute Engineのデフォルトのサービスアカウントに、バケットのStorage Object Viewer IAMロールを付与する。
3. 目的のバケットに対して署名付きURLを強制します。ユーザー管理サービスアカウントを使用するようにCloud Runサービスを作成し、更新します。サービスアカウントに、バケット上のStorage Object Viewer IAMロールを付与する。
4. 目的のバケットに対してパブリックアクセス防止を実施します。ユーザー管理サービスアカウントを使用するようにCloud Runサービスを作成し、更新します。バケット上のStorage Object Viewer IAMロールをサービスアカウントに付与します。
<details><div>
    答え：4
説明
不正解
オプションAとBは、Compute Engineのデフォルトのサービスアカウントに関係しており、Cloud Runサービスに特有ではないため、最小特権の原則に合致していません。
C. オプションCは署名付きURLを利用し、バケット内のオブジェクトへの一時的なアクセスを提供する。これはアクセスを許可する有効な方法ですが、選択肢Dほど説明したシナリオには適合しません。
正解
D. パブリックアクセス防止を実施することで、バケットが誤って公開されないようにし、プライバシーを維持します。

ユーザが管理するサービスアカウントを使用するようにCloud Runサービスを作成・更新することで、特定のサービスに必要な権限のみを付与する最小権限の原則に従うことになります。

サービスアカウントにStorage Object Viewer IAMロールを付与することで、Cloud Runサービスはより広範な権限を与えることなく、指定されたバケットからオブジェクトを読み取ることができます。
Links:
https://cloud.google.com/run/docs/securing/service-identity#user-managed_service_account
https://cloud.google.com/storage/docs/public-access-prevention
https://cloud.google.com/storage/docs/access-control/using-iam-permissions
</div></details>

## Q. 2-8
あなたは、グローバルなeコマースWebアプリケーションをホストするためにCloud Runを使用しています。あなたの会社のデザインチームは、ウェブアプリケーションの新しい配色を作成しています。あなたは、新しい配色が売上を増加させるかどうかを判断する任務を与えられています。あなたは、本番のトラフィックでテストを実施したいと考えています。

どのように調査を設計すべきでしょうか？
1. 外部のHTTP(S)ロードバランサーを使用して、あらかじめ決められた割合のトラフィックを、アプリケーションの2つの異なるカラースキームにルーティングします。結果を分析して、売上に統計的に有意な差があるかどうかを判断する。
2. 外部のHTTP(S)ロードバランサーを使用して、新しい配備を作成してテストしている間、トラフィックを元の配色にルーティングします。テストが完了したら、すべてのトラフィックを新しい配色にルーティングし直します。結果を分析し、売上に統計的に有意な差があるかどうかを判断する。
3. 外部の HTTP(S)ロードバランサーを使用して、新しいバージョンのアプリケーションにトラフィックをミラーリングする。その結果を分析し、売上に統計的に有意な差があるかどうかを判断する。
4. 全ユーザーの半数に新しい配色を表示する機能フラグを有効にする。このユーザーグループの売上が増加するかどうかをモニターする。
<details><div>
    答え：1
説明
不正解
B. C. 
オプションBとCでは同時比較ができません。また、トラフィックをミラーリング（オプションC）しても、新しいデザインとの実際のユーザーインタラクションが得られないため、ユーザーの行動を正確に反映することができません。
D. オプションDは実行可能なように見えるかもしれませんが、HTTP(S)ロードバランサーが提供するような制御された環境がありません。機能フラグを実装することは、さらなる複雑さをもたらす可能性があり、テストが均一に配布されないかもしれません。
正解です：
A. これはA/Bテストを可能にし、ユーザーベースの一部がサイトのあるバージョンを取得し、別の一部が異なるバージョンを取得します。これは、2つのバージョンを比較し、どちらがより良いパフォーマンスかを確認するための一般的なアプローチです。

既存の配色と新しい配色の両方に所定の割合のトラフィックをルーティングすることで、同じ条件下で直接比較することができ、より信頼性の高い結果が得られます。

つまり、このような研究を管理された体系的な方法で実施し、2つの配色間で売上に統計的に有意な差があるかどうかを判断しやすくするためには、選択肢Aが最良の方法ということになる。
Links:

https://cloud.google.com/load-balancing/docs/l7-internal/traffic-management#traffic_actions_weight-based_traffic_splitting

https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless
</div></details>

## Q. 2-9
あなたは大企業の開発者です。Google Cloud上で3つのGoogle Kubernetes Engineクラスタを管理しています。あなたのチームの開発者は、好みの開発ツールへのアクセスを失うことなく、クラスタを定期的に切り替える必要があります。あなたは、Googleが推奨するベストプラクティスに従いながら、これらの複数のクラスタへのアクセスを設定したいと考えています。

どうすればよいでしょうか？
1. 開発者にCloud Shellを使用するように依頼し、gcloud container clusters get-credentialを実行して別のクラスタに切り替えます。
2. 設定ファイルで、クラスタ、ユーザー、コンテキストを定義します。このファイルを開発者と共有し、kubectl configを使用してクラスタ、ユーザー、コンテキストの詳細を追加するように依頼します。
3. 開発者にワークステーションにgcloud CLIをインストールしてもらい、gcloud container clusters get-credentialsを実行して別のクラスターに切り替える。
4. 開発者にワークステーション上で3つのターミナルを開いてもらい、kubectl configを使用して各クラスタへのアクセスを設定する。
<details><div>
    答え：2
説明
不正解
A. C. 
オプションCも有効な方法ですが、共有設定ファイルに定義済みのコンテキストがあるのとは対照的に、開発者は切り替えが必要なたびにコマンドを実行する必要があるかもしれません。
D. 選択肢AとDは、Bに比べて利便性と柔軟性に劣ります。
正解
B. 設定ファイルでクラスタ、ユーザー、コンテキストを定義することで、開発者は適切なパーミッションが設定されていることを前提に、kubectl config use-contextを使用して異なる環境をすばやく切り替えることができます。

適切な詳細を含む設定ファイルを共有することで、開発者は複数のクラスタへの接続をより簡単に管理できるようになります。

異なるクラスタにコンテキストを定義することで、開発者は単純なコマンドでクラスタ間を素早く切り替えることができ、クラスタを定期的に切り替える必要があるという要件に沿う。
Links:

https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/
</div></details>

## Q. 2-10
あなたは大企業の開発者です。あなたのチームはソースコード管理（SCM）にGitを使っています。Google が推奨するベストプラクティスに従ってコードを管理し、ソフトウェアの納品率を上げたいと考えています。

あなたのチームはどのSCMプロセスを使うべきでしょうか？
1. 各開発者は、各製品のリリース前にコードをメインブランチにコミットし、テストを実施し、統合の問題が検出された場合はロールバックします。
2. 各開発者グループがリポジトリをコピーし、自分のリポジトリに変更をコミットし、各製品リリースの前にメインリポジトリにコードをマージする。
3. 各開発者は自分の作業用のブランチを作成し、そのブランチに変更をコミットし、毎日そのコードをメインブランチにマージします。
4. 各開発者グループは、自分たちの作業用にメインブランチからフィーチャーブランチを作成し、自分たちのブランチに変更をコミットし、変更諮問委員会が承認した後にコードをメインブランチにマージする。
<details><div>
    答え：3
説明
不正解
A. 製品リリースの直前まで待って統合テストを行うと、予期せぬ重大な問題が発生する可能性があるため、リスクが高い。
B. 各開発者グループがリポジトリをコピーし、各自のリポジトリに変更をコミットし、各自のコードをメインリポジトリにマージする。
D. 変更は外部の変更諮問委員会の承認を待つことになるため、統合が遅れ、継続的インテグレーションを真に受け入れることにはなりません。
正解
C. このアプローチによって、次のことが保証されます：

頻繁な統合： 毎日コードをメインブランチにマージすることで、統合に関する問題を早期に発見し修正することができます。

短命なブランチ： 開発者は機能の小さな塊に取り組むため、変更の管理がシンプルになり、マージ時にコンフリクトが発生する可能性が最小限になります。

継続的なフィードバックループ： 継続的インテグレーションでは通常、新しいコードを変更するたびに自動テストを実行します。定期的にメインブランチにマージすることで、開発者は変更に対するフィードバックを即座に得ることができ、問題に迅速に対処することができます。

継続的インテグレーションで短期間のフィーチャーブランチを使用することは、最新のソフトウェア開発の基本原則です。これはDevOps文化を支えるものであり、小規模でインクリメンタルな変更を効率的かつ確実にリリースすることに重点を置いている。

Links:
https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture
https://www.atlassian.com/git/tutorials/comparing-workflows
</div></details>

## Q. 2-11
最近、OpenTelemetry で新しいアプリケーションをインスツルメンテーションし、アプリケーションのリクエストのレイテンシを Trace でチェックしたいとします。特定のリクエストが常にトレースされるようにしたい。

どうすればいいでしょうか？
1. リクエストにX-Cloud-Trace-Contextヘッダを適切なパラメータとともに追加します。
2. 開発プロジェクトからこのタイプのリクエストを繰り返し送信するカスタムスクリプトを記述します。
3. Trace API を使用して、カスタム属性をトレースに適用する。
4. 10 分待ってから、Trace がそれらのタイプのリクエストを自動的に捕捉することを確認してください。
<details><div>
    答え：1
説明
不正解です：
B. カスタムスクリプトを書いてリクエストを繰り返し送信しても、 オプション A のように特定のトレースヘッダを組み込まない限り、 特定のリクエストが常にトレースされるわけではありません。
C. Trace API を使用してカスタム属性をトレースに適用すると、 トレースに注釈をつけることができますが、 特定のリクエストをトレースするかどうかを直接制御することはできません。
D. トレースはサンプリングやその他の設定に依存している可能性があるため、一定時間待っても特定のリクエストが常にトレースされるとは限りません。
正解
A. このヘッダーは、HTTPリクエストのトレースの動作を制御することができます。このヘッダーに適切なパラメーターを含めることで、特定のリクエストを確実にトレースすることができます。
Links:

https://cloud.google.com/trace/docs/setup#force-trace
</div></details>

## Q. 2-12
Google Kubernetes Engineの導入と、VS CodeやIntelliJを含む開発環境との統合を促進するために、開発者ツールを評価しています。

あなたは何をすべきでしょうか？
1. アプリケーションの開発には Cloud Code を使用します。
2. コードと設定ファイルを編集するには、Cloud Shell統合コードエディタを使用します。
3. Cloud Notebook インスタンスを使用して、データを取り込んで処理し、モデルをデプロイします。
4. Cloud Shellを使用して、コマンドラインからインフラストラクチャとアプリケーションを管理する。
<details><div>
    答え：1
説明
不正解
B. Cloud Shell 統合コード エディタを使用してコードと構成ファイルを編集できますが、GKE 開発用に特別に調整されているわけではなく、VS Code や IntelliJ と直接統合することはできません。
C. Cloud Notebookインスタンスは、Kubernetesの開発やデプロイよりも、データサイエンス、機械学習、アナリティクスのタスクに重点を置いている。
D. Cloud Shell は、コマンドラインからインフラストラクチャとアプリケーションを管理するために使用できますが、Cloud Code のような GKE との統合開発環境エクスペリエンスを提供しません。
正解
A. Cloud Codeは、VS CodeとIntelliJのためのプラグインのセットであり、KubernetesとGoogle Cloudで作業するための統合開発エクスペリエンスを提供します。このプラグインには、インタラクティブなクラスタとリソースの管理、ワンクリックでのKubernetesクラスタの作成、組み込みのデバッグと診断などの機能が含まれています。また、KubernetesとGoogle Cloud SDKを使用したアプリケーションの迅速なデプロイとデバッグもサポートする。さらにCloud Codeでは、開発者はアプリケーションのデプロイやデバッグ、リソースの管理、ローカル開発環境の実行などのタスクを簡単に実行できる。Cloud Codeは、KubernetesとGoogle Cloudの開発プロセスを効率化したいチームにとって最適なツールだ。
Links:

https://cloud.google.com/code
</div></details>

## Q. 2-13
Google Kubernetes Engine（GKE）への新しいコンテナイメージのデプロイを自動化するために、Cloud Buildを使用して継続的インテグレーションパイプラインを構成しています。パイプラインはソースコードからアプリケーションをビルドし、単体テストと統合テストを別々のステップで実行し、コンテナを Container Registry にプッシュします。アプリケーションはPythonウェブサーバ上で実行される。
Dockerfileは以下の通りです：

FROM python:3.7-alpine
FROM python:3.7-alpine
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
CMD [ "gunicorn", "-w 4", "main:app" ]。

Cloud Buildの実行に予想以上に時間がかかっていることに気づきました。ビルド時間を短縮したい。どうしますか？(選択肢を2つ選んでください。）
1. Cloud Buildの実行には、CPUが高い仮想マシン（VM）サイズを選択します。
2. VPC内のCompute Engine VM上にContainer Registryをデプロイし、最終イメージを保存するために使用します。
3. ビルド設定ファイルの--cache-from引数を使用して、後続のビルド用にDockerイメージをキャッシュします。
4. Dockerfileのベースイメージをubuntu:latestに変更し、パッケージマネージャユーティリティを使ってPython 3.7をインストールする。
5. アプリケーションのソースコードをクラウドストレージに保存し、gsutil を使用してソースコードをダウンロードするようにパイプラインを設定します。
<details><div>
    答え：1,3
説明
不正解
B. VM上のContainer Registryは、ビルドをスピードアップしません。
D. ubuntuコンテナイメージはpython:3.7-alpineイメージよりかなり大きくなります。
E. アプリケーション・ソース・コードをクラウド・ストレージに保存しても、アプリケーションのビルド時間は短縮されません。
正解
A. C. 
CPUの高い仮想マシンタイプはビルド速度を向上させることができるため、Aが正しい。
Cは、テストやレジストリへのプッシュのために、後続のステップで同じコンテナが使用されるため、正しい。
Links:

https://cloud.google.com/cloud-build/docs/speeding-up-builds

https://cloud.google.com/build/docs/optimize-builds/increase-vcpu-for-builds
</div></details>

## Q. 2-14
あなたは、エンドユーザーからのリクエストを処理するアプリケーションを開発しています。アプリケーションから呼び出されるクラウド関数を保護し、許可されたエンドユーザーがアプリケーション経由で関数に認証できるようにする一方で、許可されていないユーザーからのアクセスを制限する必要があります。Googleサインインをソリューションの一部として統合し、Googleが推奨するベストプラクティスに従いたいと考えています。

どうすればよいでしょうか？
1. ソースコードリポジトリからデプロイし、ユーザーにroles/cloudfunctions.viewerロールを付与します。
2. ソースコードリポジトリからデプロイし、ユーザーにroles/cloudfunctions.invokerロールを付与します。
3. gcloudを使用してローカルマシンからデプロイし、ユーザーにroles/cloudfunctions.adminロールを付与します。
4. gcloudを使用してローカルマシンからデプロイし、ユーザーにroles/cloudfunctions.developerロールを付与します。
<details><div>
    答え：2
説明
不正解
A. このロールは、ユーザーに関数を表示する権限だけを与え、関数を呼び出す権限は与えません。要件は、許可されたユーザが認証して関数を起動できるようにすることなので、このロールは基準を満たしていません。
C. このロールは、関数を削除または変更する能力を含め、関数に対する管理者権限をユーザーに付与します。これは、単にエンドユーザーが関数を呼び出すことができるようにしたい、説明したシナリオには寛容すぎる可能性があります。
D. このロールは、Cloud Functionsを開発およびデプロイする権限を付与しますが、繰り返しますが、認証して特定の関数を呼び出す必要があるだけのエンドユーザーに与えたいものではありません。
正解
B. これにより、roles/cloudfunctions.invokerロールが付与され、権限のあるユーザが関数を呼び出すために必要な正確なパーミッションが提供されます。
Links:

https://cloud.google.com/functions/docs/securing/authenticating#authenticating_function_to_function_calls
</div></details>

## Q. 2-15
アプリケーションは顧客のコンテンツをCloud Storageバケットに保存し、各オブジェクトは顧客の暗号化キーで暗号化されます。Cloud Storageの各オブジェクトのキーは、顧客がアプリケーションに入力します。あなたは、アプリケーションがCloud Storageからオブジェクトを読み取るときにHTTP 4xxエラーを受け取っていることに気づきました。

このエラーの原因として何が考えられますか？
1. あなたは顧客のbase64エンコードされたキーでオブジェクトの読み取り操作を試みました。
2. あなたは暗号化キーのBase64エンコードされたSHA256ハッシュなしで読み取り操作を試みました。
3. 顧客によって指定された暗号化アルゴリズムを、読み取り操作の際に入力した。
4. 顧客の鍵のSHA256ハッシュをbase64エンコードしたオブジェクトに対して読み取り操作を試みた。
<details><div>
    答え：2
説明
不正解
A. これが読み取り操作を実行する正しい方法です。
C. 使用されたアルゴリズムは、このコンテキストでは通常4xxエラーを引き起こさない。
D. この説明は、クラウド・ストレージにおけるCSEK(Customer-Supplied Encryption Keys)の期待される動作とは一致しません。
正解です：
B. CSEK（Customer-Supplied Encryption Keys）を使用する場合、リクエストに暗号化キーのSHA256ハッシュを含める必要があります。ハッシュを含めないとHTTP 4xxエラーになります。これは正しいオプションです。
Links:

https://cloud.google.com/storage/docs/encryption/customer-supplied-keys#response
</div></details>

## Q. 2-16
あなたは、クライアントが特定の期間、あなたのウェブサイトからファイルをダウンロードできるようにするアプリケーションを開発しています。Googleが推奨するベストプラクティスに従いつつ、このタスクを完了するためにアプリケーションをどのように設計すべきでしょうか？
1. ファイルを電子メールの添付ファイルとしてクライアントに送信するようにアプリケーションを設定する。
2. ファイルにクラウドストレージ署名付きURLを生成して割り当てます。そのURLをクライアントがダウンロードできるようにする。
3. 有効期限を指定した一時的なクラウドストレージバケットを作成し、そのバケットにダウンロード権限を与えます。ファイルをコピーし、クライアントに送信する。
4. 有効期限を指定してHTTPクッキーを生成する。時間が有効であれば、Cloud Storageバケットからファイルをコピーし、クライアントがダウンロードできるようにする。
<details><div>
    答え：2
説明
不正解
A. 電子メールの添付ファイルとしてファイルを送信することは、一時的なファイルアクセスのためのスケーラブルで安全なソリューションではなく、電子メールのサイズ制限やフィルタリングの問題につながる可能性があります。
C. 期限を指定して一時的なCloud Storageバケットを作成するのは、不必要に複雑です。多数のバケットを管理することになるかもしれませんし、個々のファイルの有効期限を処理する簡単な方法を提供しません。
D. 有効期限付きのHTTPクッキーを生成することは、本質的にクラウド・ストレージと結びついていません。また、クラウド・ストレージ・バケットからファイルをコピーすることは、署名付きURLのようなクラウド・ストレージの組み込みの安全な機能を活用することなく、複雑さを追加します。
正解
B. クラウドストレージでファイルへの一時的なアクセスを提供するための推奨アプローチは、署名付きURLを生成することです。このURLはクライアントにファイルをダウンロードするための一時的なアクセスを与え、指定された期間が経過すると失効します。
Links:

https://cloud.google.com/storage/docs/access-control/signed-urls
</div></details>

## Q. 2-17
あなたは、給与計算用の社内アプリケーションを開発している開発者です。あなたは、従業員がタイムシートを提出し、いくつかのステップが開始されるアプリケーションのコンポーネントを構築しています：

- タイムシートが提出されたことを通知するメールが、従業員とマネージャーに送信されます。
- タイムシートがベンダーのAPIの給与処理に送信されます。
- タイムシートがデータウェアハウスに送信され、人員計画のために使用される。

これらのステップは互いに依存しておらず、どの順番で完了してもよい。新たなステップが検討されており、異なる開発チームによって実装される予定である。各開発チームは、それぞれのステップに固有のエラー処理を実装します。

あなたは何をすべきでしょうか？
1. 必要なアクションを完了するために対応するダウンストリームシステムを呼び出すクラウド関数を、ステップごとにデプロイします。
2. ステップごとにPub/Subトピックを作成する。下流の開発チームごとにサブスクリプションを作成し、そのステップのトピックをサブスクライブします。
3. タイムシート提出用のPub/Subトピックを作成する。各下流開発チームがトピックをサブスクライブするためのサブスクリプションを作成します。
4. Google Kubernetes Engineにデプロイされたタイムシートマイクロサービスを作成する。マイクロサービスはダウンストリームの各ステップを呼び出し、成功した応答を待ってから次のステップを呼び出します。
<details><div>
    答え：3
説明
不正解
A. 各ステップにクラウド関数をデプロイすると、密結合になる可能性があり、複数のクラウド関数を管理するのは面倒です。また、既存のシステムを変更することなく新しいステップに対応する明確な方法も提供しない。
B. 各ステップに個別の Pub/Sub トピックを作成すると、1 つのワークフローに対して複数のトピックを管理する必要があり、新しいステップを追加するには新しいトピックを作成する必要があるため、不要な複雑さが生じます。
D. 各ステップからの正常な応答を待ってから処理を進めるタイムシートマイクロサービスを作成することは、ステップを任意の順序で完了させることができるという要件に沿ったものではありません。このアプローチは逐次処理を導入し、ステップの1つが失敗した場合に単一障害点となり、プロセス全体に影響を与える可能性があります。
正解
C. が最も適切な選択です。これは、各ステップを異なるチームによって独立して実装し、そのステップに固有のエラー処理を行うことを可能にします。また、Pub/Subは並列処理を可能にするので、ステップを互いに待つことなく同時に処理することができます。さらに、既存のアーキテクチャを変更することなく新しいサブスクリプションを追加できるため、新しいステップの追加も簡単です。

リンク

https://cloud.google.com/pubsub/docs/overview</div></details>

## Q. 2-18
あなたはシンプルなHTMLアプリケーションをインターネット上で公開することを計画しています。このサイトは、あなたのアプリケーションの FAQ に関する情報を保持します。アプリケーションは静的で、画像、HTML、CSS、Javascript を含んでいます。あなたは、できるだけ少ないステップで、このアプリケーションをインターネット上で利用できるようにしたいと考えています。

あなたは何をすべきでしょうか？
1. アプリケーションをCloud Storageにアップロードする。
2. アプリケーションをApp Engine環境にアップロードします。
3. Apache WebサーバーがインストールされたCompute Engineインスタンスを作成する。アプリケーションをホストするためにApacheウェブサーバーを設定する。
4. まずアプリケーションをコンテナ化する。このコンテナをGoogle Kubernetes Engine（GKE）にデプロイし、アプリケーションをホストするGKEポッドに外部IPアドレスを割り当てる。
<details><div>
    答え：1
説明
不正解
B. App Engineは確かにWebアプリケーションをホストできますが、サーバー側の処理を必要とする、より動的なアプリケーション向けに設計されています。単純な静的ウェブサイトの場合、これは複雑になりすぎ、不必要なコストと管理オーバーヘッドにつながります。
C. このオプションは確かに実現可能だが、仮想マシンとウェブサーバーの設定とメンテナンスを手動で行う必要がある。単純に静的ファイルをクラウドストレージにホストするのに比べて、より複雑で時間がかかる。
D. このアプローチもまたより複雑で、オーケストレーションとスケーリングを必要とするアプリケーションに適している。単純な静的Webサイトの場合、アプリケーションをコンテナ化してKubernetesクラスタを管理するのはやりすぎで、追加のコストと複雑さが発生します。
正解
A. Cloud Storageは、画像、HTML、CSS、JavaScriptを含む静的ウェブサイトをホストするシンプルな方法を提供するので、正解です。ユーザーは、静的ファイルをクラウド・ストレージに簡単にアップロードでき、最小限の構成でインターネット上に提供できます。クラウド・ストレージは高い可用性と信頼性を提供し、高速で安全なユーザー体験を保証します。

そのため、選択肢Aは、問題で説明した静的HTMLアプリケーションをインターネット上で利用可能にするための、最もシンプルで、費用対効果が高く、効率的な方法として際立っています。

リンク

https://cloud.google.com/storage/docs/hosting-static-website

</div></details>

## Q. 2-19
低レベルのLinux設定ファイルにタイプミスがあり、Compute Engineインスタンスが通常のランレベルで起動できない。あなたは今日Compute Engineインスタンスを作成したばかりで、ファイルを微調整する以外に他のメンテナンスを行っていません。

このエラーをどのように修正すべきでしょうか？
1. scpを使用してファイルをダウンロードし、ファイルを変更し、変更したバージョンをアップロードします。
2. SSHでCompute Engineインスタンスに設定、ログインし、ファイルを変更する。
3. シリアルポートからCompute Engineインスタンスに設定、ログインし、ファイルを変更する。
4. リモートデスクトップクライアントを使用してCompute Engineインスタンスを構成し、ログインし、ファイルを変更する。
<details><div>
    答え：3
説明
不正解
A. 設定エラーのためにインスタンスが正しく起動できないので、標準的なSSHおよびSCP（Secure Copy Protocol）アクセスはおそらく利用できません。この状態では、SCPを使用してファイルをダウンロードまたはアップロードすることはできません。
B. この場合も、インスタンスは通常のランレベルにブートしていないため、SSHアクセスはおそらく無効になり、この方法でログインしてファイルを変更することはできません。
D. リモートデスクトップアクセスは、インスタンスが正しく起動し、必要なサービスが実行されていることにも依存します。インスタンスが正常な状態に起動していないため、リモートデスクトップクライアントを使用して接続してファイルを変更することはできません。
正解
C. Google Compute Engineは、期待通りに動作しないインスタンスのトラブルシューティングのために、シリアルコンソール出力へのアクセスを提供します。インスタンスが正しく起動しない場合、シリアルコンソールを使用して、物理マシンのようにインスタンスと直接対話することができます。これにより、低レベルの設定ファイルを変更し、エラーを修正することができます。

リンク

https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console

</div></details>

## Q. 2-20
あなたの組織は最近、レガシー・アプリケーションをGoogle Kubernetes Engineにリプラットフォームする取り組みを始めました。モノリシックなアプリケーションをマイクロサービスに分解する必要があります。複数のインスタンスは、共有ファイルシステム上に保存されている設定ファイルへの読み取りと書き込みのアクセス権を持っています。この移行管理に必要な労力を最小限に抑え、アプリケーションコードの書き換えを避けたい。

アプリケーション・コードの書き換えは避けたい。
1. 新しいCloud Storageバケットを作成し、コンテナ内にFUSE経由でマウントします。
2. 新しい永続ディスクを作成し、そのボリュームを共有PersistentVolumeとしてマウントします。
3. 新しいFilestoreインスタンスを作成し、ボリュームをNFS PersistentVolumeとしてマウントします。
4. 新しいConfigMapとvolumeMountを作成して、構成ファイルの内容を保存します。
<details><div>
    答え：3
説明
不正解
A. Cloud Storageはファイルの共有に使用できますが、FUSEを使用してコンテナ内にマウントすると待ち時間が発生する可能性があり、共有ファイルシステムが必要な場合は通常推奨されません。また、FUSEはPOSIX準拠のファイルシステムを提供しないため、複数のインスタンスからの読み取り/書き込み操作に対応する際に不整合が生じる可能性があります。
B. GKE の永続ディスクは、複数のインスタンスが同時に読み書きアクセスでマウントするようには設計されていません。そうしようとすると、データの破損やその他の問題が発生する可能性があります。
D. ConfigMapは、コンテナ・イメージから切り離された方法で構成データを管理するためのものです。ConfigMapは読み取り専用でマウントされるため、読み取りおよび書き込みアクセスによる共有ファイルシステムの使用には適していません。
正解
C. 対照的に、オプションCは、マネージドファイルストレージサービスであるFilestoreを活用して、KubernetesのPersistentVolumeサブシステムと完全に互換性のあるNFS（ネットワークファイルシステム）を作成します。これは、複数のインスタンスが読み取りと書き込みアクセスでマウントできる共有ファイルシステムを提供し、アプリケーションコードを書き換えることなく要件に適合する。

リンク
https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
https://cloud.google.com/filestore/docs/accessing-fileshares
https://cloud.google.com/storage/docs/gcs-fuse
</div></details>

## Q. 2-21
あなたのチームはGoogle Kubernetes Engine上で動作するサービスを開発しており、Googleが推奨するプラクティスを使用してログデータを標準化する必要があります。

最も効率的な方法でデータをより有用にするために取るべき2つのステップは何ですか？(2つの選択肢を選んでください)
1. アプリケーション・ログをBigQueryに集約してエクスポートし、ログ分析を容易にします。
2. アプリケーション・ログの集約エクスポートをクラウド・ストレージに作成し、ログ分析を容易にします。
3. ログ出力を標準出力（stdout）に単一行JSONとして書き込み、構造化ログとしてCloud Loggingに取り込む。
4. Cloud Loggingに構造化ログを書き込むために、アプリケーションコードでLogging APIを使用することを義務付ける。
5. Pub/Sub APIを使用して構造化データをPub/Subに書き込むことを義務付け、Dataflowストリーミングパイプラインを作成してログを正規化し、分析のためにBigQueryに書き込む。
<details><div>
    答え：3,4
説明
不正解
A. B. E. 
他の選択肢（A,B,E）は、特定の文脈では有効であるが、ログデータの効率的な標準化に直接関係しないか、不必要な複雑さをもたらす可能性がある（例えば、選択肢EはPub/SubやDataflowのような追加のコンポーネントを導入する）。選択肢 A と B は、より広範なログ戦略の一部かもしれませんが、システム内でログデータを標準化し利用する最も効率的な方法であるとは限りません。
正解
C. D. 
KubernetesとGoogle Cloudで作業するためのベストプラクティスに基づくと、オプションCとDは、通常、ログデータの効率的な標準化と構造化に沿ったステップになります：
C. C. ログ出力を単一行のJSONとして標準出力（stdout）に書き込むことで、KubernetesとCloud Loggingはログを構造化されたデータとして認識し、扱うことができます。この標準化により、ログの取り込みプロセスが簡素化され、下流の分析やクエリが容易になります。
D. Logging APIを直接使用して構造化ログを記述することで、ログエントリが確実にフォーマットされ、Cloud Loggingのプラクティスと一致する方法で処理されます。構造化されたログは、ログのクエリーと分析を容易にします。
リンク
https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#best_practices
</div></details>

## Q. 2-22
あなたの会社の製品チームは、Google Kubernetes Engine (GKE)クラスターで稼働するステートレス分散サービスをオートスケールさせるという、顧客からの要求に基づく新しい要件を持っています。あなたは、この機能が2週間後に本番稼働するため、変更を最小限に抑えるソリューションを見つけたいと考えています。

あなたは何をすべきでしょうか？
1. Vertical Pod Autoscalerをデプロイし、CPU負荷に基づいてスケーリングします。
2. Vertical Pod Autoscalerをデプロイし、カスタムメトリックに基づいてスケーリングする。
3. 水平ポッドオートスケーラーをデプロイし、CPU負荷に基づいてスケーリングします。
4. 水平Pod Autoscalerをデプロイし、カスタムメトリックに基づいてスケーリングする。
<details><div>
    答え：3
説明
不正解
A. Vertical Pod Autoscalerをデプロイすると、既存のPodのサイズを変更することになり、スケールアウトまたはスケールインによって変動する負荷に対応する必要があるステートレス分散サービスには理想的ではありません。
B. オプションAと同様に、カスタムメトリックでVertical Pod Autoscalerを使用することになるが、水平スケーリングが必要なステートレスサービスには最適ではないかもしれない。
D. このオプションは正確な要件によっては有効なアプローチかもしれませんが、CPU負荷（オプションC）に基づくスケーリングは、ステートレスアプリケーションの自動スケーリングではより一般的なメトリックです。カスタムメトリックは、追加の設定を必要とする可能性があり、2週間の期限で変更を最小限に抑えようとする場合、最良の選択肢ではないかもしれません。
正解
C. このシナリオでは、ステートレスで分散したサービスのオートスケールに関する問題です。通常、負荷に応じてスケールアウト（インスタンスの追加）またはスケールイン（インスタンスの削除）を行う必要があります。需要に応じてスケールする必要があるステートレス・サービスの正しいアプローチは、CPU負荷などのメトリクスに基づいてサービスのインスタンス（ポッド）を追加または削除するHPA（Horizontal Pod Autoscaler）を使用することです。これは、未知の新しい要件を考慮すると、オートスケールのための最もシンプルでエントリーレベルのソリューションであるため、正しい選択である。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling
https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics
</div></details>

## Q. 2-23
あなたは最近、Google Kubernetes Engine（GKE）にGoアプリケーションをデプロイしました。運用チームは、運用トラフィックが少ないときでもアプリケーションのCPU使用率が高いことに気づきました。運用チームは、アプリケーションのCPUリソース消費を最適化するようあなたに依頼しました。どの Go 関数が最も大量の CPU を消費しているかを特定する必要があります。

あなたは何をすべきですか？
1. GKEクラスタにFluent Bitデーモンセットをデプロイして、Cloud Loggingにデータを記録します。ログを分析して、アプリケーション・コードのパフォーマンスに関する洞察を得ます。
2. アプリケーションの CPU パフォーマンス・メトリクスを評価するために、Cloud Monitoring でカスタム・ダッシュボードを作成します。
3. SSH を使用して GKE ノードに接続します。シェルで top コマンドを実行して、アプリケーションの CPU 使用率を抽出します。
4. プロファイリング・データを取得するようにGoアプリケーションを修正します。Profilerのフレーム・グラフを使用して、アプリケーションのCPUメトリクスを分析します。
<details><div>
    答え：4
説明
不正解
A. データをロギングするためにFluent Bitデーモンセットをデプロイすると、一般的なログデータをキャプチャしてCloud Loggingに送信することができますが、Goアプリケーションのパフォーマンスを分析するために必要な関数レベルのCPU消費の詳細を具体的に提供することはできません。
B. Cloud Monitoring でカスタム・ダッシュボードを作成すると、アプリケーション・レベルで CPU パフォーマンス・メトリクスを監視できます。しかし、CPUを消費している特定のGo関数に関する詳細な洞察は得られません。
C. SSH を使用して GKE ノードに接続し、top コマンドを実行すると、さまざまなプロセスの CPU 使用率に関する情報が得られます。しかし、このアプローチでは、特定のGo関数を掘り下げて、どれが最もCPUを消費しているかを判断することはできません。
正解
D. プロファイリングデータを取得するようにGoアプリケーションを修正するのが、ここでの正しいアプローチです。Goにはプロファイリングのサポートが組み込まれており、関数レベルで詳細なCPUメトリクスを取得できます。Profilerのようなツールのフレームグラフを使ってプロファイリング・データを分析することで、どのGo関数が最もCPUを消費しているかを正確に判断し、最適化の対象とすることができます。この方法によって、観測された問題に特化した、集中的かつ効果的な最適化作業が可能になります。
リンク
https://cloud.google.com/profiler/docs
https://cloud.google.com/profiler/docs/about-profiler
</div></details>

## Q. 2-24
あなたはGoogle Kubernetes Engineクラスタにデプロイされるマイクロサービスベースのアプリケーションを開発しています。このアプリケーションはSpannerデータベースを読み書きする必要があります。あなたは、コードの変更を最小限に抑えながら、セキュリティのベストプラクティスに従いたいと考えています。

Spanner認証情報を取得するために、アプリケーションをどのように構成すべきでしょうか？
1. 適切なサービスアカウントを構成し、Workload Identity を使用してポッドを実行する。
2. アプリケーションの認証情報をKubernetesシークレットとして保存し、環境変数として公開します。
3. 適切なルーティングルールを設定し、VPCネイティブクラスタを使用してデータベースに直接接続する。
4. クラウド鍵管理サービスを使用してアプリケーション認証情報を保存し、データベース接続が行われるたびにそれを取得する。
<details><div>
    答え：1
説明
不正解
B. このオプションは機能するかもしれませんが、Workload Identityを使用するほど安全ではないと考えられます。Kubernetesシークレットはクレデンシャルをハードコーディングするよりは良いが、クラスタ内で公開される可能性がある。Workload Identityは、これらの権限をより安全に管理する方法です。
C. これでは資格情報の問題にまったく対処できません。ルーティングルールを構成し、VPCネイティブクラスタを使用することは、優れたネットワーク設計の一部になり得ますが、Spannerデータベースにアクセスするための資格情報を安全に取り扱うという問題を解決するものではありません。
D. このアプローチは不必要な複雑さを追加することになる。クラウド鍵管理サービス（KMS）は暗号化鍵を管理するための強力なツールですが、このユースケースでデータベースの資格情報の保存と取得に使用するのはやりすぎです。Workload Identityは、問題で説明されているユースケースのために特別に設計されており、より簡単なソリューションを提供します。
正解
A. GKE 上で実行されるアプリケーションは、Compute Engine API、BigQuery Storage API、Machine Learning API などの Google Cloud API にアクセスする必要があります。Workload Identity を使用すると、GKE クラスタ内の Kubernetes サービスアカウントを IAM サービスアカウントとして動作させることができます。設定された Kubernetes サービスアカウントを使用する Pod は、Google Cloud API にアクセスするときに自動的に IAM サービスアカウントとして認証されます。Workload Identityを使用すると、クラスタ内の各アプリケーションに個別のきめ細かいIDと権限を割り当てることができます。Google では、可能な限りサービスアカウントと Workload Identity を使用することを推奨しています。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is
https://kubernetes.io/docs/concepts/configuration/secret/#alternatives-to-secrets
</div></details>

## Q. 2-25
Google Cloudのさまざまなプロジェクトでコンテナを作成し、実行しています。開発中のアプリケーションは、Google Kubernetes Engine (GKE) 内から Google Cloud サービスにアクセスする必要があります。

あなたは何をすべきでしょうか？
1. GKEノードにGoogleサービスアカウントを割り当てます。
2. Workload Identityを使用してポッドを実行するには、Googleサービスアカウントを使用します。
3. Googleサービスアカウントの認証情報をKubernetesシークレットとして保存する。
4. GKE ロールベースアクセス制御（RBAC）で Google サービスアカウントを使用する。
<details><div>
    答え：2
説明
不正解
A. このアプローチでは、パーミッションを個々のポッドではなくGKEノードに関連付けるため、柔軟性が低くなり、これらのノードで実行されているすべてのポッドに過度に広範なパーミッションを付与する可能性があります。最小特権の原則に従わない。
C. 認証情報をKubernetes Secretとして保存すると、クラスタ内で機密情報が公開されます。ポッドが侵害された場合、これらの認証情報の漏洩につながる可能性があります。このアプローチは、Workload Identityを使用する場合と比較して、セキュリティのベストプラクティスに準拠していません。
D. GKE RBACはクラスタ内のアクセスを制御し、Googleクラウドサービスへの認証は制御しません。このアプローチはGoogle Cloud APIに直接アクセスする方法を提供しないため、この特定の要件に適したソリューションではありません。
正解
B. Workload Identityは、KubernetesサービスアカウントをIAMサービスアカウントとして機能させる。これにより、アプリケーションはクレデンシャルを直接扱うことなくGoogle Cloudサービスに認証することができ、ベストプラクティスに合致し、きめ細かなアクセス制御を提供できる。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is
</div></details>

## Q. 2-26
あなたは複数のマイクロサービスで構成されるアプリケーションを設計しています。各マイクロサービスには独自のRESTful APIがあり、個別のKubernetesサービスとしてデプロイされます。あなたは、APIに変更があったときにこれらのAPIのコンシューマが影響を受けないようにし、またAPIの新しいバージョンがリリースされたときにサードパーティのシステムが中断されないようにしたい。

Googleが推奨するベストプラクティスに従って、アプリケーションへの接続をどのように設定すべきでしょうか？
1. APIのURLを使って適切なバックエンドにリクエストをルーティングするIngressを使う。
2. サービスディスカバリーシステムを活用し、リクエストで指定されたバックエンドに接続します。
3. 複数のクラスタを使用し、DNSエントリを利用してリクエストを別々のバージョンのバックエンドにルーティングする。
4. 複数のバージョンを同じサービスにまとめ、POSTリクエストでAPIバージョンを指定する。
<details><div>
    答え：1
説明
不正解
B. サービスディスカバリーはクラスタ内でのみ機能するため、外部クライアントは使用できません。
C. 複数のクラスタを使用するのはやりすぎです。同じサービスの複数のバージョンを1つのクラスタ内にデプロイできます。
D. リクエストボディでAPIバージョンを渡すことは、RESTのベストプラクティスではありません。
正解
A. ベストプラクティスは、/v1/foo、/v2/fooのように、URLパスでAPIのバージョンを渡すことです。このアプローチを使用すると、Ingressリソースを使用して、GKEクラスタ内の適切なバックエンドサービスにリクエストをルーティングできます。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/ingress#deprecated_annotation
https://cloud.google.com/kubernetes-engine/docs/concepts/ingress#features_of_https_load_balancing
</div></details>

## Q. 2-27
MySQLデータベースをGoogle CloudのマネージドCloud SQLデータベースに移行する予定です。このCloud SQLインスタンスに接続するCompute Engine仮想マシンインスタンスがあります。Compute EngineインスタンスがCloud SQLにアクセスするためのIPをホワイトリストに登録したくありません。

どうすればいいでしょうか？
1. Cloud SQLインスタンスのプライベートIPを有効にします。
2. Cloud SQLにアクセスするプロジェクトをホワイトリストに登録し、Compute Engineインスタンスをホワイトリストに登録されたプロジェクトに追加します。
3. Cloud SQLで、外部インスタンスからデータベースへのアクセスを許可するロールを作成し、そのロールをCompute Engineインスタンスに割り当てます。
4. あるプロジェクトにCloud SQLインスタンスを作成します。別のプロジェクトにCompute Engineインスタンスを作成します。これら2つのプロジェクト間でVPNを確立し、Cloud SQLへの内部アクセスを許可する。
<details><div>
    答え：1
説明
不正解です：
B. このオプションはホワイトリストに言及しているため無効です。
C. Cloud SQLは、この方法で外部インスタンスからのアクセスを制御するカスタムロールを作成する機能を提供していないため、このオプションは無効です。
D. このオプションは、理論的にはIPホワイトリストなしでCloud SQLとCompute Engineインスタンス間のセキュアな接続を提供できますが、プロジェクト間のVPN接続の管理など、不必要な複雑さをもたらします。プライベートIP（オプションA）を有効にすることは、よりシンプルで直接的なソリューションです。
正解
A. このページによると、Compute Engineからの接続には、'プライベートIP'、'パブリックIP'、'Cloud SQL Proxy'の3つの方法があります。Cloud SQL Proxy'オプションは回答に含まれていません。また、'Public IP'オプションはIPホワイトリストが必要です（質問によるとこれは受け入れられません）ので、唯一の有効な回答は'Private IP'です。
リンク
https://cloud.google.com/sql/docs/mysql/connect-compute-engine
https://cloud.google.com/sql/docs/mysql/connect-overview
</div></details>

## Q. 2-28
あなたはサーバーアプリケーションの負荷テストを行っています。最初の30秒の間に、以前はアクティブでなかったCloud Storageバケットが、1秒あたり2,000の書き込みリクエストと1秒あたり7,500の読み取りリクエストに対応していることを確認しました。アプリケーションは、要求が増加するにつれて、Cloud Storage JSON APIから断続的に5xxと429のHTTPレスポンスを受信するようになりました。Cloud Storage APIからの失敗した応答を減らしたい。

何をすべきでしょうか？
1. アップロードを多数の個別のストレージバケットに分散します。
2. クラウドストレージとのインターフェースにJSON APIではなくXML APIを使用する。
3. アプリケーションからアップロードを呼び出しているクライアントにHTTPレスポンスコードを戻します。
4. アプリケーションのクライアントからのアップロード速度を制限して、休止状態のバケツのピーク要求速度に徐々に到達するようにします。
<details><div>
    答え：4
説明
不正解
A. アップロードを多数の個別のストレージバケットに分散させると、失敗した応答が必ずしも減らない可能性があり、システムの複雑さが増す可能性があります。
B. JSON API の代わりに XML API を使用しても、パフォーマンスが向上するとは限らず、アプリケーションに大幅な変更が必要になる可能性がある。
C. HTTP応答コードをクライアントに戻すと、問題の根本原因に対処できない可能性があり、さらにエラーが発生する可能性があります。
正答
D. アップロードレートを制限することで、リクエストのレートを制御し、Cloud Storage APIが圧倒されるのを防ぎます。ピーク・リクエスト・レートに徐々に到達することで、システムがスケールして応答する時間を与え、応答が失敗する可能性を減らすことができる。したがって、このアプローチは観察された問題に直接対処し、与えられた選択肢の中で最良の選択である。
リンク
https://cloud.google.com/storage/docs/request-rate#ramp-up
</div></details>

## Q. 2-29
あなたの開発チームは、既存のモノリシックなアプリケーションをコンポーザブルなマイクロサービス群にリファクタリングするよう求められています。新しいアプリケーションには、どのような設計面を実装すべきでしょうか？(2つの選択肢を選んでください)
1. マイクロサービスの呼び出し元が使用するのと同じプログラミング言語でマイクロサービスのコードを開発する。
2. マイクロサービス実装とマイクロサービス呼び出し元との間でAPI契約書を作成する。
3. すべてのマイクロサービス実装とマイクロサービス呼び出し元との間で非同期通信を要求する。
4. パフォーマンス要件に対応するために、マイクロサービスの十分なインスタンスが実行されていることを確認する。
5. 現在のインターフェイスと互換性がない可能性のある将来の変更を許可するために、バージョニングスキームを実装する。
<details><div>
    答え：2,5
説明
不正解
A. 柔軟性を制限します。マイクロサービスは異なる言語で開発できます。
C. 通信は、一律の要件ではなく、特定のニーズに基づいて選択されるべきである。
D. これは基本的な設計の側面ではなく、運用上の懸念事項です。
正解
B. マイクロサービスがどのように相互作用するかを定義し、明確性を確保し、統合の問題を減らします。
E. 既存のクライアントを壊すことなく、マイクロサービスが独立して進化することを可能にする。
リンク
https://cloud.google.com/appengine/docs/standard/java/designing-microservice-api#using_strong_contracts
</div></details>

## Q. 2-30
目下の課題は、Google Kubernetes Engineクラスタ内の様々なチームのアプリケーション間でリソースを共有するためのポリシーを作成することだ。目的は、すべてのアプリケーションが円滑な運用のために必要なリソースにアクセスできることを保証することです。

この目標を達成するために、2つの具体的な対策を講じる必要があります。
1. オブジェクトの仕様でリソースの制限と要求を指定する。
2. チームごとにネームスペースを作成し、各ネームスペースにリソースのクォータを割り当てます。
3. LimitRangeを作成して、各ネームスペースのデフォルトの計算リソース要件を指定します。
4. アプリケーションごとにKubernetesサービスアカウント（KSA）を作成し、各KSAをネームスペースに割り当てます。
5. Anthosポリシーコントローラーを使用して、すべてのネームスペースにラベルアノテーションを強制します。テイントとトレレーションを使用して、ネームスペースのリソース共有を許可します。
<details><div>
    答え：2,3
説明
不正解
A. D. E. 
選択肢（A、D、E）は、説明されたコンテキストで異なるチームのアプリケーション間でリソースを共有するためのポリシーを作成するタスクに直接対応していません。
正解
B. こうすることで、チームが消費できるリソースに制限を設けることができ、1つのチームがクラスタのすべてのリソースを消費することがなくなり、すべてのチーム間でリソースが公平に共有されます。
C. LimitRangeを使用すると、特定のネームスペース内のすべてのPodに対してデフォルトの制限と要求を設定できます。また、そのネームスペース内のPodが、定義されたLimitRangeを超えるリソースを決して消費できないようにします。
リンク
https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits
https://kubernetes.io/docs/concepts/policy/resource-quotas/
https://kubernetes.io/docs/concepts/policy/limit-range/
</div></details>

## Q. 2-31
継承した App Engine 上でアプリケーションを実行しており、そのアプリケーションが安全でないバイナリを使用しているかどうか、または XSS 攻撃に対して脆弱であるかどうかを調べたいとします。

どのサービスを使うべきですか？
1. クラウド・アーマー
2. スタックドライバー・Debugger
3. クラウドセキュリティスキャナ
4. スタックドライバエラーレポート
<details><div>
    答え：3
説明
不正解
A. クラウドアーマーは、分散型サービス拒否（DDoS）保護とアプリケーションレベルの保護を提供するWebアプリケーションファイアウォール（WAF）であり、特に安全でないバイナリやXSS攻撃などの脆弱性をスキャンするようには設計されていません。
B. Stackdriver Debugger は、Google Cloud で実行されているアプリケーションのデバッグに使用され、コードの動作に関する詳細な洞察を提供しますが、特にセキュリティ・スキャン用に設計されていません。
D. Stackdriver Error Reporting は、Google Cloud 内のアプリケーションに対してリアルタイムのエラー追跡と分析を提供し、開発者がセキュリティ脆弱性ではなく、アプリケーションのエラーを特定して理解できるようにします。
正解
C. Cloud Security Scannerは、XSSを含む一般的な脆弱性について、App Engine、Compute Engine、Google Kubernetes Engineアプリケーションをスキャンするように設計されているため、このタスクの正しい選択肢です。
リンク
https://cloud.google.com/security-scanner
https://cloud.google.com/appengine/docs/standard/python/application-security
</div></details>

## Q. 2-32
AndroidアプリとiOSアプリで使用するAPIを開発中です。

HTTPSをサポートし、帯域幅コストを最小限に抑え、モバイルアプリと簡単に統合する必要がある場合、どのようなアーキテクチャが適しているでしょうか？
1. RESTful API
2. API向けMQTT
3. gRPCベースのAPI
4. SOAPベースのAPI
<details><div>
    答え：3
説明
不正解
A. HTTPS は広く使用されサポートされていますが、RESTful API は通常 JSON を使用します。
B. MQTTは、IoTアプリケーションでよく使用される軽量メッセージングプ ロトコルである。低帯域幅、高遅延のネットワーク向けに設計されているが、一般的なモバイルアプリのAPI開発には通常使用されない。
D. SOAPベースのAPI： SOAPは、構造化された情報を交換するためのプロトコルである。HTTPSをサポートできますが、他のオプションよりも冗長な傾向があり、帯域幅コストが高くなります。
正解です：
C. gRPCは、構造化データをシリアライズする手法であるプロトコル・バッファ（protobufs）を使用し、軽量かつ高速に設計されています。HTTPSをサポートし、帯域幅の面で非常に効率的であるため、モバイル・アプリケーションに適しています。
リンク
https://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them
https://grpc.io/blog/mobile-benchmarks/
</div></details>

## Q. 2-33
Cloud Buildを使用して、Cloud Source Repositoriesリポジトリへのソースコードのコミットごとに新しいDockerイメージを作成しています。アプリケーションはmasterブランチへのコミットごとにビルドされますが、自動化された方法でmasterブランチへの特定のコミットをリリースしたいとします。

どうすればいいでしょうか？
1. 新しいリリースのビルドを手動でトリガーする。
2. Gitタグパターンに基づいてビルドトリガーを作成し、新しいリリースにはGitタグ規約を使用する。
3. Gitブランチの名前パターンに基づいてビルド・トリガーを作成し、新しいリリースにはGitブランチの命名規則を使う。
4. ソースコードを別の Cloud Build トリガーで 2 つ目の Cloud Source Repositories リポジトリにコミットし、このリポジトリを新規リリースのみに使用します。
<details><div>
    答え：2
説明
不正解
A. 手動で新しいリリースのビルドをトリガーしても、必要な自動化された方法は提供されません。
C. Gitブランチ名のパターンに基づいてビルド・トリガーを作成し、新しいリリースのためにGitブランチの命名規則を使用すると、より複雑なブランチ構造になる可能性がある。これは、タグを使って特定のリリースをマークするほど明確でも簡潔でもないかもしれません。
D. 別の Cloud Build トリガーを使用してソース コードを 2 つ目の Cloud Source Repositories リポジトリにコミットすることは、より面倒なアプローチであり、リポジトリ間の重複や同期の問題につながる可能性があります。
正解
B. Git タグパターンに基づいてビルドトリガーを作成し、新規リリース用の Git タグ規約を使うことで、どのコミットをリリースするのかを指定することができます。この方法は、タグ付けされたリリースのビルドプロセスを自動化し、ベストプラクティスに沿ったものになります。
リンク
https://cloud.google.com/source-repositories/docs/integrating-with-cloud-build#create_a_build_trigger
https://docs.docker.com/docker-hub/builds/
</div></details>

## Q. 2-34
あなたは Cloud Spanner 顧客データベースのスキーマを設計しています。顧客テーブルに電話番号配列フィールドを格納し、ユーザーが電話番号で顧客を検索できるようにしたいとします。

このスキーマをどのように設計しますか？
1. Customersという名前のテーブルを作成し、顧客の電話番号を格納するArrayフィールドをテーブルに追加します。
2. Customersという名前のテーブルとPhonesという名前のテーブルを作成する。電話番号からCustomerIdを検索するために、PhonesテーブルにCustomerIdフィールドを追加します。
3. Customersという名前のテーブルを作成し、顧客の電話番号を保持するArrayフィールドをテーブルに追加します。Arrayフィールドにセカンダリインデックスを作成します。
4. Customers という名前のテーブルを親テーブルとして作成します。Phonesという名前のテーブルを作成し、このテーブルをCustomersテーブルにインターリーブします。Phonesテーブルの電話番号フィールドにインデックスを作成します。
<details><div>
    答え：4
説明
不正解
A. 配列フィールドに電話番号を格納すると、電話番号による検索を簡単にサポートできません。
B. CustomersテーブルとPhonesテーブルを別々に作成すると、電話番号による検索が可能になりますが、顧客と電話番号のリレーションシップが強い場合、ローカリティを最適化できない可能性があります。
C. Cloud SpannerはArrayフィールドのセカンダリインデックスをサポートしていないため、このオプションは実行できません。https://cloud.google.com/spanner/docs/data-types
正解
D. 
インターリーブ・テーブル構造を作成することで、CustomersとPhonesの間に親子関係が確立され、一緒にアクセスされることが多いデータのローカリティが維持されます。Phonesテーブルの電話番号フィールドにインデックスを作成することで、電話番号による効率的な検索が可能になります。
したがって、オプションDは与えられた要件に最適な設計です。
リンク
https://cloud.google.com/spanner/docs/schema-design#creating-indexes
https://cloud.google.com/spanner/docs/data-types
</div></details>

## Q. 2-35
Google Kubernetes Engineでホストされているウェブサイトの新しいヨーロッパバージョンをデプロイする必要があります。現在のウェブサイトと新しいウェブサイトは、同じHTTP(S)ロードバランサーの外部IPアドレス経由でアクセスする必要がありますが、ドメイン名は異なります。

あなたは何をすべきでしょうか？
1. 新しいドメインにマッチするホストルールで新しいIngressリソースを定義します。
2. 新しいドメインにマッチするホストルールで、既存のIngressリソースを修正する。
3. 既存のIPアドレスをloadBalancerIPとして指定して、LoadBalancerタイプの新しいサービスを作成する。
4. 新しいIngressリソースを生成し、既存のIPアドレスをkubernetes.io/ingress.global-static-ip-nameとして指定する。
<details><div>
    答え：2
説明
不正解
A. 既存のIPアドレスを指定せずに新しいIngressリソースを定義すると、通常、新しいIPアドレスを持つ新しいロードバランサーが作成されます。
C. LoadBalancerタイプの新しいサービスを作成すると、異なるドメイン名のホストルールを指定することができません。
D. 新しいIngressリソースを生成し、既存のIPアドレスを指定することは解決策のように見えるかもしれませんが、選択肢Bで説明したように、新しいIngressを作成するよりも、既存のIngressを変更する方が適しています。
正解
B. 新しいホストルールを含むように既存のIngressリソースを変更すると、同じロードバランサーとIPアドレスを使用して新しいドメイン名のトラフィックを処理できるようになります。したがって、オプションBが正しい選択です。
リンク
https://kubernetes.io/docs/concepts/services-networking/ingress/#name-based-virtual-hosting
https://cloud.google.com/kubernetes-engine/docs/tutorials/configuring-domain-name-static-ip
</div></details>

## Q. 2-36
あなたは、1つのPub/Subトピックからメッセージをサブスクライブして受信し、対応する行をデータベースに挿入するアプリケーションを設計しています。アプリケーションはLinux上で実行され、プリエンプト可能な仮想マシンを活用してコストを削減します。あなたは、グレースフル・シャットダウンを開始するシャットダウン・スクリプトを作成する必要があります。

あなたは何をすべきですか?
1. プロセス間シグナルを使用して、アプリケーション・プロセスにデータベースからの切断を通知するシャットダウン・スクリプトを記述します。
2. サインインしているすべてのユーザーに、Compute Engineインスタンスがダウンする旨のメッセージをブロードキャストし、現在の作業を保存してサインアウトするように指示するシャットダウンスクリプトを作成します。
3. アプリケーションがポーリングしている場所に、5分ごとにファイルを書き込むシャットダウンスクリプトを記述する。ファイルが読み込まれた後、アプリケーションはデータベースから切断する。
4. シャットダウンが進行中であることを知らせるメッセージをPub/Subトピックに発行するシャットダウンスクリプトを記述します。アプリケーションがメッセージを読み取ったら、データベースから切断します。
<details><div>
    答え：1
説明
不正解
B. サインインしているすべてのユーザーにメッセージをブロードキャストすることは、アプリケーションを優雅にシャットダウンし、データベースから切断するという目的とは関係ありません。
C. アプリケーションが5分ごとにポーリングするファイルを書き込むのは、特に即時のシャットダウンが必要な場合、間接的で効率が悪い方法です。
D. Pub/Subトピックへのメッセージの発行も間接的な方法であり、特に他のサブスクライバが同じトピックをリッスンしている場合は、さらに複雑な問題が発生する可能性があります。
正解
A. プロセス間シグナルを使用してアプリケーションプロセスに通知することは、グレースフル・シャットダウンを開始する一般的な方法です。これにより、アプリケーションは、開いている接続をすべて閉じ、必要なクリーンアップタスクを実行することで、シグナルに応答することができます。つまり、これはグレースフル・シャットダウンを実現する最も簡単で効果的な方法なのです。
リンク
https://cloud.google.com/compute/docs/instances/preemptible#preemption
</div></details>

## Q. 2-37
Google Kubernetes Engine（GKE）に、Pub/Subメッセージを読み込んで処理するアプリケーションをデプロイしている。各Podは1分間に固定数のメッセージを処理します。メッセージがPub/Subトピックにパブリッシュされる速度は、1日や1週間を通してかなり変化します。

タイムリーにメッセージを処理できるように、GKE デプロイメントを拡張したいとします。ワークロードを自動的に適応させるために、どの GKE 機能を使うべきですか?
1. 自動モードでの垂直ポッドオートスケーラー
2. 推奨モードのVertical Pod Autoscaler
3. 外部メトリックに基づく水平ポッドオートスケーラー
4. リソース使用率に基づく水平ポッドオートスケーラー
<details><div>
    答え：3
説明
不正解
A. AutoモードのVertical Pod Autoscalerは、既存のPodのCPUとメモリを調整しますが、外部メトリクスに基づいてPodを追加または削除することはありません。
B. 推奨モードのVertical Pod Autoscalerは、CPUとメモリの推奨設定を提供しますが、自動調整は行いません。
D. リソース使用率に基づく水平Pod Autoscalerは、CPUまたはメモリ使用率に基づいてPodをスケーリングします。
正解
C. Horizontal Pod Autoscaler（HPA）は、観察されたメトリックに応じてデプロイメント内のポッド数を自動的にスケールできるため、この作業に適したツールです。具体的には、外部メトリクス（Pub/Subキュー内の未処理メッセージ数など）を使用してスケーリングを実行します。
リンク
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
https://cloud.google.com/kubernetes-engine/docs/concepts/custom-and-external-metrics
</div></details>

## Q. 2-38
目的は、App Engineにウェブサイトをデプロイし、URL http://www.altostrat.com/ からアクセスできるようにすることです。

どのようなアクションが必要だと思いますか？
1. ウェブマスターセントラルでドメインの所有権を確認する。App Engineの正規名ghs.googlehosted.comを指すDNS CNAMEレコードを作成します。
2. Webmaster Centralでドメインの所有権を確認します。単一のグローバルApp Engine IPアドレスを指すようにAレコードを定義します。
3. dispatch.yamlでマッピングを定義して、ドメインwww.altostrat.com をApp Engineサービスに指定します。App Engineの正規名ghs.googlehosted.comを指すDNS CNAMEレコードを作成します。
4. dispatch.yamlでマッピングを定義し、ドメインwww.altostrat.com をApp Engineサービスに向けます。単一のグローバルApp Engine IPアドレスを指すように、Aレコードを定義します。
<details><div>
    答え：1
説明
不正解
B. このオプションにはドメインの所有権の確認が含まれますが、単一のグローバルIPアドレスにAレコードを定義することは、IPアドレスが変更される可能性があるため、App Engineでは推奨されません。
C. dispatch.yamlでマッピングを定義することは関連性があるように見えるかもしれませんが、単にカスタムドメインをApp Engineにマッピングする場合には必須のステップではありません。DNS CNAMEレコードの作成は正しいですが、ドメインの所有権の確認ができないため、このオプションは不完全です。
D. オプションBと同様に、これには単一のIPアドレスを指すAレコードの定義が含まれますが、これは推奨されません。また、dispatch.yamlでマッピングを定義することは、この目的には必要な手順ではありません。ドメイン所有権の確認がないため、この選択肢は正しくありません。
正解
A. このオプションは、カスタムドメインをApp Engineにマッピングする標準的な手順の概要です：
Webmaster Centralでドメインの所有権を確認します。App Engineの正式名称ghs.googlehosted.comを指すDNS CNAMEレコードを作成し、カスタムドメインの解決先をApp Engineアプリケーションにします。
リンク
https://cloud.google.com/appengine/docs/flexible/mapping-custom-domains
</div></details>

## Q. 2-39
アプリケーションはユーザーから入力を受け取り、それをユーザーの連絡先に公開します。この入力はCloud Spannerのテーブルに保存されます。あなたのアプリケーションはレイテンシに敏感で、一貫性にはあまり敏感ではありません。

このアプリケーションでは、Cloud Spannerからの読み取りをどのように実行すべきですか？
1. 読み取り専用トランザクションを実行します。
2. シングルリード方式でステールリードを実行する。
3. シングルリード方式でストロングリードを実行する。
4. 読み書きトランザクションを使用してステイルリードを実行する。
<details><div>
    答え：2
説明
不正解
A. 読み取り専用トランザクションは、特定のタイムスタンプにおけるデータベースの一貫したビューを提供します。ただし、一貫性よりも待ち時間を優先することはありません。
C. 強力な読み取りは、最新のコミット値を確実に読み取るため、一貫性は確保されるが、必ずしも低レイテンシを優先するわけではない。
D. 読み書きトランザクションは一般的に、読み取りと書き込みの両方を必要とする操作に使用されます。
正解：
B. ステイル・リードでは、古い可能性のある値を読み取ることができます。これにより、同時実行中の書き込みの完了を待たずに読み取りを進めることができるため、レイテンシが改善される可能性があります。これは、一貫性よりもレイテンシを優先するアプリケーションに沿った選択肢です。
リンク
https://cloud.google.com/spanner/docs/best-practices-gaming-database

</div></details>

## Q. 2-40
あなたはGoogle Cloud上で動作するウェブアプリケーションを開発しています。トラフィックがない日もあれば、急増する日もあります。アプリケーションを自動的にスケールアップ/スケールダウンする必要があり、アプリケーションの実行に関連するコストを最小限に抑える必要があります。

あなたは何をすべきでしょうか？
1. Firestoreをデータベースとしてアプリケーションを構築します。アプリケーションをCloud Runにデプロイします。
2. データベースとしてFirestoreを使用してアプリケーションを構築します。アプリケーションをGoogle Kubernetes Engine Standardクラスタにデプロイします。
3. データベースとしてCloud SQLを使用してアプリケーションを構築する。アプリケーションをGoogle Kubernetes Engine Autopilotクラスタにデプロイする。
4. Firestore をデータベースとしてアプリケーションを構築する。アプリケーションをオートスケーリング機能付きのCompute Engineマネージド・インスタンス・グループにデプロイする。
<details><div>
    答え：1
説明
不正解
B. GKEはゼロまでスケールしないため、Bは不正解です。
C. GKEはゼロまでスケールしないので、Cは正しくない）。また、Cloud SQLの実行にはコストがかかります。
D. Compute Engineは、ゼロスケールしないインスタンスを管理する。最低1つのインスタンスを実行する必要があります。
正解
A. Cloud Runはゼロへのスケーリングをサポートします。また、Firestoreのコストはストレージのみです。つまり、トラフィックがない場合、運用コストはゼロだ。
リンク
https://cloud.google.com/serverless-options
https://cloud.google.com/appengine/docs/the-appengine-environments
https://cloud.google.com/run
</div></details>

## Q. 2-41
開発中の Cloud Run サービスのロギングを設定しています。コンテナインスタンスは構造化されたログを標準出力（stdout）と標準エラー（stderr）ストリームに書き込みます。あなたは、自動的に作成されたリクエストログとコンテナログを関連付けたいと考えています。

どうすればよいでしょうか？
1. Cloud Traceにトレースを送信するようにアプリケーションをインスツルメンテーションします。
2. スナップショット・デバッガを使用して、各リクエストに対してランダムに生成された一意の識別子でログポイントを追加します。
3. X-Cloud-Trace-Contextヘッダー値を持つログ文にlogging.googleapis.com/traceフィールドを追加します。
4. ログ文に logging.googleapis.com/labels フィールドを追加し、各リクエストに対してランダムに生成された一意の識別子を付けます。
<details><div>
    答え：3
説明
不正解
A. このオプションはリクエストをトレースするのに役立ちますが、ログを関連付ける方法については直接言及していません。
B. このオプションは Cloud Run でのログの関連付けと直接は一致せず、特定のコード ポイントのデバッグにより関連する可能性があります。
D. このオプションは、自動的に作成される要求ログと特に相関しません。また、ランダムに生成された識別子は、望ましい相関を提供しない可能性があります。
正解
C. logging.googleapis.com/trace フィールドをログ文に追加し、X-Cloud-Trace-Context ヘッダーの値に設定することで、リクエストログとコンテナログの相関を有効にします。これにより、リクエストログとコンテナログをリンクしてトレースできるようになり、デバッグやパフォーマンス解析が容易になります。
リンク
https://cloud.google.com/run/docs/logging#correlate-logs
https://cloud.google.com/logging/docs/structured-logging
</div></details>

## Q. 2-42
トラフィックの大幅な増加に対応できるように、認証サービスからの監査イベントの取り込みを再設計する必要があります。現在、監査サービスと認証サービスは同じCompute Engine仮想マシンで実行されています。それぞれのサービスをCompute Engine VMインスタンスのプールに分割し、Pub/Subを使用して認証サービスから監査サービスにイベントを送信する予定です。

システムが大量のメッセージを処理し、効率的に拡張できるようにするには、Pub/Subトピックとサブスクリプションをどのように設定すればよいですか?
1. Pub/Subトピックを1つ作成します。プル・サブスクリプションを1つ作成します。
2. Pub/Sub トピックを 1 つ作成します。監査サービス・インスタンスごとに 1 つのプル・サブスクリプションを作成します。
3. 1つのPub/Subトピックを作成する。プッシュサブスクリプションを1つ作成する。
4. 認証サービスインスタンスごとに1つのPub/Subトピックを作成する。トピックごとに1つのプル・サブスクリプションを作成する。
5. 認証サービスインスタンスごとに1つのPub/Subトピックを作成する。トピックごとに1つのプッシュサブスクリプションを作成する。
<details><div>
    答え：1
説明
不正解
B. インスタンスごとにプル・サブスクリプションを作成すると、各サブスクリプションでメッセージが重複し、同じメッセージが複数回処理されることになります。
C. プッシュサブスクリプションを使用することもできますが、プルサブスクリプションほど大量のメッセージを処理するのに適していないかもしれません。プッシュエンドポイントの管理と再試行の処理には、さらなる複雑さとオーバーヘッドがあるかもしれません。
D. 認証インスタンスごとにトピックを作成すると、不必要に複雑になり、メッセージが監査サービスインスタンスに均等に分散されない可能性があります。これは、不均一なスケーリングと追加の管理オーバーヘッドにつながる可能性がある。
E. このオプションも、インスタンスごとにトピックとプッシュサブスクリプションを持つことによる複雑さをもたらす。さらに、プッシュサブスクリプションは大量のメッセージの処理に最適ではないため、スケーリングの問題が発生する可能性があります。
正解
A. このセットアップにより、重複することなくスケーラブルにメッセージを取り込み、処理することができます。これは柔軟で、負荷に応じて拡張でき、監査サービスが必要に応じてメッセージをプルできるようにします。
リンク
https://cloud.google.com/pubsub/docs/subscriber#push-subscription
</div></details>

## Q. 2-43
ソーシャルメディア・アプリケーションに画像アップロード機能を組み込み、ユーザーが2MBから1GBまでの画像をアップロードできるようにすることである。目的は、この機能に関連するインフラ運用のオーバーヘッドを削減することである。
1. 画像を直接受け付けるようにアプリケーションを変更し、他のユーザー情報を保存するデータベースに保存します。
2. アプリケーションを変更して、クラウド・ストレージ用の署名付きURLを作成する。これらの署名付きURLをクライアントアプリケーションに転送し、画像をクラウドストレージにアップロードする。
3. GCP上に、ユーザー画像を受け入れるためのWebサーバーをセットアップし、アップロードされたファイルを保存するためのファイルストアを作成する。ファイルストアから画像を取得するようにアプリケーションを変更する。
4. クラウドストレージにユーザーごとに個別のバケットを作成する。それぞれのバケットに書き込みアクセスを許可する別のサービスアカウントを割り当てる。ユーザー情報に基づいて、サービス・アカウントの認証情報をクライアント・アプリケーションに転送する。アプリケーションはこのサービスアカウントを使って画像をクラウドストレージにアップロードする。
<details><div>
    答え：2
説明
不正解
A. データベースに大きなファイルを保存すると、パフォーマンスと管理に重大な問題が発生する可能性があります。この方法で1GBまでのファイルを扱うと、運用が複雑になる可能性があり、通常は推奨されません。
C. このオプションでは、Webサーバーとファイルストレージを管理する必要があり、運用のオーバーヘッドが増加します。
D. このオプションは、各ユーザーに個別のバケットとサービスアカウントで不必要な複雑さを生み出し、高い運用オーバーヘッドにつながる可能性があります。
正解
B. クラウド・ストレージの強みを活かして大容量ファイルを扱い、署名付きURLを使用してクライアントが画像を直接アップロードできるようにすることで、アプリケーションのインフラストラクチャの負荷と複雑さを軽減します。
リンク
https://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-url
</div></details>

## Q. 2-44
あなたは、モノリシックなアプリケーションをマイクロサービスモデルに従うように再構築したい。この変更がビジネスに与える影響を最小限に抑えながら、これを効率的に達成したい。

どのアプローチを取るべきでしょうか？
1. アプリケーションをCompute Engineにデプロイし、オートスケールをオンにします。
2. アプリケーションの機能を適切なマイクロサービスに段階的に置き換える。
3. モノリシックなアプリケーションを適切なマイクロサービスで一気にリファクタリングし、デプロイする。
4. モノリスとは別に適切なマイクロサービスで新しいアプリケーションを構築し、新しいアプリケーションが完成したらモノリスを置き換える。
<details><div>
    答え：2
説明
不正解
A. このオプションは、実際にはマイクロサービスアーキテクチャへの移行には対応していません。単に既存のモノリスを別のホスティングソリューションに移行し、オートスケールを有効にするだけです。
C. このオプションはマイクロサービスへの移行を目的としているが、すべてを一度に行うことは非常にリスクが高い。何か問題が発生した場合、ビジネスへの影響は相当なものになる可能性がある。
D. このアプローチは有効かもしれないが、並行開発に多額の投資が必要になるかもしれない。また、オプションCと同じリスクを伴うビッグバンリリースにつながるかもしれません。
正解
B. このアプローチは、モノリスからマイクロサービスへの段階的な変換を可能にします。段階的に行うことで、ビジネスの中断を最小限に抑えて運用を継続することができ、問題が発生したときに監視して対処することができます。
リンク
https://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke
</div></details>

## Q. 2-45
Knativeで動作するオンプレミスのコンテナをGoogle Cloudに移行したい。移行がアプリケーションのデプロイ戦略に影響しないことを確認する必要があり、フルマネージドサービスを利用したいと考えています。

コンテナのデプロイには、どのGoogle Cloudサービスを使うべきでしょうか？
1. Cloud Run
2. コンピュートエンジン
3. Google Kubernetes Engine
4. App Engineの柔軟な環境
<details><div>
    答え：1
説明
不正解
B. これはIaaS（Infrastructure as a Service）サービスであり、コンテナやサーバーレスプラットフォームではありません。Knativeが提供するサーバーレス機能は提供しない。
C. GKEはマネージドKubernetesサービスであり、Knativeによって管理されたコンテナを実行することができるが、Cloud Runのようなフルマネージドのサーバーレスエクスペリエンスは提供しない。
D. これはコンテナをサポートするPlatform as a Service (PaaS)ですが、Cloud RunほどKnativeのサーバーレスモデルと密接に連携していません。
正解
A. Cloud RunはKnativeと互換性があり、Knativeベースのアプリケーションをスムーズに移行できるように設計されています。完全に管理されたサーバーレスプラットフォームを提供するので、要件に最適です。
リンク
https://cloud.google.com/blog/products/serverless/knative-based-cloud-run-services-are-ga
</div></details>

## Q. 2-46
CI/CD パイプラインに Cloud Build を使用して、Compute Engine 仮想マシンへの特定のファイルのコピーなど、いくつかのタスクを完了しています。パイプラインでは、パイプライン内のあるビルダーで生成されたフラットファイルに、同じパイプライン内の後続のビルダーがアクセスできる必要があります。

パイプライン内のすべてのビルダーがアクセスできるようにするには、ファイルをどのように保存すればよいですか？
1. Compute Engineインスタンスのメタデータを使用して、ファイルの内容を保存および取得します。
2. ファイルの内容を/workspace内のファイルに出力し、後続のビルドステップで同じ/workspaceファイルから読み取ります。
3. gsutilを使ってファイルの内容をCloud Storageオブジェクトに出力し、次のビルドステップで同じオブジェクトから読み込みます。
4. ビルド引数を追加して、別のウェブサーバに curl 経由で HTTP POST を実行し、1 つのビルダで値を永続化し、後続のビルドステップから curl 経由で HTTP GET を使用して値を読み取ります。
<details><div>
    答え：2
説明
不正解です：
A. Compute Engineインスタンスに関連するメタデータを保存するためのものであり、Cloud Buildステップ間でデータを共有するためのものではありません。
C. うまくいくかもしれませんが、/workspaceディレクトリを使用するのに比べて不必要に複雑です。
D. もまた、共有ディレクトリへの書き込みと共有ディレクトリからの読み込みだけで達成できるタスクに対して、複雑すぎるソリューションです。
正解です：
B. Cloud Buildでは、/workspaceディレクトリを使用することで、ビルドステップ間でデータを共有できます。すべてのビルド ステップは /workspace ディレクトリから読み取り、/workspace ディレクトリに書き込むことができるため、/workspace ディレクトリを使用してビルド ステップ間でデータを渡すことができます。
リンク
https://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces
</div></details>

## Q. 2-47
あなたは、自社のアプリケーションをオンプレミスからGoogle Cloudへ移行する計画を任されました。御社のモノリシックなアプリケーションはeコマースのウェブサイトです。このアプリケーションは、段階的にGoogle Cloud上にデプロイされるマイクロサービスに移行されます。御社の収益の大部分はオンライン販売によるものであるため、移行中のリスクを最小限に抑えることが重要です。機能に優先順位をつけ、最初に移行する機能を選択する必要があります。

何をすべきでしょうか？
1. 商品カタログを移行します。これはフロントエンドと商品データベースへの統合があります。
2. フロントエンド、注文データベース、サードパーティの決済ベンダーとの統合がある決済処理を移行します。
3. 注文データベース、在庫システム、サードパーティ出荷ベンダーとの統合がある。
4. ショッピングカートの移行。フロントエンド、カートデータベース、在庫システム、支払い処理システムとの統合がある。
<details><div>
    答え：1
説明
不正解
B. 注文データベースや第三者支払ベンダーのような重要なコンポーネントと統合しているため、高いリスクを伴う可能性があります。
C. 受注データベース、在庫システム、第三者出荷ベンダーとの統合もあり、複雑になる可能性がある。
D. フロントエンド、カートデータベース、在庫システム、支払処理システムへの接続があり、最も多くの統合があるようです。
正解
A. 最初に移行する部分としては、最も複雑でリスクが少ないと思われるため、最も論理的な最初のステップと思われます。これにより、チームは、より複雑なコンポーネントに取り組む前に、移行プロセスの経験と自信を得ることができます。
リンク
https://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke#choosing_an_initial_migration_effort
</div></details>

## Q. 2-48
あなたはGoogle Kubernetes Engine上でコンテナ化されたアプリケーションを実行しています。コンテナイメージはContainer Registryに保存されています。あなたのチームはCI/CDプラクティスを使用しています。既知の重大な脆弱性を持つコンテナのデプロイを防止する必要があります。
あなたは何をすべきですか？
1. 
- Web Security Scannerを使用して、アプリケーションを自動的にクロールします。
- アプリケーションのログを確認してスキャン結果を確認し、コンテナに既知の重要な脆弱性がないことを証明します。
- バイナリ認証を使用して、コンテナをデプロイする前に証明書を提供するよう強制するポリシーを実装する。
2. 
- Web Security Scanner を使用して、アプリケーションを自動的にクロールする。
- Cloud Console のスキャン詳細ページでスキャン結果を確認し、コンテナに既知の重要な脆弱性がないことを証明する。
- バイナリ認証を使用して、コンテナをデプロイする前に証明書を提供するよう強制するポリシーを実装する。
3. 
- Container Scanning API を有効にして脆弱性スキャンを実行する。
- クラウドコンソールのコンテナレジストリで脆弱性レポートを確認し、コンテナに既知の重要な脆弱性がないことを証明する。
- バイナリ認証を使用して、コンテナがデプロイされる前に証明書を提供するよう強制するポリシーを実装する。
4. 
- コンテナスキャンAPIを有効にして脆弱性スキャンを実行する
- Container Scanning API を通じて脆弱性レポートをプログラムでレビューし、コンテナに既知の重要な脆弱性がないことを証明する。
- バイナリ認証を使用して、コンテナがデプロイされる前に証明書を提供するよう強制するポリシーを実装する。
<details><div>
    答え：4
説明
不正解
A.B.C.
オプション A と B では、Web Security Scanner を使用しますが、これはコンテナイメージの脆弱性スキャンには関係ありません。むしろ、XSSやSQLインジェクションのような一般的な脆弱性について、Webアプリケーションをスキャンするのに適しています。
オプションCはほぼ正しいですが、Cloud Consoleの脆弱性レポートを手動で確認する必要があります。これは可能ですが、自動化されたCI/CDプラクティスとうまく整合しない可能性があります。
正解
D.
正しいアプローチである。これは、Container Scanning APIを活用して脆弱性スキャンを実行し、脆弱性レポートのプログラムによるレビュー（CI/CDパイプラインの一部として自動化できる）を可能にし、Binary Authorizationを使用して、既知の重要な脆弱性を持つコンテナがデプロイされないようにします。

リンク
https://cloud.google.com/container-analysis/docs/automated-scanning-howto#view-code
https://cloud.google.com/binary-authorization/docs
</div></details>

## Q. 2-49
Pub/Subにメッセージを発行するWebアプリケーションがあります。このアプリケーションの新しいバージョンをローカルでビルドする予定ですが、新しいビルドごとに Pub/Sub の統合を迅速にテストしたいと考えています。

ローカルテストはどのように構成すればよいですか？
1. 統合開発環境（IDE）にCloud Codeをインストールします。Cloud API に移動し、有効な Google プロジェクト ID に対して Pub/Sub を有効にします。ローカルで開発する場合は、pubsub.googleapis.com を呼び出すようにアプリケーションを構成します。
2. gcloudを使用してPub/Subエミュレータをインストールし、有効なGoogleプロジェクトIDでエミュレータを起動します。ローカルで開発する場合は、${gcloud beta emulators pubsub env-init}でローカルのエミュレータを使用するようにアプリケーションを設定します。
3. Google Cloudコンソールで、APIライブラリに移動し、Pub/Sub APIを有効にします。ローカルで開発する場合は、pubsub.googleapis.com を呼び出すようにアプリケーションを構成します。
4. gcloudを使用してPub/Subエミュレータをインストールし、有効なGoogleプロジェクトIDでエミュレータを起動します。ローカルで開発する場合は、PUBSUB_EMULATOR_HOST変数をエクスポートして、ローカルのエミュレータを使用するようにアプリケーションを設定します。
<details><div>
    答え：4
説明
不正解
A. このオプションはローカルでテストする方法を提供しません。代わりに、Google Cloudの実際のPub/Subサービスを呼び出すようにアプリケーションに指示します。これは、本番環境で望ましくない副作用を引き起こし、コストが発生する可能性があります。
B. このオプションは正解に近いですが、エミュレータを使うようにアプリケーションを設定する具体的な方法が間違っています。エミュレータを使用するようにアプリケーションを設定する正しい方法は、PUBSUB_EMULATOR_HOST環境変数をエクスポートすることです。
C. オプション A と同様に、このオプションはアプリケーションに Google Cloud 内の実際の Pub/Sub サービスとのやり取りを指示します。これは、本番環境に影響を与えずにローカルでテストする手段を提供しません。
正解
D. ローカルで開発し、Pub/Subの統合をテストする場合、Google Cloudの実際のPub/Subサービスを呼び出すのではなく、Pub/Subエミュレータを使用することをお勧めします。これにより、コストを発生させることなく、また本番のPub/Subシステムに影響を与えることなく、アプリケーションの開発とテストを行うことができます。

このオプションDは、アプリケーションが実際のPub/Subサービスと同様にやり取りできるローカルPub/Sub環境をセットアップする方法を提供します。
リンク

</div></details>

## Q. 2-50
あなたの e コマース・アプリケーションは外部からのリクエストを受け取り、図のようにクレジットカード処理、配送、在庫管理のためのサードパーティ API サービスに転送します。


顧客から、アプリケーションの動作が予測不可能な時間帯に遅くなるという報告を受けています。アプリケーションはメトリクスを報告しません。一貫性のないパフォーマンスの原因を特定する必要があります。

あなたは何をすべきでしょうか？
1. 各言語用の OpenTelemetry ライブラリをインストールし、アプリケーションを計測してください。
2. コンテナ内にOps Agentをインストールし、アプリケーション・メトリクスを収集するように構成します。
3. ダウンストリーム・サービスを呼び出すときに、X-Cloud-Trace-Context ヘッダーを読み取り、転送するようにアプリケーションを修正する。
4. アプリケーションメトリクスを収集するために、Google Kubernetes EngineクラスタでPrometheusのマネージドサービスを有効にする。
<details><div>
    答え：1
説明
不正解
B. このオプションは、システムとコンテナからメトリクスを収集するのに役立ちますが、サードパーティ・サービスとの相互作用を理解するのに必要な詳細なトレース情報が得られるとは限りません。
C. このオプションは Google Cloud トレースに特有であり、相互作用している外部のサードパーティ サービスには適用されない場合があります。スローダウンが発生している場所の全体像を把握できない可能性があります。
D. Managed Service for Prometheusは、GKEクラスタ上のメトリクスを収集するのに便利ですが、サードパーティのサービスを通じて要求がどのように処理されているかの詳細なトレースを提供するとは限りません。
正解
A. OpenTelemetry は、分散トレースやメトリクス収集を含む、観測可能性のための API とインスツルメンテーションを提供する複合ライブラリです。アプリケーションにOpenTelemetryを追加することで、様々なコンポーネントやサービスを流れるリクエストをトレースすることができ、どこでスローダウンが発生しているのかをピンポイントで特定することができます。

リンク
https://cloud.google.com/trace/docs/trace-app-latency
</div></details>

## Q. 3-1
あなたはNode.jsでクラウド関数を書いており、ソースコードはGitリポジトリに保存されています。ソースコードにコミットされた変更が自動的にテストされるようにしたい。ソースコードを一意に命名された Cloud Function にプッシュし、その関数をテストとして呼び出し、クリーンアップとして Cloud Function を削除する Cloud Build 構成を記述します。テストが失敗すると、クラウド関数が削除されないことがわかりました。

どうすればよいでしょうか？
1. ステップの順序を変更して、テストを実行する前にクラウド関数を削除します。
2. Cloud Buildステップに、Cloud Function テスト ステップを削除するwaitFor オプションを必須先行ステップとして含めます。
3. Cloud Build ステップでクラウド関数の結果をファイルに書き込み、0 を返すようにします。クラウド関数の削除後に、ファイルに期待された結果が含まれているかどうかをチェックし、含まれていない場合は失敗するステップを追加します。
4. クラウド ビルド テスト ステップで、result という環境変数に結果を設定し、0 を返すようにします。クラウド関数削除の後に、環境変数に期待した結果が含まれているかどうかをチェックする最終ステップを追加します。
<details><div>
    答え：3
説明
不正解：
A. テストを実行する前にクラウド関数を削除すると、関数が存在しなくなるため、テストが不可能になります。
B. この方法で waitFor オプションを使用すると、テストが失敗した場合にビルド全体が失敗し、クラウド関数は削除されません。テストが失敗した場合に関数をクリーンアップするという問題は解決しません。
D. 環境変数はステップが実行されるコンテナに対してローカルなので、ビルドプロセスの異なるステップ間で情報を渡すために使用することはできません。
正解
C. クラウド関数をデプロイし、結果を（成功または失敗に関係なく）ファイルに保存し、クラウド関数を削除し、ファイルの内容をテストすることで、適切なクリーンアップとテスト結果の正確なレポートが保証されます。
リンク
https://cloud.google.com/build/docs/configuring-builds/configure-build-step-order
https://cloud.google.com/build/docs/configuring-builds/create-basic-configuration
https://cloud.google.com/build/docs/build-config
</div></details>

## Q. 3-2
Google Kubernetes Engine（GKE）クラスタにWebアプリケーションをデプロイしました。Cloud Monitoringのメトリクスを確認していると、クラスタのCPU負荷が1日中変動していることがわかります。コストを最小限に抑えながらパフォーマンスを最大化するために、ポッドとノートの数を自動的に調整したいとします。

あなたは何をすべきですか？
1. マネージドインスタンスグループ（MIG）を修正してオートスケーリングを有効にし、CPU負荷に基づいてノードの最大量と最小量を設定します。
2. GKEクラスタでCluster Autoscalerを有効にし、CPU負荷に基づいてワークロードを自動スケールするようにHorizontal Pod Autoscaler（HPA）を構成します。
3. GKEクラスタ上でCluster Autoscalerを有効にし、カスタムメトリックに基づいてワークロードをオートスケールするようにHPAを構成します。
4. MIGを修正して、CPU負荷に基づいてノードの最大量と最小量を構成するオートスケーリングを有効にし、CPU負荷に基づいてワークロードをスケールするようにVertical Pod Autoscaler（VPA）を構成します。
<details><div>
    答え：2
説明
不正解
A. GKE には固有のオートスケーリング機能があるため、GKE によって作成されたインスタンスグループで Compute Engine のオートスケーリング機能を使用しないでください。
C. CPUメトリクスは標準であり、説明するシナリオではカスタムメトリクスを使用する必要はありません。
D. 繰り返しになるが、Compute Engineのオートスケーリング機能は、GKEで作成したインスタンスグループには使用しないこと。この問題はCPU負荷に基づくスケーリングに重点を置いているため、VPA（Vertical Pod Autoscaler）はここでは必要ありません。
正解
B. GKEクラスタでCluster Autoscalerを有効にすると、クラスタ内のノード数がワークロードの要求に基づいて自動的に調整されます。[CPU負荷]に基づいてワークロードを自動スケールするように[HPA]（Horizontal Pod Autoscaler）を設定することは、CPU負荷の変動に応じてポッド数を調整する要件と一致します。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler
https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps
</div></details>

## Q. 3-3
Cloud Run 上で Java アプリケーションを実行しています。アプリケーションのエラーメッセージがエラーレポートコンソールに表示されません。どうすればよいですか？
1. Cloud Monitoringクライアント・ライブラリがJavaアプリケーションにバンドルされていることを確認します。
2. アプリケーションログが正しい地域のストレージバケットに書き込まれていることを確認します。
3. アプリケーションエラーが標準エラー出力に書き込まれていることを確認します。
4. System.out.printlnを使用して例外をログに記録する。
<details><div>
    答え：3
説明
不正解
A. サービスが内部的にログ収集プロセスを処理するため、Cloud MonitoringクライアントライブラリはCloud Runのエラーレポートには必要ありません。
B. エラーレポートは地域のログバケットに保存されたログを分析しないため、これを検証しても問題は解決しません。
D. System.out.printlnを使用すると、標準エラー・ストリームではなく標準出力ストリーム（標準出力）に出力されるため、エラー・レポート・コンソールにエラー・メッセージが表示されません。
正解です：
C. Cloud Runでは、エラーメッセージはエラー報告で認識されるために標準エラーストリーム（stderr）に書き込まれる必要があります。アプリケーションエラーが stderr に書き込まれていることを確認することで、問題は解決します。
リンク
https://cloud.google.com/error-reporting/docs/troubleshooting
https://cloud.google.com/run/docs/error-reporting
</div></details>

## Q. 3-4
アプリケーションのパフォーマンスを分析しています。クラスタ内の特定のCloud Bigtableテーブルが他のテーブルよりも多く使用され、エンドユーザーに対するアプリケーションのパフォーマンスに一貫性がないことを確認しました。

あるタブレットでは、同じような名前の行キーのセクションが大きく使用されていますが、他のタブレットではアイドル状態です。ユーザーの郵便番号が行キーの最初のコンポーネントであり、その郵便番号から発信されたプロファイルによってアプリケーションが大量に使用されていることがわかりました。

行キーの生成方法を変更して、人間が読めるようにし、Cloud Bigtableの需要がクラスタ内でより均等になるようにします。

どうすればよいでしょうか？
1. 連続的に生成された整数値を使用します。
2. 行内容のMD5ハッシュのサブセットを使用する。
3. 複数の人間が読める属性を連結して使用する。
4. ミリ秒単位のUNIXエポックスタイルのタイムスタンプを使用する。
<details><div>
    答え：3
説明
不正解
A. 連続的に生成された整数値を使用すると、ホットスポットが発生する可能性があります。この場合、クラスタ全体でリクエストが不均等に分散されることになり、現在直面している問題と同じになります。
B. ハッシュを使用することで、行のキーをタブレット間でより均一に分散させることができますが、行の内容のMD5ハッシュを使用すると、問題のトラブルシューティングが難しくなります。また、要求事項の一つである人間が読めるものでもない。
D. UNIXのエポックスタイルのタイムスタンプを使用すると、同じミリ秒の間に複数の更新が発生した場合、衝突が発生する可能性があります。タイムスタンプをBigtableの行キーとして使用することは、一般的に推奨されません。
正解
C. 複数の人間が読み取り可能な属性を連結して行キーとすることで、属性の選択によっては、タブレット間でキーをより均等に分散させることができます。この方法は、[行キーが人間が読める]という要件も満たす。
リンク
https://cloud.google.com/bigtable/docs/schema-design#types_of_row_keys
https://cloud.google.com/bigtable/docs/schema-design-time-series#ensure_that_your_row_key_avoids_hotspotting
</div></details>

## Q. 3-5
御社は、米国で人気を博したマルチプレイヤーゲームで成功を収めている。現在、同社は他の地域に拡大したいと考えています。ユーザーがポイントを交換できる新機能を開始します。この機能は世界中のユーザが利用できます。御社の現在のMySQLバックエンドは、ゲームをホストするCompute Engineインスタンスの限界に達しています。貴社は、地域間でグローバルな一貫性と高可用性を提供する別のデータベースに移行したいと考えています。どのデータベースを選択すべきでしょうか？
1. BigQuery
2. Cloud SQL
3. Cloud Spanner
4. Cloud Bigtable
<details><div>
    答え：3
説明
不正解
A. BigQueryは主に分析データベースであり、トランザクションワークロード、特にマルチリージョンでグローバルに一貫性のあるトランザクションを含むワークロードには適していません。
B. Cloud SQLは、リージョン内の高可用性は提供するが、複数のリージョンにまたがってその機能を拡張することはできない。これはグローバルな一貫性と可用性の要件を満たさない。
D. Cloud Bigtableは高いスループットとスケーラビリティを提供するが、このケースで必要とされるグローバルな可用性は提供しない。大規模な分析および運用ワークロードには適していますが、グローバルに分散されたトランザクションシステムには適していません。
正解
C. これは正しい選択です。Cloud Spannerは、地域間の高可用性と強力な一貫性の両方を提供するグローバル分散データベースとして設計されています。世界的に人気のあるマルチプレイヤーゲームの要求を処理できるため、説明したシナリオに最適です。
リンク
Cloud Spannerドキュメント
</div></details>

## Q. 3-6
貴社はアナリティクスのユースケースの拡大を計画しています。新しいユースケースの1つでは、データアナリストがSQLを使用してイベントをほぼリアルタイムで分析する必要があります。急成長が予想されるため、できるだけマネージド・サービスを利用したいと考えています。

どうすればよいでしょうか？
1. Pub/Subトピックとサブスクリプションを作成します。ソースからPub/Subトピックにイベントをストリームします。Dataflowを活用して、これらのイベントをBigQueryに取り込みます。
2. Pub/Subトピックとサブスクリプションを作成します。ソースからPub/Subトピックにイベントをストリームします。Dataflowを活用して、これらのイベントをCloud Storageに取り込みます。
3. Pub/Sub トピックとサブスクリプションを作成する。ソースからイベントをPub/Subトピックにストリームします。データフローを活用して、これらのイベントをデータストアモードのFirestoreに取り込みます。
4. 大規模なCompute Engineインスタンス上にKafkaインスタンスを作成します。ソースからKafkaパイプラインにイベントをストリーミングする。Dataflowを活用して、これらのイベントをCloud Storageに取り込む。
<details><div>
    答え：1
説明
不正解
B. クラウド・ストレージはSQL分析用に構造化されていないため、これは個々のイベントを挿入してSQLで分析するには適していません。
C. データストア・モードのFirestoreはSQL分析用に最適化されていないため、この要件には最適ではありません。
D. このソリューションは完全に管理されたソリューションを提供しません。また、クラウド・ストレージはSQL分析に適していないため、ほぼリアルタイムでSQLを使ってデータを分析する機能も欠けています。
正解
A. これは、フルマネージドサービスを活用し、BigQueryを使用してほぼリアルタイムのSQL分析を可能にするため、正しいアプローチです。Pub/Subはストリーミングデータを処理し、Dataflowはデータを処理して変換し、BigQueryはSQLでデータを分析するのに適しています。
リンク
https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery
https://cloud.google.com/architecture/reference-patterns/overview#general_analytics
</div></details>

## Q. 3-7
Cloud Run上にデプロイされたアプリケーションは大量のトラフィックを受け取ります。アプリケーションへの変更をデプロイすると、すべてのユーザーに悪影響が及ぶことを懸念しています。コストの問題から本格的な負荷テストは避けたいと考えていますが、それでも新機能をできるだけ早くデプロイしたいと考えています。

どのアプローチを取るべきでしょうか？
1. 本番アプリケーションに対する負荷テストを毎週スケジュールします。
2. ローカルの開発環境を使用して、Google Cloud 外で負荷テストを実行する。
3. ユーザーに新機能へのアクセスを許可する前に、新バージョンとしてデプロイし、スモークテストを実施する。その後、すべてのユーザーが新機能にアクセスできるようにする。
4. トラフィック分割を使用して、一部のユーザーに新機能をテストしてもらい、すべてのユーザーが新機能を利用できるようになるまで、トラフィック分割を徐々に調整します。
<details><div>
    答え：4
説明
不正解
A. 負荷テストを定期的に実行することは、特に頻繁に実行する場合、コストがかかり、本番環境を混乱させる可能性があります。
B. ローカルのテスト環境は本番環境を正確に表していない可能性があり、このアプローチでは実際のユーザーに徐々に変更をロールアウトする方法を提供できない。
C. これはある程度のテストを提供しますが、すべてのユーザーがアクセスできるようにする前に、新機能を徐々にロールアウトしたり、実際のユーザーテストを行ったりすることはできません。
正解
D. このアプローチでは、ユーザーの一部に徐々に新機能を導入し、その行動を分析し、必要に応じてトラフィック配分を調整することができます。費用対効果の高いテストと安全な配備のバランスが取れます。
リンク
https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#split-traffic
</div></details>

## Q. 3-8
あなたのチームはCloud Runを使用して、すべてのPub/SubメッセージをCloud StorageオブジェクトとBigQueryテーブルの両方に書き込んでいます。運用上のオーバーヘッドを最小限に抑えたい。どのアーキテクチャを実装すべきですか？
1. One topic, 1 subscription, 2 Cloud Run Services
2. One topic, 2 subscriptions, 2 Cloud Run Services
3. One topic, 1 push subscription, 1 Cloud Run Service
4. Two topic, 2 subscriptions, 2 Cloud Run Services
<details><div>
    答え：2
説明
不正解：
これはメッセージの半分をBigQueryに、半分をクラウドストレージに書き込むことになり、ニーズを満たさないため、オプションAは正しくありません。
1つのシステムへのメッセージ処理に失敗すると、BigQueryまたはGoogle Cloud Storageへの書き込みが重複する可能性があるため、オプションCは正しくありません。
オプションDは、メッセージ送信料が重複し、メッセージを2つの異なるトピックに2回送信する必要があるため、コストが増加し、追加のトピックを管理する必要があり、クライアントが複雑になるため、正しくありません。
正解
各App Engineサービスは、書き込む独自のメッセージを取得し、独立して再試行/失敗できるため、オプションBは正しい。
リンク
https://cloud.google.com/run/docs/triggering/pubsub-push
https://cloud.google.com/pubsub/docs/admin
</div></details>

## Q. 3-9
Webアプリケーションからの注文を処理し、データをFirestoreのDatastoreモードのコレクションに保存するAPIエンドポイントを作成しています。アプリケーションのテスト中に、アプリケーションが Datastore API から HTTP 5xx サーバー エラーに遭遇すると、このエラーをキャッチしてクライアントに HTTP 200 OK 応答コードを返しますが、Datastore 内にはデータが保存されないことに気付きました。APIエンドポイントのコンシューマに、書き込み要求が失敗したことを知らせたい。

どうすればよいでしょうか？
1. HTTP 204 No Contentレスポンスを返す。
2. HTTP 406 Not Acceptableレスポンスを返す。
3. HTTP 500 Internal Server Error レスポンスを返す。
4. Firestore が HTTP 2xx レスポンスを返すまで、指数関数バックオフを使用して Datastore API を再試行します。
<details><div>
    答え：3
説明
不正解
A. このステータスコードはリクエストが正常に処理されたことを示しますが、送り返す情報はありません。これは成功を誤って意味するので、ここでは適さない。
B. このステータスコードは、クライアントが定義した受け入れ可能な値のリストに一致する応答をサーバが生成できないことを意味し、Datastoreで発生したエラーとは無関係です。
D. 指数関数的バックオフによる再試行を実装することは、一過性のエラーに対処する際に良いプラクティスであることが多いのですが、エラーの性質を理解せず、再試行制限を課すことなくやみくもに再試行すると、無限ループやその他の望ましくない動作につながる可能性があります。また、エラーが続く場合、クライアントはまだ何も知らされないままかもしれません。
正解
C. この応答コードは、サーバーが予期しない状況に遭遇し、リクエストを処理できなかったことを示します。このレスポンスコードは、サーバー側で障害が発生し、クライアントのリクエストが正しかったことをクライアントに知らせます。
リンク
https://www.restapitutorial.com/httpstatuscodes.html
</div></details>

## Q. 3-10
NFS共有に設定を保存するレガシーアプリケーションをコンテナ化しました。このアプリケーションをGoogle Kubernetes Engine (GKE)にデプロイする必要があり、コンフィギュレーションが取得されるまでアプリケーションにトラフィックを提供させたくありません。

あなたは何をすべきでしょうか？
1. gsutilユーティリティを使用して、起動時にDockerコンテナ内からファイルをコピーし、ENTRYPOINTスクリプトを使用してサービスを開始します。
2. GKEクラスタ上にPersistentVolumeClaimを作成します。ボリュームから設定ファイルにアクセスし、ENTRYPOINTスクリプトを使用してサービスを開始します。
3. Dockerfile の COPY ステートメントを使用して、構成をコンテナイメージにロードします。構成が利用可能であることを確認し、ENTRYPOINTスクリプトを使用してサービスを開始します。
4. GKEインスタンスグループに起動スクリプトを追加し、ノード起動時にNFS共有をマウントする。構成ファイルをコンテナにコピーし、ENTRYPOINTスクリプトを使用してサービスを開始する。
<details><div>
    答え：2
説明
不正解
A. gsutilはGoogle Cloud Storageと対話するためのコマンドラインツールであり、コンテナ化されたアプリケーションやNFS共有を管理するためのものではないため、不正解です。
C. 構成データをDockerイメージに格納することは、Kubernetesのベストプラクティスではないため、間違っています。なぜなら、Dockerイメージに設定データを保存することは、Kubernetesのベストプラクティスではないからです。これはコンテナの移植性を低下させ、アプリケーションの外部に設定と秘密を保持するというクラウドネイティブモデルに適合しません。
D. GKEは、起動スクリプトを含め、基礎となるノードインフラストラクチャを管理するため、正しくありません。また、GKEは、個々のノードの動作を直接スクリプト化することが実用的でない、あるいは可能でさえないという点で、個々のノードへのアクセスを抽象化しています。
正解です：
B. このオプションは、NFSのサポートを含む、ストレージリソースを管理するためのKubernetesのネイティブ機能を活用します。アプリケーションは、コンテナのローカルであるかのように、NFS共有から構成にアクセスできます。ENTRYPOINTスクリプトがサービスを開始するまで、コンテナはトラフィックを提供しません。
リンク
https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
https://cloud.google.com/filestore/docs/accessing-fileshares
https://cloud.google.com/storage/docs/gcs-fuse
</div></details>

## Q. 3-11
Cloud Runを使用して新しいWebアプリケーションを開発し、Cloud Source Repositoriesにコードをコミットしています。可能な限り最も効率的な方法で新しいコードをデプロイしたい。すでにコンテナをビルドするCloud Build YAMLファイルを作成し、次のコマンドを実行しています：gcloud run deploy。

次に何をすべきでしょうか？
1. コードがリポジトリにプッシュされたときに通知されるPub/Subトピックを作成します。イベントがトピックにパブリッシュされたときにビルドファイルを実行する Pub/Sub トリガーを作成します。
2. リポジトリコードが開発ブランチにプッシュされたことに応答してビルドファイルを実行するビルドトリガーを作成する。
3. Webhook URL への HTTP POST 呼び出しに応答してビルドファイルを実行する Webhook ビルドトリガーを作成します。
4. 24 時間ごとに次のコマンドを実行する Cron ジョブを作成します。
<details><div>
    答え：2
説明
不正解
A. Pub/Sub通知は通常、イベント駆動型の非同期ワークロードに使用され、特にビルドのトリガーには使用されません。
C. Webhook を使用してビルドをトリガすることはできますが、Cloud Build のネイティブ トリガ機構を使用するよりも効率が悪く、安全性も低いため、C. は正しくありません。また、コードがリポジトリにプッシュされるたびに HTTP POST 呼び出しを送信するには、手作業や追加のサービス設定が必要になります。
D. gcloud builds submit. コードのプッシュに反応するのではなく、定期的（毎日）にビルドに依存します。これは、コード変更が発生していない場合に不要なビルドが発生したり、重要なコード変更のデプロイが遅れたりする可能性があるため、効率的ではありません。
正解
B. Cloud Buildはビルドトリガーを提供し、新しいコードがソースコードリポジトリの指定されたブランチにプッシュされたときにビルドプロセスを実行するように構成できます。この戦略はしばしば継続的デプロイメントまたは継続的デリバリーと呼ばれます。オプションBを使用すると、デプロイプロセスを効率的に自動化できます。新しいコードが開発ブランチにプッシュされるたびに、ビルドが自動的にトリガーされ、変更が必要なときにだけ迅速にデプロイされるようになります。
リンク
https://cloud.google.com/build/docs/automating-builds/create-manage-triggers#connect_repo
</div></details>

## Q. 3-12
あなたの会社では、Google Cloudに保存されるすべてのデータを顧客が管理する暗号化キーで暗号化することを要求する新しいセキュリティイニシアチブを取っています。クラウド鍵管理サービス（KMS）を使用して鍵へのアクセスを設定する予定です。職務分掌の原則とGoogleが推奨するベストプラクティスに従う必要があります。

あなたは何をすべきでしょうか？（選択肢を2つ選んでください。）
1. 独自のプロジェクトでCloud KMSをプロビジョニングする。
2. Cloud KMSプロジェクトにオーナーを割り当てないでください。
3. 鍵を使用するプロジェクトにCloud KMSをプロビジョニングする。
4. Cloud KMSの鍵を使用するプロジェクトのオーナーにroles/cloudkms.adminロールを付与する。
5. Cloud KMSプロジェクトの所有者ロールを、Cloud KMSの鍵を使用するプロジェクトの所有者とは異なるユーザーに付与する。
<details><div>
    答え：1,2
説明
不正解
C. 鍵が使用されるプロジェクトにCloud KMSを併設することは、鍵の管理ミスや意図しない鍵アクセスにつながる可能性があるため、正しくない。これらの懸念は切り離して考えるのが最善である。
D. 鍵を使用するプロジェクトの所有者にcloudkms.adminロールを付与することは、職務分掌の原則に反する。これは一人のユーザーに鍵の管理と使用の両方の権限を与えることになり、不正なアクセスや変更につながる可能性があります。
E. (推奨しません) オーナー・ロールを引き続き使用する必要がある場合は、プロジェクトのオーナーであるプリンシパルとは異なるプリンシパルに付与するようにしてください。オーナーは引き続き鍵を使用できますが、1つのプロジェクト内でのみ使用できます。 
正解
A. クラウド鍵管理サービス（KMS）を独自のプロジェクトに作成することで、責任の境界が明確になり、鍵の偶発的な削除や変更のリスクを低減できる。
B. (推奨)プロジェクト・レベルでは所有者なしでプロジェクト・キーを作成し、組織レベルで付与された組織管理者を指定します。所有者とは異なり、組織管理者は鍵を直接管理したり使用したりできない。組織管理者は、鍵の管理者と使用者を制限するIAMポリシーの設定に限定される。組織レベルのノードを使用すると、組織内のプロジェクトの権限をさらに制限できる。(https://cloud.google.com/kms/docs/separation-of-duties#using_separate_project)
リンク
https://cloud.google.com/kms/docs/separation-of-duties#using_separate_project
</div></details>

## Q. 3-13
あなたのチームは、本番プロジェクトの Cloud Run 上で実行されているアプリケーションでエラーの急増を検出しました。アプリケーションは、Pub/Sub トピック A からメッセージを読み取り、メッセージを処理し、トピック B にメッセージを書き込むように構成されています。テストには一連のモック・メッセージを使用できます。

あなたは何をすべきですか?
1. ローカルマシンにPub/SubおよびCloud Runエミュレータをデプロイします。アプリケーションをローカルにデプロイし、アプリケーションのロギング・レベルを DEBUG または INFO に変更します。トピックAにモック・メッセージを書き込み、ログを分析します。
2. アプリケーションのロギングレベルをDEBUGまたはINFOに変更し、ログを分析します。
3. ローカル・マシンにPub/Subエミュレータをデプロイします。本番アプリケーションをローカルの Pub/Sub トピックに指定します。トピック A にモック・メッセージを書き込み、ログを分析します。
4. アプリケーションのログ レベルを DEBUG または INFO に変更し、ログを分析します。
<details><div>
    答え：1
説明
不正解：
B. D.
本番アプリケーションのトピック A に直接モック・メッセージを書き込むと、本番アプリケーションに影響を与え、問題が悪化する可能性があるため、オプション B と D は正しくありません。テストは、予期しない副作用を避けるために、実稼働環境では実行しないでください。
C. 
実運用アプリケーションをローカルの Pub/Sub トピックに向けることは、ネットワーク構成によっては現実的でないか、不可能である可能性が高く、実運用ワークロードの中断につながる可能性があるため、オプション C は正しくありません。さらに、テスト環境と本番環境を分離するという原則にも違反することになります。
正解
A. 実際の本番アプリケーションやデータに影響を与えることなく、本番環境をローカル・マシンで再現できます。Pub/SubおよびCloud Runエミュレータとともにアプリケーションをローカルにデプロイすることで、本番環境を模倣した隔離された環境でテストを実施し、ログを分析することができます。これにより、テストが本番のワークロードに干渉することはありません。
リンク
https://cloud.google.com/pubsub/docs/emulator
</div></details>

## Q. 3-14
Compute Engineインスタンスで実行され、任意のユーザーのGoogle Driveにファイルを書き込むWebアプリケーションを作成しています。Google Drive APIを認証するようにアプリケーションを設定する必要があります。

どうすればよいですか？
1. https://www.googleapis.com/auth/drive.file スコープを使用する OAuth クライアント ID を使用して、各ユーザーのアクセストークンを取得します。
2. ドメイン全体の権限を委譲されたOAuthクライアントIDを使用します。
3. App Engineサービスアカウントとhttps://www.googleapis.com/auth/drive.file スコープを使用して、署名付きJSON Webトークン（JWT）を生成します。
4. ドメイン全体の権限を委譲されたApp Engineサービスアカウントを使用します。
<details><div>
    答え：1
説明
不正解
B. ドメイン全体の権限は、通常、G Suiteドメイン管理者がサードパーティのアプリケーションにドメイン内のすべてのユーザーのデータへのアクセスを許可するために使用されます。このシナリオでは必要ありません。
C. D. 
App Engineではなく、Compute Engineインスタンスが使用されているため、オプションCは正しくありません。また、サービスアカウントはユーザーではなくアプリケーションを表すため、サービスアカウントを使用すると、アプリからユーザーのGoogle Driveにファイルを書き込むことができません。
オプションDも同じ理由で間違っています。さらに、ドメイン全体の権限は、このシナリオでは不要であり、要件に適合しません。
正解
A. ユーザーのドライブにファイルを書き込むために、ウェブアプリケーションが各ユーザーの代理として Google Drive API を認証する必要があります。これは通常 OAuth 2.0 を使って行われ、 ユーザのパスワードを共有することなく、 アプリケーションがユーザの代わりにアクションを実行する許可を与えることができます。
https://www.googleapis.com/auth/drive.file スコープでは、アプリが作成したGoogle Driveファイルを閲覧・管理することができます。
リンク
https://developers.google.com/drive/api/guides/api-specific-auth
https://developers.google.com/drive/api/guides/about-auth
</div></details>

## Q. 3-15
あなたのアプリケーションはGoogle Kubernetes Engine (GKE)クラスタにデプロイされています。あなたはこのアプリケーションをCloud Load Balancing HTTP(S)ロードバランサーの後ろに公開したい。

どうすればいいでしょうか？
1. GKE Ingressリソースを設定する。
2. GKEサービスリソースを構成します。
3. タイプを持つ GKE Ingress リソースを設定する： ロードバランサー
4. タイプ: LoadBalancer で GKE Service リソースを構成する： ロードバランサー。
<details><div>
    答え：1
説明
不正解
B. Serviceリソースだけでは、アプリケーションを公開することはできますが、HTTP(S)ロードバランサーのようなレベルのトラフィック管理はできません。ロードバランサーはSSL/TLS終端、ホスト、パスベースのルーティングを扱うことができ、特定のノードに限定されません。
C. D. 
オプション C と D は正しくありません： LoadBalancerは、HTTP(S)ロードバランサーではなく、ネットワークロードバランサーを作成するために、Ingressではなく、Serviceリソースで使用されます。ネットワークロードバランサーはネットワークスタックの低いレベル(トランスポートレイヤー - レイヤー4)で動作し、HTTP(S)ロードバランサーのようなHTTP(S)パスベースのルーティングルールをサポートしません。HTTP(S)ロードバランサーはネットワークスタックの上位レベル（アプリケーションレイヤー - レイヤー7）で動作し、HTTP(S)リクエストに基づくより複雑なルーティングルールを可能にします。これはGKEのIngressを介して行われます。
正解
A. Google Kubernetes Engine (GKE)では、HTTP(S)ロードバランサーの背後にあるパブリックインターネットトラフィックにアプリケーションを公開したい場合、Ingressリソースを使用する必要があります。Ingressリソースは、クラスタ内で動作するアプリケーションにHTTP(S)トラフィックをルーティングするためのルールを定義します。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
</div></details>

## Q. 3-16
セキュリティチームは、Google Kubernetes Engine で実行されているすべてのデプロイ済みアプリケーションを監査しています。監査が完了した後、チームは一部のアプリケーションがクラスタ内でトラフィックを平文で送信していることを発見しました。アプリケーションへの変更を最小限に抑え、Googleからのサポートを維持しながら、すべてのアプリケーションのトラフィックをできるだけ早く暗号化する必要があります。

どうすればいいでしょうか？
1. ネットワークポリシーを使用して、アプリケーション間のトラフィックをブロックします。
2. Istioをインストールし、アプリケーション・ネームスペースでプロキシ・インジェクションを有効にしてから、mTLSを有効にします。
3. アプリケーション内で信頼済みネットワーク範囲を定義し、それらのネットワークからのトラフィックのみを許可するようにアプリケーションを構成します。
4. 自動化プロセスを使用して、Let's Encrypt にアプリケーション用の SSL 証明書を要求し、アプリケーションに追加する。
<details><div>
    答え：2
説明
不正解
A. ネットワークポリシーはポッド間のトラフィックを制御するための貴重なツールですが、ネットワークトラフィックの暗号化はできません。
C. 信頼済みネットワークの範囲は、IP アドレスに基づいてアクセスを制御するだけで、トラフィックを暗号化しません。
D. SSL/TLS証明書はネットワークトラフィックを暗号化するための重要なコンポーネントですが、すべてのアプリケーションに手動で実装するのは大変な作業となり、アプリケーションへの変更を最小限に抑えるという要件を満たせません。さらに、これらの証明書の管理と更新が負担になる可能性があります。
正解
B. Istioは、アプリケーション（またはサービス）間のトラフィックを管理できるサービス・メッシュです。相互TLS（mTLS）と呼ばれる機能が含まれており、サービス間を行き来するすべてのトラフィックを自動的に暗号化します。つまり、アプリケーションが平文でトラフィックを送信していても、Kubernetesクラスタ内の通信はすべて暗号化される。
Istioでプロキシ・インジェクションを有効にすると、Kubernetesの各アプリケーション・ポッドに小さなヘルパー（または「サイドカー・プロキシ」）が自動的に挿入される。このサイドカー・プロキシがトラフィックの暗号化と復号化をすべて行うので、アプリケーションを暗号化対応に変更する必要はない。このため、Istioは迅速かつ効果的なソリューションとなる。
リンク
https://cloud.google.com/istio/docs/istio-on-gke/overview
</div></details>

## Q. 3-17
あなたは、クラウドストレージのバケットに保存されたプライベートな画像や動画を含むウェブアプリケーションを開発しています。ユーザは匿名で、Google アカウントを持っていません。アプリケーション固有のロジックを使用して、画像や動画へのアクセスを制御したいとします。どのようにアクセスを設定すればよいでしょうか？
1. 各WebアプリケーションユーザのIPアドレスをキャッシュし、Google Cloud Armorを使用して名前付きIPテーブルを作成します。ユーザがバックエンドバケットにアクセスすることを許可するGoogle Cloud Armorセキュリティポリシーを作成します。
2. AllUsers に Storage Object Viewer IAM ロールを付与する。Webアプリケーションで認証後、ユーザがバケットにアクセスできるようにする。
3. Identity-Aware Proxy (IAP) を設定し、Web アプリケーションでユーザを認証する。IAP による認証後、ユーザが Bucket にアクセスできるようにする。
4. バケットへの読み取りアクセスを許可する署名付きURLを生成します。Webアプリケーションで認証した後、ユーザがそのURLにアクセスできるようにする。
<details><div>
    答え：4
説明
不正解
A. Google Cloud Armorは、分散サービス拒否（DDoS）攻撃から保護するものであり、クラウドストレージ内のオブジェクトへのアクセスを提供するものではありません。
B. この場合、インターネット上のすべての人にバケットへのアクセスを許可することになり、アプリケーション固有のロジックでアクセスを制御することはできません。
C. IAP（Identity-Aware Proxy）は、Google Cloud上で動作するアプリケーションへのアクセスを制御するために使用されるものであり、Cloud Storageバケットへのアクセスを制御するために使用されるものではない。さらに、IAPはユーザーがGoogleアカウントを持っていることを要求しますが、シナリオではユーザーが匿名でGoogleアカウントを持っていないことが指定されています。
正解
D. このシナリオでは、Googleアカウントを持たないユーザーのCloud Storageオブジェクトへのアクセスを制御する必要があります。署名付きURLはこのための良いソリューションです。これらのURLは、特定のオブジェクトへの一時的なアクセスを提供し、必要なすべての認証情報を含んでいます。ウェブアプリのロジックでユーザーを認証した後、必要なリソースにアクセスするための署名付きURLを生成して提供することができます。
リンク
https://cloud.google.com/storage/docs/access-control/signed-urls#should-you-use
</div></details>

## Q. 3-18
あなたは最近、モノリシックなアプリケーションをマイクロサービスに分解してGoogle Cloudに移行しました。マイクロサービスの1つはCloud
ファンクションを使用してデプロイされます。アプリケーションを最新化する際、サービスのAPIに後方互換性のない変更を加えました。元のAPIを使用する既存の呼び出し元と、新しいAPIを使用する新しい呼び出し元の両方をサポートする必要があります。どうすればよいのでしょうか？
1. 元のクラウド関数はそのままにして、新しいAPIを持つ2番目のクラウド関数をデプロイします。ロードバランサーを使用して、バージョン間の呼び出しを分散します。
2. 元のCloud Functionはそのままにして、変更されたAPIだけを含む2番目のCloud Functionをデプロイする。呼び出しは自動的に正しい関数にルーティングされます。
3. 元のCloud Function はそのままにして、新しいAPIを持つ2番目のCloud Function をデプロイします。Cloud Endpointsを使用して、バージョン管理されたAPIを公開するAPIゲートウェイを提供する。
4. 新しいAPIをサポートするようにコードを変更した後、Cloud Functionを再デプロイします。APIの両方のバージョンに対するリクエストは、呼び出しに含まれるバージョン識別子に基づいて処理されます。
<details><div>
    答え：3
説明
不正解
A. ロードバランサーはAPIバージョンに基づいてトラフィックをルーティングするようには設計されていません。
B. クラウド・ファンクションは、APIの変更に基づいて正しいファンクションにコールを自動的にルーティングしない。
D. Cloud FunctionはAPIの両方のバージョンに対するリクエストを処理することができますが、特にAPIへの変更が大きい場合はそうならない可能性があります。また、これはより複雑で保守しにくいコードベースにつながる可能性があります。
正解
C. Cloud Endpointsは、異なるバージョンのAPIを公開する方法を提供することで、それを支援することができます。異なるCloud Functionsを指す複数のAPIパス（例えばv1とv2）を定義できる。このアプローチにより、既存のクライアントは古いバージョンのAPIを使い続けることができ、新しいクライアントは新しいバージョンのAPIを使い始めることができる。
リンク
https://cloud.google.com/endpoints/docs/openapi/get-started-cloud-functions
</div></details>

## Q. 3-19
アプリケーションでは、ホストとなるCompute Engine仮想マシンインスタンスに保存された認証情報を使用して、サービスアカウントをGCP製品に認証する必要があります。これらの認証情報をできるだけ安全にホストインスタンスに配布したいとします。
どうすればよいでしょうか。
1. HTTP署名付きURLを使用して、必要なリソースへのアクセスを安全に提供します。
2. インスタンスのサービスアカウントのアプリケーションデフォルト認証情報を使用して、必要なリソースを認証します。
3. インスタンスのデプロイ後にGCPコンソールからP12ファイルを生成し、アプリケーションを開始する前に認証情報をホストインスタンスにコピーします。
4. クレデンシャル JSON ファイルをアプリケーションのソースリポジトリにコミットし、CI/CD プロセスに、インスタンスにデプロイされるソフトウェアと一緒にパッケージ化させます。
<details><div>
    答え：2
説明
不正解
A. 署名付きURLは、リソース（Googleクラウドストレージのファイルなど）への一時的なアクセスに使用され、サービスアカウントの認証には使用されません。
C. インスタンスのデプロイ後に GCP コンソールから P12 ファイルを生成し、アプリケーションを起動する前にホスト・インスタンスに資格情報をコピーする。また、P12 ファイルは安全性が低く、Google Cloud のサービス・アカウント管理では非推奨とされています。
D. ソースコードリポジトリにクレデンシャルをコミットすることは、セキュリティ上悪い習慣です。リポジトリにアクセスできる人なら誰でも認証情報にアクセスできるので、セキュリティリスクにつながる可能性があります。
正しい答え
B. Compute EngineインスタンスがGoogle Cloudサービスとやり取りする必要がある場合、作成時にインスタンスにサービスアカウントを割り当てるのがベストプラクティスです。Googleが提供するApplication Default Credentials (ADC)ライブラリは、インスタンスに関連付けられたサービスアカウントの認証情報を自動的に取得し、Google Cloudサービスへの認証されたリクエストを可能にします。このアプローチは安全であり、手動でのクレデンシャル管理を必要としない。
リンク
https://cloud.google.com/compute/docs/api/how-tos/authorization
</div></details>

## Q. 3-20
本番環境のGoogle Kubernetes Engine（GKE）クラスタでアプリケーションを実行しています。Cloud Deployを使用して、アプリケーションを本番GKEクラスタに自動的にデプロイします。開発プロセスの一環として、アプリケーションのソースコードに頻繁に変更を加える予定であり、リモートソースコードリポジトリにプッシュする前に変更をテストするツールを選択する必要があります。

ツールセットは以下の要件を満たす必要があります：

- ローカルの頻繁な変更を自動的にテストする。

- ローカルへのデプロイは、本番環境へのデプロイをエミュレートします。

最小限のリソースを使用して、ラップトップ上でコンテナの構築と実行をテストするには、どのツールを使用すべきですか？
1. Docker Composeとdockerd
2. Terraformとkubeadm
3. MinikubeとSkaffold
4. kanikoとTekton
<details><div>
    答え：3
説明
不正解
A. ローカルのDockerコンテナオーケストレーションに焦点を当てていますが、真のKubernetes環境を提供していません。
B. 軽量なローカル開発環境ではなく、Infrastructure-as-CodeとKubernetesクラスタのブートストラップに関わる。
D. コンテナイメージの構築とKubernetesネイティブのCI/CDに関係するが、MinikubeやSkaffoldと同じようにKubernetesのローカル開発と自動テストには関係しない。
正解
C. 頻繁なローカルの変更を自動的にテストし、本番デプロイをエミュレートするローカルデプロイを持つという要件を考えると、ローカルのKubernetes開発と継続的な開発ワークフローを提供するツールが必要です。
Minikube： ラップトップ上でシングルノードのKubernetesクラスタをローカルに実行でき、本格的な環境のオーバーヘッドなしに、実際のKubernetesクラスタに対してテストと開発を行うシンプルな方法を提供します。これは、ローカルマシン上で本番デプロイをエミュレートするのに役立つ。
Skaffold： Skaffoldは、Kubernetesアプリケーションの継続的開発を容易にするコマンドラインツールだ。開発中にアプリケーションを自動的にビルド、プッシュ、デプロイし、開発から本番環境へのシームレスな移行を可能にする。
これら2つのツールを組み合わせることで、指定された要件を満たすことができる。
リンク
Minikubeドキュメント
Skaffoldドキュメント
</div></details>

## Q. 3-21
Google Kubernetes Engine (GKE) クラスタにアプリケーションをデプロイすることを計画しています。あなたのアプリケーションは水平にスケールすることができ、アプリケーションの各インスタンスは安定したネットワークIDと独自の永続ディスクを持つ必要があります。

どのGKEオブジェクトを使うべきですか？
1. デプロイメント
2. ステートフルセット
3. レプリカセット
4. レプリケーションコントローラー
<details><div>
    答え：
説明
不正解
A. C. D. 
オプションA（Deployment）、C（ReplicaSet）、およびD（ReplicationController）は、この特定のシナリオに必要な安定したネットワークIDおよび個々の永続ディスクアタッチメントを本質的に提供しません。
正解
B. StatefulSetは、Podのセットのデプロイとスケーリングを管理し、これらのPodの順序と一意性を保証するKubernetesワークロードオブジェクトです。Podが交換可能なデプロイメントとは異なり、StatefulSet内の各Podは、StatefulSetの名前とPodの序数（web-0、web-1など）からホスト名を導出します。つまり、ネットワークのアイデンティティは安定したままです。さらに、StatefulSetは一般的に永続ボリュームと共に使用されるため、各インスタンスは独自の永続ディスクを持つことができます。
リンク
- ステートフルセット｜Google Kubernetes Engine（GKE） | Googleクラウド
- 第10章. StatefulSets: レプリケートされたステートフル・アプリケーションのデプロイ - Kubernetes in Action
</div></details>

## Q. 3-22
HTTP クラウド関数を使用して、デスクトップ ブラウザとモバイル アプリケーションの両方のクライアントからのユーザー アクティビティを処理するアプリケーションがあります。この関数は、HTTP POST を使用するすべてのメトリック送信のエンドポイントとして機能します。
レガシーの制限により、この関数は、Web またはモバイル セッションでユーザーが要求するドメインとは別のドメインにマッピングする必要があります。Cloud Function のドメインは https://fn.example.com です。デスクトップクライアントとモバイルクライアントはドメインhttps://www.example.com。関数のHTTPレスポンスにヘッダーを追加する必要があります。
HTTP レスポンスにヘッダーを追加して、ブラウザとモバイルのセッションだけがメトリクスをクラウド関数に送信できるようにする必要があります。

どのレスポンス・ヘッダを追加しますか？
1. Access-Control-Allow-Origin： *
2. Access-Control-Allow-Origin：https://*.example.com。
3. Access-Control-Allow-Origin: https://fn.example.com
4. アクセス制御-許可-オリジン: https://www.example.com
<details><div>
    答え：4
説明
不正解
A. オプションA（*）は、どのドメインでもリソースにアクセスできるようにするもので、必要な特定のドメインへのアクセスを制限するものではありません。
B. オプションB(https://*.example.com)は、Access-Control-Allow-Originヘッダーの 有効な値ではない。この文脈では、ワイルドカードは単一の*文字以外 サポートされないからである。
C. オプションC（https://fn.example.com）は、Cloud Function 自身のドメインからのアクセスのみを許可することになり、デスクトップおよびモバイルクライアントのドメインからのアクセスを許可する要件と一致しません。
正解
D. このオプションは、指定されたドメイン（https://www.example.com）のみがリソースにアクセスすることを明示的に許可します。これは、記載されている要件を満たしています。
リンク
https://cloud.google.com/functions/docs/samples/functions-http-cors
</div></details>

## Q. 3-23
Compute EngineアプリケーションをGoogle Kubernetes Engineに移行することにしました。Cloud Buildを使用してコンテナイメージをビルドし、それをArtifact Registryにプッシュする必要があります。

あなたは何をすべきですか？(2つの選択肢を選んでください）
1. gcloud builds submitコマンド
2. 
3. 
4. cloudbuild.yaml
5. 
<details><div>
    答え：
説明
不正解です：
選択肢Bは、Cloud Runへのコンテナのデプロイに関するものであり、Cloud Buildを使用してイメージをビルドしてプッシュすることではありません。
選択肢 C は、既存のイメージへのタグの追加に関するものであり、Cloud Build を使用したイメージの構築とプッシュに関するものではありません。
選択肢Eは、gcloudビルダーを使用したアプリのデプロイについて説明しており、ArtifactレジストリへのDockerイメージのビルドとプッシュについて説明していません。
正解です：
選択肢Aは、gcloud builds submitコマンドを使用してCloud Buildプロセスを開始するので正しいです。
選択肢Dは、Dockerイメージをビルドし、それをレジストリにプッシュする手順の概要を示しているため、正解です。指定されたスニペットにタイプミスがあるので（steps構文のハイフンが抜けている）、上記のように修正してください。
リンク
https://cloud.google.com/artifact-registry/docs/configure-cloud-build
https://cloud.google.com/sdk/gcloud/reference/builds/submit
</div></details>

## Q. 3-24
運用中のアプリケーションがあります。これは、管理されたインスタンスグループによって制御されるCompute Engine仮想マシンインスタンス上にデプロイされています。トラフィックは、HTTP(S) ロードバランサーを介してインスタンスにルーティングされます。ユーザーはアプリケーションにアクセスできません。あなたは、アプリケーションが利用できないときに警告を発する監視技術を実装したいと考えています。どのテクニックを選択すべきですか？
1. スモークテスト
2. スタックドライバ・アップタイム・チェック
3. クラウド負荷分散-ヘルスチェック
4. マネージド・インスタンス・グループ - ヘルス・チェック
<details><div>
    答え：2
説明
不正解
A. スモークテストは、しばしば手動で、あるいは継続的インテグレーションプロセスの一部として実行されるテストの一種です。通常、本番アプリケーションの継続的な監視には使用されません。
C. クラウドロードバランシング-ヘルスチェックは、どのバックエンドインスタンスが健全でトラフィックを受け取ることができるかを判断するためにロードバランサーによって使われる。重要ではあるが、アプリケーション自体がダウンした場合のアラート機能は直接提供しない。
D. マネージド・インスタンス・グループ-ヘルス・チェックは、マネージド・インスタンス・グループ内のインスタンスの健全性を判断するために使用されます。オプションCと同様に、アプリケーションがエンドユーザーから利用できない場合に直接アラートすることはありません。
正解
B. Stackdriver uptime checks（現在はGoogle Cloud Monitoring uptime checksと呼ばれています）は、HTTPまたはHTTPSエンドポイント上でチェックを構成することができます。エンドポイントに定期的にリクエストを送信し、利用できない場合に警告を発します。このため、アプリケーションを外部から利用できない場合にアラートを出すのに適している。
リンク
https://cloud.google.com/monitoring/uptime-checks
Stackdriver Monitoring Automation Part 3: Uptime Checks｜by Charles｜Google Cloud - Community｜Medium
</div></details>

## Q. 3-25
あなたのチームメイトから、以下のコードを確認するように頼まれました。このコードの目的は、多数の小さな行をBigQueryテーブルに効率的に追加することです。

BigQuery service = BigQueryOptions.newBuilder().build().getService()；
public void writeToBigQuery (Collection<Map<String, String>> rows){ 以下のようにします。
    for (Map<String, String> rows: rows) { { { { { { { (Map<String, String> rows: rows)
        InsertAllRequest insertRequest = InsertAllRequest.newBuilder(
            "datasetId", "tableId"、
            InsertAllRequest.RowToInsert.of(row)).build()；
        service.insertAll (insertRequest)；
    }
}
チームメイトにどの改善を提案しますか？
1. 各リクエストに複数の行を含める。
2. 複数のスレッドを作成して並列に挿入を実行する。
3. 各行をクラウドストレージオブジェクトに書き込み、BigQueryにロードする。
4. 各行を並列にクラウドストレージオブジェクトに書き込み、その後BigQueryにロードする。
<details><div>
    答え：1
説明
不正解
B. 
選択肢Bの「複数のスレッドを作成して並列に挿入を実行する」は、パフォーマンスを向上させるかもしれませんが、各行に対する個別のリクエストのため、依然として高いネットワーク・オーバーヘッドが発生します。
C. D. 
オプションCとDは、別のサービス（クラウドストレージ）が関与するため、さらなる複雑さと潜在的な待ち時間が発生します。これらのオプションは、挿入操作の制限を超える非常に大きなデータセットを処理する場合に適しています。
正解
A. 
1回のリクエストで複数の行をバッチ処理することで、APIコールの回数を減らし、ネットワークのオーバーヘッドを減らし、全体的なパフォーマンスを向上させることができます。(InsertAllRequest)
リンク
BigQueryへのデータストリーミング
</div></details>

## Q. 3-26
あなたの会社の開発チームは、それぞれのローカル環境からリソースを管理したいと考えています。

あなたは、各チームの Google Cloud プロジェクトへの開発者アクセスを有効にするよう依頼されました。Google が推奨するベストプラクティスに従いつつ、効率を最大化したいと考えています。

どうすればよいでしょうか？
1. ユーザーをプロジェクトに追加し、関連するロールをユーザーに割り当て、関連する各プロジェクトIDをユーザーに提供します。
2. ユーザーをプロジェクトに追加し、関連するロールをユーザーに割り当て、関連する各プロジェクト番号をユーザーに提供する。
3. グループを作成し、グループにユーザーを追加し、関連するロールをグループに割り当て、関連する各プロジェクトIDをユーザーに提供します。
4. グループを作成し、グループにユーザーを追加し、関連するロールをグループに割り当て、関連する各プロジェクト番号をユーザーに提供する。
<details><div>
    答え：3
説明
不正解
A. 個々のユーザーの権限を管理することは、特にチームが大きくなったり変更されたりすると、扱いにくくなります。効率的でなく、ヒューマンエラーにつながる可能性があります。
B. さらに、プロジェクト番号は一般的にプロジェクトIDよりも人間にとって使いにくいものであるため、番号を提供するのは現実的ではないかもしれません。
D. グループを使用するのはベストプラクティスですが、プロジェクトIDの代わりにプロジェクト番号を提供するのは現実的ではありません。プロジェクトIDの方が人間が読みやすく、通常プロジェクトを参照するために使用されるため、ユーザーに提供するのに適しています。
正解
C. 選択肢Cは、グループベースのアクセス制御の利点を活用し、プロジェクトIDを提供して使い勝手を向上させるため、Googleが推奨するベストプラクティスに最も合致しています。
リンク
Google Cloud IAM アクセスの管理
</div></details>

## Q. 3-27
運用チームから、プロジェクト内で実行されているCloud Bigtable、Memorystore、およびCloud SQLデータベースを一覧表示するスクリプトの作成を依頼されました。スクリプトでは、ユーザーがフィルタ式を送信して、表示される結果を制限できるようにする必要があります。

どのようにデータを取得する必要がありますか？
1. HBase API、Redis API、MySQL接続を使用してデータベースのリストを取得します。結果を結合し、フィルタを適用して結果を表示する。
2. HBase API、Redis API、MySQL接続を使ってデータベースのリストを取得する。結果を個別にフィルタリングし、それらを組み合わせて結果を表示する。
3. gcloud bigtable instances list、gcloud redis instances list、gcloud sql databases listを実行する。アプリケーション内でフィルタを使用し、結果を表示する。
4. gcloud bigtableインスタンスリスト、gcloud redisインスタンスリスト、およびgcloud sqlデータベースリストを実行します。各コマンドで-filterフラグを使用し、結果を表示します。
<details><div>
    答え：4
説明
不正解
A. この場合、サービスごとにカスタム接続と追加ロジックが必要になり、不必要に複雑になります。gcloudコマンドラインツールは、この情報にアクセスするための、より効率的で標準的な方法を提供します。
B. 繰り返しになるが、個別のAPIと接続を使うと複雑さが増す。このアプローチでは、このようなタスクを単純化するために設計された、利用可能なgcloudコマンドも利用できない。
C. このオプションは、インスタンスを一覧表示するためにgcloudコマンドラインツールを正しく使用しますが、コマンドラインレベルでフィルタを適用できる組み込みの-filterフラグを利用しません。アプリケーション内でフィルタを適用するには、追加のロジックが必要です。
正しい答え
D. このオプションは、gcloudコマンドライン・ツールと-filterフラグを利用して、Cloud Bigtable、Memorystore、およびCloud SQLのインスタンスを効率的にリストし、フィルタリングします。このアプローチは、ベスト・プラクティスに沿い、組み込み機能を活用して複雑さを軽減します。
リンク
https://cloud.google.com/sdk/gcloud/reference/topic/filters
</div></details>

## Q. 3-28
あなたは、階層的なデータ構造をデータベースに保存するモバイルアプリケーションを構築しています。このアプリケーションは、オフラインで作業しているユーザーがオンラインに戻ったときに変更を同期できるようにします。バックエンドサービスは、サービスアカウントを使用してデータベース内のデータをリッチ化します。このアプリケーションは非常に人気があると予想され、シームレスかつ安全に拡張する必要がある。

どのデータベースとIAMロールを使うべきか？
1. Cloud SQLを使用し、roles/cloudsql.editorロールをサービスアカウントに割り当てます。
2. Bigtableを使用し、roles/bigtable.viewerロールをサービスアカウントに割り当てます。
3. ネイティブ・モードのFirestoreを使用し、roles/datastore.userロールをサービス・アカウントに割り当てます。
4. データストアモードのFirestoreを使用し、roles/datastore.viewerロールをサービスアカウントに割り当てます。
<details><div>
    答え：3
説明
不正解
A. Cloud SQLはリレーショナルデータベースであり、階層的なデータ構造には最適な選択ではないかもしれません。
editorロールは編集と管理へのフルアクセスを許可しますが、単にデータをリッチ化するには必要以上の権限かもしれません。
B. Bigtableは大規模用に設計されていますが、階層構造ではなく、高スループットでスケーラブルなNoSQLデータによく使用されます。
ビューアロールは読み取りアクセスしか許可しないため、データをエンリッチ（書き込み）する必要のあるサービスアカウントには適していません。
D. データストアモードのFirestoreも階層データをサポートしますが、サーバサイドのアプリケーションにより重点を置いています。
ビューアロールは読み取りアクセスしか許可しないため、データのエンリッチ化には適していません。
正解
C. ネイティブ・モードのFirestoreはモバイル・アプリケーション向けに設計されており、階層的なデータ構造とオフライン同期をサポートしています。
roles/datastore.userロールは、エンティティの読み取り、書き込み、および削除の権限を提供し、データをエンリッチするサービスアカウントに適しています。
このオプションは、シナリオで述べられている要件と一致しています。
リンク
https://cloud.google.com/architecture/building-scalable-apps-with-cloud-firestore
https://firebase.google.com/docs/firestore/manage-data/enable-offline
</div></details>

## Q. 3-29
あなたはCI/CDチームと協力して、あなたのチームが導入した新機能のトラブルシューティングを行っている開発者です。CI/CDチームはHashiCorp Packerを使用して、開発ブランチから新しいCompute Engineイメージを作成しました。イメージは正常にビルドされましたが、起動しません。あなたはCI/CDチームと問題を調査する必要があります。

あなたは何をすべきでしょうか？
1. 新しい機能ブランチを作成し、ビルドチームにイメージの再構築を依頼します。
2. デプロイされた仮想マシンをシャットダウンし、ディスクをエクスポートしてから、ディスクをローカルにマウントしてブートログにアクセスする。
3. ローカルにPackerをインストールし、ローカルにCompute Engineイメージをビルドし、個人のGoogle Cloudプロジェクトで実行する。
4. シリアルポートを使用してCompute Engine OSのログを確認し、Cloud Loggingのログを確認してシリアルポートへのアクセスを確認します。
<details><div>
    答え：4
説明
不正解
A. 新しい機能ブランチを作成し、イメージを再構築しても、現在の起動しないイメージの問題を直接解決することはできません。
B. 仮想マシンをシャットダウンし、ディスクをエクスポートしてローカルにマウントすると、プロセスが複雑になり、関連するブートログに直接アクセスできない可能性があります。
C. ローカルでCompute Engineイメージをビルドすることで、問題を再現することができますが、問題に新たな変数が追加される可能性があり、問題が発生している正確な環境を反映していない可能性があります。
正解
D. シリアルポートを使用してCompute Engine OSのログを確認すると、ブートログやその他のシステムログにアクセスできるようになり、ブートの問題に関する情報が含まれているはずです。Cloud Loggingを使用して、シリアルポートへのアクセスやその他の関連ログを確認することもできます。このアプローチは、新しい変数や不必要な複雑さを導入することなく、問題に直接対処します。
リンク
https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console
ブートディスクが起動しない原因を特定する
Anthosを使った最新のCI/CD： ソフトウェアデリバリーフレームワーク｜クラウドアーキテクチャセンター
</div></details>

## Q. 3-30
データ移行の一環として、オンプレミスの仮想マシンからGoogle Cloud Storageにファイルをアップロードしたい。これらのファイルは、GCP環境のCloud DataProc Hadoopクラスタによって消費されます。

どのコマンドを使用すべきですか？
1. gsutil cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/。
2. gcloud cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/。
3. hadoop fs cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/
4. gcloud dataproc cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/
<details><div>
    答え：1
説明
不正解
B. C. D. 
オプションBはgcloud cpを使用しようとしますが、gcloudにはこの目的のためのcpコマンドがありません。
オプションCはHadoopのファイルシステムコマンドを使おうとしていますが、ローカルマシンからクラウドストレージにコピーするため、ここでは適用できません。
オプションDは、gcloud dataprocコマンドセット内の存在しないコマンドを参照しているように見えます。
正解
A. Google Cloud Storageにファイルをアップロードするには、Cloud Storageと対話するためのCLIを提供するgsutilコマンドラインツールを使用する必要があります。提供されているオプションの中で、ローカルファイルをクラウドストレージのバケットにコピーするには、オプションAが正しいです：
gsutil cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/。
リンク
https://cloud.google.com/storage/docs/gsutil/commands/cp
https://cloud.google.com/storage/docs/uploading-objects
</div></details>

## Q. 3-31
あなたはGoで書かれ、Google Kubernetes EngineにデプロイされたWebアプリケーションを監視しています。CPUとメモリの使用率が増加していることに気づきました。どのソースコードがCPUとメモリのリソースを最も消費しているかを判断する必要があります。

あなたは何をすべきですか？
1. VMにSnapshot Debuggerエージェントをダウンロード、インストール、起動します。最も長い時間がかかる関数のデバッグスナップショットを取ります。コール・スタック・フレームを確認し、スタック内のそのレベルにあるローカル変数を特定します。
2. Cloud Profiler パッケージをアプリケーションにインポートし、Profiler エージェントを初期化します。生成されたフレームグラフを Google Cloud コンソールで確認し、時間のかかる関数を特定します。
3. OpenTelemetry と Trace エクスポートパッケージをアプリケーションにインポートし、トレースプロバイダ を作成します。Trace overview ページでアプリケーションのレイテンシデータを確認し、どこでボトルネックが発生しているかを特定します。
4. Web アプリケーションのログを収集する Cloud Logging クエリを作成します。アプリケーションの最長の関数の開始と終了のタイムスタンプの差を計算し、時間集約的な関数を特定する Python スクリプトを作成します。
<details><div>
    答え：2
説明
不正解
A. スナップショット・Debuggerは、コード内の特定の問題をデバッグしたり、ライブ・アプリケーションのスナップショットを取るために使用されます。CPU とメモリーの使用率をプロファイリングするようには設計されていません。
C. OpenTelemetry と Trace エクスポートパッケージは、観測可能性とトレースに使用されます。これは、 アプリケーションを監視し、ボトルネックを特定するのに役立ちますが、詳細な CPU とメモリのプロファイリングは提供しません。
D. Cloud Logging クエリを作成し、タイムスタンプ間の時間差を計算する Python スクリプトを記述することは、時間のかかる関数を特定するのに役立つかもしれませんが、CPU とメモリの使用率に関する具体的な洞察を提供しません。
正解です：
B. Cloud Profiler は特に CPU とメモリの使用率を分析するように設計されており、最もリソースを消費しているコードの正確な領域を特定するのに役立ちます。
リンク
https://cloud.google.com/profiler/docs/about-profiler#profiling_agent
https://cloud.google.com/profiler/docs/about-profiler#environment_and_languages
</div></details>

## Q. 3-32
新しいAPIを構築しようとしている。画像を保存するコストを最小限に抑え、画像配信の待ち時間を短縮したい。
どのアーキテクチャを使うべきか？
1. クラウド・ストレージに支えられたApp Engine
2. 永続ディスクでバックアップされたコンピュート・エンジン
3. クラウド・ファイルストアにバックアップされた転送アプライアンス
4. クラウド・ストレージに支えられたクラウド・コンテンツ・デリバリー・ネットワーク（CDN）
<details><div>
    答え：4
説明
不正解
A. これは適切なアプローチかもしれませんが、特にグローバルに分散しているユーザーにとっては、CDNほど効果的に待ち時間を最小化できないかもしれません。
B. このオプションは、CDNソリューションに比べてコストとレイテンシーが高くなる可能性がある。なぜなら、パーシステント・ディスクのストレージはより高価であり、ディストリビューションが最適化されていない可能性があるからだ。
C. Transfer Applianceは大きなデータセットをGoogle Cloudに転送するために使用され、Cloud Filestoreはマネージドファイルストレージサービスである。この組み合わせは、低レイテンシーで画像を提供するという要件には合致しません。
正解
D. このオプションは、Googleのグローバル分散システムを活用し、最も近いグローバルロケーションからユーザーにサービスを提供することで待ち時間を短縮します。クラウド・ストレージは画像の保存に費用対効果が高い。この組み合わせは、指定されたニーズに対して最高のパフォーマンスとコスト効率を提供する可能性が高い。
リンク
https://cloud.google.com/cdn/docs/overview
</div></details>

## Q. 3-33
あなたは、それぞれのクラウドストレージバケット内のユーザーのオブジェクトのメタデータ内の特定のプロパティを取得する必要がある、新しい一般向けアプリケーションを開発しています。プライバシーとデータ保存の要件により、オブジェクト・データではなくメタデータのみを取得する必要があります。検索プロセスのパフォーマンスを最大化したい。

メタデータはどのように取り出すべきでしょうか？
1. patchメソッドを使用します。
2. composeメソッドを使う。
3. copyメソッドを使用します。
4. fields リクエスト・パラメータを使用する。
<details><div>
    答え：4
説明
不正解
A. patchメソッドは、既存のオブジェクトのメタデータを更新するために使用されます。メタデータを取得するという要件には合致しません。
B. composeメソッドは、Cloud Storage内で複数の既存オブジェクトを新しいオブジェクトに結合します。これは、メタデータを取得するという要件とは関係ありません。
C. コピー・メソッドを使う： オブジェクトをコピーすると、メタデータだけでなくオブジェクト全体を操作することになるため、メタデータのみを取得するという要件に合致しません。
正解
D. fieldsリクエスト・パラメータを指定することで、APIのレスポンスを必要なフィールドのみに制限することができます。これにより、レスポンス・サイズと転送されるデータ量の両方を減らすことができ、パフォーマンスが向上し、オブジェクト・データを含まないメタデータのみを取得するという要件を満たすことができます。
リンク
https://firebase.google.com/docs/storage/web/file-metadata#get_file_metadata
https://cloud.google.com/storage/docs/json_api/v1/objects/get
</div></details>

## Q. 3-34
Compute Engineインスタンスで実行されるアプリケーションを管理しています。また、Compute Engineインスタンスで実行されるスタンドアロンDockerコンテナで実行される複数のバックエンドサービスがあります。バックエンドサービスをサポートするCompute Engineインスタンスは、複数のリージョンで管理されたインスタンスグループによってスケーリングされます。アプリケーションの呼び出しは疎結合にしたい。リクエストで見つかったHTTPヘッダーの値に基づいて選択される、個別のサービス実装を呼び出せるようにする必要があります。

バックエンドのサービスを呼び出すには、どのGoogle Cloudの機能を使うべきですか？
1. トラフィックディレクター
2. サービスディレクトリ
3. Anthosサービスメッシュ
4. 内部HTTP(S)ロードバランシング
<details><div>
    答え：1
説明
不正解
B. HTTPヘッダーに基づくルーティングではなく、サービスの登録と検出に使用します。
C. 強力ではあるが、この特定のルーティングタスクには必要以上に複雑なソリューションである。
D. これは、トラフィックの分散を処理しますが、Traffic Directorが提供するHTTPヘッダーに基づくきめ細かいルーティング制御を提供しません。
正解
A. Google CloudのTraffic Directorは、サービスメッシュ用の完全に管理されたトラフィックコントロールプレーンで、トラフィックのルーティングとロードバランシングを管理できます。これはHTTPヘッダーに基づくマッチングなどの高度なルーティング機能をサポートしており、まさにシナリオで説明されている要件です。
リンク
https://cloud.google.com/traffic-director/docs/overview#traffic_management</div></details>

## Q. 3-35
御社は、顧客の購入履歴を保存し、以下の要件を満たすデータベースソリューションを必要としています：

顧客は購入履歴を送信後すぐに照会できる。

様々なフィールドに基づいて購入履歴をソートできる。

異なるレコード形式を同時に保存できる。

これらの要件を満たすストレージ・オプションはどれですか？
1. ネイティブ・モードのFirestore
2. オブジェクト読み取りを使用するクラウドストレージ
3. SQL SELECT文を使ったCloud SQL
4. グローバルクエリを使用したデータストアモードのFirestore
<details><div>
    答え：1
説明
不正解
B. クラウド・ストレージはオブジェクト・ストレージ・サービスであり、フィールドに基づく即時クエリやソートを提供しません。非構造化データの保存には適していますが、データベースのようなクエリ機能はありません。
C. Cloud SQL（リレーショナル・データベース・サービス）ではクエリやソートは可能だが、NoSQLデータベースのように異なるレコード形式をネイティブには扱えない。異なるフォーマットに対応するためにはスキーマの変更が必要となり、説明したユースケースには適していない。
D. FirestoreのデータストアモードもNoSQLデータベースサービスであり、適していると思われるかもしれません。しかし、FirestoreのNativeモードは、より堅牢なインデックス作成機能とクエリ機能を備えており、要件により適しています。
正解
A. NativeモードのFirestoreは、柔軟でスケーラブルなNoSQLデータベース・ストレージを提供します。クライアントが書き込みを即座に読み取ることができ、さまざまなフィールドに対するインデックス機能とソート機能を提供します。異なるレコード形式は、異なるフィールドを持つドキュメントとして保存することができます。
リンク
https://cloud.google.com/datastore/docs/firestore-or-datastore
</div></details>

## Q. 3-36
あなたは急成長中の金融テクノロジー新興企業に勤めています。あなたはGoで書かれ、シンガポールリージョン（asia-southeast1）のCloud Runでホストされている決済処理アプリケーションを管理しています。決済処理アプリケーションは、同じくシンガポール地域にあるクラウドストレージバケットに保存されたデータを処理します。

この新興企業は、アジア太平洋地域にさらに拡大することを計画している。今後6ヶ月の間に、ジャカルタ、香港、台湾にペイメントゲートウェイを展開する予定です。各拠点には、顧客データをトランザクションが行われた国に置くことを要求するデータレジデンシー要件があります。これらの展開にかかるコストを最小限に抑えたい。

どうすればよいでしょうか？
1. 各リージョンにCloud Storageバケットを作成し、各リージョンに決済処理アプリケーションのCloud Runサービスを作成します。
2. 各リージョンにCloud Storageバケットを作成し、シンガポールリージョンに決済処理アプリケーションの3つのCloud Runサービスを作成します。
3. アジアのマルチリージョンに3つのCloud Storageバケットを作成し、シンガポールリージョンに決済処理アプリケーションの3つのCloud Runサービスを作成します。
4. アジアのマルチリージョンに3つのクラウドストレージバケットを作成し、シンガポールリージョンに決済処理アプリケーションの3つのクラウド実行リビジョンを作成する。
<details><div>
    答え：1
説明
不正解
B. このオプションは、Cloud Storageバケットが各リージョンにあるにもかかわらず、すべてのCloud Runサービスがシンガポールリージョンにあり、潜在的な待ち時間の問題が発生するため、データレジデンシー要件を満たしません。
C. アジアのマルチリージョンにクラウドストレージバケットを作成しても、トランザクションが行われた特定の国にデータが存在することは保証されないため、このオプションはデータレジデンシー要件を満たさない。
D. オプションCと同様に、これはデータレジデンシー要件を満たしません。さらに、シンガポールリージョンに異なるリビジョンを作成しても、異なるリージョンのユーザーの近くにアプリケーションを展開する必要性に対処できません。
正解
A. 各地域（ジャカルタ、香港、台湾）にクラウドストレージバケットを作成することで、トランザクションが行われた国にデータが存在することを保証し、データレジデンシー要件を満たします。また、各地域にCloud Runサービスを展開することで、データへのアクセスの待ち時間を最小化し、ユーザー・エクスペリエンスを向上させ、コストを最小化するという要件に沿う可能性がある。
リンク
サービスの展開と管理
バケットの作成と管理
</div></details>

## Q. 3-37
あなたは、以下の設計要件を持つ新しいアプリケーションを開発しています：

アプリケーションインフラストラクチャの作成と変更はバージョン管理され、監査可能である。

アプリケーションとデプロイのインフラストラクチャは、できる限りGoogleが管理するサービスを使用する。

アプリケーションはサーバーレスコンピュートプラットフォーム上で実行される。

アプリケーションのアーキテクチャはどのように設計すべきでしょうか？
1. 
2. 
3. 
4. 
- gcloudコマンドを使用してアプリケーションインフラストラクチャをデプロイする：
- 継続的インテグレーションパイプラインにCloud Buildを使用する
- アプリケーションのソースコードを Git リポジトリからプルし、コンテナ化されたアプリケーションを作成する
- パイプラインのステップとして、Cloud Run上に新しいコンテナをデプロイする
<details><div>
    答え：4
説明
不正解
選択肢A、B、C
他の選択肢には、すべての要件に合致しない要素がある。例えば、選択肢AのTerraformはGoogleが管理するサービスではなく、選択肢BのJenkinsはGoogleが管理するサービスではなく、選択肢CのCompute Engineはサーバーレスプラットフォームではない。
正解
選択肢D
gcloudコマンドを使用してアプリケーションインフラストラクチャをデプロイする：
これはGoogleのコマンドラインツールを使用してリソースを管理し、バージョン管理と監査が可能です。
継続的インテグレーションパイプラインにCloud Buildを使用する：
Cloud BuildはGoogleが管理するサービスで、Googleのインフラストラクチャ上でアプリケーションのビルド、テスト、デプロイを行うことができる。
アプリケーションのソースコードを Git リポジトリからプルし、コンテナ化されたアプリケーションを作成する：
これは、バージョン管理とコンテナの活用という要件に合致する。
パイプラインのステップとして、Cloud Run上に新しいコンテナをデプロイする：
Cloud RunはGoogleが管理するサーバーレス・プラットフォームであり、サーバーレス・コンピュート・プラットフォーム上で実行するという要件を満たしている。
リンク
https://cloud.google.com/docs/ci-cd
</div></details>

## Q. 3-38
開発チームは Cloud Build を使用して、App Engine でビルドされた Node.js アプリケーションをステージング環境から本番環境にプロモートしています。
このアプリケーションは、ステージング環境の webphotos-staging という名前の Cloud Storage バケットに保存された写真の複数のディレクトリに依存しています。昇格後、これらの写真は本番環境のwebphotos-prodという名前のCloud Storageバケットで利用できるようにする必要があります。可能な限りプロセスを自動化したい。
どうすればいいでしょうか？
1. 
2. 
3. 
4. 
<details><div>
    答え：3
説明
不正解です：
オプションAはプロセスを自動化しません。
選択肢Bは、アプリケーションにコピーを処理させることになりますが、これはアプリケーションの責任ではありません。
オプションDは、存在しないcpコマンドでgcloudを使用します。
正解
オプションC
Cloud Buildを使用してステージング環境から本番環境に写真を移動するプロセスを自動化する正しいアプローチは、ディレクトリをコピーするためにgsutilを使用してビルドステップを追加することです。これはオプション C と一致します：
gsutilはCloud Storageとやり取りするためのツールで、あるバケットから別のバケットなど、ロケーション間でオブジェクトをコピーするためのcpコマンドが含まれています。
rフラグは、コピーが再帰的に行われ、すべてのディレクトリとサブディレクトリを含むことを保証します。
リンク
https://cloud.google.com/storage/docs/gsutil/commands/cp
</div></details>

## Q. 3-39
Google Kubernetes Engineにコンテナをデプロイしています。コンテナの起動に時間がかかることがあるため、ライブネス・プローブを実装しました。あなたは、起動時にライブネスプローブが時々失敗することに気づきました。

どうすればよいでしょうか？
1. 起動プローブを追加します。
2. ライブネス・プローブの初期遅延を増やします。
3. コンテナの CPU リミットを増やす。
4. readinessProbeを追加する。
<details><div>
    答え：2
説明
不正解
A. 起動プローブは、コンテナ内のアプリケーションが正常に起動したかどうかを確認するために使用されます。しかし、質問で説明されている問題は、コンテナの起動が遅いために起動時にライブネス・プローブが失敗することに関連しており、アプリケーションが起動に失敗することに起因しているわけではありません。したがって、起動プローブでは根本的な問題に対処できません。
C. 遅さが CPU の制約に関連している場合、このオプションはコンテナの起動を高速化する可能性がある。しかし、問題には、コンテナが CPU 制約のために遅いという証拠が示されていないため、この解決策では記述されている特定の問題に対処できない可能性があります。
D. 準備完了プローブは、コンテナがトラフィックの受け入れを開始する準備ができているかどうかを判断するために使用されます。この問題は、コンテナがトラフィックを処理する準備ができていないのではなく、コンテナの起動が遅いことに関連しているため、readinessProbeを追加しても、起動時にreadinessProbeが失敗する問題は解決しません。
正解
B. ライブネス・プローブは、コンテナを再起動するタイミングを知るために使用されます。コンテナの起動が遅い場合、コンテナが起動するのに十分な時間がかかる前に、有効性プローブが失敗する可能性があります。初期遅延を大きくすることで、コンテナの起動時間を増やすことができ、コンテナの起動が遅いという理由だけでライブネス・プローブが失敗することを防ぐことができる。これが、説明した問題に対する正しい解決策である。
リンク
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
</div></details>

## Q. 3-40
あなたは、Google Kubernetes Engine（GKE）上でマイクロサービスアプリケーションとしてオンラインゲームプラットフォームを開発しています。ソーシャルメディア上のユーザーから、アプリケーションへの特定のURLリクエストのロード時間が長いという苦情が寄せられています。あなたはアプリケーションのパフォーマンスボトルネックを調査し、ユーザーリクエストのレイテンシが著しく高いHTTPリクエストを特定する必要があります。

あなたは何をすべきでしょうか？
1. kubectlを使用してGKEワークロードメトリクスを構成します。すべてのPodを選択して、そのメトリクスをCloud Monitoringに送信します。Cloud Monitoringでアプリケーションメトリクスのカスタムダッシュボードを作成し、GKEクラスタのパフォーマンスボトルネックを特定します。
2. HTTP リクエストメソッドと URL パスを STDOUT にログ出力するように、マイクロサービスを更新する。ログルーターを使用して、コンテナーログを Cloud Logging に送信する。Cloud Loggingでフィルターを作成して、さまざまなメソッドとURLパスにわたるユーザーリクエストの待ち時間を評価する。
3. OpenTelemetry トレースパッケージをインストールして、マイクロサービスをインスツルメンテーションする。検査と分析のためにトレースをトレースに送信するようにアプリケーションコードを更新します。トレースで分析レポートを作成し、ユーザーリクエストを分析します。
4. GKEノードにtcpdumpをインストールします。tcpdumpを実行して、長期間にわたってネットワークトラフィックをキャプチャし、データを収集する。Wiresharkを使用してデータファイルを分析し、高遅延の原因を特定する。
<details><div>
    答え：3
説明
不正解
A. このオプションは、GKEクラスタ上で実行されているワークロードに関連するメトリクスを収集するものです。これはクラスタの全体的なパフォーマンスに関する洞察を提供する可能性がありますが、HTTPリクエストの高レイテンシの調査は特に対象としていません。
B. この方法は、ロギングに依存してレイテンシを追跡します。遅いリクエストを特定するのに役立つかもしれませんが、カスタム実装が必要で、待ち時間が発生する理由を完全に把握できない可能性があります。
D. このアプローチでは、生のネットワーク・トラフィックをキャプチャし、Wiresharkで分析します。これは貴重なネットワークレベルの洞察を提供できますが、より複雑であり、アプリケーションのアーキテクチャ内の高遅延の理解に直接つながらないかもしれません。
正解
C. マイクロサービスを OpenTelemetry でインスツルメンテーションし、Google Cloud Trace にトレースを送信することで、特定の HTTP リクエストのレイテンシに関する詳細な情報を得ることができます。このアプローチは、ユーザーリクエストのレイテンシの高いスパンを特定するという問題に直接対処するもので、説明したシナリオに最も適した選択です。
リンク
https://cloud.google.com/trace/docs/setup#when-to-instrument
</div></details>

## Q. 3-41
Google Kubernetes Engine (GKE) クラスターを作成し、次のコマンドを実行します：

> gcloud container clusters create large-cluster --num-nodes 200

コマンドはエラーで失敗します：

resource "CPUS": request requires '200.0' and is short '176.0' project has quota of '24.0' with '24.0' available.

あなたはこの問題を解決したい。どうすればいいですか？
1. GCP Consoleで追加のGKEクォータを要求します。
2. GCP ConsoleでCompute Engineの追加クォータを要求する。
3. サポートケースを開き、GKEクォータの追加を要求する。
4. クラスタ内のサービスを切り離し、より少ないコア数で機能するように新しいクラスタを書き換える。
<details><div>
    答え：2
説明
不正解
A. 制限はGKE自体ではなく、Compute EngineのCPUクォータに関連しているため、これは問題を解決しません。
C. オプションAと同様に、これはCompute EngineのCPUクォータに関連しているため、問題を解決することはできません。
D. これにより、技術的には既存のクォータ内で作業できるようになるかもしれませんが、アーキテクチャに大きな変更を伴う可能性があり、意図された "大規模クラスタ "のニーズを満たさない可能性があります。
正解
B. GCP ConsoleでCPUのCompute Engineクォータを追加要求することで、エラーメッセージに記載されている特定の制限に対処し、目的のGKEクラスタを作成できるようになります。
リンク
https://cloud.google.com/kubernetes-engine/docs/how-to/node-upgrades-quota#resolving_upgrade_errors
</div></details>

## Q. 3-42
あなたのチームメイトは、Cloud Datastoreのアカウント残高にクレジットを追加している以下のコードをレビューするようあなたに依頼しました。
チームメイトにどの改善を提案しますか？

public Entity creditAccount (long accountId, long creditAmount) { 以下のようにします。
    Entity account = datastore.get(keyFactory.newKey (accountId))；
    account = Entity.builder (account).set(
        "balance", account.getLong ("balance") + creditAmount).build()
    datastore.put(account)；
    アカウントを返します；
}
1. 先祖クエリを使用してエンティティを取得します。
2. エンティティをトランザクション内で取得し、格納する。
3. 強い一貫性を持つトランザクション・データベースを使用する。
4. 関数から口座エンティティを返さない。
<details><div>
    答え：2
説明
不正解
A. 先祖クエリは、データストアクエリで強い一貫性を強制するために使用されます。ただし、読み取り操作と書き込み操作の間の潜在的な競合状態には対処できません。
C. 別のデータベースを使用することで、一貫性モデルが変更される可能性がありますが、この特定のコードスニペットにおける問題には直接対処できません。
D. 関数がアカウント・エンティティを返すかどうかは、読み取りおよび書き込み操作の一貫性に影響しません。
正解です：
B. トランザクションの中で取得と保存の操作をカプセル化することで、潜在的な競合状態を防ぎ、クレジット操作が一貫して処理されるようにします。
リンク
https://cloud.google.com/datastore/docs/concepts/transactions#uses_for_transactions：
https://cloud.google.com/datastore/docs/concepts/transactions#using_transactions
</div></details>

## Q. 3-43
あなたの会社には、数百人の従業員に分析情報を提供するBigQueryデータマートがあります。あるユーザーは、重要なワークロードを中断することなくジョブを実行したいと考えています。このユーザーは、ジョブの実行にかかる時間を気にしていません。あなたは、会社のコストとあなたの労力を最小限に抑えながら、この要求に応えたいと考えています。

あなたは何をすべきでしょうか？
1. バッチジョブとしてジョブを実行するようユーザーに依頼する。
2. ジョブを実行するユーザー用に別のプロジェクトを作成します。
3. 既存のプロジェクトに、ユーザーをjob.userロールとして追加する。
4. 重要なワークロードが実行されていないときに、ユーザーにジョブの実行を許可する。
<details><div>
    答え：1
説明
不正解
B. 別のプロジェクトを作成することは、より複雑になり、適切に構成されない限り、ユーザーのジョブが他の重要なワークロードに干渉しないことを保証できない可能性があります。また、コストも増加する可能性があります。
C. ユーザーにjob.userロールを付与すれば、ジョブを実行できるようになるが、他のワークロードを中断することなくジョブを実行するという要件には特に対処できない。
D. ユーザーのジョブを他の重要なワークロードと調整することは、論理的に複雑になる可能性があります。
正解
A. ジョブをバッチジョブとして実行するようにユーザに依頼すれば、他の重要なワークロードを中断しないという要件を満たし、コストと管理工数を最小限に抑えることができます。BigQueryのバッチクエリは通常、緊急性のないジョブに使用され、オンデマンドで実行する場合は無料である。
リンク
https://cloud.google.com/bigquery/docs/running-queries#batch
</div></details>

## Q. 3-44
Linuxを実行しているCompute Engineインスタンス上に、パッケージ化されたアプリケーションと内部で開発されたアプリケーションが混在してホストされています。これらのアプリケーションは、ログレコードをテキストとしてローカルファイルに書き込みます。ログをCloud Loggingに書き込む必要があります。

あなたは何をすべきですか？
1. ファイルのコンテンツをLinux Syslogデーモンにパイプします。
2. Google バージョンの fluentd を Compute Engine インスタンスにインストールします。
3. GoogleバージョンのcollectdをCompute Engineインスタンスにインストールします。
4. cronを使用して、ログファイルを1日1回Cloud Storageにコピーするジョブをスケジュールします。
<details><div>
    答え：2
説明
不正解
A. このオプションは、ログコンテンツをシステムのログサービスにリダイレクトしますが、Cloud Loggingと直接統合しません。これらのログをCloud Loggingに送信するには、追加の構成またはエージェントが必要です。
C. Collectdは、システムとアプリケーションのパフォーマンスメトリクスを定期的に収集するデーモンです。ログの収集やCloud Loggingへの転送よりも、モニタリングやメトリクスに重点を置いています。
D. このオプションは、ログファイルをCloud LoggingではなくCloud Storageに保存します。これはログをアーカイブしますが、ログベースのメトリクス、モニタリング、リアルタイム分析などのCloud Loggingの機能を提供しません。
正解
B. Compute EngineインスタンスにGoogleバージョンのfluentd（Logging agent）をインストールすることで、ローカルファイルからログレコードを収集し、Cloud Loggingに転送することができます。
リンク
ロギングエージェントの設定｜Google Cloud
https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent
</div></details>

## Q. 3-45
あなたの会社の企業ポリシーでは、すべてのソースファイルの先頭に著作権コメントが必要です。各ソースコミットによってトリガーされるCloud Buildのカスタムステップを書きたいとします。トリガーは、ソースにコピーライトが含まれていることを検証し、含まれていない場合は後続のステップにコピーライトを追加する必要があります。

どうすればよいでしょうか？
1. 新しいDockerコンテナをビルドして、/workspace内のファイルを検査し、各ソースファイルのコピーライトをチェックして追加します。変更されたファイルは、ソースリポジトリに明示的にコミットバックされます。
2. 新しいDockerコンテナを構築して、/workspace内のファイルを検査し、各ソースファイルのコピーライトをチェックして追加します。変更したファイルをソースリポジトリにコミットバックする必要はありません。
3. 新しいDockerコンテナをビルドして、Cloud Storageバケット内のファイルを検査し、各ソースファイルのコピーライトをチェックして追加する。変更されたファイルはCloud Storageバケットに書き戻される。
4. Cloud Storageバケット内のファイルを検査し、各ソースファイルのコピーライトをチェックして追加する新しいDockerコンテナをビルドします。変更されたファイルは明示的にソース・リポジトリにコミットバックされます。
<details><div>
    答え：1
説明
不正解
B. このオプションは/workspaceディレクトリ内のファイルを検査し、必要に応じてコピーライトコメントを追加しますが、変更はソースリポジトリにコミットバックされません。つまり、変更はビルド・プロセスのローカルなものとなり、その後のソースコードのチェックアウトでは永続化されません。
C. Cloud Buildが動作する/workspaceディレクトリではなく、Cloud Storageバケットでソースファイルを探すため、このオプションはCloud Buildのコンテキストでは意味をなさない。
D. オプションCと同様に、これは期待される/workspaceディレクトリではなくCloud Storageバケットを参照するため、典型的なCloud Buildワークフローとは一致しません。
正解
A. workspace内のファイルを検査し、各ソースファイルのコピーライトをチェックして追加し、変更されたファイルをソースリポジトリにコミットする新しいDockerコンテナを構築することで、コピーライト表示がソースレベルで確実に実施されます。変更をリポジトリにコミットし直すことで、そのソースコードのその後のすべての使用に、必要な著作権コメントが含まれるようになります。
リンク
https://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces
</div></details>

## Q. 3-46
貴社は、大量のログデータを管理するためにCloud Loggingを使用しています。処理のためにサードパーティのアプリケーションにログをプッシュするリアルタイムのログ分析アーキテクチャを構築する必要があります。

あなたは何をすべきでしょうか？
1. Pub/SubへのCloud Loggingログエクスポートを作成する。
2. BigQueryへのCloud Loggingログエクスポートを作成します。
3. クラウドストレージへのCloud Loggingログエクスポートを作成する。
4. Cloud Loggingエントリを読み取り、サードパーティ・アプリケーションに送信するクラウド関数を作成する。
<details><div>
    答え：1
説明
不正解
B. BigQueryは大量のログデータを分析するには最適ですが、サードパーティアプリケーションにリアルタイムでログをプッシュするには最適な選択ではないかもしれません。
C. このオプションはログの保存を可能にするが、リアルタイム処理やサードパーティアプリケーションとの統合を容易にしない。
D. このアプローチは機能する可能性がありますが、Cloud Loggingとの組み込みのPub/Sub統合を使用する場合と比較して、より複雑になり、より多くのカスタム開発が必要になる可能性があります。
正解
A. Google Cloud Pub/Subは、リアルタイムのメッセージング機能を提供するように設計されたメッセージングサービスです。Pub/SubへのCloud Loggingログエクスポートを作成することで、ログエントリをPub/Subトピックにプッシュできます。そこから、サブスクライバ（サードパーティのアプリケーションを含む）はリアルタイムでメッセージを消費することができ、リアルタイムのログ分析が可能になります。
リンク
https://cloud.google.com/logging/docs/export/configure_export_v2#overview
https://cloud.google.com/logging/docs/export/pubsub
</div></details>

## Q. 3-47
あなたはGoogle Kubernetes Engine（GKE）上で動作するマイクロサービスベースのアプリケーションを開発しています。いくつかのサービスは異なるGoogle Cloud APIにアクセスする必要があります。Googleが推奨するベストプラクティスに従って、クラスタ内のこれらのサービスの認証をどのように設定しますか？(2つの選択肢を選んでください)。
1. GKEノードに接続されているサービスアカウントを使用します。
2. gcloudコマンドラインツールを使用してクラスタでWorkload Identityを有効にする。
3. 秘密管理サービスからGoogleサービスアカウント鍵にアクセスする。
4. Google サービスアカウント鍵を中央秘密管理サービスに保管する。
5. gcloudを使用して、roles/iam.workloadIdentityを使用してKubernetesサービスアカウントをGoogleサービスアカウントにバインドします。
<details><div>
    答え：2,5
説明
不正解
A. ノードのサービスアカウントを使用すると、権限が過剰にプロビジョニングされ、きめ細かな制御ができなくなる可能性があります。Workload Identityを使用して、特定のGoogleサービスアカウントを必要なサービスにバインドする方がよい。
C. 秘密鍵管理サービスを使用しても、手動で鍵を管理するとエラーが発生しやすく、Workload Identityを使用する場合と比較して安全性が低下する。
D. 選択肢Cと同様に、この方法はキーの手動管理を伴うため、複雑で潜在的なセキュリティリスクにつながる可能性がある。
正解
B. Workload Identityは、GKE上で実行するアプリケーションの認証を管理する推奨方法です。これにより、KubernetesサービスアカウントをGoogleサービスアカウントにバインドすることができ、キーを手動で管理する必要なく、アプリケーションがGoogle Cloudサービスに認証できるようになります。
E. このステップはWorkload Identityの設定の一部です。KubernetesサービスアカウントをGoogleサービスアカウントにバインドすることで、アプリケーションがGoogleサービスアカウントのIDと権限を想定できるようになります。これにより、Google Cloudサービスに対するアクセスの制御と監査が容易になります。
リンク
https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform#use_workload_identity
https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts
</div></details>

## Q. 3-48
あなたはGoogle Kubernetes Engine（GKE）にマイクロサービスアプリケーションをデプロイしています。アプリケーションは毎日更新され、Linux オペレーティングシステム（OS）上で実行される多数の個別のコンテナがデプロイされることが予想されます。あなたは、Googleが推奨するベストプラクティスに従って、新しいコンテナ内の既知のOSの脆弱性を警告したい。

どうすればよいでしょうか？
1. gcloud CLIを使用してContainer Analysisを呼び出し、新しいコンテナイメージをスキャンします。各デプロイの前に脆弱性の結果を確認します。
2. Container Analysis を有効にし、新しいコンテナ イメージを Artifact Registry にアップロードします。各デプロイの前に脆弱性の結果を確認します。
3. コンテナ分析を有効にし、新しいコンテナイメージを Artifact Registry にアップロードします。各配備の前に、重要な脆弱性の結果をレビューする。
4. Container Analysis REST APIを使用してContainer Analysisを呼び出し、新しいコンテナイメージをスキャンします。各デプロイの前に脆弱性の結果をレビューする。
<details><div>
    答え：2
説明
不正解
A. gcloud CLI を使用してイメージごとにコンテナ分析を手動で呼び出すことは、Artifact Registry を使用した統合スキャンを使用するよりも効率が悪く、エラーが発生しやすい場合があります。
C. 重要な脆弱性のみをレビューすると、他の潜在的に重要なセキュリティ問題を見落とす可能性がある。一般に、重要な脆弱性だけでなく、すべての脆弱性をレビューすることが望ましい。
D. Container Analysis REST API を使用してイメージをスキャンすることもできますが、この方法には追加の開発作業と保守が必要です。Container Analysis と Artifact Registry の統合は、合理的なソリューションを提供します。
正解
B. このアプローチでは、Linux OS およびその他のコンポーネントの既知の脆弱性についてコンテナ イメージをスキャンする Google Cloud の組み込み機能を確実に利用できます。イメージを Artifact Registry にアップロードし、Container Analysis を有効にすることで、脆弱性スキャン プロセスを自動化できます。
このオプションは、コンテナのセキュリティを維持するために Google が推奨するベストプラクティスに従っているため、既知の脆弱性を持つコンテナを GKE 環境にデプロイするリスクを低減できます。
リンク
https://cloud.google.com/blog/products/application-development/understanding-artifact-registry-vs-container-registry
</div></details>

## Q. 3-49
あなたは、Google Kubernetes Engine（GKE）上でホストされるJPEG画像リサイズAPIを開発しています。サービスの呼び出し元は同じGKEクラスタ内に存在します。クライアントがサービスのIPアドレスを取得できるようにしたい。

どうすればいいでしょうか？
1. GKEサービスを定義します。クライアントは、サービスのクラスタIPアドレスを見つけるために、Cloud DNSのAレコードの名前を使用する必要があります。
2. GKEサービスを定義する。クライアントは URL でサービス名を使用してサービスに接続する必要があります。
3. GKEエンドポイントを定義します。クライアントは、クライアントコンテナ内の適切な環境変数からエンドポイント名を取得する必要があります。
4. GKEエンドポイントを定義します。クライアントはCloud DNSからエンドポイント名を取得する必要があります。
<details><div>
    答え：
説明
不正解
A. GKEサービスを定義することは正しいですが、通常Kubernetesクラスタ内では、Cloud DNSのAレコードに依存する必要はありません。クラスタ内のPodはサービス名を直接使用できます。
C. D. 
Kubernetesには "GKEエンドポイント "という概念はありません。Kubernetesのエンドポイントはサービスの一部であり、サービスがプロキシすべきポッドのIPを追跡するために内部的に使用されます。これらはクライアントが直接やりとりするものではありません。
正しい答え
B. これはクラスタ内の内部通信を許可する正しい方法です。サービス名はDNS名として機能し、サービスのクラスタIPはクラスタ内で自動的に解決されます。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/service-discovery
</div></details>

## Q. 3-50
あなたのチームはGoogle Kubernetes Engine上で動作するサービスを開発しています。チームのコードはクラウドソースリポジトリに保存されています。本番環境にデプロイする前に、コードのバグを迅速に特定する必要があります。開発者のフィードバックを改善し、プロセスを可能な限り効率化するために自動化に投資したいと考えています。

どうすればいいでしょうか？
1. Spinnakerを使って、Gitタグに基づいたコードからコンテナイメージを自動構築する。
2. Cloud Build を使用して、Git タグに基づくコードからコンテナ・イメージを自動構築する。
3. Spinnakerを使用して、本番環境へのコンテナイメージのデプロイを自動化する。
4. Cloud Build を使用して、フォークされたバージョンに基づくコードからコンテナ・イメージを自動構築する。
<details><div>
    答え：2
説明
不正解
A. Spinnakerは継続的デリバリプラットフォームですが、この問題はGitタグに基づくコンテナイメージの構築に重点を置いており、Cloud Buildとより合致しています。
C. Spinnakerはデプロイを自動化できますが、この問題はデプロイ前にバグを特定し、ビルドプロセスを自動化することに重点を置いています。
D. フォークされたバージョンに基づいてコンテナイメージをビルドすることは、バグを迅速に特定し、プロセスを効率的に自動化するという目標に合致しません。
正解
B. Git タグをトリガーとして Cloud Build を使用することで、コードに関連する変更が加えられるたびにコンテナイメージが自動的にビルドされるようになり、開発プロセスの早い段階でバグを発見できるようになります。
リンク
https://cloud.google.com/source-repositories/docs/integrating-with-cloud-build
</div></details>

## Q. 4-1
アプリケーションは管理インスタンスグループによって制御されています。管理対象インスタンスグループ内のすべてのインスタンス間で、大きな読み取り専用のデータセットを共有したい。各インスタンスが迅速に起動し、非常に低いレイテンシでファイルシステム経由でデータセットにアクセスできるようにしたい。また、ソリューションの総コストを最小限に抑えたい。

どうすればよいでしょうか。
1. データをCloud Storageバケットに移動し、Cloud Storage FUSEを使用してバケットをファイルシステムにマウントします。
2. データをCloud Storageバケットに移動し、起動スクリプトでインスタンスのブートディスクにデータをコピーする。
3. データをCompute Engineの永続ディスクに移動し、そのディスクを複数のCompute Engine仮想マシンインスタンスに読み取り専用モードでアタッチします。
4. データをCompute Engineの永続ディスクに移動し、スナップショットを取り、スナップショットから複数のディスクを作成し、各ディスクをインスタンスにアタッチする。
<details><div>
    答え：3
説明
不正解
A. Cloud Storage FUSEを使用すると、ネットワーク通信によるレイテンシが追加され、低レイテンシ要件に違反する可能性があります。
B. すべてのインスタンスの起動ディスクにデータをコピーすることになり、起動時間とストレージコストが増加する。これはクイックスタートとコスト最小化の要件に合致しない。
D. データは読み取り専用であり、各インスタンスに固有である必要はないにもかかわらず、スナップショットから複数のディスクを作成するため、不必要な複製とコストが発生します。
正解
C. 永続ディスクがVMに直接接続されるため）レイテンシーが低く、クイックスタートが可能であり、（単一の共有ディスクを使用するため）コストを最小限に抑えることができます。
リンク
https://cloud.google.com/compute/docs/disks/add-persistent-disk#use-multi-instances
</div></details>

## Q. 4-2
あなたの会社では、レポートをクラウドストレージのバケットにアップロードするアプリケーションを作成しました。レポートがバケットにアップロードされると、Cloud Pub/Subトピックにメッセージを発行したい。あなたは、実装にわずかな労力しか必要としないソリューションを実装したいと考えています。

どうすればいいでしょうか？
1. オブジェクトが変更されたときにCloud Pub/Sub通知をトリガーするようにCloud Storageバケットを設定する。
2. ファイルを受信する App Engine アプリケーションを作成します。ファイルを受信したら、Cloud Pub/Sub トピックにメッセージを発行します。
3. Cloud StorageバケットによってトリガーされるCloud Functionを作成します。Cloud Functionで、Cloud Pub/Subトピックにメッセージを発行する。
4. ファイルを受信するためにGoogle Kubernetes Engineクラスタにデプロイされたアプリケーションを作成する。ファイルを受信したら、Cloud Pub/Subトピックにメッセージをパブリッシュする。
<details><div>
    答え：1
説明
不正解
B. この場合、まったく新しい App Engine アプリケーションを作成することになり、オプション A の単純な構成よりも多くの労力と複雑さが必要になります。
C. クラウドストレージバケットによってトリガーされるCloud Function を作成し、維持する必要があります。
D. この場合、Google Kubernetes Engineクラスタにアプリケーションをデプロイする必要がありますが、これは複雑なソリューションであり、説明されている単純なタスクには過剰です。
正解
A. Cloud Storageバケットは、バケットにオブジェクトが追加または変更されたときにCloud Pub/Subトピックに通知を送信するように構成できます。これは設定が簡単で、あなたのニーズを完全に満たします。従って、Option Aは希望する動作を実現する最も簡単で効率的な方法です。
リンク
https://cloud.google.com/storage/docs/pubsub-notifications#other_notification_options
https://cloud.google.com/storage/docs/pubsub-notifications#overview
</div></details>

## Q. 4-3
あなたは、コンテナ・ファーストのアプローチを採用している金融サービス企業に勤めている。あなたのチームはマイクロサービス・アプリケーションを開発する。Cloud Buildパイプラインがコンテナイメージを作成し、リグレッションテストを実行し、イメージをArtifact Registryに公開します。リグレッション・テストに合格したコンテナだけがGoogle Kubernetes Engine（GKE）クラスタにデプロイされるようにする必要があります。あなたはすでにGKEクラスタでバイナリ認証を有効にしています。

次に何をすべきですか？
1. 証明書とポリシーを作成します。コンテナイメージが正常にリグレッションテストに合格したら、Cloud Buildを使用してKritis Signer(Binary Authorization)を実行し、コンテナイメージのアテステーションを作成します。
2. Voucher ServerおよびVoucher Clientコンポーネントをデプロイします。コンテナ・イメージが正常にリグレッション・テストに合格したら、Voucher ClientをCloud Buildパイプラインのステップとして実行します。
3. 関連するネームスペースの［Pod Security Standard］レベルを［Restricted］に設定します。Cloud Build を使用して、リグレッション・テストに合格したコンテナ・イメージにデジタル署名を付けます。
4. 認証機関とポリシーを作成します。Cloud Buildパイプラインのステップとして、リグレッション・テストに合格したコンテナ・イメージの認証を作成する。
<details><div>
    答え：1
説明
不正解
B. Voucher ServerおよびVoucher Clientコンポーネントのデプロイについてです。Voucherは認証の作成に使用できますが、シナリオではVoucherを使用するとは記載されていないため、追加のセットアップと構成が必要になります。
C. Pod Security Standards（ポッドセキュリティ基準）とコンテナイメージへのデジタル署名を参照しています。このオプションは、ポリシー施行のためのバイナリ認証や証明には直接関係しません。
D. オプション A と似ていますが、証明書の作成に Kritis Signer のようなツールを使用することには言及していません。
正解
A. 証明書とポリシーを作成し、Cloud Build を使用して Kritis Signer を実行し、リグレッション・テストに合格したコンテナ・イメージの証明書を作成する。Kritis SignerはGrafeasプロジェクトの一部であり、Binary Authorizationと統合されている。
これらのオプションの中で、オプション A が最も適している。認証者の作成、ポリシーの設定、Binary Authorizationと連携するように設計されたツール（Kritis Signer）を使用した認証の作成など、必要なステップがすべて含まれています。
リンク
https://cloud.google.com/binary-authorization/docs/creating-attestations-kritis
</div></details>

## Q. 4-4
既存のアプリケーションは、ユーザの状態情報を単一の MySQL データベースに保持しています。この状態情報は非常にユーザ固有であり、ユーザのアプリケーション使用期間に大きく依存します。MySQLデータベースは、さまざまなユーザーに対するスキーマの維持と拡張に課題をもたらしています。

どのストレージオプションを選択すべきでしょうか？
1. Cloud SQL
2. クラウドストレージ
3. クラウドスパナー
4. クラウドデータストア／ファイアストア
<details><div>
    答え：4
説明
不正解
A. 
SQLをサポートする完全に管理されたリレーショナルデータベースです。従来のリレーショナルデータベース構造を必要とするアプリケーションに適している。スキーマの柔軟性の必要性には直接対応できないが、垂直方向に拡張できる。
B. 
画像、動画、バックアップなどのバイナリ・データの保存に適したオブジェクト・ストレージ・ソリューション。構造化されたユーザー状態情報、特に頻繁なクエリーが必要な場合には適さない。
C. 
リレーショナル・データベース構造と非リレーショナル・スケールの両方を提供するグローバル分散データベース・サービス。従来のRDBMSの利点とNoSQLデータベースの利点を兼ね備えている。しかし、スキーマの更新を管理することができますが、グローバルな一貫性と水平方向のスケーラビリティが主な関心事でない場合は、やり過ぎかもしれません。
正解
D. 
Webおよびモバイルアプリケーション用に設計されたNoSQLデータベース。スキーマレスデータの柔軟性を提供するため、柔軟なスキーマを必要とするアプリケーションに適しています。特にFirestoreはDatastoreの次期バージョンであり、改善されたリアルタイム機能とリッチなクエリ機能を提供する。
このようなシナリオを踏まえると、課題はスキーマ管理と様々なユーザーのための機能強化にある。柔軟なスキーマ設計を本質的にサポートするNoSQLデータベースは、最も適したソリューションかもしれない。
リンク
https://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for
</div></details>

## Q. 4-5
あなたは、Cloud Run上にデプロイされた本番環境のビジネスクリティカルなアプリケーションをサポートしています。アプリケーションはHTTP 500エラーを報告しており、アプリケーションの操作性に影響を与えています。エラーの数が特定の時間ウィンドウ内のリクエストの15%を超えたときにアラートを出したいと考えています。

どうすればよいでしょうか？
1. Cloud MonitoringAPIを消費するクラウド関数を作成します。Cloud Schedulerを使用して、毎日Cloud Functionをトリガーし、エラーの数が定義されたしきい値を超えた場合に警告します。
2. Google CloudコンソールのCloud Runページに移動し、サービスリストからサービスを選択します。Metricsタブを使用して、そのリビジョンのエラー数を視覚化し、毎日ページを更新する。
3. エラーの数が定義されたしきい値を超えた場合に警告する警告ポリシーをCloud Monitoringで作成します。
4. Cloud Monitoring APIを消費するCloud Functionを作成します。Cloud Composerを使用して、毎日Cloud Functionをトリガーし、エラーの数が定義されたしきい値を超えた場合にアラートを送信します。
<details><div>
    答え：3
説明
不正解
A. このオプションは、Cloud Functionの作成とCloud Schedulerでのスケジューリングを必要とするため、不必要な複雑さを追加します。Cloud Monitoringはすでにビルトインのアラート機能を提供しているため、このアプローチは冗長になります。
B. このオプションでは、エラー率を手動でチェックする必要があり、自動アラートを設定するには実用的ではありません。また、しきい値を超えた場合にアラートを送信する方法もありません。
D. オプションAと同様に、このオプションは不必要な複雑さを伴います。Cloud Functionを作成し、Cloud Composerを使用してそれをオーケストレーションすると、管理するコンポーネントが増えます。Cloud Monitoringのビルトインアラートシステムは、これらの余分なステップを必要とせずにこれを処理できます。
正解
C. 
これは正しいアプローチです：
メトリクスを直接監視し、しきい値を設定することができます。
手動でチェックする必要がなく、指定した条件に基づいてアラートを送信できる。
Cloud Functions、Cloud Scheduler、Cloud Composerのセットアップのような複雑さが必要ない。
リンク
https://cloud.google.com/monitoring/alerts/policies-in-api#metric-polices
</div></details>

## Q. 4-6
あなたのコードはプロジェクトAのCloud Functions上で実行されています。プロジェクトBが所有するCloud Storageバケットにオブジェクトを書き込むことになっています。
この問題を解決するにはどうすればいいですか？
1. ユーザーアカウントに、Cloud Storageバケットのroles/storage.objectCreatorロールを付与します。
2. あなたのユーザアカウントに、service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com サービスアカウントの roles/iam.serviceAccountUser ロールを付与します。
3. service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com サービスアカウントに、Cloud Storage バケットの roles/storage.objectCreator ロールを付与します。
4. プロジェクトBでCloud Storage APIを有効にする。
<details><div>
    答え：3
説明
不正解
A. あなたのユーザーアカウントに権限を与えても問題は解決しません。なぜなら、コードはあなたのユーザーアカウントではなく、サービスアカウントを使って実行されるからです。
B. roles/iam.serviceAccountUserロールは、クラウド・ストレージの権限付与とは関係ないため、サービス・アカウントのユーザー・アカウントにこのロールを付与しても問題は解決しません。
D. 
プロジェクトBでCloud Storage APIを有効にしても、"403 Forbidden "のようなパーミッションエラーが修正されるとは限りません。バケットを使用しているプロジェクトでは、APIはすでに有効になっているはずであり、エラーメッセージはAPIの有効化ではなく、パーミッションに関連しています。
正解
C. クラウドファンクションでコードを実行する場合、コードはファンクションに関連付けられたサービスアカウントの権限で実行されます。この場合、"403 Forbidden "というエラーは、通常、サービスアカウントがCloud Storageバケットに書き込もうとしていることに関連したパーミッションの問題を示しています。
コードはプロジェクトAのCloud Functions上で実行され、プロジェクトBが所有するCloud Storageバケットにオブジェクトを書き込むことになっているので、プロジェクトBのCloud StorageバケットのCloud Functionsインスタンス(service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com)に関連付けられたサービスアカウントに適切なパーミッションを与える必要があります。
リンク
https://cloud.google.com/functions/docs/concepts/iam#troubleshooting_permission_errors
</div></details>

## Q. 4-7
バージョン管理システム、Cloud Build、Container Registryで構成されるCI/CDパイプラインを構築しています。新しいタグがリポジトリにプッシュされるたびに、Cloud Buildジョブがトリガーされ、新しいコードでユニットテストを実行し、新しいDockerコンテナイメージをビルドし、Container Registryにプッシュします。パイプラインの最後のステップでは、新しいコンテナを本番のGoogle Kubernetes Engine（GKE）クラスタにデプロイする必要があります。以下の要件を満たすツールとデプロイ戦略を選択する必要があります：

ダウンタイムが発生しない

テストが完全に自動化されている

ユーザーにロールアウトする前にテストできる

必要に応じて迅速にロールバックできる

何をすべきか？
1. 新しいコードのA/Bテストとして構成されたSpinnakerパイプラインをトリガーし、成功したらコンテナを本番環境にデプロイします。
2. 新しいコードのカナリアテストとして構成された Spinnaker パイプラインをトリガーし、成功したらコンテナを本番環境にデプロイします。
3. Kubernetes CLI ツールを使用する別の Cloud Build ジョブをトリガーして、新しいコンテナを GKE クラスターにデプロイし、そこでカナリアテストを実行します。
4. Kubernetes CLIツールを使用する別のCloud Buildジョブをトリガーして、新しいコンテナをGKEクラスタにデプロイし、そこでシャドーテストを実行します。
<details><div>
    答え：4
説明
不正解
A. 
A/Bテストは通常、本番環境でユーザーにロールアウトする前のテストではなく、2つのバージョンを比較してどちらがより優れたパフォーマンスを発揮するかを理解するために使用されます。
B. C. 
このオプションにはカナリアテストが含まれますが、Kubernetes CLIツールに依存し、ダウンタイムゼロと迅速なロールバックに必要な包括的なオーケストレーションを提供しない可能性があります。
正解
D. 
シャドーテストでは、実際のユーザーに影響を与えることなく、新しいバージョンを古いバージョンと一緒にデプロイします。古いバージョンへのリクエストはすべて新しいバージョンに複製されますが、新しいバージョンからの結果は破棄されます。これにより、ユーザーに影響を与えることなく、新バージョンが本番環境でどのように動作するかを観察することができます。
リンク
https://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_a_shadow_test
</div></details>

## Q. 4-8
あなたのチームはGoogle Cloud上で動作するサービスを開発しています。データ処理サービスを構築する必要があり、Cloud Functionsを使用します。関数で処理されるデータは機密性の高いものです。許可されたサービスからのみ起動できるようにし、Googleが推奨するファンクションのセキュリティに関するベストプラクティスに従う必要があります。

どうすればよいでしょうか？
1. プロジェクトでIdentity-Aware Proxyを有効にします。その許可を使用して、機能アクセスを保護します。
2. Cloud Functions Viewer ロールを持つサービスアカウントを作成します。そのサービスアカウントを使って関数を呼び出す。
3. Cloud Functions Invokerロールを持つサービスアカウントを作成します。そのサービスアカウントを使用して関数を呼び出します。
4. セキュアにしたい関数と同じプロジェクトで、呼び出しサービスの OAuth 2.0 クライアント ID を作成します。その認証情報を使って関数を呼び出します。
<details><div>
    答え：3
説明
不正解
A. Identity-Aware Proxy (IAP) は、Google Cloud で実行されるアプリケーションへのアクセスを制御するために使用されますが、Cloud Functions を保護するために特別に設計されているわけではありません。
B. Cloud Functions Viewerロールは、関数に関する情報の表示は許可するが、関数の呼び出しは許可しない。
D. 
OAuth 2.0 クライアント ID を作成することは、認証フローの一部にはなりますが、それだけでは、どのサービスが関数を呼び出すことを許可されているかを適切に制御することはできません。
正解
C. 
Googleが推奨するベストプラクティスに従って、クラウド・ファンクションを保護し、許可されたサービスだけがファンクションを呼び出せるようにするには、ファンクションの呼び出しを許可するロールを持つサービス・アカウントを作成し、そのサービス・アカウントを使って呼び出しを行います。
リンク
https://cloud.google.com/functions/docs/securing/authenticating#authenticating_function_to_function_calls
</div></details>

## Q. 4-9
あなたの開発チームは、対応する統合テストとサービス・テストとともに、Javaを使用していくつかのクラウド・ファンクションを構築しました。Cloud Buildを使用して、関数をビルドしてデプロイし、テストを開始しています。Cloud Buildジョブが、コードの検証に成功した直後にデプロイの失敗を報告しています。

あなたは何をすべきでしょうか？
1. Cloud Function インスタンスの最大数を確認します。
2. Cloud Buildトリガーが正しいビルドパラメーターを持っていることを確認します。
3. 切り捨て指数バックオフポーリング戦略を使用してテストを再試行する。
4. Cloud Build サービスアカウントに Cloud Functions Developer ロールが割り当てられていることを確認してください。
<details><div>
    答え：4
説明
不正解
A. クラウドファンクションインスタンスの最大数をチェックすることは、デプロイの失敗ではなく、スケーリングと同時実行に関連しています。
B. Cloud Buildトリガーのパラメータを確認することは、ビルドプロセス自体が失敗している場合に有用かもしれませんが、この質問は、失敗がコード検証に成功した後のデプロイ中に発生していることを示しています。
C. 切り捨て指数バックオフポーリング戦略を使用してテストを再試行することは、デプロイ時の失敗ではなく、テスト中の一時的なエラーの処理に関連しています。
正解です：
D. 
Cloud Functions Developerロールは、Cloud Functionsをデプロイするために必要な権限を提供し、Cloud Buildサービスアカウントがこのロールを持っていない場合、デプロイの失敗が発生する可能性があります。
したがって、デプロイの失敗を解決するための正しいアクションは、オプションDで説明するように、Cloud BuildサービスアカウントがCloud Functionsをデプロイするために必要なロールを持っていることを確認し、確実にすることです。
リンク
https://cloud.google.com/build/docs/troubleshooting#build_trigger_fails_due_to_missing_cloudbuildbuildscreate_permission
</div></details>

## Q. 4-10
あなたは大企業の開発者です。本番環境のGoogle Kubernetes Engine（GKE）クラスタでGoで書かれたアプリケーションを実行しています。BigQueryへのアクセスを必要とする新機能を追加する必要があります。Googleが推奨するベストプラクティスに従って、GKEクラスタへのBigQueryアクセスを許可したいとします。

どうすればよいですか？
1. BigQueryアクセス権を持つGoogleサービスアカウントを作成します。JSONキーをSecret Managerに追加し、Goクライアントライブラリを使用してJSONキーにアクセスします。
2. BigQueryにアクセスできるGoogleサービスアカウントを作成します。GoogleサービスアカウントのJSONキーをKubernetesシークレットとして追加し、このシークレットを使用するようにアプリケーションを設定します。
3. BigQueryにアクセスできるGoogleサービスアカウントを作成します。GoogleサービスアカウントのJSONキーをSecret Managerに追加し、アプリケーションが使用するシークレットにアクセスするためにinitコンテナを使用します。
4. GoogleサービスアカウントとKubernetesサービスアカウントを作成します。GKEクラスタでWorkload Identityを構成し、アプリケーションデプロイメントでKubernetesサービスアカウントを参照する。
<details><div>
    答え：4
説明
不正解
A. 
シークレットマネージャでサービスアカウントのキーを管理することは、ソースコードやその他の安全でない場所にキーを保存するよりも優れていますが、手動でキーをローテーションして管理する必要があります。
B. 
サービスアカウントのJSONキーをKubernetesシークレットとして保存すると、RBACポリシーが適切に設定されていない場合、クラスタ内でシークレットが不正アクセスにさらされる可能性があります。
C. 
initコンテナでSecret Managerを使用することは、必要以上に複雑であり、サービスアカウントのキーを手動で処理する必要があります。
正解
D. 
Google Kubernetes Engine（GKE）上で実行され、他のGoogleクラウドサービス（BigQueryなど）と相互作用するアプリケーションを認証する推奨方法は、Workload Identityを使用することです。Workload Identityを使用すると、KubernetesサービスアカウントをGoogleサービスアカウントにバインドすることができ、個別のキーを扱ったり管理したりする必要がなくなります。
オプションD「GoogleサービスアカウントとKubernetesサービスアカウントを作成する。GKEクラスタ上でWorkload Identityを設定し、アプリケーションのDeploymentでKubernetesサービスアカウントを参照する "は、このベストプラクティスと一致している。
リンク
https://cloud.google.com/kubernetes-engine/docs/quickstarts/deploy-app-container-image#deploying_to_gke
</div></details>

## Q. 4-11
あなたは最近、Cloud Spannerデータベースインスタンスを本番稼動させている新しいチームに参加しました。あなたの上司は、データベースの高い信頼性と可用性を維持しながらコストを削減するために、Spannerインスタンスを最適化するようあなたに依頼しました。

あなたは何をすべきですか？
1. Cloud Loggingを使用してエラーログをチェックし、必要な最小容量が見つかるまでSpannerの処理ユニットを少しずつ減らします。
2. Cloud Monitoringを使用してCPU使用率を監視し、必要な最小容量が見つかるまでSpanner処理ユニットを少しずつ減らします。
3. Cloud Traceを使用して、Spannerへの受信リクエストの1秒あたりのリクエストを監視し、必要な最小容量が見つかるまでSpannerの処理ユニットを少しずつ減らします。
4. スナップショット デバッガを使用してアプリケーション エラーをチェックし、必要な最小容量を 見つけるまでSpannerの処理ユニットを少しずつ減らします。
<details><div>
    答え：2
説明
不正解
A. Cloud Loggingはエラーとイベントロギングに使用され、処理ユニットの最適化に関する情報に基づいた決定を下すために必要なCPU使用率と負荷要件に関する洞察を提供しません。
C. Cloud Traceはアプリケーション・リクエストの待ち時間を理解するのに役立ちますが、処理単位を決定する上でより重要な要素であるSpannerインスタンスのCPU使用率を直接示すことはできません。
D. スナップショットデバッガは、アプリケーションエラーのデバッグに使用され、Spannerインスタンスのリソース使用率と容量要件を理解するのには役立ちません。
正解です：
B. 高い信頼性と可用性を維持しながら、Cloud Spannerデータベースインスタンスをコスト面で最適化するには、処理ユニットとシステムの実際の需要のバランスを見つける必要があります。Cloud SpannerのCPU使用率メトリクスは、容量要件を理解するための重要な指標となります。

オプションBの「クラウドモニタリングを使用してCPU使用率を監視し、必要な最小容量が見つかるまでSpannerの処理ユニットを少しずつ減らす」は正しいアプローチです。

リンク
https://cloud.google.com/spanner/docs/compute-capacity#increasing_and_decreasing_compute_capacity
</div></details>

## Q. 4-12
あなたのチームは、Google Kubernetes Engine（GKE）クラスタ上でホストされるアプリケーションを作成しました。このアプリケーションを、2つの異なる地域にある2つのGKEクラスタにデプロイされたレガシーRESTサービスに接続する必要があります。アプリケーションを弾力性のある方法でターゲットサービスに接続したい。また、別のポートでレガシーサービスのヘルスチェックを実行できるようにしたい。

どのように接続を設定しますか。(2つの選択肢を選んでください)
1. アプリケーションをサービスに接続するためにサイドカープロキシでTraffic Directorを使用します。
2. プロキシレスTraffic Director設定を使用して、アプリケーションをサービスに接続します。
3. プロキシから発信されるヘルスチェックを許可するように、レガシーサービスのファイアウォールを設定します。
4. アプリケーションから発信されるヘルスチェックを許可するように、レガシーサービスのファイアウォールを設定する。
5. Traffic Directorのコントロールプレーンから発信されるヘルスチェックを許可するように、レガシーサービスのファイアウォールを設定する。
<details><div>
    答え：1,3
説明
不正解
B. プロキシレスgRPCはgRPCサービス専用であり、質問はレガシーRESTサービスを指定しているため、このオプションは適用できません。
D. 完全に不可能というわけではありませんが、このオプションは、ヘルスチェックがプロキシではなくアプリケーションから直接来ることを前提としています。Traffic Directorとサイドカープロキシを使用している場合（オプションAの場合）、ヘルスチェックはプロキシ自身から発信される可能性が高くなります。
E. Traffic Directorはヘルスチェック自体を実行しないので、これは間違っています。Traffic Directorはヘルスチェックを実行するためにプロキシ（またはプロキシレスセットアップのアプリケーション）を設定します。
正解です：
A. これは、サイドカープロキシを持つTraffic Directorが負荷分散と回復力を提供できるため、地域を越えてGKEアプリケーションを接続するための合理的な選択です。Traffic Director はマイクロサービスのトラフィックを管理するように設計されており、Envoy のようなサイドカープロキシとうまく機能します。
C. Traffic Directorをサイドカープロキシを使用している場合、ヘルスチェックはおそらくプロキシ自身から発信されるので、レガシーサービスのファイアウォールルールでこれを許可する必要があります。
リンク
https://cloud.google.com/traffic-director/docs/advanced-setup#routing-rule-maps
https://cloud.google.com/traffic-director/docs/advanced-setup
https://cloud.google.com/load-balancing/docs/health-check-concepts
</div></details>

## Q. 4-13
開発時間を最小限に抑えながら、本番稼動中のサービス低下をオンコールエンジニアに通知したい。

どうすればよいでしょうか？
1. Cloud Functionを使用してリソースを監視し、アラートを発生させます。
2. Cloud Pub/Subを使用してリソースを監視し、アラートを発生させる。
3. Stackdriver Error Reportingを使用してエラーをキャプチャし、アラートを発生させます。
4. リソースの監視とアラートの発生にはStackdriver Monitoringを使用します。
<details><div>
    答え：4
説明
不正解
A. リソースを監視するCloud Functionを記述することは可能ですが、これにはかなりのカスタム開発が必要になり、この目的のために特別に設計された既存のMonitoring toolsを活用することはできません。
B. Cloud Pub/Subは主にメッセージング・サービスであり、リソースの監視やアラートの発生に特化したものではない。
C. Stackdriver Error Reporting（現在のCloud Error Reporting）は、アプリケーションのエラーをキャプチャして追跡するために使用されますが、Cloud Monitoringのような広範なリソースおよびパフォーマンス監視機能を提供しません。
正解
D. ここでの最も適切な選択です。様々なメトリクスを追跡し、アラートをトリガーする条件を設定し、必要な担当者に通知するための組み込み機能を提供します。
リンク
https://cloud.google.com/error-reporting/docs/notifications
https://cloud.google.com/blog/products/gcp/drilling-down-into-stackdriver-service-monitoring
</div></details>

## Q. 4-14
あなたは、Google Kubernetes Engineクラスタで動作する複数のマイクロサービスで構成されるアプリケーションを開発しています。あるマイクロサービスは、オンプレミスで稼働しているサードパーティのデータベースに接続する必要があります。あなたはデータベースへの認証情報を保存し、セキュリティのベストプラクティスに従いつつ、これらの認証情報がローテーションできるようにする必要があります。

あなたは何をすべきでしょうか？
1. 認証情報をサイドカーコンテナプロキシに格納し、それを使用してサードパーティデータベースに接続します。
2. 認証情報をKubernetesシークレットとして保存し、Cloud Key Management Serviceプラグインを使用して暗号化と復号化を処理します。
3. クレデンシャルを暗号化されたボリュームマウントに格納し、クライアントPodにPersistent Volume Claimを関連付ける。
4. サービスメッシュを構成して、マイクロサービス内のPodからデータベースへのトラフィックを許可または制限します。
<details><div>
    答え：2
説明
不正解
A. サイドカーコンテナプロキシは通常、監視、ロギング、ネットワークトラフィックの制御など、他の目的に使用されます。サイドカーコンテナプロキシは、主に認証情報を安全に格納するために設計されていません。
D. Istio などのサービスメッシュは、主にサービス間通信、トラフィック管理、およびポリシーを管理しますが、クレデンシャルの安全な保管には重点を置きません。
C. ボリュームマウントを暗号化することは、機密データを格納するための可能な方法ですが、Kubernetes Secretsを使用することは、そのようなクレデンシャルを格納するためのより標準的なプラクティスです。永続ボリュームは特にシークレット用ではなく、一般的なデータストレージ用です。
正解
B. Kubernetesシークレットは、機密情報の保存と管理を可能にします。
Google CloudのKey Management Service (KMS)をKubernetesと共に使用することで、シークレットが転送中と静止時の両方で暗号化されることを保証できます。KMSはあなたのサービスが使用する暗号鍵を提供し、まさにこのユースケースのためにGKEとシームレスに統合されています。
したがって、オプションBは、与えられたシナリオのための最良の選択です。
リンク
https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
</div></details>

## Q. 4-15
ユーザが管理するキーを持つユーザが管理するサービスアカウントを使用してCloud Storage APIを認証するオンプレミス・アプリケーションがあります。このアプリケーションは、Dedicated Interconnectリンクを介してPrivate Google Accessを使用してCloud Storageに接続します。アプリケーションからのCloud Storageバケット内のオブジェクトへのアクセス要求が、403 Permission Deniedエラー・コードで失敗していることがわかりました。

この問題の原因として何が考えられますか？
1. バケット内のフォルダ構造とオブジェクトのパスが変更された。
2. サービスアカウントの定義済みロールのパーミッションが変更された。
3. サービスアカウントキーはローテーションされましたが、アプリケーションサーバー上で更新されていません。
4. オンプレミスのデータセンターから Google Cloud へのインターコネクトリンクに一時的な障害が発生しています。
<details><div>
    答え：3
説明
不正解
A. 
バケツ内のフォルダ構造やオブジェクトパスが変更された場合、通常 "403 Permission Denied" エラーではなく、"404 Not Found" エラーが発生します。
B. 
サービスアカウントの事前定義されたロールのパーミッションが変更された場合、それは異常で意図的な行動である。しかし、サービスアカウントが必要なリソースにアクセスできなくなるような方法でパーミッションが減らされた場合、"403 Permission Denied "エラーになる可能性があります。
D. 
インターコネクトリンクの停止は、接続性の問題やネットワークのタイムアウトや障害に関連するエラーにつながる可能性が高く、特にアクセス許可に関連する "403 Permission Denied" エラーにはつながりません。
正解
C. 
エラーコード "403 Permission Denied "は、通常、接続性の問題ではなく、認証または認可の問題を示します。
提供された選択肢の中で、このエラーの最も可能性の高い原因は、サービスアカウントのキーがローテートされたにもかかわらず、アプリケーションサーバ上で更新されておらず、認証の試みが失敗したことでしょう。
つまり、選択肢 C は、"403 Permission Denied "エラーの最も典型的なシナリオであり、記述されている問題の最も可能性の高い原因です。
リンク
403エラーメッセージのトラブルシューティング
</div></details>

## Q. 4-16
あなたはPub/Subサブスクリプションからクレジットカードデータを読み取るアプリケーションを開発しています。あなたはコードを書き、ユニットテストを完了しました。Google Cloud にデプロイする前に
をテストする必要があります。

あなたは何をすべきですか？
1. メッセージを発行するサービスを作成し、Pub/Subエミュレータをデプロイします。パブリッシング・サービスでランダムなコンテンツを生成し、エミュレータにパブリッシュします。
2. アプリケーションにメッセージをパブリッシュするサービスを作成します。本番環境のPub/Subからメッセージを収集し、パブリッシング・サービスで再生します。
3. メッセージを発行するサービスを作成し、Pub/Subエミュレータをデプロイします。本番環境のPub/Subからメッセージを収集し、エミュレータに発行します。
4. メッセージを発行するサービスを作成し、Pub/Subエミュレータをデプロイします。パブリッシング・サービスからエミュレータに、テスト用の標準メッセージ・セットをパブリッシュします。
<details><div>
    答え：4
説明
不正解
A. 
Pub/Subエミュレータを使用することになりますが、ランダムなコンテンツを生成すると、一貫性のある意味のあるテストシナリオが得られない場合があります。
B. 
このオプションはエミュレータを使用しません。また、本番環境からメッセージを再生すると、特にクレジットカード情報のような機密データについて、プライバシーやセキュリティに影響を及ぼす可能性があります。
C. 
選択肢Bと同様に、エミュレータで本番データを使用することになるため、プライバシーとセキュリティに関する同様の懸念が生じる可能性があります。
正解
D. 
管理されたテスト環境を提供します。エミュレータを使用し、標準的なテスト・メッセージ・セットを発行することで、実運用データに依存したり、他のサービスに依存したりすることなく、アプリケーションの統合と動作を検証できます。したがって、このオプションは、制御された安全な方法でPub/Sub統合をテストするための最も適したアプローチです。
リンク
Pub/Sub用エミュレータの使用
</div></details>

## Q. 4-17
デプロイメントリソースを使用した新しいアプリケーションリビジョンを、本番環境のGoogle Kubernetes Engine（GKE）にデプロイする予定です。コンテナが正しく動作しない可能性があります。リビジョンをデプロイした後に問題が発生した場合のリスクを最小限に抑えたい。Googleが推奨するベストプラクティスに従いたい。

どうすればよいでしょうか？
1. PodDisruptionBudgetを80%にしてローリングアップデートを実行します。
2. HorizontalPodAutoscalerのスケールダウンポリシー値を0にして、ローリングアップデートを実行します。
3. デプロイメントをStatefulSetに変換し、PodDisruptionBudgetを80%にしてローリングアップデートを実行します。
4. デプロイメントをStatefulSetに変換し、HorizontalPodAutoscalerのスケールダウンポリシー値を0にしてローリングアップデートを実行します。
<details><div>
    答え：1
説明
不正解
B. 
HPA（HorizontalPodAutoscaler）は、制御されたローリングアップデートではなく、主にCPUやメモリの使用量などのメトリクスに基づく自動スケーリングを扱うため、これは導入時のリスクの最小化とは直接関係ありません。
C. 
StatefulSetは、安定したネットワークIDと安定した永続ストレージを必要とするワークロードに使用され、通常、ステートレス・アプリケーションのローリングアップデート中のリスクを最小限に抑えるために使用されることはありません。
D. 
オプションCと同様に、更新時のリスクを最小限に抑えることを目的としたステートレスアプリケーションには適切ではありません。
正解
A. 
ローリングアップデートを実行すると、一度にダウンするPodの割合が一定になり、システムの可用性が維持されます。PodDisruptionBudget を使用することで、中断のレベルを制御し、リスクを最小限に抑えることができます。
このアプローチにより、段階的な更新が可能になり、必要なレプリカの80%を維持できます。PodDisruptionBudget（PDB）は、自発的な中断時に同時に中断されるPodの数を制限します。このオプションは、更新中の可用性を維持するのに役立ち、リスクを最小化するという目標に合致します。
リンク
https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/
https://cloud.google.com/blog/products/containers-kubernetes/ensuring-reliability-and-uptime-for-your-gke-cluster
</div></details>

## Q. 4-18
アプリケーションをローカルでテストするとうまく動作しますが、Compute Engineインスタンスにデプロイすると著しく動作が遅くなります。問題を診断する必要があります。

何をすべきでしょうか？
1. アプリケーションがローカルでより速く実行されることを示すチケットをクラウド・サポートに提出します。
2. Cloud Debugger スナップショットを使用して、アプリケーションのポイント・イン・タイムの実行を調べます。
3. Cloud Profiler を使用して、アプリケーション内のどの関数に最も長い時間がかかるかを判断します。
4. アプリケーションにロギングコマンドを追加し、Cloud Loggingを使用して、レイテンシの問題が発生している場所を確認します。
<details><div>
    答え：3
説明
不正解
A. 
クラウド サポートにチケットを提出するのは時期尚早である可能性が高いです。まず根本的な問題を理解し、速度低下の原因について具体的な証拠を収集する方が有益です。
B. 
Cloud Debugger のスナップショットは、特定の瞬間のアプリケーションの状態を理解するのに便利ですが、実行に時間がかかる関数などのパフォーマンス問題の診断には通常使用されません。
D. 
アプリケーションにロギング・コマンドを追加することは、待ち時間が発生する場所を特定するのに役立つかもしれませんが、この方法は時間がかかり、アプリケーションのパフォーマンス特性の詳細なビューを十分に提供しない可能性があります。
正解
C. 
Cloud Profilerを使用すると、アプリケーションのパフォーマンスを分析し、どの関数に最も時間がかかっているかを特定し、速度低下の根本原因を明らかにすることができます。これはパフォーマンス最適化のために特別に設計された強力なツールであり、このシナリオに最も適した選択です。
リンク
Google Cloud Profilerドキュメント
</div></details>

## Q. 4-19
アプリケーションの一部をGoogle Cloudに移行した。オンプレミスとクラウドの両方のアプリケーションで、オンプレミスにデプロイされたレガシーの監視プラットフォームを使用しています。クラウド・アプリケーションのタイムクリティカルな問題に対する通知システムの応答が遅いことに気づきました。

どうすればよいでしょうか？
1. 監視プラットフォームをCloud Monitoringに置き換えます。
2. Compute EngineインスタンスにCloud Monitoringエージェントをインストールします。
3. 一部のトラフィックを古いプラットフォームに移行します。2つのプラットフォームで同時にA/Bテストを行います。
4. Cloud LoggingとCloud Monitoringを使用して、ログを取得し、監視し、アラートを送信します。既存のプラットフォームに送信します。
<details><div>
    答え：4
説明
不正解
A. 
混合環境で、レガシー・プラットフォームがオンプレミスとクラウド展開されたアプリケーションの両方を監視している場合は、実行できない可能性があります。
B. 
この場合、Compute Engineインスタンスからしかメトリクスを収集できないため、既存の通知システムとの統合には対応できません。
C. 
これは、モニタリングというよりむしろ、アプリケーションのパフォーマンスに関連しているようです。
正解
D. 
Google Cloud のMonitoring toolsを利用して、必要なログとアラートを取得しながら、既存の監視プラットフォームと統合することができます。これにより、既存の監視システムを全面的に見直すことなく、タイムクリティカルな問題への迅速な対応が可能になる。
リンク
Cloud Loggingでのログのエクスポート
モニタリングAPI v3の概要
</div></details>

## Q. 4-20
あなたのチームはPostgreSQLデータベースとCloud Runを使って新しいアプリケーションを開発しています。あなたは、Googleクラウド上ですべてのトラフィックが非公開であることを保証する責任があります。
Cloud 上ですべてのトラフィックがプライベートに保たれるようにする責任があります。マネージドサービスを使用し、Googleが推奨するベストプラクティスに従いたいと考えています。あなたは何をすべきですか？
1. 
- 同じプロジェクトでCloud SQLとCloud Runを有効にします。
- Cloud SQL用にプライベートIPアドレスを構成する。プライベートサービスへのアクセスを有効にする。
- Serverless VPC Accessコネクタを作成します。
- Cloud Runがコネクターを使用してCloud SQLに接続するように設定する。
2. 
3. 
4. 
<details><div>
    答え：
説明
不正解です：
オプションBは、Compute Engine VM上でPostgreSQLを自分で管理することになり、マネージドサービスを使用する目的とは一致しないため、推奨されません。
オプションCは不要です。なぜなら、異なるプロジェクトを使用し、VPNを設定することは、このユースケースにとって明確な利点がないまま複雑さを増すからです。
オプションDも不必要な複雑さを伴い、Compute Engine VM上でPostgreSQLを自分で管理する必要があります。
正解です：
オプションA
1. 同じプロジェクトでCloud SQLとCloud Runを有効にします。
2. Cloud SQL用にプライベートIPアドレスを構成する。プライベートサービスへのアクセスを有効にする。
3. Serverless VPC Accessコネクタを作成します。
4. Cloud Runがコネクターを使用してCloud SQLに接続するように設定する。
Cloud SQLとCloud Runを同じプロジェクトで有効にする： Cloud SQL（マネージドサービス）を使うことで、PostgreSQLを自分で管理する必要がなくなる。同じプロジェクトで管理することで、ネットワークとアクセス管理が簡単になります。
Cloud SQL用にプライベートIPアドレスを設定します。プライベートサービスへのアクセスを有効にします： これにより、Cloud SQLとCloud Run間の接続がプライベートになり、Googleのネットワーク内に留まるようになります。
Serverless VPC Accessコネクタを作成します： これによりCloud RunはVPCネットワークに接続し、Cloud SQLインスタンスにプライベートでアクセスできるようになります。
Cloud Runがコネクターを使用してCloud SQLに接続するように設定します： これにより、Cloud RunからCloud SQLへの接続は、Serverless VPC Accessコネクタによって提供されるプライベート接続を使用するようになります。
したがって、オプションAは、マネージドサービスを使用してトラフィックをプライベートに保ち、Googleが推奨するベストプラクティスに従うという目標に最も合致する。
リンク
https://cloud.google.com/sql/docs/postgres/connect-run#configure
https://cloud.google.com/sql/docs/postgres/connect-run#private-ip
</div></details>

## Q. 4-21
あなたはGoogle Kubernetes Engine上で継承したWebアプリケーションを実行しています。あなたは、アプリケーションが既知の脆弱性を持つライブラリを使用しているかどうか、またはXSS攻撃に対して脆弱であるかどうかを判断したい。

どのサービスを使うべきですか？
1. Google Cloud Armor
2. デバッガ
3. ウェブ・セキュリティ・スキャナ
4. エラー報告
<details><div>
    答え：3
説明
不正解
A. 
分散型サービス拒否（DDoS）防御およびWebアプリケーションファイアウォール（WAF）サービスです。さまざまな種類の攻撃からアプリケーションを保護するのに役立ちますが、コードやライブラリの脆弱性をスキャンして特定するようには設計されていません。
B. 
Google Cloud のデバッガを使用すると、開発者は実行中のアプリケーションを停止したり遅くしたりすることなく、任意のコード位置でアプリケーションの状態を検査できます。これはトラブルシューティングに便利ですが、脆弱性スキャン機能はありません。
D. 
このツールは、Google Cloud で実行中のアプリケーションによって生成されたエラーを集約して表示します。アプリケーションのランタイムエラーの理解と分析に役立ちますが、XSS攻撃や既知の脆弱性を持つライブラリに関連する問題などのセキュリティ脆弱性は検出できません。
正解
C. 
正しいサービスであるWeb Security Scanner（オプションC）は、XSSを含む一般的なWebアプリケーションの脆弱性に関連する問題をスキャン、特定、報告することで、まさにこの種の脆弱性検出を実行するように設計されています。
リンク
https://cloud.google.com/security-command-center/docs/concepts-web-security-scanner-overview
</div></details>

## Q. 4-22
クラウド・クライアント・ライブラリを使用して、アプリケーションの画像をクラウド・ストレージにアップロードしています。アプリケーションのユーザーから、アップロードが完了せず、クライアント・ライブラリがHTTP 504 Gateway Timeoutエラーを報告することがあるという報告がありました。アプリケーションをエラーに強くしたい。

アプリケーションにどのような変更を加えるべきですか?
1. クライアント・ライブラリ呼び出しの周囲に指数関数的バックオフ処理を記述します。
2. クライアント・ライブラリ呼び出しの周囲に、1秒間の待ち時間バックオフ処理を記述する。
3. アプリケーションにリトライボタンを設計し、エラーが発生したらクリックするようにユーザに要求する。
4. オブジェクトのキューを作成し、アプリケーションが10分後に再試行することをユーザーに通知する。
<details><div>
    答え：1
説明
不正解
B. 
固定された1秒間の待機時間は状況に適応せず、（システムが回復するのに多くの時間を必要とする場合）短すぎるか、（より短い待機時間で十分な場合）不必要に長い可能性があります。これでは、指数関数的バックオフ戦略のような適応的な利点は得られない。
C. 
これは一時的な回避策にはなるかもしれませんが、ユーザーに解決の負担を強いることになり、ユーザーエクスペリエンスが悪くなります。さらに、根本的な問題には対処できません。
D. 
このオプションは、アプリケーションをより弾力的にするかもしれませんが、記述された特定のエラーに対処しておらず、不必要な複雑さと遅延をもたらす可能性があります。
正解
A. 
指数関数的バックオフ戦略は、再試行間の時間を徐々に増加させ、システムの負荷を軽減し、障害が発生したコンポーネントに回復するための時間を与えます。このアプローチは、一過性のエラーに対処するためにクラウドプロバイダーが推奨することが多く、これが最良の選択肢となります。
リンク
https://cloud.google.com/storage/docs/retry-strategy#exponential-backoff
</div></details>

## Q. 4-23
あなたは、GoogleドライブのAPIにアクセスし、Googleドライブにファイルを保存する許可をユーザーから取得する必要があるJavaScriptウェブアプリケーションを開発しました。あなたは、アプリケーションの認証方法を選択する必要があります。

あなたは何をすべきでしょうか？
1. APIキーを作成する。
2. SAML トークンを作成する。
3. サービスアカウントを作成する。
4. OAuthクライアントIDを作成します。
<details><div>
    答え：4
説明
不正解
A. 
APIキーは、呼び出し元のプロジェクト(あなたのアプリケーション)をGoogleに識別させるために使用されますが、ユーザーの認証やパーミッションの処理には適していません。APIキーはユーザーレベルのアクセス制御を提供しません。
B. 
SAML (Security Assertion Markup Language) はシングルサインオン (SSO) に使われるもので、Google Drive へのアクセスに個別のユーザ認証が必要なこのコンテキストには適用されません。
C. 
サービスアカウントはアプリケーションの認証と認可に使用され、個々のユーザーには使用されません。サービスアカウントを使用する場合、アプリケーションは個々のユーザーの代理ではなく、独自のIDでGoogle Driveにアクセスすることになります。
正解
D. 
OAuthは、ユーザーのパスワードを公開することなく、サードパーティのアプリケーションにユーザーリソースへのアクセスを許可するために特別に設計されています。OAuthクライアントIDを作成することで、ユーザーをGoogleがホストするページにリダイレクトし、そこでユーザーがアプリケーションに必要な権限を付与することができます。
リンク
https://developers.google.com/drive/api/guides/api-specific-auth
</div></details>

## Q. 4-24
Cloud SQLと通信するCompute Engineインスタンスにアプリケーションをデプロイします。Cloud SQL Proxyを使用して、アプリケーションのインスタンスに関連付けられたサービスアカウントを使用して、アプリケーションがデータベースと通信できるようにします。Googleが推奨するベストプラクティスに従って、サービスアカウントに割り当てられたロールに最小限のアクセスを提供したいとします。

どうすればよいでしょうか？
1. Project Editorロールを割り当てます。
2. Project Ownerロールを割り当てる。
3. Cloud SQL Client ロールを割り当てます。
4. Cloud SQL Editorロールを割り当てます。
<details><div>
    答え：3
説明
不正解
A. 
このロールには、プロジェクト全体にまたがる広範な権限があります。潜在的なセキュリティリスクにつながる可能性があります。
B. 
このロールには、Editor ロールよりもさらに広範な権限があり、この特定のタスクに対して過度に寛容な構成になる。
D. 
このロールは、サービスアカウントがCloud SQLインスタンス上で管理アクションを実行することを許可します。
正解
C. 
このロールは、サービスアカウントがCloud SQLインスタンスに接続するために必要な権限を提供しますが、不必要なリスクにさらされる可能性のある広範な権限は提供しません。そのため、目の前のタスクに必要な権限のみを付与するというベストプラクティスに従った選択肢Cが適切です。
リンク
https://cloud.google.com/sql/docs/mysql/sql-proxy
https://cloud.google.com/sql/docs/mysql/roles-and-permissions#proxy-roles-permissions
</div></details>

## Q. 4-25
あなたはオンラインのeコマースサイトを運営する企業に勤めています。あなたの会社は、世界中に拡大することを計画していますが、現在、Eストアはある特定の地域にサービスを提供しています。あなたは SQL データベースを選択し、組織の成長に合わせて拡張できるスキーマを構成する必要があります。すべての顧客トランザクションを格納するテーブルを作成し、顧客(CustomerId)とトランザクション(TransactionId)が一意であることを確認します。

どうすればよいですか?
1. TransactionIdとCustomerIdを主キーとして構成したCloud SQLテーブルを作成します。TransactionIdには増分番号を使用します。
2. TransactionIdとCustomerIdを主キーに設定したCloud SQLテーブルを作成する。TransactionIdにはランダムな文字列（UUID）を使用する。
3. TransactionIdとCustomerIdを主キーとして構成したCloud Spannerテーブルを作成します。TransactionIdにはランダムな文字列（UUID）を使用します。
4. TransactionIdとCustomerIdを主キーとして構成したCloud Spannerテーブルを作成する。TransactionIdには増分番号を使用します。
<details><div>
    答え：3
説明
不正解
A. 
これはCloud SQLを使用しますが、グローバルに拡大するプラットフォームではCloud Spannerほど楽に拡張できないかもしれません。
B. 
これもCloud SQLを使用する。
D. 
これはCloud Spannerを使用しますが、ホットスポットを引き起こす可能性のある増分番号を推奨します。
正解
C. 
これはCloud Spannerを使用し、ホットスポットの問題を避けるためにUUIDを使用するので、正しい選択肢です。
リンク
https://cloud.google.com/spanner/docs/schema-design#uuid_primary_key
</div></details>

## Q. 4-26
あなたのチームは金融機関向けのアプリケーションを構築しています。アプリケーションのフロントエンドはCompute Engine上で実行され、データはCloud SQLと1つのCloud Storageバケットに存在します。アプリケーションはPIIを含むデータを収集し、Cloud SQLデータベースとCloud Storageバケットに保存します。PIIデータを保護する必要があります。

あなたは何をすべきですか？
1. 
- 関連するファイアウォールルールを作成し、フロントエンドのみがCloud SQLデータベースと通信できるようにします。
- IAMを使って、フロントエンドのサービスアカウントだけがクラウドストレージバケットにアクセスできるようにする。
2. 
- 関連するファイアウォールルールを作成し、フロントエンドのみがCloud SQLデータベースと通信できるようにする。
- プライベートアクセスを有効にして、フロントエンドがクラウドストレージバケットに個人でアクセスできるようにする。
3. 
- Cloud SQL用にプライベートIPアドレスを構成する
- VPC-SCを使用してサービス境界を作成する
- Cloud SQLデータベースとCloud Storageバケットを同じサービス境界へ追加します。
4. 
- Cloud SQL用にプライベートIPアドレスを構成する
- VPC-SCを使用してサービス境界を作成する。
- Cloud SQLデータベースとCloud Storageバケットを異なるサービス境界へ追加します。
<details><div>
    答え：3
説明
不正解
A.
フロントエンドのみがCloud SQLと通信できるようにファイアウォールルールを作成するのは良いステップですが、VPCサービスコントロールが提供するようなデータ流出に対する隔離と保護を提供するものではありません。
IAMを使ってCloud Storageバケットへのアクセスを制限するのも良い方法だが、VPC Service Controlsの堅牢性には欠ける。IAMポリシーだけでは、定義されたネットワークの外からデータにアクセスするような特定の脅威から保護できない可能性がある。
B.
オプションAと同様に、ファイアウォールルールを作成することは良いことですが、VPC-SCが提供する高度な保護には欠けています。
クラウド・ストレージ・バケットのプライベート・アクセスを有効にすることは、セキュリティのレイヤーを追加しますが、VPC-SCの境界と同じレベルのデータ流出制御を提供しません。
D.
Cloud SQL用にプライベートIPアドレスを構成することは、オプションCと同様に積極的なステップです。
VPC-SCを使用するのも良いですが、Cloud SQLデータベースとCloud Storageバケットを異なる境界線に配置すると、特にこれらのリソースが互いに通信する必要がある場合、不必要な複雑さが生じる可能性があります。特に、これらのリソースが互いに通信する必要がある場合はなおさらです。これらを異なる境界線に分離することは、両方のリソースにわたってPIIデータを保護することが主な関心事である、指定されたユースケースに合致しないかもしれません。
正解
C.
このシナリオでPIIデータを保護するには、オプションCが最適です。その理由を確認してみましょう：
Cloud SQLにプライベートIPアドレスを設定する： このステップでは、Cloud SQLインスタンスがパブリック・インターネットに公開されないようにします。
VPC Service Controls（VPC-SC）を使用してサービス境界を作成する： VPC Service Controls を使用すると、Google Cloud リソースのセキュリティ境界を定義できます。サービス境界を設定することで、境界を越える通信を制限し、境界内のリソースを潜在的な脅威から隔離します。
Cloud SQLデータベースとCloud Storageバケットを同じサービス境界へ追加する： Cloud SQLデータベースとCloud StorageバケットはどちらもPIIデータの保存に使用されるため、同じサービス境界内にグループ化することで、一貫したセキュリティ・ポリシーを提供し、不正アクセスから隔離しつつ、互いの通信を保証します。
リンク
https://cloud.google.com/vpc-service-controls/docs/service-perimeters

</div></details>

## Q. 4-27
Google Cloud への新しいアプリケーションのデプロイ手法を設計しています。デプロイ計画の一環として、ライブトラフィックを使用して、新規および既存のアプリケーションのパフォーマンスメトリクスを収集したいとします。ローンチする前に、完全な本番負荷に対してテストする必要があります。

どうすればよいでしょうか？
1. カナリア配置を使用する
2. ブルー/グリーンデプロイメントを使用する
3. ローリングアップデートの導入
4. 導入時にトラフィックミラーリングとA/Bテストを使用する
<details><div>
    答え：4
説明
不正解
A. 
新バージョンをユーザーのサブセットでテストするのに便利ですが、シナリオで説明されているように、起動前に本番環境全体の負荷に対してテストする機能はありません。
B. 
このアプローチでは、完全な本番負荷の下で両方のバージョンを同時にテストすることはできません。
C. 
これは、段階的な置き換えを含み、完全な実稼働負荷の下で横に並べて比較することはできません。
正解
D. 
A/Bテストとトラフィックのミラーリングを組み合わせたオプションDは、本番の全負荷に対して新しいアプリケーションと既存のアプリケーションの両方をテストする機能を提供します。
リンク
アプリケーションの展開とテスト戦略｜Cloud Architecture Center
</div></details>

## Q. 4-28
あなたは大企業の開発者です。あなたはGoogle Kubernetes Engine（GKE）にWebアプリケーションをデプロイしています。DevOpsチームは、Cloud Deployを使用してアプリケーションをGKEのDev、Test、ProdクラスタにデプロイするCI/CDパイプラインを構築しました。Cloud DeployがDevクラスタへのアプリケーションのデプロイに成功した後、それを自動的にTestクラスタに昇格させたいとします。

Googleが推奨するベストプラクティスに従って、このプロセスをどのように構成すればよいでしょうか？
1. 
- clouddeploy-operations トピックからの SUCCEEDED Pub/Sub メッセージをリッスンする Cloud Build トリガーを作成します。
- アプリケーションをTestクラスタにプロモートするステップを含むようにCloud Buildを構成します。
2. 
- Google Cloud Deploy API を呼び出してアプリケーションを Test クラスタにプロモートする Cloud Function を作成します。
- この関数は、cloud-builds トピックからの SUCCEEDED Pub/Sub メッセージによってトリガーされるように構成します。
3. 
- Google Cloud Deploy APIを呼び出してアプリケーションをTestクラスタに昇格させるCloud Functionを作成します。
- clouddeploy-operationsトピックからのSUCCEEDED Pub/Subメッセージによってトリガーされるようにこの関数を構成する。
4. 
- gke-deploy ビルダーを使用する Cloud Build パイプラインを作成します。
- cloud-builds トピックからの SUCCEEDED Pub/Sub メッセージをリッスンする Cloud Build トリガーを作成します。
- このパイプラインを構成して、Test クラスタへのデプロイメントステップを実行します。
<details><div>
    答え：3
説明
不正解
A.
アプリケーションをプロモートするためのCloud Deploy APIではなくCloud Buildが関係するため、正しくありません。Cloud Deployは環境間のデプロイを管理するように設計されています。
B.
クラウド・デプロイではなく、クラウド・ビルドに関連する cloud-builds トピックを参照しているため、正しくありません。
D.
Cloud Buildパイプラインに依存し、Cloud Deploy操作ではなくcloud-buildsトピックからのメッセージをリッスンし、Cloud Deploy APIではなくgke-deployビルダーを使用するため、不正解です。
正解です：
C.
このオプションには、Google Cloud Deploy APIを呼び出すCloud Functionの作成が含まれ、CI/CDパイプラインの環境間でアプリケーションをプロモートするのに適しています。また、Cloud Deploy操作に関連するclouddeploy-operationsトピックからのSUCCEEDED Pub/Subメッセージを正しくリッスンします。
リンク
https://cloud.google.com/functions/docs/calling/pubsub
</div></details>

## Q. 4-29
あなたの会社には、アプリケーション情報をBigQueryで保持するデータウェアハウスがあります。BigQueryデータウェアハウスには2PBのユーザーデータが保存されています。最近、貴社はEUユーザを含むユーザベースを拡大しました：

ユーザーの要求に応じて、すべてのユーザーアカウント情報を削除できる必要があります。

すべてのEUユーザーデータは、EUユーザー専用の単一リージョンに保存する必要があります。

あなたはどの2つのアクションを取るべきですか？(2つの選択肢を選んでください)
1. BigQuery連携クエリを使用して、クラウドストレージからデータをクエリします。
2. EU ユーザーの情報のみを保持するデータセットを EU 地域に作成する。
3. EUリージョンにCloud Storageバケットを作成し、EUユーザーの情報のみを保存します。
4. クラウドデータフローパイプラインを使用して、ユーザーレコードをフィルタリングしてデータを再アップロードします。
5. BigQueryのDMLステートメントを使用して、リクエストに基づいてユーザーレコードを更新/削除する。
<details><div>
    答え：2,5
説明
不正解
A. C. D. 
オプション A. 連携クエリにより、Google Cloud サービス全体でデータを分析できる。このオプションは、地域ストレージやユーザー情報の削除機能という特定の要件には対応していません。
オプション C. EUリージョンにCloud Storageバケットを作成しても、データウェアハウスが存在するBigQueryには直接関係しません。このオプションは所定の要件を満たしていません。
オプション D. ユーザーレコードをフィルタリングしてデータを再アップロードすることは、EU ユーザーデータを特定のリージョンに保存したり、要求に応じてユーザーデータを削除したりする要件に直接関係しません。
正解
B. E. 
オプション B. EU 地域に別のデータセットを作成することで、必要に応じて特定の地域の EU ユーザーの情報を保存できます。これにより、すべてのEUユーザーデータを単一のリージョンに保存するというコンプライアンス要件を満たすことができます。
オプションE. BigQueryのDML（Data Manipulation Language）ステートメントを利用することで、ユーザーの要求に基づいて特定のユーザーレコードを更新または削除することができます。これにより、ユーザーの要求に応じてすべてのユーザーアカウント情報を削除するという要件に準拠できます。
リンク
https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language
https://cloud.google.com/architecture/bigquery-data-warehouse
</div></details>

## Q. 4-30
あなたのチームは、多くの異なるソースからニュース記事を集約するアプリケーションの保守を担当しています。監視ダッシュボードには、一般にアクセス可能なリアルタイムのレポートが含まれ、WebアプリケーションとしてCompute Engineインスタンス上で実行されます。外部の利害関係者やアナリストは、認証なしのセキュアなチャネルを介してこれらのレポートにアクセスする必要があります。

このセキュアチャネルはどのように構成する必要がありますか?
1. インスタンスにパブリックIPアドレスを追加します。インスタンスのサービスアカウントキーを使用してトラフィックを暗号化します。
2. Cloud Schedulerを使用して、1時間ごとにCloud Buildをトリガーし、レポートからエクスポートを作成します。パブリック・クラウド・ストレージ・バケットにレポートを保存します。
3. モニタリングダッシュボードの前にHTTP(S)ロードバランサーを追加する。Identity-Aware Proxy を設定して、通信チャネルをセキュアにする。
4. HTTP(S)ロードバランサーを監視ダッシュボードの前に追加します。トラフィックを暗号化するために、Googleが管理するSSL証明書をロードバランサーにセットアップする。
<details><div>
    答え：4
説明
不正解
A. 
インスタンスのサービス・アカウント・キーを使用してトラフィックを暗号化することは、HTTP通信を保護する適切または標準的な方法ではありません。SSL/TLSは、このような接続をセキュアにする一般的な方法です。
B. 
パブリック・クラウド・ストレージ・バケットにレポートを保存すると、レポートにアクセスできますが、リアルタイム・アクセスの要件を満たしません。この方法では、即時のリアルタイム・アクセスではなく、1時間ごとにレポートが生成・保存されるため、遅延が発生する。
C. 
IAP（Identity-Aware Proxy）は、Google Cloudで実行されるアプリケーションへのアクセスを制御するために使用される。要件は認証なしでアクセスを提供することであるため、このオプションはシナリオに適合しません。
正解
D. 
この問題は、認証なしで一般にアクセス可能なリアルタイムレポートへの安全なチャネルアクセスの必要性を強調している。したがって、SSL証明書は安全な通信に必要な暗号化を提供する。
HTTP(S)ロードバランサーを監視ダッシュボードの前に置き、Googleが管理するSSL証明書を使用することで、ユーザーとアプリケーション間のトラフィックは暗号化され、安全なアクセスが保証される。
リンク
https://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs
</div></details>

## Q. 4-31
Cloud Runを使用してウェブアプリケーションをホストしています。アプリケーションのプロジェクトIDとアプリケーションの実行地域を安全に取得し、この情報をユーザーに表示する必要があります。最もパフォーマンスの高いアプローチを使用したい。

あなたは何をすべきですか？
1. HTTP リクエストを使用して、http://metadata.google.internal/ エンドポイントで利用可能なメタデータ・サーバーに Metadata-Flavor： Googleヘッダを使用します。
2. Google Cloudコンソールで、プロジェクトダッシュボードに移動し、設定の詳細を収集します。Cloud Run の「Variables & Secrets」タブに移動し、必要な環境変数を Key:Value 形式で追加します。
3. Google Cloud コンソールで、プロジェクト ダッシュボードに移動し、設定の詳細を収集します。アプリケーション設定情報を Cloud Run のインメモリコンテナファイルシステムに書き込みます。
4. アプリケーションからCloud Asset Inventory APIにAPIコールを行い、インスタンス・メタデータを含むようにリクエストをフォーマットする。
<details><div>
    答え：1
説明
不正解
B. 
Googleクラウドコンソールから手動で環境変数を追加します。この方法は動的ではないため、情報が変更された場合は手動で更新する必要がある。
C. 
オプション B と似ていますが、コンテナのファイルシステムに情報を書き込みます。これも動的アプローチではなく、手動更新が必要です。
D. 
Cloud Asset Inventory APIにAPIコールを行う必要があり、不必要な複雑さとオーバーヘッドが追加される可能性があります。
正解
A. 
Cloud Run コンテナ内でメタデータを取得する直接的な方法を提供します。メタデータサーバーへの問い合わせは、サービスに関するランタイム情報を取得するための一般的なパターンです。この方法はまさにこのユースケースのために設計されており、オプションの中で最もパフォーマンスの高い方法です。
リンク
https://cloud.google.com/run/docs/container-contract#metadata-server
</div></details>

## Q. 4-32
あなたのチームはGoogle Cloud上で動作するサービスを開発しています。あなたはPub/Subトピックに送信されたメッセージを処理し、それらを格納したいと考えています。データの重複やデータの衝突を避けるために、各メッセージは正確に一度だけ処理されなければなりません。最も安価でシンプルなソリューションを使用する必要があります。

あなたは何をすべきでしょうか？
1. Dataprocジョブでメッセージを処理し、出力をストレージに書き込む。
2. Apache BeamのPubSubIOパッケージを使用したDataflowストリーミングパイプラインでメッセージを処理し、出力をストレージに書き込む。
3. Cloud Functionでメッセージを処理し、結果をBigQueryの場所に書き込み、そこでジョブを実行してデータを重複排除します。
4. Dataflowストリーミングパイプラインでメッセージを取得し、Cloud Bigtableに格納し、別のDataflowストリーミングパイプラインを使用してメッセージを重複排除する。
<details><div>
    答え：2
説明
不正解
A. 
Dataproc は主にバッチ処理に使用されます。Pub/Subトピックからのメッセージをリアルタイムで処理する必要がある場合、Dataprocは最適な選択ではないかもしれません。
C. 
Cloud Functionを使用し、BigQueryでデータを重複排除するジョブを実行すると、ソリューションが複雑になります。さらに、Cloud Functionsは適切に構成されていない場合、正確な1回限りの処理を保証しない可能性があり、重複の可能性があります。
D. 
Dataflowストリーミングパイプラインでメッセージを取得し、Cloud Bigtableに格納し、別のDataflowストリーミングパイプラインを使用してメッセージを重複排除することは、Apache BeamのPubSubIOパッケージでDataflowを使用することに比べて、過度に複雑でコストがかかります。
正解
B. 
オプションBは、ジャストワンス処理を実現するための簡単で費用対効果の高い方法を提供します。Apache BeamのPubSubIOパッケージはPub/Subで動作するように設計されており、各メッセージが正確に一度だけ処理されるように必要なロジックを実装することができます。出力を同じパイプラインの一部としてストレージに書き込むことで、要件を満たすまとまりのあるソリューションが保証されます。
リンク
https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub
</div></details>

## Q. 4-33
貴社はソースコードをクラウドソースリポジトリに保存しています。貴社は、リポジトリへのソースコードのコミットごとにコードをビルドし、テストしたいと考えており、管理され、運用のオーバーヘッドが最小限に抑えられるソリューションが必要です。

どの方法を使うべきでしょうか？
1. ソース コード コミットごとにトリガーを構成して Cloud Build を使用します。
2. ソースコードのコミットを監視するように設定された、Google Cloud Platform Marketplace経由でデプロイされたJenkinsを使用します。
3. ソースコードのコミットを監視するように設定されたオープンソースの継続的インテグレーションツールで、Compute Engineの仮想マシンインスタンスを使用する。
4. ソース コード コミット トリガーを使用して、App Engine サービスがソース コードを構築するトリガとなるメッセージを Cloud Pub/Sub トピックにプッシュする。
<details><div>
    答え：1
説明
不正解
B. 
Google Cloud Platform Marketplace経由でデプロイされたJenkinsを使えば確かにうまくいくかもしれませんが、Cloud Buildに比べて運用のオーバーヘッドが大きくなります。Jenkinsはセットアップ、設定、継続的なメンテナンスが必要で、運用のオーバーヘッドを最小限に抑えるソリューションの要件には合致しない。
C. 
オープンソースの継続的インテグレーションツールでCompute Engineの仮想マシンインスタンスを使用する場合、セットアップ、設定、メンテナンスが必要になります。これは、最小限の運用負担で管理されたソリューションの必要性とは一致しません。
D. 
ソース コード コミット トリガーを使用してカスタム システムを作成し、メッセージを Cloud Pub/Sub トピックにプッシュし、それが App Engine サービスをトリガーするというのは、より複雑なソリューションです。この場合、かなりの量の開発、テスト、および継続的なメンテナンスが必要になり、管理されたソリューションの要件には一致しません。
正解
A. 
選択肢Aは、Google Cloudの管理された継続的インテグレーションと継続的デリバリーのプラットフォームを使用する、最もシンプルで簡単なアプローチです。Cloud Buildは、ソースコードのコミットごとにコードを自動的にビルドしてテストするように簡単に設定でき、運用上のオーバーヘッドを最小限に抑えたマネージドソリューションの要件によく合致しています。
リンク
https://cloud.google.com/build/docs/automating-builds/create-manage-triggers
</div></details>

## Q. 4-34
分析システムはBigQueryデータセットに対してクエリを実行します。SQLクエリはバッチで実行され、SQLファイルの内容をBigQuery CLIに渡します。その後、BigQuery CLIの出力を別のプロセスにリダイレクトします。しかし、クエリの実行時にBigQuery CLIからパーミッションエラーが発生します。
この問題を解決したいと思います。

どうすればよいですか？
1. サービスアカウントにBigQueryデータビューワロールとBigQueryジョブユーザロールを付与します。
2. サービスアカウントにBigQuery Data EditorロールとBigQuery Data Viewerロールを付与します。
3. SQLクエリからBigQueryでビューを作成し、CLIでビューからSELECT*を実行します。
4. 新しいデータセットとテーブルにCLIからクエリを実行します。
<details><div>
    答え：1
説明
不正解
B. 
BigQueryデータエディタロールは、データを表示するだけでなく、データを変更する権限を付与します。このオプションの権限は、必要なものに対して寛容すぎる可能性があります。
C. 
SQLクエリからビューを作成しても、サービスアカウントがデータにアクセスしてクエリを実行するには適切なパーミッションが必要なため、パーミッションの問題は解決しません。
D. 
ソーステーブルを新しいデータセットにコピーしても、権限の問題が解決するとは限りません。サービスアカウントが新しいデータセットとテーブルに対して適切な権限を持っていることを確認する必要があります。
正解
A. 
このシナリオでは、オプションAが正しい選択肢です。サービスアカウントにBigQuery Data Viewerロールを付与することで、データセットへの読み取りアクセスを提供し、サービスアカウントがデータを表示できるようにします。BigQueryジョブユーザロールを使用すると、サービスアカウントはBigQueryでクエリを含むジョブを作成および実行できます。
リンク
https://cloud.google.com/bigquery/docs/access-control#bigquery
</div></details>

## Q. 4-35
あなたは、Istioを使用してGoogle Kubernetes Engine（GKE）上でマイクロサービスアプリケーションを管理しています。あなたは、Istio AuthorizationPolicy、Kubernetes NetworkPolicy、およびmTLSをGKEクラスタに実装することで、マイクロサービス間の通信チャネルを保護します。あなたは、2つのPod間の特定のURLへのHTTPリクエストが失敗し、他のURLへの他のリクエストが成功することを発見しました。

接続の問題の原因は何ですか？
1. Kubernetes NetworkPolicyリソースがPod間のHTTPトラフィックをブロックしています。
2. HTTPリクエストを開始するPodが、不正なTCPポート経由でターゲットPodに接続しようとしている。
3. クラスタの認証ポリシーが、アプリケーション内の特定のパスに対するHTTPリクエストをブロックしています。
4. クラスタではmTLSがパーミッシブモードで構成されていますが、Podのサイドカープロキシが暗号化されていないトラフィックをプレーンテキストで送信しています。
<details><div>
    答え：3
説明
不正解
A. 
Kubernetes NetworkPolicyは、特定のURLだけでなく、Pod間のすべてのトラフィックをブロックします。これは説明されている問題と一致しません。
B. 
TCPポートが正しくない場合も、特定のURLだけでなく、Pod間のすべてのトラフィックが失敗します。
D. 
パーミッシブモードで構成されたmTLSは、暗号化されたトラフィックと暗号化されていないトラフィックの両方を許可するため、他のURLが成功する一方で特定のURLが失敗することは説明できません。
正解です：
C. 
クラスターのAuthorization Policyが、アプリケーション内の特定のパスに対するHTTPリクエストをブロックしているという選択肢Cが、この問題の最も可能性の高い原因です。IstioのAuthorizationPolicyは、アプリケーション内の特定のサービスやパスに対して、誰が何を行うことができるかというアクセス制御を可能にします。
リンク
https://istio.io/latest/docs/tasks/security/authorization/authz-http/
https://kubernetes.io/docs/concepts/services-networking/network-policies/
https://istio.io/latest/docs/tasks/security/authentication/mtls-migration/
</div></details>

## Q. 4-36
Compute Engineの仮想マシンインスタンスにアプリケーションをデプロイしています。アプリケーションは、ログファイルをディスクに書き込むように設定されています。アプリケーションコードを変更せずにStackdriver Loggingでログを表示したい。

あなたは何をすべきですか？
1. Stackdriverロギングエージェントをインストールし、アプリケーションログを送信するように構成します。
2. Stackdriver Logging Libraryを使用して、アプリケーションからStackdriver Loggingに直接ログを記録します。
3. アプリケーション・ログを送信するように構成するために、インスタンスのメタデータにログ・ファイル・フォルダ・パスを提供する。
4. アプリケーションのログを /var/log に変更し、そのログが自動的に Stackdriver Logging に送信されるようにする。
<details><div>
    答え：1
説明
不正解
B. 
Stackdriver Logging Libraryを使用すると、アプリケーション・コードを変更してStackdriver Loggingに直接ログを記録する必要があります。
C. 
インスタンスのメタデータでログファイルフォルダのパスを提供することは、Stackdriver Loggingに送信するログを構成するための方法として認められていません。
D. 
アプリケーションのログを/var/logに変更するだけでは、ログは自動的にStackdriver Loggingに送信されません。これらのログをどこで見つけて送信するかを知るために、ロギングエージェントを設定する必要があります。
正解
A. 
エージェントは、アプリケーションによって生成されたカスタムログファイルなど、システム上のさまざまな場所からログを収集し、Cloud Loggingに送信するように構成できます。したがって、正しい選択肢はAで、必要なログをStackdriver Loggingに送信するようにLogging Agentをインストールして構成します。
リンク
https://cloud.google.com/logging/docs/agent/logging/installation
</div></details>

## Q. 4-37
API呼び出し元を認証し、クォータを実施し、メトリクスをレポートするパブリックAPIを構築する必要があります。
このアーキテクチャを完成させるために、どのツールを使用すべきですか？
1. App Engine
2. クラウドエンドポイント
3. アイデンティティ アウェア プロキシ
4. HTTP(S) ロードバランシングのための GKE Ingress
<details><div>
    答え：2
説明
不正解
A. 
Cloud Runにはこれらの機能はありません。
C. 
IAPは認証のみを提供します。
D. 
GKE Ingress はこれらの機能を提供しません。
正解
B. 
認証、クォータ/レート制限、メトリクスの3つの機能はすべて、Cloud Endpointsのコア機能です。
リンク
https://cloud.google.com/endpoints/docs/openapi/quotas-configure
https://cloud.google.com/endpoints/docs/openapi/monitoring-your-api
</div></details>

## Q. 4-38
タイムスタンプ、口座番号（文字列）、取引金額（数値）の3つの列を含むログファイルを解析しています。一意の口座番号ごとに、すべての取引金額の合計を効率的に計算したい。
どのデータ構造を使うべきか？
1. 連結リスト
2. ハッシュ・テーブル
3. 二次元配列
4. カンマ区切り文字列
<details><div>
    答え：2
説明
不正解
A. 
リンクリストは口座番号の効率的な検索や挿入を提供しません。毎回正しい口座番号を見つけるためにリストを横断する必要があり、その結果、検索や挿入のたびにO(n)の非効率な時間複雑度になります。
C. 
2次元配列も、取引額を更新するたびに正しい口座番号を見つけるために配列を検索する必要があり、非効率的なO(n)時間複雑性につながる。
D. 
カンマ区切りの文字列は、算術計算や効率的な検索を行うための実用的なデータ構造ではありません。値を更新したり検索したりするたびに文字列を解析する必要があるからです。
正しい答え
B. 
このシナリオでは、ハッシュテーブルが正しい選択肢です。口座番号をキーとし、取引金額の合計を値とすることができます。これにより、検索と挿入の両方の平均時間複雑度が効率的にO(1)になり、各口座番号の取引金額の追跡が容易になります。
リンク
https://open4tech.com/array-vs-linked-list-vs-hash-table/
</div></details>

## Q. 4-39
新しいアプリケーションコードを本番環境に移行する前に、様々な異なるユーザに対してテストを実施したい とします。この計画はリスクを伴いますが、本番ユーザでアプリケーションの新バージョンをテストし、オペレー ティングシステムに基づいて、どのユーザがアプリケーションの新バージョンに転送されるかを制御したいでしょう。新バージョンにバグが発見された場合、新しくデプロイされたバージョンのアプリケーションをできるだけ早くロールバックしたい。

どうすればよいでしょうか？
1. アプリケーションをCloud Runにデプロイします。トラフィック分割を使用して、リビジョンタグに基づいて、ユーザートラフィックのサブセットを新しいバージョンに誘導します。
2. Anthosサービスメッシュを使用してGoogle Kubernetes Engineにアプリケーションをデプロイします。トラフィックの分割を使用して、ユーザーエージェントヘッダーに基づく新しいバージョンにユーザートラフィックのサブセットを誘導する。
3. アプリケーションを App Engine にデプロイします。トラフィックの分割を使用して、IP アドレスに基づいてユーザー トラフィックのサブセットを新しいバージョンに誘導します。
4. Compute Engineにアプリケーションをデプロイします。Traffic Directorを使用して、事前に定義された重みに基づいて、ユーザートラフィックのサブセットを新しいバージョンに誘導します。
<details><div>
    答え：2
説明
不正解
A. 
オプションA（Cloud Run）はリビジョンタグに基づくトラフィック分割を可能にしますが、オペレーティングシステム（ユーザーエージェントヘッダー）に基づくユーザートラフィックの制御を提供しません。
C. 
オプションC（App Engine）ではトラフィックの分割が可能ですが、IPアドレスに基づく分割は、オペレーティングシステムに基づいてユーザーをルーティングする必要性に合致しません。
D. 
オプションD（Traffic Directorを備えたCompute Engine）は、事前に定義された重みに基づくトラフィック制御を可能にしますが、オペレーティングシステムに基づいてユーザーをルーティングするために必要なきめ細かさを提供しません。
正解
B. 
オプションBでは、ユーザーエージェントなどのHTTPヘッダに基づいてトラフィックを分割するなど、高度なルーティングとトラフィック制御が可能です。これにより、ユーザーをオペレーティングシステムに基づいて異なるバージョンのアプリケーションにルーティングすることが可能になり、記載されている要件を満たすことができます。
リンク
https://cloud.google.com/traffic-director/docs/ingress-traffic#sending-traffic
</div></details>

## Q. 4-40
あなたは新しいAPIのデプロイを担当している。そのAPIは3つの異なるURLパスを持っている：

https://yourcompany.com/students

https://yourcompany.com/teachers

https://yourcompany.com/classes

各API URLパスがコード内の異なる関数を呼び出すように設定する必要があります。どうすればよいだろうか。
1. HTTPSロードバランサーを使用して公開されるバックエンドサービスとして、1つのCloud Functionを作成します。
2. 直接公開される3つのクラウド関数を作成する。
3. 直接公開するクラウド関数を1つ作成する。
4. HTTPSロードバランサーを使用して公開される3つのバックエンドサービスとして3つのCloud Function を作成します。
<details><div>
    答え：4
説明
不正解
A. 
ロードバランサーを使用して1つのCloud Functionを作成すると、ルーティングを処理するためにCloud Function自体に余分なロジックを追加しない限り、URLに基づいて異なるコードパスを使用できません。
B. 
直接公開される3つのクラウドファンクションを作成すると、それぞれが別々のURLを持つことになるので、"https://yourcompany.com "の下で望ましいURL構造を維持できなくなります。
C. 
直接公開される1つのクラウド関数を作成すると、オプションAと同様に、内部ルーティングロジックなしでURLに基づいて異なるコードパスを使用できなくなります。
正解
D. 
コード内の異なる関数を呼び出す必要がある3つの異なるURLパスがあるので、各パスを別々のバックエンドサービス（この場合はCloud Functions）にマッピングしたいと思います。3つの別々のCloud Functionsを作成することで、それぞれが特定のURLパスのロジックを処理するように設計できます。
オプションDにはHTTPSロードバランサーの使用も含まれており、URLパスに基づいて正しいバックエンドサービス（Cloud Function）にトラフィックをルーティングするように構成できます。これにより、単一のドメインを維持しながら、パスに基づいて異なるCloud Function にトラフィックをルーティングすることができます。
リンク
https://cloud.google.com/load-balancing/docs/https/setup-global-ext-https-serverless
</div></details>

## Q. 4-41
Google Kubernetes Engine（GKE）クラスタに数百のマイクロサービスをデプロイする予定です。マネージドサービスを使用して、GKE上のマイクロサービス間の通信をどのように保護する必要がありますか？
1. マネージドSSL証明書を使用したグローバルHTTP(S)ロードバランシングを使用してサービスを保護します。
2. オープンソースのIstioをGKEクラスタに導入し、Service MeshでmTLSを有効にする。
3. SSL証明書を自動的に更新するために、GKEにcert-managerをインストールする。
4. Anthos Service Mesh をインストールし、Service Mesh で mTLS を有効にしてください。
<details><div>
    答え：4
説明
不正解の答え
A. 
マネージドSSL証明書を使用したグローバルHTTP(S)ロードバランシングは、クラスタ内のマイクロサービス間の内部通信ではなく、外部トラフィックを処理するように設計されています。
B. 
オープンソースのIstioをデプロイし、mTLS（mutual Transport Layer Security）を有効にすることは、サービス間の通信をセキュアにする方法です。しかし、このオプションではIstioを自分で管理する必要があり、特にマネージド・サービスを探している場合には理想的ではないかもしれない。
C. 
Cert-manager on GKEは、TLS証明書の管理と発行を自動化することに重点を置いていますが、マイクロサービス間の通信を保護するための包括的なソリューションを提供していません。
正解
D. 
Anthos Service Mesh をインストールし、Service Mesh で mTLS を有効にすることで、GKE クラスター内のマイクロサービス間の通信を保護するためのマネージドソリューションを提供します。これはトラフィックを暗号化するだけでなく、マイクロサービス間の通信のベストプラクティスに沿って、サービス間の身元確認を簡素化します。
したがって、オプション D は、GKE クラスタ内のマイクロサービス通信を保護するための完全に管理されたアプローチとして最良の選択です。
リンク
https://cloud.google.com/architecture/service-meshes-in-microservices-architecture#security_2
https://cloud.google.com/service-mesh/docs/overview#security_benefits
</div></details>

## Q. 4-42
あなたはeコマースサイトを管理する組織で働いています。あなたのアプリケーションは、グローバルなHTTP(S)ロードバランサの後ろにデプロイされています。あなたは新しい商品推奨アルゴリズムをテストする必要があります。あなたは、A/B テストを使用して、新しいアルゴリズムが売上に与える影響を無作為に判断することを計画しています。

この機能をどのようにテストすべきでしょうか？
1. 重みを使ってバージョン間でトラフィックを分割する。
2. 単一のインスタンスで新しいレコメンデーション機能のフラグを有効にします。
3. 新しいバージョンのアプリケーションにトラフィックをミラーリングする。
4. HTTPヘッダーベースのルーティングを使用する。
<details><div>
    答え：1
説明
不正解
B. 
単一のインスタンスで新しいレコメンデーション機能を有効にすると、2つのバージョンを適切に比較できる管理された実験ができません。
C. 
新しいバージョンにトラフィックをミラーリングすると、両方のバージョンに同じリクエストを送りますが、通常は古いバージョンからのレスポンスだけを使用します。このアプローチはパフォーマンスを評価するためにより適しており、ユーザとのインタラクションとコンバージョンを測定したいA/Bテストでは役に立ちません。
D. 
HTTPヘッダーベースのルーティングは、ユーザーエージェントやカスタムヘッダーなど、特定のヘッダー値に基づいてトラフィックを誘導するために使用されます。A/Bテストに使用することもできますが、このアプローチが必要であることを示す情報は質問にはありません。ウェイトを使用してトラフィックをランダムに分割することは、A/Bテストの目標を達成するためのより簡単な方法を提供します。
正解
A. 
重みを使用して2つのバージョン（現在のアルゴリズムと新しいアルゴリズム）間でトラフィックを分割することにより、各バージョンを見るユーザーの割合を制御することができます。これにより、ユーザーの一部が現行バージョンを体験し、一部が新バージョンを体験するというランダム化実験を行うことができます。そして、売上への影響を比較することができる。
リンク
https://cloud.google.com/traffic-director/docs/advanced-traffic-management#weight-based_traffic_splitting_for_safer_deployments
</div></details>

## Q. 4-43
あなたはプロジェクトAでCompute Engineホストアプリケーションを書いていて、プロジェクトBのCloud Pub/Subトピックを安全に認証する必要があります。
あなたは何をすべきですか？
1. プロジェクトBが所有するサービスアカウントでインスタンスを構成します。プロジェクトAにCloud Pub/Sub publisherとしてサービスアカウントを追加します。
2. プロジェクトAが所有するサービスアカウントでインスタンスを構成する。サービスアカウントをトピックのパブリッシャーとして追加します。
3. プロジェクト B が所有するサービス・アカウントの秘密鍵を使用するようにアプリケーション・デフォルト資格情報を構成します。
4. プロジェクト A が所有するサービス アカウントの秘密鍵を使用するようにアプリケーションの既定の資格情報を構成する。
<details><div>
    答え：2
説明
不正解
A. 
プロジェクトAにCloud Pub/Subパブリッシャーとしてサービスアカウントを追加すると、プロジェクトBのトピックにアクセスする権限が付与されないため、このオプションは正しくありません。
C. 
このオプションは、オプション A と同様に、サービス アカウントをプロジェクト A にクラウド Pub/Sub パブリッシャーとして追加するため、プロジェクト B のトピックに必要な権限が付与されないため、正しくありません。
D. 
このオプションはオプション B と似ていますが、特に Application Default Credentials に言及しています。重要なのは、サービスアカウントに適切な権限を設定することです。
正解
B. 
この選択肢は正しい。プロジェクト A が所有するサービス アカウントでプロジェクト A のインスタンスを構成し、そのサービス アカウントにプロジェクト B の特定のトピックで必要な権限 (パブリッシャーなど) を付与します。
リンク
https://cloud.google.com/pubsub/docs/access-control
</div></details>

## Q. 4-44
あなたのチームは、Google Kubernetes Engine（GKE）上で動作するステートレスサービスを開発しています。あなたは、GKEクラスタで実行されている他のサービスからのみアクセスされる新しいサービスをデプロイする必要があります。このサービスは、負荷の変化に対応するために、可能な限り迅速にスケールする必要があります。

あなたは何をすべきでしょうか？
1. Vertical Pod Autoscalerを使用してコンテナをスケールし、ClusterIPサービス経由でコンテナを公開します。
2. Vertical Pod Autoscalerを使用してコンテナをスケーリングし、NodePortサービス経由でコンテナを公開します。
3. Horizontal Pod Autoscalerを使用してコンテナをスケールし、ClusterIPサービス経由でコンテナを公開します。
4. コンテナのスケーリングにはHorizontal Pod Autoscalerを使用し、NodePort Serviceを介してコンテナを公開します。
<details><div>
    答え：3
説明
不正解
A. 
VPA（Vertical Pod Autoscaler）は、ポッドのCPUとメモリの要求と制限を変更し、基本的に水平方向ではなく垂直方向にスケーリングします。この場合、ポッドの新しいインスタンスは作成されないため、迅速なスケーリングはできません。
B. 
繰り返しますが、VPAは迅速なスケーリングには適していません。NodePort Serviceは、各ノードのIP上の静的ポートにサービスを公開するが、これはサービスがクラスタ内でのみアクセスされる必要がある場合は不要である。
D. 
Horizontal Pod Autoscalerは迅速なスケーリングを実現するための正しいアプローチですが、このコンテキストではNodePortサービスを使用する必要はありません。NodePortはサービスをクラスタの外部に公開するため、クラスタ内のみのアクセスという要件に一致しません。
正解です：
C. 
同じクラスタ内の他のサービスからのみアクセスされる必要があり、負荷の変化に応じて迅速にスケーリングする必要があるサービスでは、CPUまたはメモリの使用率（またはカスタムメトリクス）に基づいてデプロイメント内のポッド数をスケーリングするために、HPA（Horizontal Pod Autoscaler）を使用する必要があります。これにより、サービスは必要に応じてレプリカを追加または削除でき、需要に応じた迅速なスケーリングが可能になります。
ClusterIPサービスは、クラスタ内のアクセスのみが必要な場合に使用するのに適したタイプです。内部クライアントがサービスのどのポッドにも接続できる単一のIPアドレスを提供します。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/service#services_of_type_clusterip
https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
</div></details>

## Q. 4-45
ANSI-SQL準拠のデータベースに、同じカラムを持つ2つのテーブルがあり、結果セットから重複する行を削除して、1つのテーブルにすばやく結合する必要があります。

どうすればよいでしょうか？
1. SQLのJOIN演算子を使用してテーブルを結合します。
2. 入れ子のWITH文を使用してテーブルを結合する。
3. SQLのUNION演算子を使用してテーブルを結合する。
4. SQL の UNION ALL 演算子を使用してテーブルを結合します。
<details><div>
    答え：3
説明
不正解
A. JOINは、2つ以上のテーブルから、関連する列に基づいて行を結合するために使用されます。2つの同じテーブルを重複なく結合するという要件には適合しません。
B. 入れ子のWITH文（共通テーブル式）は、複雑なクエリに使用できますが、2つの同じテーブルを単純に結合し、重複を削除する最も簡単な方法ではありません。
D. UNION ALLは2つのクエリの結果セットを結合しますが、重複を含みます。要件は重複を削除することなので、この選択肢は正しくありません。
正解
C. UNION演算子を使用してテーブルを結合し、重複行を自動的に削除します。
リンク
https://www.techonthenet.com/sql/union_all.php
</div></details>

## Q. 4-46
あなたはプロジェクトAとプロジェクトBという2つのGoogle Cloudプロジェクトを持っています。あなたはプロジェクトBのクラウドストレージバケットに出力を保存するクラウド関数をプロジェクトAに作成する必要があります。あなたは何をすべきですか？
1. 
- プロジェクトBでGoogleサービスアカウントを作成する。
- プロジェクトAのサービスアカウントでCloud Function をデプロイします。
- このサービスアカウントに、プロジェクトBにあるストレージバケットのroles/storage.objectCreatorロールを割り当てます。
2. 
- プロジェクトAにGoogleサービスアカウントを作成する。
- プロジェクトAのサービスアカウントでCloud Function をデプロイする。
- このサービスアカウントに、プロジェクトBにあるストレージバケットのroles/storage.objectCreatorロールを割り当てる。
3. 
- プロジェクト A のデフォルトの App Engine サービスアカウント (PROJECT_ID@appspot.gserviceaccount.com) を決定します。
- プロジェクトAのデフォルトのApp Engineサービスアカウントを使用して、Cloud Function をデプロイします。
- デフォルトのApp Engineサービスアカウントに、プロジェクトBに存在するストレージバケットのroles/storage.objectCreatorロールを割り当てます。
4. 
- プロジェクト B のデフォルトの App Engine サービスアカウント (PROJECT_ID@appspot.gserviceaccount.com) を決定します。
- プロジェクトAのデフォルトApp EngineサービスアカウントでCloud Functionをデプロイする。
- デフォルトのApp Engineサービスアカウントに、プロジェクトBに存在するストレージバケットのroles/storage.objectCreatorロールを割り当てます。
<details><div>
    答え：2
説明
不正解：
A.C.D.
プロジェクトBのサービスアカウントは、プロジェクトAのCloud Function のデプロイに使用できないため、このオプションは正しくありません。
技術的にはうまくいくかもしれませんが、デフォルトの App Engine サービス アカウントには必要以上に広い権限が設定されている可能性があるため、最小権限の原則には従っていません。
選択肢 D. この選択肢は、プロジェクト B のデフォルトの App Engine サービス アカウントを使用してプロジェクト A にCloud Function をデプロイすることに言及しているため、正しくありません。
これは実行不可能です：
B.
オプションBは、プロジェクトBのターゲットバケット上に、必要最小限の権限（roles/storage.objectCreator）を持つ特定のサービスアカウントをプロジェクトAに作成することで、最小権限の原則に正しく沿います。
リンク
https://cloud.google.com/docs/authentication/production#providing_credentials_to_your_application
</div></details>

## Q. 4-47
あなたは小さな新興企業のウェブ開発チームで働いています。あなたのチームは、Cloud Storage や Cloud Build などの Google Cloud サービスを使用して Node.js アプリケーションを開発しています。チームはバージョン管理に Git リポジトリを使用しています。週末にマネージャーから電話があり、会社の Web サイトの 1 つを緊急アップデートするよう指示されました。アップデートを行うには Google Cloud にアクセスする必要がありますが、仕事用のラップトップがありません。会社以外のコンピュータにソースコードをローカルに保存することは許可されていません。

開発者環境をどのようにセットアップすべきでしょうか？
1. テキストエディタとGitコマンドラインを使って、ソースコードの更新をプルリクエストとして公共のコンピューターから送信する。
2. テキストエディタとGitコマンドラインを使って、ソースコードの更新を、公共のコンピューター上で動いている仮想マシンからのプル・リクエストとして送信する。
3. 開発にはCloud Shellと内蔵コード・エディタを使います。ソースコードの更新をプル・リクエストとして送信します。
4. クラウドストレージのバケットを使用して、編集が必要なソースコードを保存します。バケットをドライブとしてパブリック・コンピュータにマウントし、コード・エディタを使ってコードを更新する。バケットのバージョニングをオンにし、チームの Git リポジトリを指定する。
<details><div>
    答え：3
説明
不正解
A. 
公共のコンピューターでテキストエディタとGitコマンドラインを使うと、ソースコードと認証情報が安全でない環境にさらされる可能性があります。
B. 
公共のコンピューター上で仮想マシンを実行すると、機密情報が安全でない環境にさらされる危険性があります。
D. 
クラウドストレージのバケットを使い、それを公共のコンピューターにマウントすることは、ソースコードを潜在的なリスクにさらすことにもなります。
正解
C. 
Cloud Shell は、Google Cloud Console からアクセスできる、セキュアで完全に管理された開発環境を提供します。これにより、企業以外のマシンに機密データを保存することなく、必要な変更を行うことができ、問題で言及されている制約と一致します。
リンク
https://cloud.google.com/shell/docsあなたは小さな新興企業のウェブ開発チームで働いています。
</div></details>

## Q. 4-48
Cloud Build を使用して、Cloud Source Repositories に保存されているアプリケーションのソース コードをビルドおよびテストしています。ビルドプロセスには、Cloud Build 環境で利用できないビルドツールが必要です。
どうすればよいですか？
1. ビルド・プロセス中にインターネットからバイナリをダウンロードします。
2. カスタムクラウドビルダーイメージを作成し、ビルド手順でそのイメージを参照する。
3. バイナリを Cloud Source Repositories リポジトリにインクルードし、ビルドスクリプトで参照します。
4. Cloud BuildパブリックIssue Trackerに対して機能要求を提出することで、バイナリをCloud Build環境に追加してもらう。
<details><div>
    答え：2
説明
不正解です：
A. 
ビルドプロセス中にインターネットからバイナリをダウンロードすることは、特にソースが信頼できない場合、安全でない可能性があります。
C. 
バイナリを Cloud Source Repositories リポジトリに含めると、リポジトリのサイズが大きくなり、特にバイナリを頻繁に更新する必要がある場合、バージョン管理の問題につながる可能性があります。
D. 
Cloud Build public Issue Trackerに対して機能要求を提出しても、必要なバイナリが標準のCloud Build環境に追加されるとは限りませんし、追加されたとしてもプロセスに時間がかかる可能性があります。そのため、緊急のニーズに対する実用的なソリューションではありません。
正解
B. 
カスタムクラウドビルダーイメージを作成することで、必要なビルドツールやビルドプロセスが必要とするその他の依存関係を含めることができます。その後、ビルド構成でこのカスタムイメージを参照し、ビルドプロセス中に必要なツールが利用できるようにします。
リンク
https://cloud.google.com/cloud-build/docs/configuring-builds/use-community-and-custom-builders#creating_a_custom_builder
</div></details>

## Q. 4-49
Google Kubernetes Engineのデプロイメント更新中に、アプリケーションが強制シャットダウンされたことに気づきました。アプリケーションは終了する前にデータベース接続を閉じませんでした。アプリケーションを更新して、グレースフル・シャットダウンが完了するようにしたいと思います。

どうすればよいでしょうか？
1. 受信したSIGTERMシグナルを処理するようにコードを更新して、データベースから優雅に切断する。
2. PodDisruptionBudgetを構成して、Podが強制シャットダウンされないようにします。
3. アプリケーションの terminationGracePeriodSeconds を増やします。
4. PreStopフックを設定してアプリケーションをシャットダウンする。
<details><div>
    答え：1
説明
不正解
B. 
PodDisruptionBudgetを構成すると、アプリケーションの可用性に影響する中断を管理するのに役立ちますが、アプリケーション自体の内部でグレースフル・シャットダウン・ロジックを処理することはできません。
C. 
terminalGracePeriodSecondsを増やすと、アプリケーションがシャットダウンする時間を増やすことができますが、SIGTERMシグナルを処理しないと、アプリケーションはグレースフル・シャットダウンを実行する必要があることを認識できない可能性があります。
D. 
PreStopフックを設定することでもアプリケーションを優雅にシャットダウンできますが、通常、シャットダウンを処理するために別のスクリプトやロジックを作成する必要があります。SIGTERM シグナルをアプリケーション・コードで直接処理する方法（選択肢 A）の方が、一般的には直接的で簡単なアプローチです。
正解
A. 
Kubernetes環境でアプリケーションのグレースフル・シャットダウンを保証したい場合、SIGTERMシグナルを処理することが不可欠になることがよくあります。このシグナルは、Kubernetesがアプリケーションをシャットダウンしたいときにアプリケーションに送信されます。SIGTERMシグナルをリッスンし、データベース接続を閉じるなど、必要なクリーンアップを実行するようにコードを更新できます。
というわけで、正解はA
リンク
https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace
</div></details>

## Q. 4-50
あなたは最近アプリケーションを開発しました。パブリックIPアドレスを持たないCompute EngineインスタンスからCloud Storage APIを呼び出す必要があります。

どうすればいいでしょうか？
1. キャリア・ピアリングを使用する
2. VPCネットワークピアリングを使用する
3. 共有VPCネットワークの使用
4. プライベート Google アクセスを使用する
<details><div>
    答え：4
説明
不正解
A. 
キャリアピアリングは、オンプレミスのネットワークとGoogleのネットワークとの直接接続を可能にしますが、このシナリオには関係ありません。
B. 
VPC Network Peeringは、VPCネットワークを相互に接続することができますが、パブリックIPアドレスなしでGoogle Cloudサービスにアクセスする要件には対応していません。
C. 
共有 VPC ネットワークは、組織内のネットワーク リソースの共有を可能にしますが、パブリック IP アドレスなしで Google Cloud サービスにアクセスすることには直接対応していません。
正解
D. 
パブリックIPアドレスを持たないCompute EngineインスタンスからCloud Storage APIのようなGoogle Cloudサービスにアクセスするには、プライベートGoogleアクセスを使用します。
プライベートGoogleアクセスを使用すると、外部IPアドレスを持たないVPCネットワーク内のリソースが、パブリックインターネットではなくプライベートIPアドレスを使用してGoogle APIやサービスにアクセスできるようになります。
リンク
https://cloud.google.com/vpc/docs/private-google-access
</div></details>

## Q. 5-1
Cloud Buildを使ってDockerイメージをビルドしています。単体テストと統合テストを実行するためにビルドを変更する必要があります。障害が発生した場合、ビルド履歴にビルドが失敗した段階を明確に表示させたいとします。

どうすればいいでしょうか？
1. ユニットテストと統合テストを実行するために、DockerfileにRUNコマンドを追加します。
2. ユニットテストと統合テストをコンパイルする単一のビルドステップを持つCloud Buildビルド設定ファイルを作成する。
3. 単体テストと統合テスト用に別々のCloud Buildパイプラインを生成するCloud Buildビルド設定ファイルを作成します。
4. ユニットテストと統合テストをコンパイルして実行するために、個別のクラウドビルダーステップを持つCloud Buildビルド構成ファイルを作成します。
<details><div>
    答え：4
説明
不正解
A. 
DockerfileにRUNコマンドを追加すると、イメージのビルドプロセス中にテストが実行されますが、ビルド履歴にビルドが失敗した段階が明確に表示されません。
B. 
単体テストと統合テストの両方に対して単一のビルドステップを作成しても、ビルドがどの段階で失敗したかを理解するのに必要なきめ細かさは得られません。
C. 
単体テストと統合テスト用に別々のCloud Buildパイプラインを生成すると、不必要に複雑になり、必要な明確なビルド履歴が得られない可能性があります。
正解
D. 
Dockerイメージのビルドと単体テストおよび統合テストの実行を同時に処理し、障害が発生した段階を明確に表示する最善の方法は、各タスクに個別のステップを持つCloud Buildビルド設定ファイルを作成することです。
コンパイル、単体テストの実行、統合テストの実行を別々のステップにすることで、Cloud Buildはビルド履歴にビルドが失敗した段階を明確に表示します。
リンク
https://cloud.google.com/build/docs/build-config-file-schema
</div></details>

## Q. 5-2
あなたのチームは、給与計算アプリケーションをサポートする対話型音声応答（IVR）システムのビジネスロジックを実装するバックエンドアプリケーションを書いています。IVRシステムには、以下の技術的特徴があります：

- 各顧客からの電話は一意のIVRセッションに関連付けられます。

- IVRシステムは、セッションごとにバックエンドへの永続的なgRPC接続を作成します。

- 接続が中断されると、IVRシステムは新しい接続を確立するため、そのコールにわずかな遅延が発生します。

バックエンドアプリケーションのデプロイに使用するコンピュート環境を決定する必要があります。現在のコールデータを使用して、次のことがわかります：

- 通話時間は1～30分。

- 通話時間は1～30分。

- 特定の既知の日付（例：給料日）の前後や、大規模な給与計算の変更が発生したときに、通話が大幅に急増する。

コスト、労力、運用上のオーバーヘッドを最小限に抑えたい。バックエンドアプリケーションはどこに配置すべきでしょうか？
1. Compute Engine
2. 標準モードのGoogle Kubernetes Engineクラスタ
3. Cloud Functinos
4. Cloud Run
<details><div>
    答え：4
説明
不正解
A. 
DockerファイルにRUNコマンドを追加すると、イメージのビルドプロセス中にテストが実行されますが、ビルド履歴にビルドが失敗した段階が明確に表示されません。
B. 
単体テストと統合テストの両方に対して単一のビルドステップを作成しても、ビルドがどの段階で失敗したかを理解するのに必要な粒度は得られない。
C. 
単体テストと統合テストのために別々のCloud Buildパイプラインを生成すると、不必要に複雑になり、必要な明確なビルド履歴を提供できない可能性があります。
正解
D. 
Dockerイメージのビルドと単体テストおよび統合テストの実行を同時に処理し、障害が発生した段階を明確に表示する最善の方法は、各タスクに個別のステップを持つCloud Buildビルド設定ファイルを作成することです。
コンパイル、単体テストの実行、統合テストの実行を別々のステップにすることで、Cloud Buildはビルド履歴にビルドが失敗した段階を明確に表示します。
リンク
https://cloud.google.com/run/docs/tutorials/secure-services
</div></details>

## Q. 5-3
Cloud Runにデプロイされた一連のREST APIエンドポイントをロードテストする必要があります。APIはHTTP POSTリクエストに応答します。負荷テストは以下の要件を満たす必要があります：
- 負荷は複数の並列スレッドから開始される。
- APIへのユーザートラフィックは複数のソースIPアドレスから発信される。
- 負荷は、追加のテストインスタンスを使用してスケールアップできる。
Googleが推奨するベストプラクティスに従いたい。

負荷テストはどのように構成すればよいでしょうか？
1. cURLがインストールされたイメージを作成し、テストプランを実行するようにcURLを構成します。このイメージをマネージド・インスタンス・グループに配備し、各VMに対してイメージのインスタンスを1つ実行します。
2. cURL がインストールされたイメージを作成し、テストプランを実行するように cURL を構成します。このイメージをアンマネージド・インスタンス・グループに展開し、各VMに対してイメージのインスタンスを1つ実行します。
3. Cloud Shellで分散負荷テストフレームワークのコンテナイメージをダウンロードする。Cloud Shell上でコンテナの複数のインスタンスを順次起動し、APIの負荷を増加させる。
4. Google Kubernetes Engineのプライベートクラスタに分散負荷テストフレームワークをデプロイします。より多くのトラフィックを開始し、同時ユーザー数をサポートするために、必要に応じて追加のPodをデプロイします。
<details><div>
    答え：3
説明
不正解
A. 
B. 
cURLとVMインスタンスグループを使用すると、複数の並列スレッドと異なるソースIPアドレスでエンドポイントを効果的にテストするために必要な柔軟性とスケーラビリティが得られません。このセットアップを手動で管理するのは面倒である。
D. 
Cloud Shellから負荷テストフレームワークを順次実行することは、複数の並列スレッドと異なるソースIPアドレスから負荷を開始する要件に合致しません。さらに、Cloud Shellは大規模な負荷テストを処理するようには設計されていません。
正解
C. 
分散負荷テストフレームワークでGKEクラスタを使用すると、テストのスケーラビリティと並列性が可能になります。必要なマルチスレッドを容易に処理し、さまざまなソース IP アドレスからのトラフィックをシミュレートできます。このセットアップは、負荷テストのベストプラクティスに沿っており、必要に応じてテストパラメータを調整する柔軟な方法を提供します。
リンク
https://cloud.google.com/run/docs/about-load-testing https://cloud.google.com/architecture/distributed-load-testing-using-gke#build_the_container_image
</div></details>

## Q. 5-4
ユーザーに代わって Google Cloud API を介して Google Cloud サービスと対話する必要がある Java Web サーバーを開発しています。ユーザーは、Google Cloud IDを使用してGoogle Cloud APIに認証できる必要があります。

Webアプリケーションに実装すべきワークフローはどれですか？
1. 
2. 
3. 
4. 
- ユーザがアプリケーションに到着したら、要求された権限のリストが表示された Google Cloud の同意画面にユーザをルーティングし、Google アカウントへの SSO によるサインインをユーザに促します。
- ユーザーがサインインして同意すると、アプリケーションは Google サーバーから認証コードを受け取ります。
- アプリケーションはGoogleサーバーに認証コードとアクセストークンの交換を要求する。
- Google サーバーは、アプリケーションが Google Cloud API を呼び出すために使用するアクセストークンを返信します。

<details><div>
    答え：4
説明
不正解
オプション A、オプション B、オプション C
他の選択肢には、アプリケーション内でユーザー名/パスワードの入力を促したり（AとB）、API呼び出しに認証コードを直接使用しようとしたり（C）など、セキュリティ上の脆弱性やOAuth 2.0フローの不適切な処理が含まれています。
正解
D.
アプリケーションが Google サーバーに認証コードとアクセストークンの交換を要求する。
Google サーバーは、アプリケーションが Google Cloud API を呼び出す際に使用するアクセストークンを返信します。
このオプションは、標準的な OAuth 2.0 の認証コードの流れに沿ったもので、アプリケーション内でユーザ認証情報を保存したり直接扱ったりしないので、安全に扱えます。このオプションは、Google のアイデンティティプラットフォームと同意メカニズムを利用してユーザーを認証し、ユーザーに代わって Google Cloud サービスにアクセスする権限を取得します。
リンク
https://developers.google.com/identity/protocols/oauth2#webserver
</div></details>

## Q. 5-5
APIバックエンドが複数のクラウドプロバイダー上で動作している。あなたはAPIのネットワークレイテンシのレポートを作成したい。

どの2つのステップを踏むべきですか？(2つの選択肢を選んでください)
1. Zipkin コレクターを使用してデータを収集します。
2. Fluentd エージェントを使用してデータを収集します。
3. Stackdriver Traceを使ってレポートを作成する。
4. Stackdriver Debuggerを使ってレポートを作成する。
5. レポートを作成するには、Stackdriver Profilerを使用します。
<details><div>
    答え：1,3
説明
不正解
B. Fluentdはレイテンシトレースではなく、ロギングに向いています。
D. Stackdriver Debugger は、コードのデバッグに使用され、ネットワークのレイテンシーを分析するものではない。
E. Stackdriver Profiler は、アプリケーションのCPU使用率とメモリ割り当てを継続的にプロファイリングするために使用され、ネットワーク待ち時間の分析には使用されません。
正解
A. Zipkinは、リクエストを満たすために動作しているすべての異種サービスのタイミングデータを収集するのに役立つ分散トレースシステムです。このデータの収集と検索の両方を管理します。
C. Stackdriver Trace（Google Cloud のオペレーション・スイートの一部）は、リクエストがアプリケーションをどのように伝搬するかを示すトレースを視覚化し分析するために使用される。これにより、アプリケーションのパフォーマンスや、ネットワーク遅延などのボトルネックが発生している場所を理解することができます。
リンク
https://cloud.google.com/trace/docs/zipkin#frequently_asked_questions
https://cloud.google.com/trace/docs/quickstart#analysis_reports_window
</div></details>

## Q. 5-6
Google Kubernetes Engine (GKE) クラスタにアプリケーションをデプロイする予定です。
アプリケーションは HTTP ベースのヘルスチェックを /healthz で公開しています。あなたはこのヘルスチェックエンドポイントを使用して、ロードバランサーによってトラフィックがPodにルーティングされるべきかどうかを判断したいと考えています。
Pod設定に含めるべきコードスニペットはどれですか？
1. 
2. 
readinessProbe：
  httpGet：
   path： /healthz
   port： 80
3. 
4. 
<details><div>
    答え：2
説明
不正解
オプションA、C、D
生存性プローブ（オプションA）は、コンテナを再起動するタイミングを知るために使用され、ルーティングの決定には使用されません。オプションCとDは、このユースケースのための有効なKubernetes構成に対応していません。
正解
B.
Kubernetesのコンテキストで、ロードバランサーによってポッドにトラフィックがルーティングされるべきかどうかを判断したい場合、通常はreadinessProbeを使用する。このタイプのプローブは、コンテナがリクエストを処理する準備ができているかどうかを判断するために使用される。readinessProbeが失敗すると、コンテナはサービスのロードバランサーからトラフィックを受け取らなくなる。
つまり、正しいコードスニペットは
リンク
https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes
</div></details>

## Q. 5-7
MySQLからCloud Bigtableに移行するテーブルのスキーマを設計しています。MySQLテーブルは以下の通りです：



このテーブルのCloud Bigtable用の行キーをどのように設計しますか？
1. Account_idをキーとして設定します。
2. Account_id_Event_timestampをキーとして設定します。
3. Event_timestamp_Account_idをキーとして設定します。
4. Event_timestampをキーとして設定します。
<details><div>
    答え：2
説明
不正解
A. C. D. 
正解
B. 
Cloud Bigtableでは、行キーの設計は良好なパフォーマンスを確保するために重要であり、多くの場合、データのアクセスパターンに依存します。
与えられたMySQLテーブルでは、プライマリキーはAccount_idとEvent_timestampで構成されており、クエリはこの2つのフィールドでフィルタリングやソートを行うことが多いことを示唆しています。Bigtableの目標は、想定されるクエリパターンに対して効率的な読み取りをサポートする行キーを設計することです。
Account_idとEvent_timestampを連結するオプションBは、Account_idによる効率的なクエリを可能にし、Event_timestampも含むクエリをサポートします。この設計では、特定のアカウントのすべてのレコードをBigtableの近くに保持し、特定のアカウントのすべてのイベントにアクセスする必要があるクエリをサポートします。
リンク
https://cloud.google.com/bigtable/docs/schema-design#row-keys
</div></details>

## Q. 5-8
あなたは、マイクロサービスアーキテクチャを使用するアプリケーションを設計しています。アプリケーションはクラウドとオンプレミスにデプロイする予定です。アプリケーションをオンデマンドでスケールアップできるようにし、マネージドサービスも可能な限り利用したいと考えています。

どうすればいいでしょうか？
1. Anthosによって管理される複数のGoogle Kubernetes Engine (GKE)クラスタ上のマルチクラスタデプロイメントにオープンソースのIstioをデプロイします。
2. Anthosを使って各環境にGKEクラスタを作成し、Cloud Run for Anthosを使ってアプリケーションを各クラスタにデプロイします。
3. Anthosを使って各環境にGKEクラスタをインストールし、Cloud Buildを使って各クラスタにアプリケーションのDeploymentを作成します。
4. クラウド上にGKEクラスタを作成し、オンプレミスにオープンソースのKubernetesをインストールする。外部のロードバランサーサービスを使用して、2つの環境間でトラフィックを分散させる。
<details><div>
    答え：2
説明
不正解
A. 
オープンソースのIstioをマルチクラスタ環境にデプロイすれば、サービスメッシュ機能を実現できるが、マネージドサービスの利用を極力重視しないため、より多くの手動管理が必要になる。
C. 
Anthosを使って各環境にGKEクラスタをインストールし、Cloud Buildを使ってデプロイするのは有効なアプローチですが、スケーリングを自動的に処理するCloud Run for Anthosが提供するサーバーレス機能が欠けています。
D. 
クラウドにGKEクラスタを作成し、オンプレミスにオープンソースのKubernetesをインストールすると、より多くの手動管理が必要になり、外部のロードバランサーを使用すると、クラウド環境とオンプレミス環境の間のシームレスな統合ができません。
正解
B. 
Anthosを使って各環境にGKEクラスタを作成し、Cloud Run for Anthosを使ってアプリケーションをデプロイすることで、完全に管理されたソリューションが提供されます。Cloud Run for Anthosは、基盤となるインフラストラクチャを気にすることなくコンテナを実行し、完全に管理することを可能にします。また、トラフィックがない時にはゼロにスケールダウンするなど、自動スケーリングも提供します。
リンク
https://cloud.google.com/anthos/run/docs/deploy-application
</div></details>

## Q. 5-9
あなたは、ユーザーがニュース記事を読んでコメントを投稿できるアプリケーションを開発しています。Firestore を使用して、ユーザーが投稿したコメントを保存し、表示するようにアプリケーションを構成したいと考えています。

未知の数のコメントと記事をサポートするために、スキーマをどのように設計すべきでしょうか？
1. 各コメントを記事のサブコレクションに格納します。
2. 各コメントを記事の配列プロパティに追加します。
3. 各コメントをドキュメントに格納し、コメントのキーを記事の配列プロパティに追加する。
4. 各コメントをドキュメントに保存し、コメントのキーをユーザープロファイルの配列プロパティに追加する。
<details><div>
    答え：1
説明
不正解
B. 
配列は、個々の配列要素に対するクエリをサポートしていないことや、同時更新を処理する際の課題など、Firestoreではいくつかの制限があります。多数のコメントがある場合、これは問題になる可能性があります。
C. 
各コメントを個別のドキュメントに格納し、それらのコメントを記事上の配列で参照することは、オプション B と同様の制限につながる可能性があります。
D. 
コメントのキーをユーザプロファイルの配列プロパティに格納することは、コメントをユーザにリンクしますが、直接記事にはリンクしません。それは特定の記事に対するすべてのコメントを取得する効率的な方法を提供しません。
正解
A. 
このアプローチはFirestoreのネストされたコレクションのサポートを利用します。各コメントを記事のサブコレクションにドキュメントとして格納することで、指定されたアーティクルのすべてのコメントを簡単にクエリでき、アーティクルごとに無制限のコメント数をサポートします。
リンク
https://firebase.google.com/docs/firestore/data-model#subcollections
https://firebase.google.com/docs/firestore/best-practices#high_read_write_and_delete_rates_to_a_narrow_document_range
</div></details>

## Q. 5-10
あなたは金融機関の開発者です。Cloud Shell を使って Google Cloud サービスとやりとりしています。ユーザーデータは現在エフェメラルディスクに保存されていますが、最近可決された規制により、機密情報をエフェメラルディスクに保存することはできなくなりました。ユーザーデータ用に新しいストレージソリューションを実装する必要があります。コードの変更は最小限に抑えたい。

ユーザーデータはどこに保存すべきでしょうか？
1. ユーザーデータをCloud Shellのホームディスクに保存し、削除を防ぐために少なくとも120日ごとにログインします。
2. Compute Engineインスタンスの永続ディスクにユーザーデータを保存します。
3. ユーザーデータをクラウドストレージのバケットに保存する。
4. ユーザーデータをBigQueryのテーブルに格納する。
<details><div>
    答え：2
説明
不正解
A. 
Cloud Shellのホームディスクは機密データや永続的なデータを保存するためのものではありません。長期間(120日間)アクセスされないと削除される可能性があり、機密性の高いユーザー情報を保存するには信頼できません。
C. 
クラウド・ストレージ・バケットを使用してデータを安全に保管することができるが、このシナリオではコードの変更を最小限に抑える必要がある。現在のアーキテクチャにもよるが、データをクラウドストレージに移すにはコードに大幅な変更を加える必要があり、コードの変更を最小限に抑えるという要件と矛盾する可能性がある。
D. 
BigQueryは、サーバーレスで、拡張性が高く、費用対効果の高いマルチクラウド・データウェアハウスです。BigQueryは分析用に設計されており、一般的に機密性の高いユーザーデータの保存には使用されません。
正解
B. 
永続ディスクは、より安定した信頼性の高いストレージソリューションを提供します。データは機密性の高いものであるため、制御されたCompute Engineインスタンス内の永続ディスクに格納することで、適切なアクセス制御と管理が可能になり、規制コンプライアンスに沿うことができます。
新しい規制を遵守し、コードの変更を最小限に抑えるという要件を考慮すると、このコンテキストにおける正しい選択肢は、Compute Engineインスタンスで永続ディスクを使用することでしょう。
リンク
https://cloud.google.com/shell/docs/how-cloud-shell-works#persistent_disk_storage
</div></details>

## Q. 5-11
Google Kubernetes Engine (GKE)上で動作しているアプリケーションがあります。アプリケーションは現在ロギングライブラリを使用しており、標準出力に出力しています。ログをCloud Loggingにエクスポートする必要があり、ログに各リクエストに関するメタデータを含める必要があります。これを達成するために最も簡単な方法を使用したい。

どうすればいいでしょうか？
1. アプリケーションのロギングライブラリを Cloud Logging ライブラリに変更し、Cloud Logging にログをエクスポートするようにアプリケーションを構成します。
2. JSON形式でログを出力するようにアプリケーションを更新し、必要なメタデータをJSONに追加します。
3. CSV形式でログを出力するようにアプリケーションを更新し、必要なメタデータをCSVに追加する。
4. 各 GKE ノードに Fluent Bit エージェントをインストールし、エージェントに /var/log からすべてのログをエクスポートさせます。
<details><div>
    答え：1
説明
不正解
B. 
ロギングにJSONを使用するとメタデータを含めることができますが、このアプローチでは、アプリケーションの既存のロギング・コードに、より広範な変更が必要になる可能性が高いです。
C. 
CSVは構造化されたロギングの典型的な形式ではなく、Cloud Loggingとうまく適合しないため、この方法はさらに面倒になります。
D. 
この方法は、追加の設定とメンテナンスが必要になる可能性があり、要件を満たすための最も簡単な方法ではありません。Fluent Bit はログプロセッサーとフォワーダーであり、それを使用することは、このシナリオでは不必要な複雑さを伴うかもしれません。
正解
A. このオプションは Cloud Logging と直接統合し、各リクエストに関するメタデータを含めることを可能にします。Cloud Logging ライブラリは Google Cloud サービスとシームレスに動作するように設計されているため、GKE アプリケーションからログをエクスポートするための論理的でシンプルな選択肢となります。
リンク
https://cloud.google.com/run/docs/logging#container-logs
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine
</div></details>

## Q. 5-12
Compute Engineインスタンス上で起動するアプリケーションを、複数のプロジェクトにまたがって開発しています。

各プロジェクトのインスタンスは、同じアプリケーションコードを持ちますが、構成は異なります。デプロイ時に、各インスタンスは、提供する環境に基づいてアプリケーションの構成を受け取る必要があります。このフローを構成するためのステップ数を最小限に抑えたい。

どうすればよいでしょうか？
1. インスタンスを作成するときに、gcloudコマンドを使用して起動スクリプトを構成し、正しい環境を示すプロジェクト名を決定します。
2. 各プロジェクトで、提供する環境に対応する値を持つ「environment」という名前のメタデータキーを構成します。デプロイツールを使用してインスタンスメタデータを照会し、環境の値に基づいてアプリケーションを構成します。
3. 選択したデプロイツールを各プロジェクトのインスタンスにデプロイします。デプロイジョブを使用してバージョン管理システムから適切な設定ファイルを取得し、各インスタンスにアプリケーションをデプロイするときに設定を適用します。
4. 各インスタンスの起動時に、インスタンスが提供する環境に対応する値を持つ「environment」という名前のインスタンスカスタムメタデータキーを構成します。デプロイメント ツールを使用してインスタンス メタデータをクエリし、環境値に基づいてアプリケーションを構成します。
<details><div>
    答え：2
説明
不正解
A. 
この方法は機能する可能性がありますが、プロジェクト名から環境を決定するスタートアップスクリプトを記述し、維持することは、特にプロジェクトの命名規則が将来変更された場合に、より複雑でエラーが発生しやすくなる可能性があります。
C. 
この方法では、各プロジェクトのインスタンスにデプロイツールをデプロイし、デプロイ ジョブを設定する必要があるため、より複雑で時間がかかる可能性があります。
D. 
このアプローチでは、インスタンスごとに手動で構成する必要があり、プロジェクト全体の設定を利用しないため、不整合や管理オーバーヘッドの増加につながる可能性があります。
正解
B. 
この方法はシンプルで柔軟だ。環境にプロジェクト全体のメタデータキーを設定することで、環境を識別するための明確で一貫性のある方法が作成されます。デプロイツールはこのメタデータを簡単に照会し、カスタムスクリプトや手動介入を必要とせずに、適切な設定を適用できます。
リンク
https://cloud.google.com/compute/docs/metadata/querying-metadata
</div></details>

## Q. 5-13
貴社は、ゾーン障害によるGoogle Kubernetes Engine（GKE）APIの停止を経験したばかりです。あなたは、将来ゾーン障害が発生した場合にユーザーへのサービス中断を最小限に抑える、可用性の高いGKEアーキテクチャをデプロイしたいと考えています。

あなたは何をすべきでしょうか？
1. ゾーンクラスタを展開する
2. リージョナルクラスターの展開
3. マルチゾーン・クラスタの導入
4. オンプレミスクラスターの導入
<details><div>
    答え：2
説明
不正解
A. ゾーン型クラスタは1つのゾーンに配置されます。そのゾーンに障害が発生すると、クラスタ全体が利用できなくなります。
C. マルチゾーンクラスタは、同じリージョン内の複数のゾーンにまたがっているため、ゾーンクラスタよりも高い可用性を提供しますが、GKEで高可用性を実現するための推奨アプローチとして、リージョナルクラスタがマルチゾーンクラスタに取って代わりました。リージョナルクラスタは、優れたフォールトトレランスを提供します。
D. オンプレミスのクラスタをデプロイしても、Google Cloudインフラストラクチャのゾーン障害という特定の懸念には対処できません。
正解
B. リージョナルクラスターは、リージョン内の3つのゾーンにまたがっています。1つのゾーンに障害が発生しても、他の2つのゾーンは機能し続けることができ、高可用性を提供します。
リンク
https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster
https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#regional_clusters
</div></details>

## Q. 5-14
あなたはイベント駆動型のアプリケーションを開発しています。Pub/Sub に送信されたメッセージを受信するトピックを作成しました。これらのメッセージはリアルタイムで処理する必要があります。アプリケーションは他のシステムから独立しており、新しいメッセージが到着したときにのみコストが発生する必要があります。

どのようにアーキテクチャを構成しますか？
1. Compute Engineにアプリケーションをデプロイします。Pub/Subプッシュサブスクリプションを使用して、トピック内の新しいメッセージを処理します。
2. コードをCloud Functionsにデプロイする。Pub/Sub トリガーを使用してクラウド関数を呼び出します。Pub/Sub API を使用して、Pub/Sub トピックへのプル・サブスクリプションを作成し、そこからメッセージを読み取ります。
3. Google Kubernetes Engineにアプリケーションをデプロイする。Pub/Sub APIを使用して、Pub/SubトピックへのPullサブスクリプションを作成し、そこからメッセージを読み込む。
4. コードをCloud Functionsにデプロイする。Pub/Sub トリガーを使用して、トピック内の新しいメッセージを処理します。
<details><div>
    答え：4
説明
不正解
A. 
これは、新しいメッセージが到着したときにのみコストを発生させるという要件に合致しません。
B. 
Cloud Functions上にコードをデプロイするのは良い選択ですが、Pub/Subトリガーとプル・サブスクリプションの両方を使用するのは冗長です。Pub/Sub トリガーだけでメッセージを処理できます。
C. 
Compute Engineのオプションと同様に、これはリソースの継続的な実行を伴い、メッセージがない場合でもコストが発生します。
正解
D. 
クラウドファンクションでは、クラウドイベントによってトリガーされる単一目的の関数を書くことができます。Pub/Sub トリガーと一緒に使用すると、指定したトピックにメッセージが送信されるたびに関数が呼び出されます。関数の実行時間に対してのみ支払いが発生するため、新しいメッセージが到着したときにのみコストが発生するという要件を満たすことができます。
Cloud Functions で Pub/Sub トリガーを使用する場合、手動でサブスクリプションを作成する必要はありません。代わりに、関数をデプロイすると、指定されたトピックに対してサブスクリプションが自動的に作成されます。そのトピックにメッセージがパブリッシュされるたびに関数がトリガーされ、そのメッセージが関数の引数として渡されます。
したがって、オプション D は有効であり、記述されたシナリオに適しています。これはPub/Subメッセージのリアルタイム処理を可能にし、メッセージがトピックにパブリッシュされたときにのみコストが発生します。
リンク
https://cloud.google.com/solutions/event-driven-architecture-pubsub
</div></details>

## Q. 5-15
あなたは、顧客からの購入を処理するeコマース・アプリケーションを管理しています。注文量は非常に変動しやすく、バックエンドの注文処理システムは一度に 1 つのリクエストしか処理できないことがわかりました。使用量に関係なく、顧客にシームレスなパフォーマンスを保証したい。顧客の注文更新要求が生成された順序で実行されることが重要です。

どうすればよいでしょうか？
1. 購入リクエストと変更リクエストを WebSocket 経由でバックエンドに送信します。
2. 購入リクエストと変更リクエストをRESTリクエストとしてバックエンドに送信する。
3. Pub/Subサブスクライバをプルモードで使用し、データストアを使用して順序を管理する。
4. プッシュモードで Pub/Sub サブスクライバを使用し、データストアを使用して注文を管理する。
<details><div>
    答え：3
説明
不正解
A. 
WebSocket 経由で購入リクエストと変更リクエストを送信しても、リクエストが順番に処理されることは本質的に保証されません。また、既存のアーキテクチャに大きな変更を加える必要があるかもしれません。
B. 
購入リクエストと変更リクエストをRESTリクエストとしてバックエンドに送信すると、同時処理が発生する可能性があり、一度に1つのリクエストのみを処理し、順序を維持するという要件と矛盾する。
D. 
プッシュ・モードで Pub/Sub サブスクライバを使用すると、メッセージが到着するとサブスクライバ・エンドポイントにプッシュされます。メッセージが正しい順序で処理されることを保証するのは難しいかもしれません。
正解
C. 
与えられたシナリオでは、顧客の注文更新要求が生成された順序で処理されることを保証する必要があり、一度に1つの要求しか処理できないバックエンドシステムに対処する必要があります。Google Cloud Pub/Subを使用してメッセージの順序を管理し、プルモードでサブスクライバを使用することで、一度に1つのメッセージを順番に処理することができます。
リンク
https://cloud.google.com/pubsub/docs/pull
</div></details>

## Q. 5-16
あなたの会社の開発チームは、Dockerビルドで様々なオープンソースのオペレーティングシステムを使用したいと考えています。御社の環境で公開コンテナでイメージが作成された場合、共通脆弱性・暴露（CVEs）についてスキャンする必要があります。スキャンプロセスは、ソフトウェア開発の俊敏性に影響を及ぼしてはならない。可能な限りマネージド・サービスを利用したい。

どうすればよいでしょうか？
1. Container Registryの脆弱性スキャン設定を有効にする。
2. コードのチェックイン時にトリガーされるクラウド関数を作成し、CVEsのためにコードをスキャンします。
3. 開発環境において、商用サポートされていないベースイメージの使用を許可しない。
4. Cloud Monitoringを使用してCloud Buildの出力を確認し、脆弱なバージョンが使用されているかどうかを判断する。
<details><div>
    答え：1
説明
不正解
B. 
チェックイン時にCVEsのコードをスキャンするCloud Functionを作成することは、ビルドされたDockerイメージではなく、ソースコードそのものをスキャンすることです。公開されたコンテナイメージのCVEスキャンという特定の要件には合致しません。
C. 
非商用でサポートされているベースイメージの使用を禁止することで、いくつかのリスクは軽減されるかもしれませんが、様々なオープンソースのオペレーティングシステムのCVEをスキャンするという特定のニーズを満たすことはできません。
D. 
Cloud Monitoringを使用してCloud Buildの出力を確認するには、かなりのカスタムロジックが必要であり、コンテナイメージの脆弱性スキャン用に設計された既存のマネージドサービスを活用することはできません。
正解
A. 
GoogleのContainer Registryは、Dockerイメージに統合された脆弱性スキャンを提供しており、イメージのパッケージやバイナリにある既知のセキュリティ脆弱性を検出することができます。この機能を有効にすると、開発プロセスを減速させることなく、イメージが自動的にスキャンされます。これは、提供された要件と一致している。
リンク
https://cloud.google.com/container-analysis/docs/os-overview
https://docs.docker.com/engine/scan/
</div></details>

## Q. 5-17
あるアプリケーションを本番環境にデプロイしています。新しいバージョンがデプロイされたとき、すべての本番トラフィックが新しいバージョンのアプリケーションにルーティングされるようにしたい。また、新バージョンで問題が発生した場合に旧バージョンに戻すことができるように、旧バージョンをデプロイしたままにしておきたいと考えています。

どのデプロイメント戦略を使うべきでしょうか？
1. ブルー/グリーンデプロイメント
2. カナリアデプロイメント
3. ローリングデプロイメント
4. デプロイの再作成
<details><div>
    答え：1
説明
不正解
B. 
このアプローチでは、新バージョンを一部のユーザーに徐々に導入し、そのパフォーマンスと機能を評価します。すべての本番トラフィックをすぐに新しいバージョンに切り替えるわけではないので、この選択肢はシナリオに当てはまりません。
C. 
この戦略では、新バージョンを徐々にロールアウトし、旧バージョンのインスタンスを段階的に置き換えていきます。すべての本番トラフィックを新しいバージョンに即座に切り替えるわけではないので、説明した要件には当てはまらない。
D. 
この戦略では、旧バージョンを停止して新バージョンをデプロイするため、ダウンタイムが発生します。すべてのトラフィックを新バージョンにルーティングしますが、ロールバックの可能性がある旧バージョンを利用できないため、シナリオに適合しません。
正解
A. 
2つのバージョンを即座に切り替えることができ、必要に応じて以前のバージョンに戻すことができます。
リンク
https://www.redhat.com/en/topics/devops/what-is-blue-green-deployment
</div></details>

## Q. 5-18
あなたのアプリケーションは、Google Kubernetes Engine（GKE）クラスタでコンテナとして実行されています。Kubernetes APIサーバーへの呼び出しによって秘密が明らかになるのを防ぐ安全なアプローチを使用して、アプリケーションに秘密を追加する必要があります。

あなたは何をすべきでしょうか？
1. Kubernetesシークレットを作成し、シークレットを環境変数としてコンテナに渡します。
2. クラウド鍵管理サービス（KMS）キーを使用して、クラスタ上でGKEアプリケーションレイヤーシークレット暗号化を有効にします。
3. シークレットをCloud KMSに保存します。Cloud KMSからSecretを読み取るためのGoogleサービスアカウントを作成します。サービスアカウントのキーをJSON形式でエクスポートし、そのJSONファイルをCloud KMSからSecretを読み込めるConfigMapボリュームとしてコンテナにマウントする。
4. シークレットをシークレットマネージャに保存します。Secret ManagerからSecretを読み取るためのGoogleサービスアカウントを作成します。コンテナを実行するKubernetesサービスアカウントを作成します。Workload Identityを使用してGoogleサービスアカウントとして認証します。
<details><div>
    答え：4
説明
不正解
A. 
Kubernetesシークレットは文字列をエンコードするだけで、シークレットを読める人なら誰でもデコードできるため、オプションAは正しくありません。
B. 
GKE Application-layer Secrets Encryptionは依然としてKubernetes APIサーバーからBase64エンコードされた文字列としてシークレットを提示するため、オプションBは正しくありません。
C. Cloud KMSは暗号化キーの保存に使用され、実際のSecretは保存されないため、オプションCは正しくありません。また、Googleサービスアカウントのキーをボリュームとして渡しており、Secretsを読むことができれば誰でも読むことができます。
正解です：
D. 
シークレットを保存するためのセキュアなサービスと、Workload Identityを使用してシークレットを取得するためのセキュアなアプローチを提供します。
リンク
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
https://cloud.google.com/kms/docs
https://kubernetes.io/docs/concepts/configuration/secret/
https://cloud.google.com/secret-manager/docs/overview
</div></details>

## Q. 5-19
Compute Engineのインスタンスグループが、全体のCPU使用率に応じて自動的にスケールするように設定しました。
しかし、クラスタがインスタンスの追加を完了する前に、アプリケーションの応答レイテンシが急激に増加します。インスタンスグループのオートスケーラの構成を変更することで、エンドユーザーにより一貫したレイテンシ体験を提供したいとします。

どの2つの構成を変更する必要がありますか？(2つの選択肢を選んでください)
1. インスタンスグループテンプレートにAUTOSCALEというラベルを追加します。
2. グループに追加されたインスタンスのクールダウン期間を短くする。
3. インスタンスグループのオートスケーラーのターゲットCPU使用率を増やす。
4. インスタンスグループのオートスケーラーのターゲットCPU使用率を減らす。
5. インスタンスグループ内の個々のVMのヘルスチェックを削除する。
<details><div>
    答え：1,4
説明
不正解
A. 
ラベルは通常、整理のために使用され、オートスケールの動作を本質的に変更するものではありません。これは望ましい効果をもたらさないでしょう。
C. 
ターゲットCPU使用率を増やすと、CPU使用率の増加に対するオートスケーラーの感度が低下する。
E. 
ヘルスチェックは、グループ内のインスタンスの健全性を判断するために使用される。ヘルスチェックを削除しても、CPU使用量の増加に対してオートスケーリングがより迅速に反応するようになるとは限りません。
正解
B. 
クールダウン期間は、インスタンスから使用情報を収集する前に、オートスケーラがインスタンスの起動後に待機すべき時間です。この期間を短くすることは、インスタンスが追加された後、オートスケーラがスケーリングの決定をより迅速に行うことができることを意味し、状況の変化により迅速に対応できる可能性があります。
D. 
ターゲットCPU使用率を下げると、オートスケーラーはCPU使用率により敏感になり、CPU使用率がターゲットに近づくと、より迅速にインスタンスを追加するようになります。これにより、システムはレイテンシーの増加により迅速に対応できるようになるはずだ。
これら2つの設定変更は、インスタンス・グループ・オートスケーラーの設定を変更することで、より一貫したレイテンシー・エクスペリエンスを提供するのに役立つはずだ。
リンク
https://cloud.google.com/compute/docs/autoscaler#cool_down_period
https://cloud.google.com/compute/docs/autoscaler/scaling-cpu#scaling_based_on_cpu_utilization
</div></details>

## Q. 5-20
あなたのチームは大規模なGoogle Kubernetes Engine（GKE）クラスタを管理しています。現在、複数のアプリケーションチームが同じ名前空間を使用して、クラスタ用のマイクロサービスを開発しています。あなたの組織では、マイクロサービスを作成するチームをさらに増やす予定です。各チームの作業のセキュリティと最適なパフォーマンスを確保しながら、複数の環境を構成する必要があります。コストを最小限に抑え、Googleが推奨するベストプラクティスに従いたい。

どうすればいいでしょうか？
1. 既存のクラスタの各チームに新しいロールベースのアクセス制御（RBAC）を作成し、リソースのクォータを定義します。
2. 既存のクラスタ内の環境ごとに新しいネームスペースを作成し、リソースクォータを定義します。
3. チームごとに新しいGKEクラスタを作成する。
4. 既存のクラスタにチームごとに新しいネームスペースを作成し、リソースのクォータを定義する。
<details><div>
    答え：1
説明
不正解
B. 
これもリソース・クォータを使用しますが、チーム別ではなく環境別に編成すると、特に複数のチームが同じ環境で作業している場合、必要な分離と組織の明確性が得られない可能性があります。
C. 
このアプローチはチーム間の完全な分離を実現しますが、コストがかかり、運用上のオーバーヘッドが大きくなる可能性があります。
D. 
このオプションも、リソースの使用量を制御しながらネームスペース・レベルでの分離を実現できるため、有力な候補となるでしょう。しかし、RBACを実装しないと、パーミッションのきめ細かな制御ができなくなり、セキュリティに影響する可能性があります。
正解
A. 
この選択により、クラスタ内で誰が何にアクセスできるかを制御でき、チームごとに異なる権限を持つことができます。リソース・クォータを追加することで、各チームが定義された量のリソースしか消費できないようにし、1つのチームがクラスタのリソースを独占することを防ぎます。このアプローチは最小特権の原則に従い、統一された管理構造を維持しながら最適なパフォーマンスを保証します。
RBACをリソース・クォータと一緒に実装することで、セキュリティ、パフォーマンス、コスト効率のバランスを提供し、既存のクラスタ内でのアクセス制御とリソース消費を管理することでベストプラクティスに従います。
リンク
https://cloud.google.com/kubernetes-engine/docs/best-practices/rbac
</div></details>

## Q. 5-21
Google Cloud上で動作するステートレスWebアプリケーションを開発しています。トラフィックがない日もあれば、急増する日もあります。アプリケーションを自動的にスケールアップ/スケールダウンする必要があり、アプリケーションの実行に関連するコストを最小限に抑える必要があります。

あなたは何をすべきでしょうか？
1. FirestoreをデータベースとしてPythonでアプリケーションを構築します。アプリケーションをCloud Runにデプロイします。
2. FirestoreをデータベースとしてC#でアプリケーションを構築します。アプリケーションをApp Engine柔軟環境にデプロイします。
3. CloudSQLをデータベースとしてPythonでアプリケーションを構築する。アプリケーションをApp Engine標準環境にデプロイする。
4. データベースとしてFirestoreを使用してPythonでアプリケーションを構築します。アプリケーションをオートスケーリング機能付きのCompute Engineマネージド・インスタンス・グループにデプロイする。
<details><div>
    答え：1
説明
不正解
B. 
App Engineの柔軟な環境も自動的にスケールしますが、Cloud Runとは異なり、インスタンスがゼロになるまでスケールダウンすることはありません。
C. 
App Engine標準環境は自動的にスケールし、インスタンスをゼロまでスケールダウンできるため、コストを最小限に抑えることができる。ただし、特定のニーズによっては、CloudSQLはFirestoreに比べて運用上のオーバーヘッドやコストが増える可能性があります。
D. 
このアプローチも自動スケーリングを提供しますが、Cloud RunやApp Engineのようなサーバーレスプラットフォームを使用する場合と比較して、運用の複雑さとコストが高くなる可能性が高くなります。
正解
A. 
Cloud Runではステートレス・コンテナをデプロイでき、需要に応じて自動的にスケールアップとスケールダウンが行われる。使用していないときはゼロまでスケールできるため、使用した分だけ料金を支払うことになる。FirestoreはスケーラブルなNoSQLデータベースを提供し、Pythonが適さないような言語固有の要件はない。自動スケーリングとコスト最小化の必要性を考慮すると、Cloud Runが提供するサーバーレスアーキテクチャを活用し、アプリケーションを需要に応じて正確にスケールさせ、コストを最小化できるオプションAが最良の選択と思われる。
リンク
Cloud Run - 自動スケーリング
</div></details>

## Q. 5-22
あなたのウェブサイトはCompute Engineにデプロイされています。マーケティングチームは、3つの異なるWebサイトのデザイン間でコンバージョン率をテストしたいと考えています。

どのアプローチを使うべきですか？
1. App Engine にウェブサイトをデプロイし、トラフィック分割を使用します。
2. App Engineにウェブサイトを3つの別々のサービスとしてデプロイします。
3. ウェブサイトをCloud Functionsにデプロイし、トラフィック分割を使用する。
4. ウェブサイトを3つの別々の機能としてCloud Functionsにデプロイする。
<details><div>
    答え：1
説明
不正解
B. 
App Engine のサービスは通常、アプリケーションを論理コンポーネントに分割するために使用されます。異なるバージョンのサービス間でトラフィックを分割することは、異なるデザイン間のコンバージョン率をテストするのに適切な方法であり、完全に別々のサービスとしてデプロイすることはできません。
C. 
Cloud Functionsは、App Engineのようにビルトインのトラフィック分割をサポートしていません。機能間でトラフィックを分割するカスタムソリューションを実装することはできますが、より複雑になり、App Engineのネイティブトラフィック分割を使用するほど簡単ではありません。
D. 
このオプションは、組み込みのトラフィック分割メカニズムを使用せず、異なる Cloud Functions 間でカスタムのトラフィック分割を実装することは、このユースケースにとって不必要に複雑になります。
正解
A. 
Google App Engineでは、アプリケーションの複数のバージョンをデプロイし、トラフィック分割を使用してそれらのバージョン間で要求を分散できます。これにより、実験を行い、異なるバージョンのパフォーマンスを比較することができます。これは、まさにシナリオが3つの異なるWebサイトのデザインで行っていることです。
リンク
https://cloud.google.com/appengine/docs/standard/python/splitting-traffic
</div></details>

## Q. 5-23
あなたは、オンプレミスのデータセンターをGoogle Cloudに移行する最終段階にいます。期限が間近に迫り、廃止予定のサーバーでWeb APIが実行されていることに気づきました。Google Cloudに移行しながら、このAPIを最新化するソリューションを提案する必要があります。

最新化された Web API は、以下の要件を満たす必要があります：

- 毎月末の高トラフィック時にオートスケールする。

- Python 3.xで記述されていること

- 開発者は、頻繁なコード変更に応じて新しいバージョンを迅速にデプロイできなければならない。この移行にかかるコスト、労力、運用上のオーバーヘッドを最小限に抑えたい。

どうすればいいでしょうか？
1. コードを最新化し、App Engineの柔軟な環境にデプロイします。
2. コードを最新化し、App Engine標準環境にデプロイする。
3. コードを最新化し、アプリケーションをn1-standard-1 Compute Engineインスタンスにデプロイする。
4. Google Kubernetes Engine上のDockerコンテナとして実行するよう、開発チームにアプリケーションの書き換えを依頼する。
<details><div>
    答え：2
説明
不正解
A. 
App Engineの柔軟な環境は要件を満たすかもしれませんが、一般的に標準環境と比べて運用上のオーバーヘッドとコストが高くなります。
C. 
Compute Engineインスタンスにデプロイすると、スケーリングと新バージョンのデプロイのための手動設定と管理が必要になり、不必要な複雑さと運用オーバーヘッドが増える。
D. 
Kubernetes Engineは優れた柔軟性とスケーラビリティを提供するが、この特定のユースケースには過剰な可能性が高い。特に、既存のアプリケーションがまだコンテナ化を使用していない場合は、移行プロセスに複雑さとコストを追加する可能性があります。
正解
B. 
最新化したコードをApp Engineの標準環境にデプロイすれば、コストと複雑さを最小限に抑えながら、前述の要件をすべて満たすことができます。標準環境は自動スケーリングを提供し、Python 3.xをサポートし、新しいバージョンの迅速なデプロイを可能にします。
リンク
https://cloud.google.com/appengine/docs/standard#standard_environment_languages_and_runtimes
</div></details>

## Q. 5-24
あなたは最近、ログデータを毎日クラウドストレージのバケットに転送するウェブアプリケーションを開発しました。認証されたユーザーは、重要なイベントがないかどうか、2週間前のログを定期的にレビューする。その後、ログは年に1回、外部の監査人によってレビューされる。データは7年以上保存しなければならない。これらの要件を満たし、コストを最小限に抑えるストレージ・ソリューションを提案したい。

あなたはどうすべきでしょうか？(選択肢を2つ選んでください）
1. バケットロック機能を使用して、データの保持ポリシーを設定します。
2. 14日以上前のオブジェクトのストレージクラスをColdlineに設定するスケジュールジョブを実行します。
3. Coldline ストレージバケットへのアクセスが必要なユーザー用に JSON ウェブトークン (JWT) を作成する。
4. ライフサイクル管理ポリシーを作成し、14日以上古いオブジェクトのストレージクラスをColdlineに設定する。
5. ライフサイクル管理ポリシーを作成し、14 日以上古いオブジェクトのストレージクラスを Nearline に設定する。
<details><div>
    答え：1,4
説明
不正解
B. 
ライフサイクル管理ポリシーで同じ目標を自動的に達成できるのに、ストレージクラスを管理するためにスケジュールされたジョブを実行すると、不必要な複雑さが加わります。
C. 
Coldline ストレージバケットにアクセスするための JWT を作成することは、コストの最小化や、データ保持およびアクセスに関する指定された要件を満たすこととは関係がない。
E. 
ニアライン・ストレージは、アクセス頻度の低いデータ用に設計されているが、最初の14日以降に年1回見直しを行うデータについては、コールドラインと比較してより高価なオプションとなる可能性がある。このユースケースでは、コールドラインの方が費用対効果が高いでしょう。
正解
A. 
これにより、最低7年間はデータが削除されないようにすることができ、保持要件を満たすことができます。
D. 
Coldlineストレージは、長期アーカイブ用で、アクセス頻度の低いデータ用に設計されているため、外部監査人による年次レビューに適しています。また、コストを最小限に抑えることができます。
リンク
https://cloud.google.com/storage/docs/bucket-lock https://cloud.google.com/storage/docs/lifecycle
</div></details>

## Q. 5-25
アプリケーションは Google Kubernetes Engine (GKE) クラスタにデプロイされています。
アプリケーションの新しいバージョンがリリースされると、CI/CDツールはspec.template.spec.containers[0].imageの値を更新し、新しいアプリケーションバージョンのDockerイメージを参照するようにします。Deploymentオブジェクトが変更を適用する際、新しいバージョンのレプリカを少なくとも1つデプロイし、新しいレプリカが健全になるまで以前のレプリカを維持したいとします。

以下の GKE Deployment オブジェクトにどの変更を加えるべきですか？

apiVersion: apps/v1
種類 デプロイメント
メタデータ
    name: ecommerce-frontend-deployme
スペック
    レプリカ 3
    セレクタ
        matchLabels：
            app: eコマースフロントエンド
    テンプレート
        metadata：
            labels：
                app: ecommerce-frontend
spec：
    コンテナ
        名前: ecommerce-frontend-webapp
        image: ecommerce-frontend-webapp:1.7.9
        ポート
            コンテナポート: 80
1. デプロイメント戦略を RollingUpdate に設定し、maxSurge を 0、maxUnavailable を 1 に設定します。
2. 配置戦略をRollingUpdateに設定し、maxSurgeを1、maxUnavailableを0に設定します。
3. デプロイメント戦略をRecreateに設定し、maxSurgeを0、maxUnavailableを1に設定します。
4. デプロイメント戦略をRecreateに設定し、maxSurgeを1、maxUnavailableを0に設定する。
<details><div>
    答え：2
説明
不正解
A. C. D. 
これらのオプションは、新しいレプリカが健全になるまで既存のレプリカを維持するという要件を満たさないか、新しいレプリカを作成する前に既存のレプリカをすべて終了するRecreateストラテジーを含むため、望ましい動作に合致しません。
正しい答え
B. 配置を更新する場合、RollingUpdate戦略では、少なくとも一定数のレプリカが常に使用可能な状態に保たれ、古いレプリカが終了する前に新しいレプリカが作成されます。
新しいバージョンのレプリカを少なくとも1つ配置し、新しいレプリカが健全になるまで以前のレプリカを維持する」という要件では、ストラテジをRollingUpdateに設定し、maxSurgeを1に設定し、maxUnavailableを0に設定する必要があります。
maxSurge： 更新中に作成できる追加レプリカの数。これを 1 に設定すると、レプリカを 1 つ追加作成できます。
maxUnavailable： 更新中に使用できなくなるレプリカの最大数。これを0に設定すると、新しいレプリカが健全になるまで、既存のレプリカが停止しないようになります。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades
</div></details>

## Q. 5-26
あなたは、HTTPとHTTPSの両方でアクセスでき、Compute Engineインスタンス上で実行されるWebアプリケーションを開発しています。アプリケーションのメンテナンスを行うために、リモートのラップトップからCompute EngineインスタンスにSSH接続する必要があります。Googleが推奨するベストプラクティスに従って、インスタンスをどのように設定すればよいでしょうか？
1. TCPプロキシロードバランサーの後ろにプライベートIPアドレスを持つCompute Engineウェブサーバーインスタンスでバックエンドを設定します。
2. ファイアウォールルールを設定して、すべてのイングレストラフィックがCompute EngineのWebサーバーに接続できるようにします。
3. SSHアクセス用にCloud Identity-Aware Proxy APIを構成します。次に、アプリケーションのWebトラフィック用にHTTP(S)ロードバランサーの後ろにプライベートIPアドレスを持つCompute Engineサーバーを構成します。
4. HTTP(S)ロードバランサーの背後にプライベートIPアドレスを持つCompute Engine Webサーバーインスタンスを持つバックエンドをセットアップする。パブリックIPアドレスとオープンファイアウォールポートを持つBastionホストをセットアップする。bastion hostを使ってWebインスタンスに接続する。
<details><div>
    答え：3
説明
不正解
A. 
TCPプロキシロードバランサーはHTTP/HTTPSトラフィック用に設計されておらず、特定のSSHアクセスソリューションを提供しません。
B. 
すべてのイングレストラフィックを許可すると、不必要なセキュリティリスクが発生します。
D. 
bastionホストのアプローチは機能しますが、Cloud IAPと比べてより複雑なソリューションであり、必要なアプリケーション・トラフィックにHTTP(S)ロードバランシングを使用することに特に言及していません。
正解
C. 
オプションCは、Googleが推奨するベストプラクティスに準拠しながら要件を満たす最善のアプローチである。
Cloud IAPはIAM（Identity and Access Management）と統合され、VMをパブリック・インターネットに公開することなくSSHアクセスを可能にし、セキュリティを強化する。
HTTP(s)ロードバランサーの使用により、Webトラフィックをインスタンス間でバランスさせることができ、リクエストに応じてHTTPとHTTPSの両方がサポートされます。
Compute EngineインスタンスのプライベートIPアドレスは、パブリックインターネットへの露出を減らし、セキュリティをさらに強化します。
リンク
https://cloud.google.com/solutions/connecting-securely#storing_host_keys_by_enabling_guest_attributes
https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_ssh_connections
https://cloud.google.com/compute/docs/connect/standard-ssh#cloud_iap
</div></details>

## Q. 5-27
あなたの開発チームは、.NETレガシーアプリケーションの保守を任されています。アプリケーションは時々変更され、最近更新されました。あなたの目標は、環境から環境へとCI/CDパイプラインを移動しながら、アプリケーションが一貫した結果を提供することを確実にすることです。ホスティング環境間の外部要因や依存関係が問題にならないようにしながら、デプロイのコストを最小限に抑えたい。あなたの組織では、コンテナはまだ承認されていません。

どうすべきでしょうか？
1. .NET Coreを使用してアプリケーションを書き直し、Cloud Runにデプロイします。リビジョンを使用して環境を分離します。
2. Cloud Buildを使用して、ビルドごとに新しいCompute Engineイメージとしてアプリケーションをデプロイします。このイメージを各環境で使用する。
3. MS Web Deployを使用してアプリケーションをデプロイし、Compute Engineでは常に最新のパッチが適用されたMS Windows Serverのベースイメージを使用するようにする。
4. Cloud Buildを使ってアプリケーションをパッケージ化し、Google Kubernetes Engineクラスタにデプロイする。名前空間を使用して環境を分離する。
Google Kubernetes Engine (GKE)クラスタにデプロイすることは、コンテナ化を意味するが、これも組織で承認されていない。
<details><div>
    答え：2
説明
不正解
A. 
.NET Coreを使用してアプリケーションを書き直し、Cloud Runにデプロイすることは、アプリケーションをコンテナ化することを意味します。
C. 
MS Web Deployを使用すると、環境間で依存関係が異なる可能性があり、アプリケーションの動作が異なる可能性があるため、異なる環境間で一貫した結果を得る必要性に対処できません。さらに、最新のパッチが適用されたWindows Serverベースイメージを確実に使用することは良いことですが、必ずしも環境間の一貫性を保証するものではありません。
D. 
正解
B. 
これは、ビルドごとに新しいCompute Engineイメージを作成し、すべての依存関係がすべての環境で一貫していることを保証し、コンテナを使用しないので、正しいアプローチです。
リンク
https://cloud.google.com/architecture/modernization-path-dotnet-applications-google-cloud#take_advantage_of_compute_engine
https://cloud.google.com/architecture/modernization-path-dotnet-applications-google-cloud#phase_1_rehost_in_the_cloud
</div></details>

## Q. 5-28
マネージド・インスタンス・グループによって管理されているアプリケーションがあります。アプリケーションの新バージョンをデプロイする場合、コストを最小限に抑え、インスタンス数を増やしてはなりません。新しいインスタンスが作成されるたびに、新しいインスタンスが健全である場合にのみデプロイが継続されるようにしたい。

どうすればよいでしょうか？
1. maxSurgeを1、maxUnavailableを0に設定してローリングアクションを実行します。
2. maxSurgeを0、maxUnavailableを1に設定してローリングアクションを実行します。
3. maxHealthyを1、maxUnhealthyを0に設定してローリングアクションを実行する。
4. maxHealthyを0、maxUnhealthyを1にしてローリングアクションを実行する。
<details><div>
    答え：2
説明
不正解
A. 
maxSurgeを1に設定すると、更新中に追加のインスタンスが許可され、コストが増加する可能性があります。
C. D. 
CとD。maxHealthyとmaxUnhealthyは管理インスタンスグループのローリングアップデートのパラメータとして認識されていません。
正解
B. 
maxSurgeを0に設定することで、更新中にインスタンス数が必要な数を超えないようにします。一時的にインスタンスが追加作成されることはないため、コストを最小限に抑えるという要件に一致します。
maxUnavailableを1に設定すると、更新のために一度に1つのインスタンスを停止できますが、新しいインスタンスが健全であるとマークされた場合にのみプロセスが続行されます。
リンク
MIGでVMの設定更新を自動的に適用する｜Compute Engine Documentation｜Google Cloud
</div></details>

## Q. 5-29
あなたはCloud Run上にデプロイされ、Cloud Functionsを使用する新しいアプリケーションに取り組んでいます。新しい機能が追加されるたびに、新しい Cloud Functions と Cloud Run サービスがデプロイされます。ENV変数を使用してサービスを追跡し、サービス間通信を可能にしていますが、ENV変数を維持するのが難しくなっています。動的ディスカバリーをスケーラブルに実装したい。

どうすればいいでしょうか？
1. Google CloudプロジェクトにデプロイされたCloud RunサービスとCloud Functionsをクエリするために、Cloud Run AdminとCloud Functions APIを使用するようにマイクロサービスを構成します。
2. Service Directoryネームスペースを作成する。デプロイ時にAPIコールを使用してサービスを登録し、実行時にクエリを実行します。
3. きちんと文書化された命名規則を使用して、Cloud FunctionsとCloud Runサービスのエンドポイントの名前を変更する。
4. 単一のCompute EngineインスタンスにHashicorp Consulをデプロイする。デプロイ時にConsulにサービスを登録し、実行時にクエリを実行する。
<details><div>
    答え：2
説明
不正解
A. 
Cloud Run Admin APIとCloud Functions APIを直接クエリしても動作しますが、複雑さが増し、レートが制限される可能性があります。
C. 
十分に文書化された命名規則に依存することは、真の動的発見メカニズムを提供しない。命名規則が変更されたり、命名に間違いがあったりすると、問題につながる可能性がある。
D. 
単一のCompute EngineインスタンスにHashicorp Consulをデプロイすると、運用がさらに複雑になり、単一障害点となる可能性がある。また、Service DirectoryのようなGoogleが管理するサービスほどシームレスではありません。
正解
B. 
Service Directoryはサービスエンドポイントのリアルタイム検索を提供し、保守とサービスの動的発見を容易にします。
サービスを名前空間に整理し、デプロイ時にサービスを登録し、実行時にクエリを実行できる。
Google Cloudによるマネージド・サービスなので、既存のアーキテクチャとうまく統合でき、必要に応じて拡張できる。
そのため、オプションBはスケーラブルでダイナミックなディスカバリーの要件に最も合致している。
リンク
https://medium.com/google-cloud/fine-grained-cloud-dns-iam-via-service-directory-446058b4362e
https://cloud.google.com/service-directory/docs/overview
</div></details>

## Q. 5-30
あなたは、App Engineの標準環境とMemorystore for Redisを使用するeコマースWebアプリケーションを開発しています。ユーザーがアプリケーションにログインすると、アプリケーションはユーザーの情報（セッション、名前、住所、プリファレンスなど）をキャッシュし、チェックアウト時にすばやく取得できるように保存します。
ブラウザでアプリケーションをテストしているとき、502 Bad Gateway エラーが発生しました。アプリケーションはMemorystoreに接続していないと判断しました。

このエラーの原因は何ですか？
1. お客様のMemorystore for RedisインスタンスはパブリックIPアドレスなしでデプロイされました。
2. Serverless VPC AccessコネクタをApp Engineインスタンスとは異なるリージョンで構成しました。
3. App EngineとMemorystore間の接続を許可するファイアウォールルールが、DevOpsチームによるインフラストラクチャの更新中に削除されました。
4. App Engine インスタンスとは異なるアベイラビリティ ゾーンの異なるサブネットで Serverless VPC Access コネクタを使用するようにアプリケーションを構成しました。
<details><div>
    答え：2
説明
不正解
A. 
Memorystore for Redisインスタンスは、App EngineからアクセスするためにパブリックIPアドレスを必要としないため、これがエラーの原因である可能性は高くありません。
C. 
App Engineの標準環境では、Memorystoreへの接続に特定のファイアウォールルールを必要としないため、それらを削除してもこの問題は発生しません。
D. 
サブネット情報は重要ですが、App Engine の標準環境ではアベイラビリティ ゾーン レベルでの構成は行わないため、このオプションは適用できません。
正解
B. 
App Engineの標準環境でMemorystore for Redisを使用する場合、App EngineサービスはServerless VPC Accessコネクタを使用してMemorystoreと通信する必要があり、両方が同じリージョンにある必要があります。この要件を反映したオプションがオプションBです。
リンク
レスポンスエラーのトラブルシューティング｜OpenAPIを使用したクラウドエンドポイント
</div></details>

## Q. 5-31
アプリケーションがカスタム・マシン・イメージとしてビルドされている。マシンイメージの複数の固有のデプロイがあります。各デプロイは、独自のテンプレートを持つ個別の管理対象インスタンスグループです。各デプロイメントには、一意の設定値のセットが必要です。これらの固有の値を各配備に提供しますが、すべての配備で同じカスタ ムマシンイメージを使用します。Compute Engineのすぐに使える機能を使用したい。

どうすればよいですか?
1. 一意の構成値を永続ディスクに配置します。
2. 固有の構成値をCloud Bigtableテーブルに置く。
3. インスタンステンプレートのスタートアップスクリプトに固有の構成値を配置する。
4. インスタンス・テンプレートのインスタンス・メタデータに一意の構成値を配置します。
<details><div>
    答え：4
説明
不正解
A. 
構成値を永続ディスクに配置すると、構成とストレージが緊密に結合されるため、柔軟性が低下し、異なるインスタンス間で管理するのが難しくなります。
B. 
Cloud Bigtableを使用すると、この特定のユースケースに明確なメリットをもたらすことなく、さらなる複雑さとコストが発生する。
C. 
起動スクリプトを使用してインスタンスを構成することはできますが、起動スクリプトに一意の構成値を直接配置することは、インスタンスメタデータを使用する場合、特に複数の一意のデプロイがある場合、柔軟性と保守性が劣る可能性があります。
正解
D. 
インスタンスメタデータは、より柔軟でエレガントなソリューションを提供し、異なるデプロイメント間で同じイメージを使用しながら、各デプロイメントに固有の構成を提供できます。したがって、オプションDが正しい選択です。
リンク
インスタンスグループ｜Compute Engineドキュメント｜Google Cloud
https://cloud.google.com/compute/docs/storing-retrieving-metadata#custom
</div></details>

## Q. 5-32
このアーキテクチャ図は、何千台ものデバイスからデータをストリーミングするシステムを表しています。データをパイプラインに取り込み、データを保存し、SQLステートメントを使用してデータを分析したいとします。

ステップ 1、2、3、および 4 では、どの Google Cloud サービスを使用する必要がありますか？
1. App Engine,Pub/Sub,BigQuery
2. データフロー,Pub/Sub,Firestore,BigQuery
3. Pub/Sub,データフロー,BigQuery,Firestore
4. Pub/Sub,データフロー,Firestore,BigQuery
<details><div>
    答え：
説明
不正解
オプション A：
App Engine： App Engineはアプリケーションの構築には使用できますが、デバイスからのストリーミングデータの取り込みには通常使用されません。
Pub/Sub: データを取り込むことはできますが、データの変換や処理には通常使用されません。
BigQuery： BigQueryはSQLを使用してデータを分析するために使用され、処理されたストリーミング・データを保存するために使用されるわけではないので、このステップには不適切である。
Firestore： FirestoreのようなNoSQLデータベースはデータを保存することはできますが、一般的にSQLを使用してデータを分析するために使用されることはありません。
オプションB：
データフロー： Dataflowはデータを処理・変換するためのものであり、ストリーミングデータを取り込むためのものではない。
Pub/Sub：データを取り込むことはできるが、データを処理したり変換したりするためには使用されない。
Firestore： Firestoreはデータを保存することはできるが、SQLステートメントを使ったデータ分析には適していないため、このステップには不適切。
BigQuery： データを分析することはできるが、分析前に処理したデータを保存するという要件に合致しないため、このステップには不適切である。
オプションC：
Pub/Sub：データの取り込みには適しているが、加工や変換には適していない。
データフロー： データの処理と変換については正しい。
BigQuery： SQLを使用したデータ分析に使用され、処理されたデータの保存には使用されないため、このステップには使用されない。
Firestore： データを保存するために使用されますが、通常SQLベースの分析には使用されません。
正解です：
D. 1. Pub/Sub 2. データフロー 3. ファイアストア 4. BigQuery
Pub/Sub：メッセージ・バッファとして機能し、数千台のデバイスからリアルタイムのストリーミング・データを取り込む。
データフロー： ストリーミング・データの処理と変換を行う。バッチ処理とストリーム処理の両方に対応できる。
ファイアストア 処理されたデータを格納するNoSQLデータベース。大規模データの処理とリアルタイムアクセスが可能。
BigQuery： SQLステートメントを使用してデータを分析する。アドホック・クエリーを実行し、保存されたデータに対してリアルタイム分析を実行できる。
このパイプラインにより、データの取り込み、処理、保存、分析が必要に応じて確実に行われる。
リンク
Pub/Sub ドキュメント
データフロー・ドキュメント
Firestore ドキュメント
BigQueryドキュメント
</div></details>

## Q. 5-33
アプリケーション用に `fully baked` または `golden` Compute Engine イメージを作成したい。

アプリケーションの実行環境(test, staging, production)に応じて、適切なデータベースに接続するようにアプリケーションをブートストラップする必要があります。

何をすべきでしょうか？
1. 適切なデータベース接続文字列をイメージに埋め込みます。環境ごとに異なるイメージを作成する。
2. Compute Engineのインスタンスを作成する際に、接続するデータベース名のタグを追加する。アプリケーションでは、Compute Engine APIに問い合わせて現在のインスタンスのタグを取得し、タグを使用して適切なデータベース接続文字列を構築します。
3. Compute Engineインスタンスを作成するときに、DATABASEのキーと適切なデータベース接続文字列の値を持つメタデータ項目を作成します。アプリケーションでは、DATABASE環境変数を読み取り、その値を使用して適切なデータベースに接続します。
4. Compute Engineインスタンスを作成するときに、DATABASEのキーと適切なデータベース接続文字列の値を持つメタデータ項目を作成します。アプリケーションで、メタデータ・サーバーにDATABASE値を問い合わせ、その値を使用して適切なデータベースに接続します。
<details><div>
    答え：4
説明
不正解
A. 
データベース接続文字列をイメージに埋め込み、環境ごとに異なるイメージを作成することは、複数のイメージを管理することになり面倒です。柔軟性がなく、接続文字列を変更すると新しいイメージが必要になる。
B. 
タグを使用すると、データベース接続文字列のような機密情報がインスタンスの詳細を閲覧できる人に公開され、潜在的なセキュリティリスクにつながる可能性があります。
C. 
環境変数の読み取りは、メタデータ・サーバーへの問い合わせほど簡単ではないかもしれません。さらに、このアプローチでは、インスタンスの作成時または起動時に環境変数が何らかの方法で設定されることを前提としていますが、これがどのように発生するかは明確になっていません。
正解
D. 
これにより、柔軟性とセキュリティが向上します。接続文字列をインスタンスのメタデータに格納することで、メタデータに基づいて異なる環境に適応する単一のイメージを持つことができます。メタデータ・サーバーへの問い合わせはGoogle Cloudで一般的にサポートされている方法であり、このオプションは説明したシナリオに最適です。インスタンスメタデータの保存と取得に関するドキュメントに、この方法の詳細が記載されています。
リンク
https://cloud.google.com/compute/docs/metadata/querying-metadata
https://cloud.google.com/compute/docs/metadata/setting-custom-metadata
</div></details>

## Q. 5-34
貴社はオンプレミスのHadoop環境をクラウドに移行する計画を立てています。HDFSに保存されているデータのストレージコストとメンテナンスの増加は、貴社にとって大きな懸念事項です。また、既存のデータ分析ジョブや既存のアーキテクチャへの変更は最小限に抑えたいと考えています。

どのように移行を進めるべきでしょうか？
1. Hadoopに保存されているデータをBigQueryに移行します。オンプレミスのHadoop環境ではなく、BigQueryから情報を取得するようにジョブを変更します。
2. コスト削減のため、SSDの代わりにHDDを搭載したCompute Engineインスタンスを作成する。そして、既存の環境をCompute Engineインスタンスで新しい環境に完全移行する。
3. Google Cloud Platform上にCloud Dataprocクラスタを作成し、Hadoop環境を新しいCloud Dataprocクラスタに移行する。HDFSデータをより大きなHDDディスクに移動し、ストレージコストを節約する。
4. Google Cloud Platform上にCloud Dataprocクラスタを作成し、Hadoopコードオブジェクトを新しいクラスタに移行する。データをCloud Storageに移動し、Cloud Dataprocコネクタを活用してそのデータ上でジョブを実行する。
<details><div>
    答え：4
説明
不正解
A. 
データをBigQueryに移行し、BigQueryから情報を取得するようにジョブを変更するには、既存のデータ分析ジョブや既存のアーキテクチャを大幅に変更する必要があります。これでは、既存のジョブに最小限の変更を加えるという要件を満たせません。
B. 
単にHDDを搭載したCompute Engineインスタンスを作成し、既存のHadoop環境を移行するだけでは、ストレージコストとメンテナンスの増加という懸念には対処できない。また、Cloud Dataprocのようなマネージドサービスのメリットも活用できない。
C. 
HDFSのデータをより大きなHDDディスクに移動しても、必ずしもストレージコストが削減できるとは限らないし、メンテナンスの負担も軽減されないかもしれない。Cloud DataprocはGoogle Cloud Storageとシームレスに動作するように設計されており、このオプションはその機能を活用しません。
正解
D. 
オプションDは、フルマネージドApache SparkおよびApache HadoopサービスであるCloud DataprocとGoogle Cloud Storageを活用する。データをCloud Storageに移行することで、HDFSに比べてストレージコストとメンテナンスオーバーヘッドを削減できる。既存のHadoopジョブは最小限の変更でCloud Dataprocクラスタに移行でき、Cloud Dataprocコネクタを使ってCloud Storageに保存されたデータに対してジョブを実行できる。このアプローチは既存のジョブへの変更を最小限に抑え、ストレージコストとメンテナンスを削減するという目標に合致している。
リンク
https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-overview
</div></details>

## Q. 5-35
あなたは、分散マイクロサービス・アーキテクチャを使用するアプリケーションを構築しています。あなたはJavaで書かれたマイクロサービスの1つで、パフォーマンスとシステムリソースの使用率を測定したいと考えています。

あなたは何をすべきでしょうか？
1. Cloud Profilerでサービスを計測し、サービスのCPU使用率とメソッドレベルの実行時間を計測します。
2. サービス・エラーを調査するために、Debuggerでサービスをインスツルメンテーションします。
3. Cloud Traceでサービスを計測し、リクエストの待ち時間を測定する。
4. サービスのレイテンシを測定するためにOpenCensusでサービスをインストルメントし、カスタムメトリクスをCloud Monitoringに書き込む。
<details><div>
    答え：1
説明
不正解
B. 
クラウドDebuggerは、実行中のアプリケーションのパフォーマンスに影響を与えたり、停止したり遅くしたりすることなく、任意のコードロケーションでアプリケーションの状態を調査するために使用されます。これはデバッグには有用ですが、パフォーマンスやリソース使用率を測定することはできません。
C. 
Cloud Traceはリクエストのレイテンシーをレポートするのに使えますが、CPU使用率やメソッドレベルの実行時間に関する情報は提供しません。
D. 
OpenCensusを使用してレイテンシを測定し、カスタムメトリクスを作成することができますが、Google Cloudは、CPU使用率およびメソッドレベルの実行時間を含むアプリケーションパフォーマンスを分析するために設計された専用ツールとしてCloud Profilerを提供しているため、このユースケースにはCloud Profilerの方が適しています。
正解
A. 
Cloud Profilerを使用することは、指定されたJavaマイクロサービスのパフォーマンスとシステム・リソース利用率の両方を測定するという指定された目標を達成する最も簡単な方法です。
リンク
https://cloud.google.com/profiler/docs/profiling-java
https://cloud.google.com/appengine/docs/legacy/standard/java/microservice-performance
</div></details>

## Q. 5-36
あなたは、ユーザーのファイルをクラウドストレージに保存する必要があるアプリケーションを開発しています。
各ユーザーがクラウドストレージに自分のサブディレクトリを持つようにしたい。新しいユーザが作成されると、対応する空のサブディレクトリも作成されなければなりません。
どうすればいいでしょうか？
1. 長さ0バイトの、末尾のスラッシュ（'/'）で終わるサブディレクトリ名のオブジェクトを作成します。
2. サブディレクトリの名前でオブジェクトを作成し、そのサブディレクトリ内のオブジェクトをすぐに削除します。
3. 長さが0バイトで、WRITERアクセス制御リスト権限を持っているサブディレクトリの名前でオブジェクトを作成する。
4. サブディレクトリの名前で、長さが0バイトのオブジェクトを作成する。Content-TypeメタデータをCLOUDSTORAGE_FOLDERに設定する。
<details><div>
    答え：1
説明
不正解
B. 
サブディレクトリ内にオブジェクトを作成し、すぐに削除しても、実際にはサブディレクトリ自体は作成されません。
C. 
WRITERアクセス制御リスト権限を持ち、長さが0バイトのオブジェクトを作成することは、ディレクトリを表現する標準的な方法ではありません。アクセス制御リストはパーミッションを管理するために使用されるのであって、ディレクトリを作成するために使用されるのではありません。
D. 
Content-TypeメタデータをCLOUDSTORAGE_FOLDERに設定することは、クラウド・ストレージでは特別な意味を持ちません。Google Cloud Storageでは、この目的のためにCLOUDSTORAGE_FOLDERのような認識された値はありません。
正解
A. 
Google Cloud Storageでは、ディレクトリやサブディレクトリは物理オブジェクトではありません。代わりに、オブジェクトの命名規則の一部です。フォルダやサブディレクトリの概念は、オブジェクト名にスラッシュ（「/」）を使用することでシミュレートされます。
したがって、ディレクトリやサブディレクトリを別のオブジェクトとして作成するという概念はクラウドストレージには存在しません。サブディレクトリを別オブジェクトとして作成する必要はなく、単にサブディレクトリ名をプレフィックスとしてオブジェクト名に含めることができます。例えば、"user1 "というサブディレクトリを作成したい場合、オブジェクト名を "user1/filename "とするだけです。
しかし、空のディレクトリの作成をシミュレートしたり、いくつかのツールがディレクトリとして認識するパターンに従ったりしたい場合は、末尾のスラッシュで終わる名前のゼロバイトオブジェクトを作成することができます。これがオプションAの提案である。
リンク
https://cloud.google.com/storage/docs/folders#overview
</div></details>

## Q. 5-37
あなたはJavaアプリケーションをCloud Runにデプロイしました。アプリケーションはCloud SQL上でホストされているデータベースにアクセスする必要があります。規制要件のため、Cloud SQLインスタンスへの接続は内部IPアドレスを使用しなければなりません。

Googleが推奨するベストプラクティスに従って、どのように接続を設定すればよいでしょうか？
1. Cloud SQL接続でCloud Runサービスを構成します。
2. サーバーレスVPCアクセスコネクターを使用するようにCloud Runサービスを構成します。
3. Cloud SQL Java コネクタを使用するようにアプリケーションを構成します。
4. Cloud SQL Auth プロキシのインスタンスに接続するようにアプリケーションを構成する。
<details><div>
    答え：2
説明
不正解
A. 
このオプションは通常、Cloud SQLへのパブリック接続を使用します。要件は内部IPアドレスを使用して接続することなので、これは要件を満たしていません。
C. 
Cloud SQL Java コネクタは通常、接続を管理するために使用され、接続がCloud SQLインスタンスの内部IPアドレスに行われるか外部IPアドレスに行われるかを本質的に制御しません。これだけでは、内部 IP アドレス経由の接続は保証されません。
D. 
Cloud SQL Auth プロキシは、Cloud SQL への接続を保護するために使用されます。内部IPに接続するように構成することもできますが、そうするには通常VPCピアリングかVPNが必要になります。Cloud Runはサーバーレスなので、Cloud RunからピアリングやVPNを設定することは実行不可能です。
正解
B. 
Cloud Runサービスから内部IPアドレスを使用してCloud SQLインスタンスに接続する必要がある場合、推奨されるアプローチはServerless VPC Accessコネクターを使用することです。これにより、Cloud RunサービスはGoogle Cloud Virtual Private Cloud（VPC）ネットワーク内の内部IPアドレスに接続できるようになります。
リンク
https://cloud.google.com/sql/docs/mysql/connect-run#vpc-access
</div></details>

## Q. 5-38
Compute Engineにデプロイされたアプリケーションのメモリ使用量を確認したい。

どうすればいいでしょうか？
1. Stackdriverクライアント・ライブラリをインストールします。
2. Stackdriver Monitoring Agentをインストールします。
3. Stackdriver Metrics Explorerを使用します。
4. Google Cloud Platform Console を使用する。
<details><div>
    答え：2
説明
不正解
A. 
クライアント・ライブラリは通常、さまざまなクラウド・サービスとプログラムでやり取りするために使用され、Compute Engineインスタンスのメモリ使用量の監視には直接関係しません。
C. 
Metrics Explorerは、収集したメトリクスを表示および分析するために使用できるCloud Monitoringの機能です。このデータを収集するには、Monitoring Agentが必要です。
D. 
Google Cloud Platform Consoleは、Cloud Monitoringを含む様々な監視機能へのアクセスを提供しますが、Monitoring Agentをインストールしない限り、コンソールを使用するだけでは、詳細なメモリ使用量を得ることはできません。
正解
B. 
Monitoring Agentは、VMインスタンスからシステムとアプリケーションのメトリクスを収集し、Cloud Monitoringに送信する役割を担っています。メモリ使用量はこれらのメトリクスの1つです。したがって、Compute Engine上にデプロイされたアプリケーションのメモリ使用量を表示するには、選択肢Bが適切です。
リンク
https://cloud.google.com/monitoring/agent
Google Cloud Platform: VMインスタンスのメモリ使用量を監視する方法 - Stack Overflow
</div></details>

## Q. 5-39
アプリケーションをGoogle Cloud Platformに移行し、既存の監視プラットフォームはそのまま使用しています。現在、タイムクリティカルな問題に対して通知システムが遅すぎることに気づきました。

どうしたらよいでしょうか？
1. 監視プラットフォーム全体をStackdriverに置き換える。
2. Compute EngineインスタンスにStackdriverエージェントをインストールする。
3. Stackdriverを使用してログをキャプチャし、アラートを出し、それを既存のプラットフォームに送信する。
4. 一部のトラフィックを旧プラットフォームに戻し、2つのプラットフォームで同時にA/Bテストを実施する。
<details><div>
    答え：3
説明
不正解
A. 
Stackdriverは強力な監視とロギングのソリューションですが、既存の監視プラットフォーム全体を置き換えるのは大変な作業です。通知が遅いという特定の問題を解決するには、このオプションはやり過ぎかもしれません。
B. 
エージェントをインストールすることで、メトリクスとログを収集できるようになりますが、通知システムが遅いという問題に対処できるとは限りません。エージェントのインストールに加えて、アラートを設定する必要があります。
D. 
このオプションは、通知が遅いという問題に直接対処しておらず、核心的な問題を解決せずに不必要な複雑さをもたらす可能性が高い。
正解
C. 
Stackdriver（現在はCloud Monitoring and Cloud Loggingとして知られている）を利用することで、タイムクリティカルな問題に対してGoogle Cloudのアラート機能を利用することができます。こうすることで、既存の監視プラットフォームと統合しながら、迅速な通知というメリットを得ることができます。Stackdriverで特定のアラート条件を設定し、必要に応じてログとメトリクスを既存のシステムに転送できます。オプションCでは、Google Cloudのアラート機能を活用しながら、既存の監視プラットフォームの部分を残すことができます。
リンク
https://cloud.google.com/monitoring/alerts/concepts-indepth#notification-latency
</div></details>

## Q. 5-40
あなたは、機密性の高い非構造化データオブジェクトをクラウドストレージのバケットに保存し、アクセスするアプリケーションを開発しています。規制要件に準拠するため、すべてのデータオブジェクトが最初に作成されてから少なくとも7年間は利用可能であることを保証する必要があります。3年以上前に作成されたオブジェクトへのアクセス頻度は非常に低い（年に1回以下）。ストレージ・コストを最適化しながらオブジェクト・ストレージを構成する必要があります。

あなたは何をすべきでしょうか？(選択肢を2つ選んでください）
1. バケットに7年間の保持ポリシーを設定します。
2. IAM条件を使用して、オブジェクトの作成日から7年後にオブジェクトへのアクセスを提供する。
3. オブジェクト作成後7年間は、オブジェクトが誤って削除されないように、オブジェクトのバージョニングを有効にする。
4. バケットにオブジェクトのライフサイクルポリシーを設定し、3年後にオブジェクトをStandard StorageからArchive Storageに移動する。
5. バケツ内の各オブジェクトの年齢をチェックし、3年より古いオブジェクトをArchive Storageクラスを持つ2番目のバケツに移動するCloud Functionを実装する。Cloud Schedulerを使用して、毎日のスケジュールでCloud Functionをトリガーする。
<details><div>
    答え：1,4
説明
不正解
B. 
IAM条件を使用して、特定の時間後にオブジェクトへのアクセスを提供することは、オブジェクトの可用性を確保したり、ストレージコストを最適化したりする要件に対応しません。
C. 
オブジェクトのバージョニングを有効にすると、オブジェクトが上書きされたり削除されたりしても、以前のバージョンは保持されますが、7年間の保持期間を強制したり、アクセス頻度に基づいてストレージコストを最適化したりすることはできません。
E. 
オブジェクトを移動するCloud Functionを実装すれば目的は達成できるかもしれませんが、Cloud Storageのネイティブのライフサイクル管理機能（オプションD）を使用するのに比べて、より複雑でエラーが発生しやすいアプローチになります。
正解
A. 
これにより、少なくとも7年間保存されるまではオブジェクトを削除できないようになり、規制要件を満たすことができます。
D. 
これにより、アクセス頻度の低いオブジェクトを3年後に低コストのストレージクラスに移動することで、ストレージコストを最適化することができる。
リンク
https://cloud.google.com/storage/docs/bucket-lock
https://cloud.google.com/storage/docs/lifecycle
</div></details>

## Q. 5-41
BigQuery APIを使用して、数分ごとにBigQuery上で数百のクエリを実行する分析アプリケーションがあります。これらのクエリの実行にかかる時間を調べたいとします。

どうすればよいでしょうか？
1. Stackdriver Monitoringを使用して、スロットの使用状況をプロットします。
2. API 実行時間をプロットするには、Stackdriver Trace を使用します。
3. Stackdriver Traceを使用して、クエリ実行時間をプロットします。
4. Stackdriver Monitoringを使用して、クエリ実行時間をプロットします。
<details><div>
    答え：4
説明
不正解
A. 
スロットの使用状況をプロットすると、クエリの具体的な実行時間ではなく、使用されている計算リソースに関する情報が得られます。
B. 
Stackdriver Traceは、アプリケーションリクエストの待ち時間の分析に使用され、BigQueryクエリの実行時間に特化したものではありません。
C. 
オプションBと同様に、Stackdriver TraceはBigQueryクエリの実行時間を直接プロットするために使用されません。
正しい答え
D. 
これにより、クエリの実行にかかる時間を把握することができ、記載されている要件を満たすことができます。
リンク
https://cloud.google.com/bigquery/docs/monitoring
</div></details>

## Q. 5-42
異なるマイクロサービスを使用してアプリケーションを開発しています。各マイクロサービスに特定の数のレプリカを設定する機能が必要です。また、マイクロサービスがスケールするレプリカ数に関係なく、他のマイクロサービスから特定のマイクロサービスに統一された方法でアクセスできる機能が必要です。あなたはこのソリューションをGoogle Kubernetes Engineに実装する予定です。

あなたは何をすべきでしょうか？
1. 各マイクロサービスをデプロイメントとしてデプロイします。Serviceを使用してクラスタ内のDeploymentを公開し、クラスタ内の他のマイクロサービスからアドレス指定するためにService DNS名を使用します。
2. 各マイクロサービスをDeploymentとしてデプロイします。Ingressを使用してクラスタ内のDeploymentを公開し、Ingress IPアドレスを使用してクラスタ内の他のマイクロサービスからDeploymentをアドレス指定します。
3. 各マイクロサービスをPodとしてデプロイします。クラスタ内のPodをServiceを使用して公開し、クラスタ内の他のマイクロサービスからマイクロサービスをアドレス指定するためにService DNS名を使用します。
4. 各マイクロサービスをPodとしてデプロイします。Ingressを使用してクラスタ内のPodを公開し、Ingress IPアドレスを使用してクラスタ内の他のマイクロサービスからPodをアドレス指定します。
<details><div>
    答え：1
説明
不正解
B. 
Ingressは主に、Kubernetesクラスタ内のサービスに対する外部アクセス（通常はHTTP）を管理するために使用されます。クラスタ内のマイクロサービス間の内部通信には必要ありません。
C. 
デプロイメントを使用せずに各マイクロサービスをスタンドアロンのPodとしてデプロイすると、レプリカを簡単に管理したりマイクロサービスをスケールしたりすることができません。
D. 
オプションBと同様に、Ingressを使用して単一のPodを公開することは、内部通信のための適切なソリューションではなく、レプリカを管理するための要件に対処できません。
正解
A. 
Kubernetesクラスタでは、マイクロサービスのレプリカを管理し、内部で公開する適切な方法は、Serviceと一緒にDeploymentを使用することです。Deploymentは、同一のPodのセットに対して必要な数のレプリカを管理し、指定された数のレプリカが常に実行されるようにします。Serviceは、クラスタ内でこれらのデプロイメントを公開する方法を提供し、アプリケーションの異なる部分間の通信を可能にします。
リンク
Kubernetesデプロイメント
Kubernetesサービス
</div></details>

## Q. 5-43
あなたのチームは、Google Kubernetes Engine（GKE）で実行するアプリケーションのビルドパイプラインをセットアップしています。セキュリティ上の理由から、パイプラインによって生成されたイメージのみをGKEクラスタにデプロイしたいと考えています。

Google Cloudサービスのどの組み合わせを使うべきですか？
1. Cloud Build、クラウドストレージ、バイナリ認証
2. Googleクラウドデプロイ、クラウドストレージ、Googleクラウドアーマー
3. Googleクラウド・デプロイ、アーティファクト・レジストリ、Googleクラウド・アーマー
4. Cloud Build、アーティファクト・レジストリ、バイナリ認証
<details><div>
    答え：4
説明
不正解
A. 
B. 
C. 
これらのオプションは、シナリオに記述されている要件にあまり一致しません。たとえば、Cloud Storageは一般的にこのコンテキストではコンテナ・イメージ・ストレージには使用されませんし、Google Cloud Armorはイメージの認証よりもネットワークとアプリケーション層のセキュリティに重点を置いています。
正解
D. 
各サービスの理由は次のとおりです：
Cloud Build： Cloud Build：このサービスは、ソースコードをコンパイル、ビルド、パッケージ化し、CI/CDパイプラインの一部としてコンテナイメージを作成するために使用されます。
アーティファクト・レジストリ： これはスケーラブルで管理されたサービスで、コンテナ・イメージの安全で便利なストレージを提供する。Cloud Buildによって生成されたイメージを保管するために使用される。
バイナリ認証： これはGKEのセキュリティ機能で、定義したポリシーに対して検証された信頼できるコンテナイメージのみがクラスタにデプロイされるようにします。
リンク
https://cloud.google.com/architecture/app-development-and-delivery-with-cloud-code-gcb-cd-and-gke#objectives
</div></details>

## Q. 5-44
Google Kubernetes Engineに新しいアプリケーションをデプロイし、パフォーマンスの低下が発生しています。ログは Cloud
Logging に書き込まれ、メトリクスを取得するために Prometheus サイドカーモデルを使用しています。コストを最小限に抑えながら、パフォーマンス問題をトラブルシューティングし、リアルタイムのアラートを送信するために、メトリクスとログからのデータを関連付ける必要があります。

どうすればよいでしょうか？
1. Cloud Loggingログからカスタムメトリクスを作成し、Prometheusを使用してCloud Monitoring REST APIを使用して結果をインポートします。
2. Cloud LoggingのログとPrometheusのメトリクスをCloud Bigtableにエクスポートします。クエリを実行して結果を結合し、Google Data Studioで分析する。
3. Cloud Loggingログをエクスポートし、PrometheusメトリクスをBigQueryにストリーミングします。定期的なクエリを実行して結果を結合し、Cloud Tasks を使用して通知を送信します。
4. Prometheus メトリクスをエクスポートし、Cloud Monitoring を使用して外部メトリクスとして表示します。ログからログベースのメトリクスを作成し、Prometheusデータと関連付けるようにCloud Monitoringを構成します。
<details><div>
    答え：4
説明
不正解
A. 
Cloud Loggingログからカスタムメトリクスを作成し、Prometheusを使用して結果をインポートすることは可能ですが、最も効率的で費用対効果の高い方法ではないかもしれません。また、Cloud Monitoring REST APIを直接使用すると、より複雑になる可能性があります。
B. 
ログとPrometheusメトリクスをCloud Bigtableにエクスポートし、Google Data Studioで分析するのは、状況を複雑にしすぎているかもしれません。Cloud Bigtableは、高スループットでスケーラブルなNoSQLストレージに最適化されており、このユースケースには最適ではないかもしれません。
C. 
ログをBigQueryにエクスポートし、PrometheusのメトリクスをBigQueryにストリーミングすることで、データの相関を取ることができますが、Cloud Tasksを使用して定期的なクエリと通知をセットアップするのは複雑で、リアルタイムのアラートには最適なソリューションではないかもしれません。
正解
D. 
オプションDは、メトリクスとログを分析するための組み込みツールを提供し、PrometheusメトリクスとCloud Loggingログを含む異なるソースからのデータを関連付けることができるCloud Monitoringを活用します。このアプローチは、パフォーマンス問題をトラブルシューティングするための、より簡単で費用対効果の高い方法を提供します。
リンク
ログのモニタリングデータで GKE をより迅速にトラブルシューティング｜Google Cloud Blog
https://cloud.google.com/stackdriver/docs/solutions/gke/prometheus#viewing_metrics
</div></details>

## Q. 5-45
Cloud Build buildを使ってDockerイメージをDevelopment環境、Test環境、Production環境にプロモートしようとしています。同じDockerイメージがそれぞれの環境にデプロイされるようにする必要があります。

ビルドのDockerイメージはどのように識別すべきですか？
1. 最新のDockerイメージタグを使用してください。
2. ユニークなDockerイメージ名を使う。
3. Dockerイメージのダイジェストを使用します。
4. Dockerイメージタグのセマンティックバージョンを使う。
<details><div>
    答え：3
説明
不正解
A. 
latest」Dockerイメージタグを使用すると、「latest」タグが異なる時点で異なるイメージを参照する可能性があるため、矛盾が生じる可能性があります。
B. 
一意なDockerイメージ名を使っても、異なるバージョンに一意な名前が付けられる可能性があるため、環境間で同じイメージが使われるとは限りません。
D. 
Dockerイメージタグのセマンティックバージョンを使用することは、オプションAやBよりも優れていますが、タグが異なるイメージに移動され、矛盾が生じる可能性があります。
正しい答え
C. 
ダイジェストは特定のイメージに対する一意の識別子であり、タグやその他の命名規則に関係なく、まったく同じイメージが使用されることを保証します。したがって、このシナリオではオプションCが最も信頼できる選択です。
リンク
https://cloud.google.com/architecture/using-container-images
</div></details>

## Q. 5-46
Pythonで書かれたアプリケーションがCloud Run上で本番稼動しています。アプリケーションは、同じプロジェクトの Cloud Storage バケットに保存されたデータを読み書きする必要があります。最小特権の原則に従ってアプリケーションにアクセスを許可したい。

どうすればいいでしょうか？
1. カスタムIAM（Identity and Access Management）ロールを持つユーザー管理サービスアカウントを作成します。
2. Storage Admin Identity and Access Management (IAM)ロールを持つユーザー管理サービスアカウントを作成する。
3. Project Editor ID and Access Management（IAM）ロールを持つユーザー管理サービスアカウントを作成する。
4. 本番環境でクラウド実行リビジョンにリンクされているデフォルトのサービスアカウントを使用する。
<details><div>
    答え：1
説明
不正解
B. 
Storage Admin IAMロールを付与すると、オブジェクトとバケットを完全に制御するなど、必要以上の権限が付与されます。
C. 
Project Editor IAMロールを付与すると、プロジェクト内のすべてのリソースへの編集アクセスなど、必要以上の権限も付与されます。
D. 
Cloud Runリビジョンにリンクされたデフォルトのサービスアカウントを使用すると、特定のタスクに必要な権限がない可能性があります。
正解
A. 
最小特権の原則に従い、そのタスクに必要な権限のみを付与する。この場合、タスクはCloud Storageバケットへのデータの読み書きです。
オプションAの "カスタムIAM（Identity and Access Management）ロールを持つユーザー管理サービスアカウントを作成する "では、アプリケーションがCloud Storageバケットへのデータの読み書きに必要な特定の権限だけを持つロールを作成することができます。
リンク
https://cloud.google.com/iam/docs/understanding-roles#storage.admin
</div></details>

## Q. 5-47
シングルプレーヤーのモバイルゲームバックエンドを開発中で、昼夜を問わずユーザーがゲームに参加するため、予測不可能なトラフィックパターンが発生します。

リクエストを処理するのに十分なリソースを確保しつつ、過剰なプロビジョニングを最小限に抑えることで、コストを最適化したいと考えています。また、トラフィックの急増を効率的に処理できるシステムも必要です。

どのコンピュート・プラットフォームを使うべきでしょうか？
1. Cloud Run
2. マネージド・インスタンス・グループを備えたコンピュート・エンジン
3. アンマネージド・インスタンス・グループによるコンピュート・エンジン
4. クラスタオートスケーリングを使用するGoogle Kubernetes Engine
<details><div>
    答え：1
説明
不正解
B. 
マネージドインスタンスグループは自動スケーリングを提供できますが、特にトラフィックパターンが非常にバースト的である可能性があるシングルプレーヤーのモバイルゲームバックエンドでは、迅速かつ予測不可能なスケーリング要件に対処する上で効率的でない可能性があります。
C. 
アンマネージド・インスタンス・グループは自動スケーリングを提供しないため、スケーリングを自分で管理する必要があります。これは予測不可能なトラフィックパターンを処理するには理想的ではありません。
D. 
クラスタオートスケーリングを使用したGKEはスケーリング要件に対応できますが、シングルプレイヤーモバイルゲームのバックエンドには過剰かもしれません。また、Cloud Runよりも多くの管理とオーバーヘッドが必要になる可能性があります。
正解
A. 
Cloud Runは完全に管理されたサーバーレスプラットフォームで、リクエスト数に応じて自動的にスケールアップ/スケールダウンします。このため、問題で説明されているような予測不可能で負荷が変化するアプリケーションに最適です。
リンク
https://cloud.google.com/run/docs
</div></details>

## Q. 5-48
Terraformを使ってラップトップからGoogle Cloudにリソースをデプロイする必要がある。

Google Cloud 環境のリソースはサービスアカウントを使って作成する必要があります。あなたの Cloud Identity は roles/iam.serviceAccountTokenCreator Identity and Access Management (IAM) ロールと Terraform を使ってリソースをデプロイするのに必要な権限を持っています。Google が推奨するベストプラクティスに従って、必要なリソースをデプロイするための開発環境をセットアップしたい。

あなたは何をすべきでしょうか？
1. 
2. 
- コマンドラインから次のコマンドを実行します: gcloud config set auth/impersonate_service_account service-account-name@project.iam.gserviceacccount.com.
- GOOGLE_OAUTH_ACCESS_TOKEN環境変数を、gcloud auth print-access-tokenコマンドによって返される値に設定します。
3. 
4. 
<details><div>
    答え：
説明
不正解
オプションA、C、D
オプションAでは、サービスアカウントのキーファイルをダウンロードしてローカルマシンに保存します。この方法は、鍵ファイルが誤って公開される可能性があるため、セキュリティリスクにつながる可能性があります。
オプションCは、サービス・アカウントではなくユーザー・アカウントの認証に依存するため、推奨されない。
オプションDは、HashiCorp Vaultへの鍵の安全な保存を含みますが、Google CloudでTerraformを認証する標準的な方法でも推奨される方法でもありません。
正解
B.
オプションBは正しい方法です。なぜならgcloudコマンドラインツールを使ってサービスアカウントをなりすますため、Terraformは鍵ファイルを直接管理することなくサービスアカウントとして認証することができるからです。この方法はサービスアカウントのなりすましに関するGoogleのベストプラクティスに従い、サービスアカウントの鍵の取り扱いに関連するリスクを軽減します。
リンク
https://cloud.google.com/sdk/gcloud/reference/config/set#impersonate_service_account
https://cloud.google.com/blog/topics/developers-practitioners/using-google-cloud-service-account-impersonation-your-terraform-code
</div></details>

## Q. 5-49
Google Kubernetes Engine（GKE）にインターネットに面したマイクロサービス・アプリケーションをデプロイする必要がある。A/Bテストメソッドを使用して新機能を検証したい。新しいコンテナイメージのリリースをデプロイするために、以下の要件があります：

- 新しいコンテナイメージのデプロイ時にダウンタイムを発生させない。

- 新しい本番リリースは、本番ユーザーのサブセットを使用してテストおよび検証されます。

あなたは何をすべきですか？
1. 
2. 
3. 
- GKEクラスタにAnthos Service Meshをインストールします。
- GKEクラスタ上に2つのデプロイメントを作成し、異なるバージョン名でラベルを付けます。
- 新しいバージョンのアプリケーションを参照するデプロイメントにトラフィックのごく一部を送信する Istio ルーティングルールを実装します。
4. 
<details><div>
    答え：3
説明
不正解
オプションA. このアプローチでは、ダウンタイムなしでアップデートを行うことができるかもしれませんが、A/Bテストのために本番ユーザーのサブセットを使用して新しい本番リリースを検証するという要件には特に対応していません。
オプション B. このアプローチでは、別のネームスペースを作成することで不必要な複雑さが生じ、A/B テストの特定の要件に対処できません。
オプション D. ローリングアップデートパターンは、アップデート中のダウンタイムはありませんが、A/B テストのために本番ユーザーのサブセットを使用して新しいリリースを検証するという要件には対応していません。
正解
C.
このオプションは、Anthos Service Mesh（ASM）とIstioを活用します。ASMとIstioは、まさに質問に記載されているようなユースケースを処理するように設計されています。ASMとIstioは強力なトラフィックルーティング機能を提供し、特定の割合のトラフィックを異なるバージョンのアプリケーションに送ることができます。これはA/Bテストを容易にし、要件を満たします。
リンク
https://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_an_ab_test
</div></details>

## Q. 5-50
ディレクトリlocal-scriptsとそのすべてのコンテンツをローカルワークステーションからCompute Engine仮想マシンインスタンスにコピーする必要があります。

どのコマンドを使用する必要がありますか？
1. 
2. 
3. gcloud compute scp --project "my-gcp-project" --recurse ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone "us-east1-b".
4. 
<details><div>
    答え：3
説明
不正解
オプションAとB：gsutilは、Compute Engineインスタンスにファイルをコピーするためではなく、Cloud Storage上の操作に使用されるため、これらのオプションは正しくありません。
選択肢D：gcloud computeにはmvコマンドがないので、この選択肢は正しくありません。
正解です：
C. 
オプションCは、Compute Engineインスタンスとの安全なコピー操作のために設計されたgcloud compute scpコマンドを使用します。recurseフラグは、ディレクトリの内容が再帰的にコピーされることを保証します。
リンク
gcloud compute copy-files｜Google Cloud CLI ドキュメント
</div></details>

## Q. 5-51
Cloud Source RepositoriesとCloud Buildを使用して、PythonアプリケーションをCloud Runにデプロイしています。Cloud Buildパイプラインを以下に示します：

ステップ
- 名前: python
エントリポイント: pip
args: ["install", "-r", "requirements.txt", "--user"]。
 
-name: 'gcr.io/cloud-builders/docker'
args: ['build', '-t'、
'us-centrall-docker.pkg.dev/$(PROJECT_ID)/$_REPO_NAME}/myimage: $(SHORT_SHA}', '.'].
 
- name: 'gcr.io/cloud-builders/docker'
args: ['push'、
'us-centrall-docker.pkg.dev/${PROJECT_ID}/$(_REPO_NAME}/myimage: $(SHORT_SHA)'].
 
- name: 'google/cloud-sdk'
args: ['gcloud', 'run', 'deploy', 'helloworld-$(SHORT_SHA)'、
'--image-us-centrall-docker.pkg.dev/${PROJECT_ID)/$(_REPO_NAME)/myimage: $(SHORT_SHA)'、
'--region', 'us-centrall', '--platform', 'managed'、
'--allow-unauthenticated']。
デプロイ時間を最適化し、不要な手順を避けたい。

どうすればいいでしょうか？
1. コンテナをArtifact Registryにプッシュするステップを削除します。
2. 新しいDockerレジストリをVPCにデプロイし、VPC内のCloud Buildワーカープールを使用してビルドパイプラインを実行します。
3. Cloud Runインスタンスと同じリージョンのCloud Storageバケットにイメージアーティファクトを保存します。
4. ビルド設定ファイルのDockerビルドステップに-cache-from引数を追加します。
<details><div>
    答え：4
説明
不正解
A. 
コンテナを Artifact Registry にプッシュするステップを削除すると、コンテナがデプロイに使用できなくなるため、これは正しい選択肢ではありません。
B. 
新しいDockerレジストリをVPC内にデプロイすると、複雑さが増す可能性があり、必ずしもビルド時間が短縮されるとは限りません。
C. 
イメージアーティファクトをCloud Storageバケットに保存することは、このユースケースでは不要であり、デプロイメント時間に大きな影響を与える可能性は低い。イメージは通常、Cloud Storageのような汎用オブジェクトストレージサービスではなく、Artifact RegistryやContainer Registryのようなコンテナレジストリに格納する必要があります。
正解
D. 
ビルドパイプラインでデプロイメント時間を最適化する場合、Dockerのキャッシュメカニズムを利用して、変更のないレイヤーの再構築を避けるという方法があります。-cache-from引数を使用すると、docker buildコマンドを実行する際にキャッシュソースとして使用するDockerイメージを指定できます。
リンク
https://cloud.google.com/build/docs/optimize-builds/speeding-up-builds#using_a_cached_docker_image
</div></details>

## Q. 5-52
あなたのチームは、あるアプリケーションが稼働しているGoogle Kubernetes Engine（GKE）クラスタを管理しています。別のチームがこのアプリケーションとの統合を計画しています。彼らが統合を開始する前に、他のチームがあなたのアプリケーションに変更を加えることはできませんが、GKE上に統合をデプロイできることを確認する必要があります。

あなたは何をすべきでしょうか?
1. Identity and Access Management（IAM）を使用して、クラスタ・プロジェクトのViewer IAMロールを他のチームに付与します。
2. 新しいGKEクラスタを作成します。Identity and Access Management (IAM) を使用して、クラスタプロジェクトの Editor ロールを他のチームに付与します。
3. 既存のクラスタに新しいネームスペースを作成します。Identity and Access Management (IAM) を使用して、クラスタプロジェクトの Editor ロールを他のチームに付与します。
4. 既存のクラスタに新しいネームスペースを作成します。Kubernetesのロールベースのアクセス制御（RBAC）を使用して、新しいネームスペースのAdminロールを他のチームに付与します。
<details><div>
    答え：4
説明
不正解
A. 
Viewer IAMロールを付与すると、読み取り専用アクセスしか許可されず、統合を展開するのに十分ではありません。
B. 
新しい GKE クラスタを作成し、Editor ロールを付与すると、他のチームがプロジェクト全体に変更を加えることができるようになり、既存のアプリケーションに影響を与える可能性がある。
C. 
クラスタプロジェクト全体にEditorロールを付与すると、必要以上の権限が付与され、既存のアプリケーションへの変更が可能になる可能性があります。
正解
D. 
このシナリオでは、他のチームがGKEクラスタに統合をデプロイすることを許可したいが、既存のアプリケーションに変更を加えることができないようにしたい。これを行う最善の方法は、既存のクラスタに新しいネームスペースを作成し、Kubernetes Role-Based Access Control（RBAC）を使用して、そのネームスペース内で他のチームに特定の権限を付与することです。
オプションDが正しい選択肢です：
こうすることで、他のチームに特定のネームスペースへの管理アクセス権を与えることができ、他のネームスペース内の既存のアプリケーションに影響を与えることなく、そのネームスペース内のリソースをデプロイおよび管理できるようになります。
リンク
https://cloud.google.com/kubernetes-engine/docs/concepts/access-control#rbac
https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
</div></details>

## Q. 5-53
あなたのサービスが、クラウドストレージから読み込んだ画像にテキストを追加しています。混雑する時期に、クラウドストレージへのリクエストがHTTP 429 "Too Many Requests "ステータスコードで失敗します。

このエラーにどのように対処すべきですか？
1. オブジェクトにcache-controlヘッダーを追加します。
2. GCPコンソールからクォータ増加を要求する。
3. 切り捨て指数バックオフ戦略でリクエストを再試行する。
4. Cloud StorageバケットのストレージクラスをMulti-regionalに変更する。
<details><div>
    答え：3
説明
不正解
A. 
オブジェクトにcache-controlヘッダーを追加しても、クラウド・ストレージへのリクエストが多すぎるという問題は軽減されません。
B. 
クォータの増加を要求することは一時的な解決策になるかもしれないが、サービスが連続して要求を出し続ける場合、根本的な問題を解決することはできない。
D. 
クラウドストレージバケットのストレージクラスを変更しても、リクエストレートの制限には影響しません。
正解
C. 
このストラテジーは、サービスが最初に少し待ってからリクエストを再試行することを意味します。再試行が失敗した場合、サービスは次の試行までにさらに長い時間待ちます。この方法によって、サービスはリクエストを緩和することができ、レート制限の問題を軽減することができる。
リンク
https://cloud.google.com/storage/docs/json_api/v1/status-codes#429_Too_Many_Requests
</div></details>

## Q. 5-54
貴社は、人気のアプリケーションのユーザーを米国外に拡大したいと考えています。同社は、アプリケーション用のデータベースの99.999%の可用性を確保し、世界中のユーザーの読み取り待ち時間を最小化したいと考えています。

どの2つのアクションを取るべきでしょうか？(2つの選択肢を選んでください)
1. nam-asia-eur1」構成でマルチリージョンのCloud Spannerインスタンスを作成します。
2. nam3」構成でマルチリージョンのCloud Spannerインスタンスを作成します。
3. 少なくとも3つのSpannerノードでクラスタを作成します。
4. 少なくとも1つのSpannerノードでクラスタを作成します。
5. 少なくとも1つのノードで、別々のリージョンに最低2つのCloud Spannerインスタンスを作成する。
6. 異なるデータベース間でデータをレプリケートするためにCloud Dataflowパイプラインを作成する。
<details><div>
    答え：1,3
説明
不正解
B. 
nam3」構成は北米にしか及ばないため、グローバルユーザーの読み取り待ち時間を最小限に抑えることはできません。
D. 
Spannerノードが1つしかないと、予想される負荷を処理し、望ましい可用性を達成するのに十分でない可能性があります。
E. 
別々のインスタンスを2つ作成すると、レプリケーションを手動で管理する必要がありますが、Cloud Spannerは複数リージョン構成ですでに処理しています。
F. 
Cloud Dataflowは、このシナリオでは異なるデータベース間でデータを複製するようには設計されていない。それは、データベースの複製ではなく、データの変換と処理タスクに使用されます。
正解
A. 
nam-asia-eur1 "のようなマルチリージョン構成は、北米、アジア、およびヨーロッパにまたがり、これらの地域のユーザーのためのグローバルな配布と低い読み取りレイテンシを保証します。
C. 
Spannerクラスタに少なくとも3つのノードを持つことで、より高い可用性を提供し、より多くのクエリと読み取り/書き込み操作を処理できます。
リンク
https://cloud.google.com/spanner/docs/instances
https://cloud.google.com/spanner/docs/latency
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

## Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>
