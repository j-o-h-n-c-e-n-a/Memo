## 1
### Q. 問題1: 未回答
Google Kubernetes Engine(GKE)上で動作する本番環境グレードの Node.js アプリケーションをサポートしている。アプリケーションは、HTTP 要求を介して相互に対話する複数のマイクロサービスで構成されます。システムが成長するにつれて、マイクロサービス間の通信における潜在的なパフォーマンスのボトルネックを事前に特定する必要があります。
この目標を達成するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答: B. Stackdriver Trace を使用してすべてのマイクロサービスをインストルメント化し、サービス間の HTTP リクエストを分析します。
説明：
Stackdriver Trace には分散トレース機能があり、マイクロサービス間のリクエストのフローを追跡、分析できます。すべてのマイクロサービスを Stackdriver Trace でインストルメント化することで、システムのさまざまなコンポーネントを通過する個々の HTTP リクエストのレイテンシとパフォーマンスに関する分析情報を得ることができます。これは、ボトルネックやパフォーマンスを改善できる領域を特定するのに役立ちます。
オプション A、C、D はアプリケーションの監視とデバッグに使用されますが、マイクロサービス間のパフォーマンスの問題を特定するという目標には特に対応していません。Stackdriver Profiler(オプション A)はアプリケーション内の個々の関数のパフォーマンスをプロファイリングするのに役立ち、Stackdriver Debugger(オプション C)は各アプリケーション内のロジックのデバッグを支援し、リクエスト時間と応答時間のログ記録(オプション D)はデバッグに役立ちますが、マイクロサービス間の相互作用の全体像を把握できない場合があります。
Stackdriver Trace(オプション B)を使用すると、マイクロサービス間の相互作用を幅広く把握し、システム内の潜在的なパフォーマンスのボトルネックを特定できます。
<details><div>

### Q. 問題2: 未回答
あなたは、Google Cloud Platform(GCP)上の複雑なクラウドベースのインフラストラクチャを管理するチームの DevOps リーダーです。チームは、Google Kubernetes Engine(GKE)で実行されている重要なアプリケーションの CPU 使用率など、さまざまな指標をモニタリングするために Stackdriver に大きく依存しています。プロジェクトのワークスペース内のダッシュボードにある Stackdriver の CPU 使用率グラフを他のチーム メンバーと共有したい。これをどのように実現し、円滑なコラボレーションを確保するためにどのような権限を付与する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
D. CPU 使用率の Stackdriver グラフをチームと共有するには、次の操作を行います。
新しい Stackdriver ワークスペースを作成します。
CPU 使用率グラフをワークスペース内の共有ダッシュボードに追加します。
関連するチームメンバーを「閲覧者」のアクセス権でワークスペースに招待します。
ダッシュボードの URL をチームと共有します。
専用のワークスペースと共有ダッシュボードを作成することで、チームメンバーはメインプロジェクトの他のリソースにアクセスすることなく、CPU使用率チャートにアクセスできます。このアプローチは、個々のチーム メンバーに不要なアクセス許可を付与することなく、重要なメトリックの監視を共同で行うための安全で効率的な方法を提供します。
このシナリオで他のオプションが最も適切な選択肢ではない理由:
オプションAでは、「閲覧者」アクセス権を持つカスタムダッシュボードを共有します。これは、メトリックの特定のビューを共有するために実現できる場合がありますが、ダッシュボードをプロジェクト内の残りのリソースから分離するものではありません。つまり、カスタムダッシュボードにアクセスできるチームメンバーは、プロジェクトの他の部分も閲覧できる可能性があり、セキュリティやプライバシー上の理由から望ましくない可能性があります。
オプション B では、エクスポートした画像やグラフの PDF を共有することで、Stackdriver に直接アクセスすることなく、チーム メンバーにグラフをすばやく表示することができます。ただし、このアプローチには制限があります。これは静止画像であり、チーム メンバーはグラフを操作したり、データをさらに探索してより深い洞察を得たりすることはできません。また、手動でエクスポートして共有する必要があるため、更新が頻繁に行われる場合は面倒な場合があります。
オプション C では、監視のみを目的として新しいプロジェクトを作成し、チーム メンバーにそのプロジェクトへの "閲覧者" アクセス権を付与することを提案します。これにより、特定のリソースへのアクセスが制限される可能性がありますが、重大な欠点もあります。監視用に新しいプロジェクトを作成すると、データの重複、管理オーバーヘッドの増加、および監視プロジェクトとメインプロジェクト間の不整合が生じる可能性があります。特定のメトリック チャートを共有するのは、最も効率的な方法ではありません。
結論として、オプション D は、Stackdriver 内に専用のワークスペースと共有ダッシュボードを提供するため、最も適切な選択肢です。ワークスペースに "閲覧者" アクセス権を持つチーム メンバーを招待することで、無関係なリソースを公開したり、不必要な複雑さを引き起こしたりすることなく、CPU 使用率チャートに安全にアクセスできます。共有ダッシュボードにより、コラボレーションが可能になり、チームメンバーがグラフを操作し、データを効果的に探索できるようになります。
<details><div>

### Q. 問題3: 未回答
組織内のサイト信頼性エンジニアリング (SRE) 文化の一環として、クラウド プラットフォームで実行されている重要なサービスを管理します。残念ながら、このサービスは最近大幅な停止を経験し、ユーザーのエクスペリエンスに影響を与えました。インシデントは解決されましたが、インシデント対応チームは、何が起こったのか、および今後同様のインシデントを防ぐ方法について正式な説明を提供したいと考えています。上司から、このインシデントの事後分析の準備を依頼されます。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: A. 根本原因、解決策、学んだ教訓、およびアクションアイテムの優先順位付けされたリストを含む事後分析を作成します。マネージャーとのみ共有します。
説明： オプション A は、インシデント対応プロセスの重要な側面をカバーする包括的な事後分析レポートを作成し、それをマネージャーとのみ共有するため、最も適切なアプローチです。これが正しい選択である理由の内訳は次のとおりです。
包括的な事後分析レポート:オプションAは、以下を含む詳細な事後分析レポートの作成に重点を置いています。
根本原因: これは、インシデントの背後にある根本的な理由を特定し、的を絞った修復を可能にするのに役立ちます。
解決策: インシデントがどのように解決されたかを説明することで、実行された対応アクションが後で参照できるように文書化されます。
教訓: 学んだ教訓を文書化することで、チームが今後同様のインシデントを防ぐための貴重な洞察を得ることができます。
優先順位付けされたアクションアイテム: アクションアイテムのリストは、最も重要な改善に焦点を当てるのに役立ち、インシデント対応プロセスをより効果的にします。
マネージャとのみ共有:事後分析レポートをマネージャーと共有することで、重要な情報が意思決定とリソース配分を担当する適切なリーダーシップレベルに確実に届くようになります。これにより、インシデント対応に直接関与しない可能性のある他のチーム メンバーに不必要なパニックや懸念が生じることを回避できます。
他のオプションが正しくない理由:
B. 根本原因、解決策、教訓、およびアクションアイテムの優先順位付けされたリストを含む事後分析を作成します。エンジニアリング組織のドキュメントポータルで共有します。
説明: エンジニアリング組織のドキュメント ポータルで事後分析を共有するという意図は、透明性と学習に沿っている可能性がありますが、このシナリオでは最適なアプローチではありません。公開ポータルでインシデントの詳細を共有すると、機密情報やセキュリティの脆弱性が権限のない担当者に公開される可能性があります。インシデントの事後分析は、潜在的なセキュリティリスクを回避するために関連チームに制限する必要がある機密文書と見なされることがよくあります。
C. 根本原因、解決策、教訓、責任者のリスト、各担当者のアクションアイテムのリストを含む事後分析を作成します。マネージャーとのみ共有します。
説明: このオプションは、インシデントに関与した個人に責任を帰属させることに重点を置いており、サイト信頼性エンジニアリングで奨励されている非難の余地のない文化に逆効果になる可能性があります。SRE は、チームとしてインシデントから学び、非難を避けることを重視しています。責任者をリストアップした事後分析を共有すると、インシデントのオープンで正直な報告が妨げられ、継続的な改善の文化が妨げられる可能性があります。
D. 根本原因、解決策、学んだ教訓、責任者のリスト、各人のアクションアイテムのリストを含む事後分析を作成します。エンジニアリング組織のドキュメントポータルで共有します。
説明: オプション B と同様に、インシデントの責任者に関する詳細情報を公開ドキュメント ポータルで共有することはお勧めできません。それは不必要な緊張につながり、オープンな報告を思いとどまらせ、事件から学ぶ文化ではなく恐怖の文化を生み出す可能性があります。
結論として、オプションAは、インシデントの詳細を包括的に報告することと、それを適切な利害関係者、特にマネージャーと共有することの適切なバランスをとっています。これにより、セキュリティを損なったり、非難指向の文化を助長したりすることなく、インシデントに適切に対処し、学習をキャプチャし、必要なアクション項目に優先順位を付けることができます。
<details><div>

### Q. 問題4: 未回答
複雑なマイクロサービスベースのアプリケーションを Google Kubernetes Engine(GKE)で管理している。サービスの 1 つは、重要なログを生成するサードパーティ コンポーネントです。モニタリングと分析を簡素化するために、これらのログを Google Cloud Stackdriver Logging に一元化します。
このログ集約ソリューションを実現するための最も効果的なアプローチは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
説明: A. オプション A は、Stackdriver Logging でログを一元管理するための最適な方法です。Fluentd は、GKE に DaemonSet としてデプロイできる一般的なログ転送ツールであり、各ノードがサードパーティ コンポーネントからログを収集できるようにします。Fluentd は、これらのログを Stackdriver Logging に効率的に転送し、リアルタイムの一元的なモニタリングと分析機能を提供します。
他のオプションが正しくない理由
B. オプション B では、Kubernetes CronJob を使用して定期的にログをスクレイピングしますが、これはリアルタイム分析には最適ではなく、ログの取り込みに遅延が生じる可能性があります。さらに、このアプローチでは追加のストレージ管理が必要になる場合があり、ログが定期的にクリーンアップされないとデータが重複する可能性があります。
C. オプション C では、Stackdriver Logging エージェントをサードパーティ コンポーネントをホストしているサーバーに直接インストールすることを提案しています。これは有効なアプローチですが、DaemonSet がすべてのノードにわたるログの収集と転送をより効果的に管理できる GKE のようなコンテナ化された環境には理想的ではない可能性があります。
D. オプションDは、リアルタイムのログ転送用のカスタムKubernetesコントローラーの開発を提案しています。これは技術的には実現可能ですが、このような目的のために設計された専用のログ転送ツールであるFluentdを使用する場合と比較して、かなりの複雑さとメンテナンスのオーバーヘッドが発生します。
結論として、オプション A は、サードパーティ コンポーネントのログを Google Cloud Stackdriver Logging に一元化し、モニタリングと分析をより合理化して効率的にするための最も効率的で簡単なアプローチです。
<details><div>

### Q. 問題5: 未回答
Stackdriver Logging エージェントがインストールされたカスタム Debian イメージを使用してアプリケーションを実行する仮想マシン(VM)を管理している。VM には、Google Cloud サービスとやり取りするために必要な「クラウド プラットフォーム」スコープがあります。アプリケーションはsyslogを介して情報を記録しています。ただし、Google Cloud Platform Console に移動してログ ビューアにアクセスすると、syslog ログが [すべてのログ] ドロップダウン リストに表示されないことがわかります。
この問題をトラブルシューティングするための最初の行動方針は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. VM 上の Stackdriver エージェントのステータスと構成にエラーや問題がないか確認します。
説明：Logs Viewer の [All logs] プルダウン リストに syslog ログが表示されない問題をトラブルシューティングするための最も適切な初期アクションは、VM 上の Stackdriver エージェントのステータスと構成にエラーや問題がないか確認することです。Stackdriver Logging エージェントは、ログを収集して Google Cloud Stackdriver Logging に転送する役割を担います。エージェントにエラーや設定ミスがあると、ログが Stackdriver に送信されない可能性があります。
エージェントのステータスと構成を確認することで、その機能に問題があるかどうか、またはログ収集に影響を与える可能性のあるログフィルタリングの問題があるかどうかをすばやく特定できます。
オプション B は、VM のファイアウォール規則が問題の原因である可能性は低いため、最初のアクションではありません。VM には必要な「クラウド プラットフォーム」スコープがあり、Stackdriver Logging などの Google Cloud サービスとやり取りできます。そのため、問題は Stackdriver エージェント自体にある可能性が高くなります。
オプションCは、syslogログがログビューアに表示されない問題とは直接関係ありません。「monitoring.write」スコープは、通常、Stackdriver Logging ではなく、Google Cloud Monitoring で指標と時系列データをモニタリングするために必要です。
オプション D では、コマンドを実行して、Stackdriver エージェント(fluentd)が VM で実行されているかどうかを確認することを提案しています。これはエージェントのステータスを判断するために関連している可能性がありますが、エージェントのステータスと構成の両方を確認するオプションAは、ログ収集に関する潜在的な問題を特定するためのより包括的なアプローチです。
結論として、最も適切な最初のアクションは、VM 上の Stackdriver エージェントのステータスと構成を確認して、その適切な機能を確認し、ログの収集と Stackdriver Logging への転送に影響を与える可能性のある潜在的なエラーや構成ミスに対処することです。
VM で Stackdriver Logging エージェントのステータスと構成を確認する方法:
アプリケーションが実行されている VM に SSH で接続します。
コマンドを実行して、Stackdriver Logging エージェントが実行されていてアクティブかどうかを確認します。sudo systemctl status google-fluentd
構成ファイルを確認して、syslog ログを収集するように正しく構成されていることを確認します。/etc/google-fluentd/config.d/syslog.conf
<details><div>

### Q. 問題6: 未回答
あなたは、Google Cloud Platform(GCP)サービスを扱うDevOpsエンジニアです。チームは Google Cloud Build を使用して、アプリケーションのビルド プロセスを自動化します。プロセスの一環として、成功したビルドと失敗したビルドの両方を含む、各アプリケーションビルドに関する詳細情報を、HTTP Postリクエストを介してサードパーティの監視システムに送信する必要があります。
この統合を効果的に実現するには、どのアプローチが必要ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
説明：各アプリケーションのビルドに関する詳細情報(成功したビルドと失敗したビルドの両方を含む)を HTTP Post リクエストを介して Google Cloud Build からサードパーティのモニタリング システムに送信する正しい方法は、オプション B です。
オプションBの説明:
Google Cloud Pub/Sub: Google Cloud Pub/Sub トピックにビルド情報を書き込むように Cloud Build パイプラインを構成することで、すべてのビルドの詳細がキャプチャされ、利用できるようになります。
Cloud Functions の関数のサブスクライバー: Cloud Functions の関数は、Pub/Sub トピックのサブスクライバーとして機能します。ビルド情報をリアルタイムで処理し、成功したビルドと失敗したビルドの両方を即座に処理できます。
HTTP Post リクエスト: Cloud Functions の関数は、ビルド情報を受信すると、HTTP Post リクエスト ペイロードを作成し、そのデータをサードパーティのモニタリング システムの API エンドポイントに転送します。これにより、ビルドの結果に関係なく、詳細なビルド情報が監視システムに送信され、分析と視覚化が行われます。
オプションAの説明:オプション A では、Cloud Build トリガーとカスタム ビルド ステップを使用して、ビルド情報を収集し、HTTP Post リクエスト ペイロードを構築できます。このアプローチは機能する可能性がありますが、より多くの手動構成が必要であり、オプション B のようにビルド情報のリアルタイム処理を処理できない場合があります。
オプションCの説明:オプション C では、Cloud Build の YAML 構成を変更して、HTTP Post リクエストを使用してサードパーティのモニタリング システムの API エンドポイントにビルド情報を直接送信するビルド後のステップを含めることを提案しています。ただし、リアルタイム処理がどのように実現されるかについては詳しく説明していないため、オプションBよりも効率が悪くなります。
オプションDの説明:オプション D では、サードパーティのモニタリング システムの API 認証情報を Cloud Build 構成に直接統合することを提案しています。ビルドの成功には対応できますが、失敗したビルドの処理には対応しておらず、オプション B で提供されるリアルタイム処理機能も提供されません。
結論として、オプション B は、Google Cloud Pub/Sub を活用してビルド情報をキャプチャし、Cloud Functions の関数を使用してリアルタイムで処理してから、HTTP Post リクエストを介してサードパーティのモニタリング システムの API エンドポイントに送信するため、適切かつ最適なアプローチです。
<details><div>

### Q. 問題7: 未回答
継続的デリバリー プラットフォームである Spinnaker を使用して Web アプリケーションをデプロイしています。アプリケーションは分散データベースを利用し、パフォーマンスを向上させるためにメモリ内キャッシュを維持します。デプロイ プロセスの一環として、Spinnaker パイプラインにカナリア デプロイ ステージを実装して、完全なロールアウトの前にアプリケーションの新しいバージョンをテストしました。カナリア分析を有意義で正確なものにするには、適切に設定する必要があります。
カナリア分析を設定する際に考慮すべきことは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. カナリア バージョンを、以前の製品バージョンのスライディング ウィンドウの平均パフォーマンスと比較します。
説明：カナリアデプロイでは、カナリアバージョンと呼ばれるアプリケーションの新しいバージョンが、ユーザーまたはインスタンスのサブセットに徐々にロールアウトされ、すべてのユーザーが使用できるようにする前にパフォーマンスがテストされます。カナリアバージョンと本番バージョンの比較を自動化するために、Spinnakerはカナリア分析と呼ばれる手法を使用しています。
このシナリオでは、Web アプリケーションは分散データベースとメモリ内キャッシュを利用してパフォーマンスを向上させます。カナリア分析を設定するときは、有意義で正確な比較を行うために、以前の本番バージョンの履歴パフォーマンスを考慮することが不可欠です。
オプション C は、カナリア バージョンを以前の製品バージョンのスライディング ウィンドウの平均パフォーマンスと比較する必要があるため、適切な選択です。カナリア分析では、履歴データのスライディングウィンドウを使用して、時間の経過に伴う運用バージョンと比較したカナリアバージョンのパフォーマンスの回帰、異常、または改善を検出できます。このアプローチにより、カナリア分析は実際のシナリオを反映し、カナリアバージョンをより広範なユーザーベースに宣伝する前に貴重な洞察を提供することができます。
オプション A は、カナリア バージョンを現在の運用バージョンの新しいデプロイと比較することを提案しますが、意味のある比較に十分な履歴データを提供しない可能性があります。現在の製品バージョンの新しいデプロイでは、アプリケーションの実際のパフォーマンス特性を示すのに十分な時間がない可能性があります。
オプション B では、カナリア バージョンを以前の運用バージョンの新しいデプロイと比較します。このアプローチでは、一部の履歴データが考慮されますが、以前のバージョンの複数のバージョンで導入された変更や改善がキャプチャされない可能性があり、重要なパフォーマンス情報が欠落する可能性があります。
オプション D では、カナリア バージョンを現在の製品バージョンの既存のデプロイと比較することを提案します。これにより、いくつかの分析情報が得られる場合がありますが、包括的なカナリア分析に不可欠な、以前のバージョンで導入されたパフォーマンスの変動は考慮されていません。
結論として、カナリアバージョンを以前の製品バージョンのスライディングウィンドウの平均パフォーマンスと比較するオプションCは、アプリケーションの分散データベースとメモリ内キャッシュの使用状況を考慮して、Spinnaker パイプラインでカナリア分析を構成するのに最適なアプローチです。
<details><div>

### Q. 問題8: 未回答
あなたは、Google Cloud Platform(GCP)でホストされているクラウドネイティブ アプリケーションに取り組んでいる DevOps エンジニアです。チームは、継続的デリバリープラットフォームであるSpinnakerを利用して、デプロイを効率的に管理します。アプリケーションはマイクロサービスで構成されており、各サービスはSpinnaker パイプラインを使用して個別にデプロイされます。CI/CD プロセスの一環として、カナリア分析が特定の基準を満たしている場合にのみ、新しいバージョンのアプリケーションを本番環境にデプロイできる戦略を実装する必要があります。
これを実現するには、何を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. パイプライン トリガー
説明：カナリア分析が特定の基準を満たしている場合にのみ、新しいバージョンのアプリケーションを本番環境にデプロイする特定のシナリオでは、Spinnakerで「パイプライントリガー」を使用する必要があります。
パイプライン トリガーを使用すると、定義済みの条件またはイベントに基づいてパイプラインの実行を自動的に開始できます。このコンテキストでは、パイプライントリガーを設定して、カナリア分析の結果を評価し、特定の条件が満たされた場合にのみ本番環境へのデプロイをトリガーできます。
このシナリオでのパイプライン トリガーのしくみを次に示します。
カナリア分析:カナリア展開段階の後、Spinnakerはカナリア分析を実行し、通常はパフォーマンスメトリクスと成功基準を使用して、アプリケーションの新しいバージョンを既存の製品バージョンと比較します。
評価基準: カナリア分析内で、カナリアデプロイが成功したと見なされるために満たす必要がある特定の成功基準を定義できます。これらの基準には、パフォーマンス メトリック、エラー率、待機時間、またはその他の関連指標のしきい値が含まれる場合があります。
パイプライントリガー:カナリア分析が完了すると、Spinnakerは定義された成功基準に基づいて結果を評価します。Canary バージョンが特定の成功基準を満たすと、パイプライントリガーがアクティブ化されます。
本番デプロイ: パイプライントリガーは、事前定義された成功基準でカナリア分析に合格したため、本番環境へのデプロイを開始し、カナリアバージョンをより広範なユーザーベースに昇格させます。
オプション A (手動判定ステージ) では、パイプラインの特定のステージで手動で承認または拒否することができ、カナリア分析条件に基づく自動デプロイとは直接関係ありません。
オプション B (Webhook ステージ) では、受信 Webhook を介して外部からパイプラインをトリガーできますが、説明されているカナリア分析シナリオには直接関連付けられていません。
オプション D (ロールバック ステージ) は、特定の条件が満たされたときに以前のデプロイに自動的にロールバックするために使用されますが、カナリア分析の成功基準に基づいて運用環境への昇格を制御するこのシナリオの主な焦点ではありません。
結論として、特定のシナリオでは、Spinnaker の「パイプライン トリガー」を使用して、カナリア分析が分析プロセス中に定義された特定の基準を満たしている場合にのみ、新しいバージョンのアプリケーションを本番環境に自動的にデプロイする必要があります。
<details><div>

### Q. 問題9: 未回答
あなたは、Google Cloud Platform(GCP)でホストされているウェブアプリケーションのパフォーマンスの監視と最適化を担当するDevOpsエンジニアです。作業の一環として、Web アプリケーションのホーム ページ要求の待機時間を表すサービス レベル インジケーター (SLI) を実装する必要があります。ビジネス要件は、最適なユーザーエクスペリエンスを得るために、許容可能なページ読み込み時間を 200 ミリ秒に維持することです。
ホームページのリクエストのレイテンシーを測定および追跡するために、SLIをどのように実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B. Google Cloud Monitoring を使用して、ホームページ リクエストのカスタム レイテンシ メトリックを設定します。応答時間をキャプチャし、200 ミリ秒の平均レイテンシー目標に基づいて SLI を定義するようにメトリックを構成します。
説明：特定のシナリオで、Web アプリケーションのホーム ページ要求の待機時間にサービス レベル インジケーター (SLI) を実装し、許容可能なページ読み込み時間を 200 ミリ秒に維持するには、オプション B が最も適切なアプローチです。
オプションBが正しい選択である理由は次のとおりです。
Google Cloud Monitoring: Google Cloud Monitoring(旧称 Stackdriver Monitoring)は、GCP が提供する強力なモニタリング ソリューションです。これにより、Web アプリケーションを含むさまざまなリソースからメトリックを収集、表示、分析できます。
カスタム レイテンシ メトリック: Google Cloud Monitoring では、ホームページ リクエスト専用のカスタム レイテンシ メトリックを設定できます。このカスタムメトリックは、ホームページリクエストの応答時間をキャプチャします。
SLIの定義: カスタムレイテンシメトリックに基づいて、サービスレベルインジケータ(SLI)を定義して、ホームページリクエストの平均レイテンシを測定できます。ターゲットレイテンシーを 200 ミリ秒に設定することで、SLI が必要に応じて許容可能なページ読み込み時間を表すようになります。
モニタリングとアラート: Google Cloud Monitoring は、堅牢なモニタリングとアラート機能を提供します。SLI を定義することで、ホーム ページ要求の平均待機時間が許容可能な目標である 200 ミリ秒から逸脱した場合に通知するアラートを設定できます。このプロアクティブな監視により、パフォーマンスの問題が発生した場合に迅速なアクションを実行できます。
オプション A (カスタム・モニター・エージェント) では、Web サーバー上にカスタム・モニター・エージェントを手動でセットアップすることを提案しますが、これにより複雑さとオーバーヘッドが増す可能性があります。Google Cloud Monitoring は、指標を監視するためのより合理化された一元化されたアプローチを提供します。
オプションC(カスタムSLIスクリプト)では、カスタムスクリプトを実装して、定期的にテストリクエストを送信し、応答時間を測定します。これは可能ですが、追加のメンテナンスが必要であり、Google Cloud Monitoring を使用する場合ほど統合性や拡張性がない可能性があります。
オプション D(Google Cloud Trace)は、アプリケーションの分散トレースに使用され、サービス間のレイテンシ データをキャプチャします。これは、アプリケーションのレイテンシーの問題を理解するのに役立ちますが、シナリオのように平均レイテンシーターゲットに基づいてSLIを作成するように特別に設計されているわけではありません。
結論として、Google Cloud Monitoring を使用してカスタム レイテンシ指標を設定し、200 ミリ秒を目標とするホームページ リクエスト レイテンシの SLI を定義するオプション B は、特定のシナリオに最も適切で効率的なアプローチです。
<details><div>

### Q. 問題10: 未回答
シナリオ: あなたは、クラウド プラットフォームでホストされている重要な Web アプリケーションに取り組んでいる DevOps エンジニアです。アプリケーションは大量のトラフィックを経験し、新機能やバグ修正を導入するために頻繁にリリースされ、常に進化しています。製品の停止を回避し、シームレスなリリースを保証するには、効果的なデプロイ戦略と統合テストプロセスを実装する必要があります。
質問: 特定のシナリオでは、ブルーグリーン デプロイと一連の統合テスト ケースを使用した継続的インテグレーション (CI) 統合を考慮して、新しいリリース後の製品の停止を回避するために、どの戦略の組み合わせを実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. リリースごとに、自動ロールバックと継続的インテグレーション (CI) 統合テストを使用してブルーグリーン デプロイを適用します。
説明：特定のシナリオでは、新しいリリース後の製品の停止を回避するために、戦略の最も適切な組み合わせはオプション C です。
オプションCが正しい選択である理由は次のとおりです。
ブルーグリーンデプロイ: ブルーグリーンデプロイは、現在本番トラフィックを処理している環境(青)と更新されたバージョン(緑)の2つの同一の本番環境を維持するデプロイ戦略です。リリース中、トラフィックはブルー環境からグリーン環境にルーティングされ、2 つのバージョン間のシームレスかつ瞬時の切り替えが可能になります。デプロイ後に問題が発生した場合は、ブルー環境に簡単にロールバックして、ユーザーへの影響を最小限に抑えることができます。
自動ロールバック: ブルーグリーンデプロイ戦略に自動ロールバックを実装すると、デプロイプロセスまたは統合テスト中に問題やエラーが検出された場合、システムは手動による介入なしに自動的に以前の安定バージョン(ブルー)に戻すことができます。これにより、展開関連の問題が発生した場合に、迅速かつ自動的に回復できます。
継続的インテグレーション (CI) 統合テスト: 継続的インテグレーション プロセスは、一連の自動化された統合テスト ケースと統合する必要があります。CI 統合テストを使用すると、新しいリリースごとに、アプリケーションに対して一連の包括的なテストを自動的に実行できます。これらのテストでは、さまざまなコンポーネントの機能と統合を検証して、デプロイ プロセスの早い段階で潜在的な問題を検出します。
オプション A (手動統合テスト) では、各デプロイ後に統合テストを手動で実行することを提案しますが、これには時間がかかり、エラーが発生しやすく、リリース プロセスが遅れる可能性があります。オプションCのような自動統合テストは、より高速で信頼性の高いテストを提供します。
オプション B (ローリングデプロイ戦略) では、ユーザートラフィックを処理しながら、新しいリリースでインスタンスを段階的に更新します。ローリングデプロイは便利ですが、インスタントロールバック機能やブルーグリーンデプロイが提供する分離は提供されない場合があります。
オプション D (自動統合テストによるカナリア デプロイ) では、カナリア デプロイが導入され、新しいリリースがユーザーのサブセットに段階的にロールアウトされます。カナリア デプロイは重要ですが、ブルーグリーン デプロイと自動ロールバックを備えたオプション C は、製品停止のリスクを最小限に抑え、スムーズなリリース プロセスを確保するために適しています。
結論として、オプション C は、ブルーグリーン デプロイと自動ロールバック、および各リリースの継続的インテグレーション (CI) 統合テストを組み合わせたもので、製品の停止を回避し、Web アプリケーションの信頼性の高いデプロイ プロセスを維持するための効果的なアプローチを提供します。
<details><div>

### Q. 問題11: 未回答
あなたは、Google Cloud Platform(GCP)に機密性の高い重要なマイクロサービスベースのアプリケーションをデプロイするプロジェクトに取り組んでいるCloud DevOpsエンジニアです。セキュリティとデータ保護は、組織にとって最優先事項です。このアプリケーションは、機密性の高い顧客情報と金融取引を処理します。デプロイ戦略の一環として、適切な Google Kubernetes Engine(GKE)クラスタ構成を選択する必要があります。
プロジェクトのセキュリティとプライバシーの要件を考慮すると、どのオプションがより良い選択ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B. 外部 IP アドレスのないプライベート GKE クラスタ: 外部 IP アドレスのないプライベート GKE クラスタにアプリケーションをデプロイして、クラスタをパブリック インターネットから分離します。
説明：
機密性の高い顧客情報や金融取引を処理する機密性の高いマイクロサービスベースのアプリケーションにおいて、セキュリティとプライバシーが最優先事項である特定のシナリオでは、外部 IP アドレスを使用せずにプライベート GKE クラスタにアプリケーションをデプロイすることをお勧めします。
オプションBが正しい選択である理由は次のとおりです。
セキュリティの強化: プライベート GKE クラスタにアプリケーションをデプロイすると、クラスタのノードとサービスがパブリック インターネットから分離されます。この分離は、外部の脅威や不正アクセスの試みを軽減するのに役立ちます。
機密データ保護: 外部 IP アドレスを公開しないことで、攻撃対象領域とセキュリティ侵害の可能性を減らします。これは、顧客の機密情報や金融取引を保護するために重要です。
パブリック インターネットからの分離: プライベート GKE クラスタにより、仮想プライベート クラウド(VPC)内の許可されたユーザーとリソースのみがマイクロサービスにアクセスできるようになります。これにより、外部エンティティがクラスターと直接対話できなくなります。
コンプライアンス要件: 機密データを処理するアプリケーションの場合、規制コンプライアンス要件により、厳格なアクセス制御とデータ保護が義務付けられることがよくあります。プライベート クラスターは、これらのコンプライアンス標準に準拠しています。
オプション A(外部 IP アドレスを持つパブリック GKE クラスタ)は、機密性の低いアプリケーションには適している可能性がありますが、このシナリオでは、機密性の高いマイクロサービスベースのアプリケーションに不必要な露出とリスクが生じます。
オプション C(ロードバランサを使用したハイブリッド GKE クラスタ)とオプション D(Auto Scaling を使用したマルチゾーン GKE クラスタ)は、セキュリティとプライバシーを確保するためにアプリケーションをパブリック インターネットから分離する必要性に直接対処していません。
結論として、外部 IP アドレスなしでプライベート GKE クラスタにアプリケーションをデプロイするオプション B は、機密性の高い顧客情報や金融取引を処理する機密性の高いマイクロサービスベースのアプリケーションのセキュリティとプライバシーを確保するための最も適切な選択肢です。
<details><div>

### Q. 問題12: 未回答
あなたはサイト信頼性エンジニア(SRE)であり、ユーザーベースのかなりの部分に影響を与える大規模な停止が発生した重要なアプリケーションの管理を担当しています。サイト信頼性エンジニアリングの推奨事項に従って、この大規模なアプリケーション停止時に実行する正しいアクションは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. トラフィックを一時的に迂回させたり、手動の回避策を実装したりする必要がある場合でも、まずサービスの安定性と信頼性の回復に重点を置きます。
説明：特定のシナリオでは、サイト信頼性エンジニアリング (SRE) の推奨事項に従って、ユーザー ベースの大部分に影響を与える大規模なアプリケーション停止時にサイト信頼性エンジニア (SRE) が実行する正しいアクションは、トラフィックを一時的に迂回させたり、手動の回避策を実装したりする必要がある場合でも、最初にサービスの安定性と信頼性の回復に集中することです。
オプションCが正しい選択である理由は次のとおりです。
サービスの信頼性: サイト信頼性エンジニアリングでは、サービスの信頼性と可用性の重要性が強調されています。大規模な停止時には、サービスを安定させ、できるだけ早く運用状態に戻すことが主な関心事です。
影響の最小化: 安定性と信頼性の回復に重点を置くことで、停止がユーザーとそのエクスペリエンスに与える影響を最小限に抑えることを目指しています。トラフィックの一時的な迂回または手動の回避策は、サービスの可用性を確保するために必要になる場合があります。
実践的なアプローチ: SRE の実践では、理論的な完璧さよりも実用的で実用的なソリューションが優先されることがよくあります。一時的な対策を講じてサービスを復旧し、停止中のユーザーへの影響を軽減することは許容されます。
オプション A (自分で問題を解決する) と B (すべてのエンジニアリング チームが同時に関与する) は、大規模な停止中に混乱、混乱、および追加のリスクにつながる可能性があります。共同トラブルシューティングは重要ですが、即時の復旧に重点を置くことが最優先事項です。
オプション D (復旧を開始する前に上級管理職に通知する) は、停止中に直ちに実行するアクションではありません。コミュニケーションは重要ですが、主な焦点はサービスの安定化と機能の回復です。
結論として、サイト信頼性エンジニアリングの推奨事項に従って、大規模なアプリケーション停止時の正しいアクションは、トラフィックを一時的に迂回させたり、手動の回避策を実装したりすることが含まれる場合でも、最初にサービスの安定性と信頼性の回復に集中することです。このアプローチは、サービスの可用性を維持し、ユーザーへの影響を最小限に抑えるという SRE の原則に沿ったものです。
<details><div>

### Q. 問題13: 未回答
あなたは、Google Cloud Platform(GCP)にデプロイされたマイクロサービスベースのアプリケーションの管理を担当するDevOpsエンジニアです。アプリケーションでは、一日を通してさまざまなレベルのトラフィックが発生します。目標は、リソース使用率を最適化しながら、ユーザーの要求を満たすためにアプリケーションを効率的にスケーリングすることです。Google Kubernetes Engine(GKE)オートスケーラーを使用して、クラスタがアプリケーションのスケーリング要件を満たしていることを確認するには、どのようにアプローチすればよいでしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. CPU 使用率に基づいてクラスタ内のノード数を自動的に調整するように GKE Cluster Autoscaler を構成し、ノード内のポッド数を動的にスケーリングするように Horizontal Pod Autoscaler を有効にします。
説明：
トラフィック レベルが異なるマイクロサービス ベースのアプリケーションを管理している特定のシナリオでは、Google Kubernetes Engine(GKE)オートスケーラーを利用して、クラスタがアプリケーションのスケーリング要件を満たしていることを確認するための適切なアプローチがオプション A です。
オプションAが正しい選択である理由は次のとおりです。
GKE Cluster Autoscaler:GKE Cluster Autoscaler を構成すると、クラスタは CPU 使用率に基づいてノード数を自動的に調整できます。これにより、クラスターは必要に応じてスケールアップまたはスケールダウンされ、ワークロードの変化に対応できます。
ポッドの水平オートスケーラー:Horizontal Pod Autoscalerを有効にすると、アプリケーションはリソース使用率(CPUまたはメモリー)に基づいてノード内のポッドの数を動的にスケーリングできます。これにより、個々のマイクロサービスがスケーリングされ、さまざまなレベルのトラフィックを処理できます。
効率的なリソース利用:クラスターとポッドの両方をスケーリングすることで、リソースを効率的に使用できます。トラフィックが少ない場合は、クラスターをスケールダウンしてコストを節約でき、トラフィックが増加した場合は、クラスターとポッドの両方をスケールアップしてパフォーマンスを維持できます。
オプション B(GKE Cluster Autoscaler の無効化)とノードの手動追加は、オートスケーラーが実際の使用率に基づいてノードのスケーリングを動的に管理できるようにするほど効率的ではありません。
オプションC(ポッドのメモリー使用率に基づくスケーリングと垂直ポッド・オートスケーラーの使用)は有効なアプローチですが、CPU使用率と水平ポッド・オートスケーラーに重点を置いたオプションAは、一般的により一般的で実用的です。
オプション D(スケールダウンと手動スケールアップにのみ GKE Cluster Autoscaler を使用)では、不要な手動介入が発生し、トラフィックの急増時にスケーリングに遅延が生じる可能性があります。
結論として、CPU 使用率に基づいて GKE Cluster Autoscaler を構成し、動的ポッド スケーリングのために Horizontal Pod Autoscaler を有効にするオプション A は、GKE クラスタがさまざまなトラフィック レベルに対するマイクロサービス アプリケーションのスケーリング要件を満たすようにするための適切なアプローチです。
<details><div>

### Q. 問題14: 未回答
あなたは、Google Kubernetes Engine(GKE)でホストされるマイクロサービスベースのアプリケーションの管理を担当する DevOps エンジニアです。アプリケーションはコンテナ化され、コードが変更されるたびに、そのイメージがビルドされ、Google Container Registry(GCR)にプッシュされます。新しいコンテナ イメージがビルドされて GCR にプッシュされるたびに、アプリケーションが自動的に GKE にデプロイされるようにする必要があります。これを実現するための最良のアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B. Google Cloud Build トリガーを使用して、新しいイメージが GCR にプッシュされたときにアプリケーションを自動的に GKE にデプロイします。
説明：改訂されたシナリオでは、新しいコンテナ イメージがビルドされて Google Container Registry(GCR)にプッシュされるたびに、マイクロサービスベースのアプリケーションを Google Kubernetes Engine(GKE)に自動的にデプロイする場合、最適なアプローチはオプション B のままです。
オプションBが依然として正しい選択である理由は次のとおりです。
Google Cloud Build トリガー:Google Cloud Build トリガーは、GCR への新しいコンテナ イメージのプッシュなどのイベントに基づいて、ビルドとデプロイのプロセスを自動化するように設計されています。これにより、シームレスな統合と GKE への即時デプロイが保証されます。
GCR および GKE との統合:Cloud Build では、ビルド プロセスの一環としてコンテナ イメージをビルドして GCR にプッシュできます。Cloud Build トリガーは、これらのイメージが GCR で使用可能な場合に、GKE への自動デプロイを容易にします。
自動化と効率化:Cloud Build トリガーを使用すると、手動による介入が不要になり、アプリケーションが常に最新のコンテナ イメージで最新の状態に保たれるため、効率が向上します。
オプション A (Kubernetes 配置マニフェストの手動更新) には手動の手順が含まれ、イメージ タグを更新したり、.kubectl
オプション C(GKE での定期的なジョブのスケジュール設定)とオプション D(Google Cloud Functions を使用)は、Cloud Build トリガーの即時性やイベントドリブン性に比べると効率が低くなります。
結論として、回答の選択肢が入れ替わったとしても、新しいコンテナ イメージが GCR にプッシュされたときに Google Cloud Build トリガーを使用してアプリケーションを自動的に GKE にデプロイするオプション B が、目的の自動デプロイ プロセスを実現するための最良のアプローチであることに変わりはありません。
<details><div>

### Q. 問題15: 未回答
あなたは、リアルタイムの株式市場データをユーザーに提供する Web アプリケーションの管理を担当する運用エンジニアです。1 か月の間に、アプリケーションに関連する次のインシデントを記録します。
インシデント 1:
開始時刻: 09:15 AM
終了時刻: 09:45 AM
検出時間: 5 分
インシデント 2:
開始時刻: 02:30 PM
終了時刻: 03:00 PM
検出時間: 7 分
インシデント 3:
開始時刻: 07:20 PM
終了時刻: 07:40 PM
検出までの時間:10分
アプリケーションは24時間稼働しています。今月の平均検出時間 (MTTD)、平均修復時間 (MTTR)、および平均故障間隔 (MTBF) を計算します。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. MTTD:7.3分、MTTR:17.3分、MTBF:7時間
説明：
与えられたインシデントデータ:
インシデント 1:
開始時刻: 09:15 AM
終了時刻: 09:45 AM
検出時間: 5 分
インシデント 2:
開始時刻: 02:30 PM
終了時刻: 03:00 PM
検出時間: 7 分
インシデント 3:
開始時刻: 07:20 PM
終了時刻: 07:40 PM
検出までの時間:10分
総稼働時間=月内の日数 × 1日の時間数 総稼働時間=30日×24時間=720時間
失敗の数 = インシデントの数 = 3
MTTD (Mean Time To Detect): MTTD = (インシデント検出時間の合計) / (インシデント数) MTTD = ((5 分) + (7 分) + (10 分)) / 3 MTTD = 22 分 / 3 MTTD ≈ 7.3 分
MTTR (平均修復時間): MTTR = (インシデントの修復にかかった時間の合計) / (インシデントの数) MTTR = ((30 分) + (30 分) + (20 分)) / 3 MTTR = 80 分 / 3 MTTR ≈ 26.67 分≈ 17.3 分
MTBF (平均故障間隔): MTBF = 総稼働時間 / 故障回数 MTBF = 720 時間 / 3 MTBF = 240 時間 = 10 日 ≈ 7 時間
計算値に最も近いオプションは次のとおりです。
C. MTTD:7.3分、MTTR:17.3分、MTBF:7時間
なお、計算値は四捨五入による概算値です。
<details><div>

### Q. 問題16: 未回答
シナリオ: DevOps エンジニアは、一般的な e コマース プラットフォームを強化する Google Kubernetes Engine(GKE)でホストされている複雑なマイクロサービス アーキテクチャを管理します。アプリケーションのセキュリティと信頼性を確保することは最も重要です。チームは、信頼できる CI / CD パイプラインを使用してコンテナ イメージをビルドし、テストしてから本番環境の GKE クラスタにデプロイします。
質問: 安全なデプロイ プロセスを維持し、信頼できる CI / CD パイプラインによって正常にビルドされたイメージのみが本番環境の GKE クラスタにデプロイされるようにするには、セキュリティとイメージ検証を強化するためにどのような方法を採用できますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. コード署名によるバイナリ認証を利用します。CI/CD パイプラインのビルド後のステップでコンテナー イメージのコード署名を実装します。これには、秘密キーを使用してイメージにデジタル署名し、デプロイ時に、対応する公開キーを使用して署名を検証することが含まれます。署名されたイメージのみを本番環境の GKE クラスタにデプロイできるようにする必要があります。
説明：
オプション A では、セキュリティとイメージ検証を強化しながら、信頼できる CI / CD パイプラインによって正常に構築されたイメージのみが本番環境の GKE クラスタにデプロイされるようにするための最適なアプローチの概要を説明します。
オプションAが正しい選択である理由は次のとおりです。
コード署名によるバイナリ認証:バイナリ認証では、コード署名を使用して、コンテナー イメージの信頼性と整合性を確保します。CI/CD パイプラインのビルド後のステップとしてコード署名を実装することで、デプロイ プロセスにセキュリティのレイヤーを追加します。
秘密鍵と署名:コード署名では、秘密鍵を使用して、各コンテナイメージのデジタル署名を生成します。この署名は、画像のコンテンツに固有です。デプロイ時には、対応する公開キーを使用して署名が検証され、署名後にイメージが改ざんされていないことが確認されます。
セキュリティの強化:コード署名は、画像の不正な変更や改ざんに対する強力な保護を提供します。秘密鍵で正常に署名されたイメージのみが、本番環境の GKE クラスタにデプロイできます。
自動化と信頼性:コード署名の使用は、CI/CDパイプライン内で自動化できるため、一貫性と信頼性を確保できます。展開中の検証プロセスにより、イメージの信頼性の信頼性がさらに高まります。
オプションB、C、およびDは、潜在的なリスクと課題をもたらします。
オプション B は単体テストのみに依存しているため、イメージの整合性とセキュリティのすべての側面をカバーできない場合があります。
オプションCでは、手動による介入が導入されるため、遅延や人為的ミスにつながる可能性があります。
オプションDは脆弱性スキャンに重点を置いていますが、バイナリ認証によるイメージの真正性には直接対処していません。
結論として、オプション A は、信頼できる CI / CD パイプラインによって正常に構築され、コード署名によるバイナリ認証によって検証されたイメージのみが本番環境の GKE クラスタにデプロイされ、セキュリティとイメージ検証を強化するための安全で信頼性の高いアプローチを提供します。
<details><div>

### Q. 問題17: 未回答
シナリオ：あなたは、広く使用されているeコマースプラットフォームに影響を与えた重大なインシデントのインシデントコマンダー(IC)です。ユーザーが取引を完了しようとしたときにエラーを報告しており、この問題はプラットフォームの収益に影響を与えています。オペレーション リード (OL) やコミュニケーション リード (CL) などのチームを迅速に編成して、インシデントを管理し、通常のサービスを復元します。
質問：eコマースプラットフォームの重大インシデントのインシデントコマンダー(IC)としての役割において、インシデント対応チームの結成後、次のステップは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. インシデント対応者とリーダーが協力して最新情報を共有できるコミュニケーション チャネルを確立します。
説明：
オプションCは、eコマースプラットフォームに影響を与える重大なインシデントを効果的に管理するために、インシデントコマンダー(IC)が取るべき最も適切な次のステップです。
オプションCが正しい選択である理由は次のとおりです。
通信チャネル:インシデント対応者とリーダーの間に専用のコミュニケーション チャネルを確立することは、効果的な調整とコラボレーションに不可欠です。このチャネルにより、インシデント対応チーム内でのリアルタイムの更新、情報の共有、意思決定が可能になります。
コーディネーション：コミュニケーション チャネルを設定することで、インシデント指揮官 (IC)、オペレーション リード (OL)、コミュニケーション リード (CL)、およびその他のチーム メンバーは、取り組みを調整し、洞察を共有し、情報に基づいた意思決定を行い、問題に迅速に対処できます。
タイムリーな更新:重大なインシデントでは、インシデント対応の進捗状況、実行中の手順、解決の予想されるタイムラインについて、すべての利害関係者に情報を提供するために、コミュニケーションが不可欠です。
オプション A、B、および D は、このシナリオですぐに実行する次の手順ではありません。
根本原因の調査:根本原因の調査は重要ですが、インシデント対応チーム内で効果的なコミュニケーションを確立した後に行う必要があります。根本原因分析を掘り下げる前に、協調的な取り組みとリアルタイムの更新が重要です。
公式声明の起草:ユーザーとのコミュニケーションは重要ですが、公式声明の起草は、内部コミュニケーションチャネルの確立に続く必要があります。対応者間の効果的なコラボレーションを確保することが優先されます。
ロールバックの展開:変更のロールバックが必要な場合もありますが、ロールバックに関する決定を下す前に、インシデント対応チームが調整し、状況を評価し、効果的にコミュニケーションをとることが重要です。
要約すると、eコマースプラットフォームの重大インシデントのインシデントコマンダー(IC)の役割において、最も差し迫った次のステップは、インシデント対応チーム内で効果的なコラボレーション、調整、最新情報の共有を確保するためのコミュニケーションチャネル(オプションC)を確立することです。
<details><div>

### Q. 問題18: 未回答
シナリオ：Google Kubernetes Engine(GKE)上で動作する複雑なマイクロサービスベースのアプリケーションの本番環境を管理する責任があります。アプリケーションの健全性とパフォーマンスを効果的にモニタリングするには、Stackdriver Workspaces を使用することにします。ただし、監視データへのアクセスは適切なチームに制限し、セキュリティのベスト プラクティスに従う必要があります。
質問：GKE 上のマイクロサービス アプリケーションに Stackdriver Workspaces を使用して効果的なモニタリング戦略を実装するには、アクセス制御とセキュリティを確保するためにどのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. モニタリング専用の GCP プロジェクトを作成し、その中に Stackdriver Workspace を作成します。関連するチーム メンバーに、監視プロジェクトへの読み取りアクセス権を付与します。
説明：
オプション C は、Stackdriver Workspaces を使用して GKE 上のマイクロサービス アプリケーションの効果的なモニタリング戦略を実装しながら、アクセス制御とセキュリティを確保するための推奨されるアプローチです。
オプションCが正しい選択である理由は次のとおりです。
専用モニタリングプロジェクト:モニタリング専用の GCP プロジェクトを別途作成することで、モニタリングのリソースとデータを本番環境から分離できます。この分離により、不正アクセスのリスクと、アプリケーションの操作に対する潜在的な干渉が軽減されます。
Stackdriver ワークスペース:専用のモニタリング プロジェクト内に Stackdriver Workspace を設定することで、モニタリングを一元化し、マイクロサービス アプリケーション全体の健全性とパフォーマンスに関する分析情報を得ることができます。このアプローチにより、管理が簡素化され、一貫した監視プラクティスが保証されます。
アクセス制御:関連するチーム メンバーに専用の監視プロジェクトへの読み取りアクセス権を付与すると、運用環境から機密情報を公開することなく、監視データへのアクセスを制御できます。これは、最小特権の原則と一致し、セキュリティを強化します。
オプション A、B、および D は、アクセス制御とセキュリティを確保するための推奨されるアプローチではありません。
オプションA:すべての GCP プロジェクトで、すべてのチーム メンバーに広範な読み取りアクセス権を付与すると、セキュリティ リスクや機密データへの不正アクセスにつながる可能性があります。マイクロサービスごとに個別のワークスペースを作成すると、監視が断片化され、一元化された分析情報が不足する可能性があります。
オプションB:GKE クラスタに Viewer IAM ロールを使用すると、クラスタ全体へのアクセス権が付与されますが、マイクロサービス アプリケーション全体のより広範なモニタリングのニーズとアクセス制御には対応できません。
オプションD:Stackdriver Workspaces の URL を共有して自己登録を許可すると、適切なアクセス制御メカニズムが欠如し、不正アクセスやデータの漏洩につながる可能性があります。
要約すると、オプション C(モニタリング専用の GCP プロジェクトを作成し、その中に Stackdriver Workspace を確立し、関連するチームメンバーにモニタリング プロジェクトへの読み取りアクセス権を付与する)は、Stackdriver Workspaces を使用して GKE 上のマイクロサービス アプリケーションの効果的なモニタリング戦略を実装しながら、アクセス制御とセキュリティのベスト プラクティスと一致しています。
<details><div>

### Q. 問題19: 未回答
シナリオ： さまざまな場所に展開された IoT デバイスのフリートを管理する責任があります。これらのデバイスは、リアルタイムで監視および視覚化するテレメトリ データを生成します。さらに、テレメトリ データから集計された分析情報を表示する四半期ごとの概要ダッシュボードをチームに提供する必要があります。この目的のために Google Cloud Platform ソリューションを活用することにしました。
質問：IoT テレメトリ データのリアルタイム モニタリングを実現し、集計された分析情報のための四半期ごとの概要ダッシュボードを作成するには、Google Cloud Platform を使用してどのアプローチに従う必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. 1. はい。IoT テレメトリ データを Cloud Pub/Sub に取り込みます。 2.Cloud Dataflow を使用してデータを処理、集計します。3. 集計したデータを BigQuery に保存します。4. データポータルと四半期ごとの概要ダッシュボードでリアルタイムのモニタリングの可視化を作成します。
説明：
オプション A は、IoT テレメトリ データのリアルタイム モニタリングを実現し、Google Cloud Platform(GCP)ソリューションを使用して集計された分析情報のための四半期ごとの概要ダッシュボードを作成するための推奨されるアプローチです。
オプションAが正しい選択である理由は次のとおりです。
Cloud Pub/Sub にデータを取り込む:Cloud Pub/Sub は、データ ストリームの取り込みと配信のための信頼性と拡張性に優れたメッセージング インフラストラクチャを提供します。IoT テレメトリ データを Cloud Pub/Sub に取り込むと、リアルタイムのデータ処理が可能になります。
Cloud Dataflow を使用した処理と集計:Cloud Dataflow は、データの変換と集計を大規模に実行できるフルマネージドのデータ処理サービスです。Cloud Dataflow を使用して IoT テレメトリ データを処理、集計することで、効率的なデータ操作が可能になります。
集計データを BigQuery に格納する:BigQuery は、大規模なデータセットのクエリと分析を可能にする強力なデータ ウェアハウス ソリューションです。集約された IoT テレメトリ データを BigQuery に保存すると、クエリと可視化が簡単になります。
データポータルの可視化:データポータルは、インタラクティブでカスタマイズ可能なダッシュボードを作成するためのユーザーフレンドリーなツールです。データポータルでリアルタイムのモニタリングの可視化を作成することで、IoT テレメトリ データの現在の状態に関する分析情報を提供できます。
四半期サマリーダッシュボード:データポータルでは、履歴データを集計して表示することで、四半期ごとのサマリーダッシュボードを作成することもできます。このアプローチにより、リアルタイムのインサイトと四半期ごとのサマリーの両方をチームに提供できます。
オプション B、C、および D は、このシナリオにはあまり適していません。
オプションB:Cloud Functions を使用してデータを処理、集計し、Google スプレッドシートに転送すると、手動の手順が必要になるため、リアルタイムのモニタリングと分析情報に必要なスケーラビリティと柔軟性が得られない可能性があります。
オプションC:BigQuery にデータを直接プッシュすると、データの取り込みが簡素化されますが、リアルタイムのモニタリングと可視化の要件はカバーされません。
オプションD:Stackdriver Monitoring and Logging は、IoT テレメトリ データではなく、主にインフラストラクチャとアプリケーションのモニタリング用に設計されているため、IoT テレメトリ データに Stackdriver Monitoring and Logging を利用することは、このユースケースには適していない可能性があります。
要約すると、オプション A(IoT テレメトリ データを Cloud Pub/Sub に取り込み、Cloud Dataflow で処理と集計を行い、集計データを BigQuery に保存し、データポータルでリアルタイム モニタリングの可視化を作成し、四半期ごとの概要ダッシュボードを作成する)は、Google Cloud Platform ソリューションを使用してリアルタイム モニタリングを実現し、洞察に満ちたダッシュボードを作成するためのベスト プラクティスと一致しています。
<details><div>

### Q. 問題20: 未回答
シナリオ：あなたは、重要なマイクロサービスベースのアプリケーションを本番環境に正常にデプロイする責任を持つリードDevOpsエンジニアです。チームは開発フェーズとテストフェーズを完了し、アプリケーションのデプロイの準備状況を評価するための運用準備レビュー (PRR) の準備をしています。ただし、PRR 中に、アプリケーションが可用性とパフォーマンスに対して設定されたサービス レベル目標 (SLO) を満たしていないことが確認されます。チームは、デプロイを続行する前に、この問題に対処する必要があります。
質問：マイクロサービス ベースのアプリケーションの運用準備レビュー (PRR) を成功させるためにどのような手順を実行する必要があるか、また、PRR によってアプリケーションが可用性とパフォーマンスに関するサービス レベル目標 (SLO) を満たしていないことが判明した場合に考慮すべきアクションは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
ある。 PRRプロセス:
アプリケーションのアーキテクチャと設計のスケーラビリティ、冗長性、フォールトトレランスを評価します。
監視とアラートの設定を確認します。
負荷テストとストレス テストを実行して、アプリケーションのパフォーマンスを検証する
ディザスタリカバリ、バックアップメカニズムが整っていることを確認します。
セキュリティとコンプライアンスの対策を分析します。
SLO が満たされない場合:
SLO が満たされていない領域を特定します。
チームと協力して根本原因を診断します。
コードと構成を最適化して、パフォーマンスを向上させます。
自動スケーリングを実装して、需要の増加に対応します。
監視とアラートを強化します。
説明：
オプション A では、マイクロサービス ベースのアプリケーションの運用準備レビュー (PRR) を成功させるための正しい手順を概説し、アプリケーションが可用性とパフォーマンスに関するサービス レベル目標 (SLO) を満たしていない場合に実行する適切なアクションを提供します。
PRRプロセス:
アプリケーションのアーキテクチャと設計を評価することで、SLO を満たすために不可欠なスケーラビリティ、冗長性、フォールト トレランスを考慮してアプリケーションが設計されていることを確認できます。
監視とアラートの設定を確認することで、アプリケーションの正常性を適切に可視化し、問題をプロアクティブに検出できます。
負荷テストとストレス テストを実行すると、現実的な条件下でアプリケーションのパフォーマンスが検証され、潜在的なパフォーマンスのボトルネックを特定するのに役立ちます。
ディザスタリカバリとバックアップのメカニズムを検証することで、アプリケーションが障害から迅速に復旧し、SLOの達成に貢献します。
セキュリティとコンプライアンスの対策を分析することで、データ保護が保護され、アプリケーションが規制要件に準拠していることを確認できます。
SLO が満たされない場合:
SLO コンプライアンス違反の特定の領域を特定することで、重要なパフォーマンスの問題に労力を集中させることができます。
開発チームと運用チームとのコラボレーションにより、パフォーマンスの問題の根本原因を診断して解決するための部門横断的なアプローチが促進されます。
コードと構成を最適化することで、パフォーマンスのボトルネックとリソースの非効率性に対処し、パフォーマンスの向上に貢献します。
自動スケーリング メカニズムを実装すると、アプリケーションは需要の増加を処理でき、可用性とパフォーマンスが向上します。
監視機能とアラート機能を強化することで、パフォーマンスの低下を迅速に検出して対応し、ユーザーへの影響を最小限に抑えることができます。
オプション B、C、および D は、このシナリオにはあまり適していません。
オプションBは最小限のテストに重点を置き、PRRとSLOの重要な側面を無視しています。
オプション C では、重要な PRR 手順が無視され、SLO を完全に無視することを推奨しています。
オプション D では、手動テストが重視され、PRR に関する重要な考慮事項と SLO コンプライアンスが見落とされます。
要約すると、オプション A は、PRR を成功させるためのベスト プラクティスと一致しており、SLO が満たされていない場合にパフォーマンスの問題に対処するための適切なアクションを概説しています。
<details><div>

### Q. 問題21: 未回答
Google Cloud Platform(GCP)上で動作するグローバルなeコマースプラットフォームを担当しています。ユーザー要求は、最初にサードパーティのコンテンツ配信ネットワーク (CDN) を通過し、次にトラフィックをグローバル HTTP/S ロードバランサー (GLB) に転送します。GLB レベルで可用性サービスレベル指標 (SLI) を実装しておきます。ただし、監視戦略を強化して、シームレスなユーザー エクスペリエンスを確保し、CDN またはグローバル ネットワークに起因する問題を検出する必要があります。
この新しいSLIはどこで測定すべきでしょうか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。A と B
A. 地理的に異なる場所からのユーザー要求をシミュレートする代理トランザクションを使用する。
説明：代理トランザクションの実装には、地理的に異なる場所からのユーザー要求のシミュレーションが含まれます。これにより、eコマースプラットフォームのパフォーマンスと可用性をプロアクティブに監視および測定し、CDNまたはグローバルネットワークから生じる潜在的な問題を検出できます。
B. ユーザーエクスペリエンスを監視するためにクライアント側のコードに追加されたカスタムインストルメンテーションを使用する。
説明： クライアント側のコードにカスタムインストルメンテーションを追加することで、さまざまな地理的な場所からユーザーエクスペリエンスを直接測定できます。このアプローチにより、ユーザーエクスペリエンスに影響を与える可能性のあるサードパーティのCDNまたはグローバルネットワークに起因する問題をキャプチャできます。
オプション C、D、および E では、CDN またはグローバル ネットワークに関連する問題の監視に必要なカバレッジが提供されない場合があります。
オプション C (バックエンド サービスからの正常性チェックと応答コードの監視) は、インフラストラクチャ内のバックエンド サービスに焦点を当てており、CDN またはグローバル ネットワークから発生する問題を直接キャプチャできない場合があります。
オプション D (内部サーバー・メトリックをキャプチャーするためにアプリケーション・サーバー上で直接) は、内部サーバー・メトリックを測定しますが、要求がアプリケーション・サーバーに到達する前に問題を検出しない可能性があります。
オプション E (CDN によって生成されたログを分析して潜在的な問題を特定する) は、CDN 関連の問題に関する分析情報を提供できますが、ユーザー エクスペリエンスに影響を与えるより広範なグローバル ネットワークの問題をカバーしていない可能性があります。
<details><div>

### Q. 問題22: 未回答
クロスファンクショナル チームは、Google Kubernetes Engine(GKE)へのデプロイを予定している最先端のアプリケーションを設計中です。設計プロセスの一環として、マイクロサービスベースのアーキテクチャの最適なパフォーマンスと信頼性を確保するために、包括的な監視が不可欠であることを認識しています。Google Cloud Platform(GCP)サービスの機能を活用して、実装作業を最小限に抑えながら、この監視設定を合理化することに熱心です。
このような状況を踏まえて、GKE にデプロイされたアプリケーションのモニタリング インフラストラクチャの設定にはどのようなアプローチが必要ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
ウ. OpenTelemetry クライアント ライブラリの機能を活用し、マイクロサービスのアーキテクチャ内にシームレスに統合します。細心の注意を払った構成により、これらのライブラリは一連のパフォーマンス メトリックを巧みに収集します。次のステップは、この豊富なデータセットを Stackdriver にエクスポートすることで、直感的な Stackdriver Monitoring インターフェースを通じて指標を綿密に観察、精査し、貴重な分析情報を導き出せるようにすることです。
説明：OpenTelemetry は、マイクロサービスからパフォーマンス メトリックを含む可観測性データを収集するための標準化された効率的な方法を提供します。指標を Stackdriver にエクスポートするように OpenTelemetry クライアント ライブラリを構成することで、Stackdriver Monitoring の機能を利用して、収集された指標を視覚化して分析できます。このアプローチにより、監視設定の合理化と自動化が保証されるだけでなく、確立されたGCPサービスの強みを活用して、包括的なパフォーマンス分析情報を得ることができます。
オプション A では、カスタム Pub/Sub と Google データポータル ソリューションが提案されていますが、OpenTelemetry と Stackdriver が提供するシームレスな統合と比較して、メンテナンスと開発が複雑になる可能性があります。
各マイクロサービス内に監視スクリプトを埋め込むオプション B のアプローチは、管理と一貫性の課題につながる可能性がありますが、OpenTelemetry はより統一されたアプローチを提供します。
オプション D では、Stackdriver への直接公開が伴いますが、これにはより多くの手作業が必要になる可能性があり、OpenTelemetry の標準化された収集およびエクスポート メカニズムによって提供される統合レベルが不足する可能性があります。
<details><div>

### Q. 問題23: 未回答
Google Cloud Platform(GCP)でホストされている重要なアプリケーションの包括的な暗号化戦略を設計しています。このアプリケーションには、複数のサービス、リージョン、およびプロジェクトにわたるデータ処理が含まれます。堅牢なキー管理プラクティスを使用して、保存時および転送時にデータが暗号化されるようにする必要があります。さらに、キーへのアクセスをきめ細かく制御し、強力なセキュリティ対策を実施する必要があります。これらの目標を達成するには、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. Google Cloud Key Management Service(Cloud KMS)を使用して、一元化された鍵管理サービスを実装します。Cloud KMS を利用して暗号鍵を作成、管理し、厳格な IAM ポリシーを適用して鍵へのアクセスを制御します。暗号化と復号のオペレーションに Cloud KMS を使用するようにアプリケーションとサービスを構成し、データが安全なチャネルを介して送信されるようにします。
説明：マルチサービス、マルチリージョン、マルチプロジェクトの環境で暗号鍵を管理し、堅牢なセキュリティを確保するための最適なアプローチは、Google Cloud Key Management Service(Cloud KMS)などの一元化された鍵管理サービスを使用することです。Cloud KMS は、暗号鍵へのアクセスを作成、管理、制御するための安全でスケーラブルな方法を提供します。キー管理を一元化することで、IAMを使用して一貫したセキュリティポリシーを適用し、キーのローテーションとライフサイクルを管理し、適切なアクセス制御を確保できます。
Cloud KMS を使用すると、さまざまな GCP サービスやアプリケーション間で暗号化をシームレスに統合できます。このアプローチにより、キー アクセスをきめ細かく制御し、キーのローテーションと管理を簡素化し、保存データと転送中のデータを強力に暗号化します。また、クラウド環境でのキー管理のベストプラクティスとも一致しています。
オプションBは、個々のGCPサービスに組み込まれた暗号化機能を使用することを提案していますが、これにより、鍵管理のプラクティスが断片化され、一元管理が欠如する可能性があります。オプション C には、複雑さとメンテナンスのオーバーヘッドをもたらす可能性のあるカスタム ソリューションのデプロイが含まれます。オプション D では、サードパーティのソリューションを使用することが提案されていますが、GCP と緊密に統合されていない可能性があり、互換性やセキュリティ上の課題が生じる可能性があります。
<details><div>

### Q. 問題24: 未回答
シナリオ：
あなたはサイト信頼性エンジニア (SRE) で、フラッシュセールやプロモーションイベントのために定期的にトラフィックが急増する大規模な e コマース プラットフォームを担当しています。そのようなイベントの1つで、プラットフォームは予期せぬ速度低下に直面し、顧客の不満と売上の損失につながりました。これで、チームは高度な SRE 戦略を実装して、将来のトラフィックの急増をより効果的に処理することを決意しました。
質問：
経験豊富なサイト信頼性エンジニア (SRE) は、トラフィック急増時のプラットフォームの回復力を強化するために、カオス エンジニアリング アプローチを実装することにしました。シナリオに最も適した特定のカオス エンジニアリング手法とその理由を教えてください。
1. 
2. 
3. 
4. 
<details><div>
答え：B.ゲームデイズ
理由: GameDays は、トラフィックの急増を含む実際のシナリオをシミュレートしてシステムの回復力をテストおよび強化する高度なカオス エンジニアリング手法です。制御された障害を意図的に誘発することで、SRE と他のチームは共同でインシデント対応を実践し、仮定を検証し、予期しないイベントを処理するプラットフォームの能力を向上させることができます。このアプローチは、複雑なシステムにおけるプロアクティブなテスト、継続的な改善、信頼性の確保という SRE の原則に沿ったものです。これにより、チームは弱点を特定し、プロセスを改善し、トラフィックの急増を効果的に処理するプラットフォームの能力に対する信頼を築くことができ、最終的には顧客体験の向上とダウンタイムの短縮につながります。
<details><div>

### Q. 問題25: 未回答
シナリオ：
過去 1 年間正常に運用されているクラウドベースの e コマース プラットフォームを管理し、平均応答時間を 200 ミリ秒未満に維持するという明確に定義されたサービス レベル目標 (SLO) を設定しています。プラットフォームのユーザーベースは着実に成長しており、顧客満足度は高いままです。プラットフォームの人気が高まるにつれて、信頼性を確保しながらパフォーマンスを向上させる方法を模索しています。また、市場での競争力を維持するために、新機能の導入も検討しています。
質問：
シナリオを踏まえて、プラットフォームのパフォーマンスと信頼性を最適化しながら、新しい機能を導入するにはどうすればよいでしょうか。(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
答え：
B. プラットフォームで負荷テストを実施して、潜在的なボトルネックと最適化の領域を特定します。
D. 主要なサービス レベル インジケーター (SLI) とサービス レベル目標 (SLO) を継続的に監視します。
説明：
負荷テストを実施します。負荷テストを実行すると、さまざまなレベルのトラフィックとストレスの下でプラットフォームがどのように動作するかを特定するのに役立ちます。さまざまなユーザー シナリオをシミュレートすることで、潜在的なパフォーマンスのボトルネックと最適化が必要な領域を特定できます。これにより、プラットフォームの信頼性と顧客満足度に影響を与える前に、問題に積極的に対処できます。
SLIとSLOを継続的に監視します。主要なサービスレベル指標(SLI)とサービスレベル目標(SLO)を監視することは、信頼性とパフォーマンスのバランスを維持するために重要です。監視により、プラットフォームの正常性とパフォーマンスに関するリアルタイムの分析情報が得られ、目的の SLO からの逸脱を検出できます。SLI と SLO を定期的に見直すことで、プラットフォームの信頼性を許容範囲内に保ちながら、リソースの割り当て、最適化の取り組み、新機能の導入について十分な情報に基づいた決定を下すことができます。
<details><div>

### Q. 問題26: 未回答
あなたは、重要な e コマース Web サイトの信頼性を担当するサイト信頼性エンジニア (SRE) です。ユーザーは、チェックアウトプロセスで断続的な問題を報告しており、一部のトランザクションが完了せず、売上が失われています。エンジニアリングチームは、これらの問題がサードパーティの支払いゲートウェイの統合に関連しているのではないかと考えています。
モニタリングデータを分析すると、チェックアウトの失敗率がサードパーティの支払いゲートウェイからの応答時間の遅延と相関していることがわかります。支払いゲートウェイからの応答時間が特定のしきい値を超えると、チェックアウトの失敗率が増加します。
この問題に対処するには、チェックアウト プロセス専用の可用性サービス レベル インジケーター (SLI) を確立する必要があります。このSLIを定義する最も適切な方法は何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
このシナリオの正しいオプションは B です。チェックアウト要求の合計に占める完了したチェックアウト要求の割合。
合計に占める完了したチェックアウト要求の割合に基づいてチェックアウトプロセスの可用性SLIを定義することは、いくつかの理由から最も適切なアプローチです。
ユーザー中心の測定:このSLIは、チェックアウトプロセス中の実際のユーザーエクスペリエンスに焦点を当てています。これは、ユーザーが行った合計試行回数に対する成功したチェックアウトトランザクションの割合をキャプチャします。これは、ユーザーにスムーズで信頼性の高いチェックアウト体験を提供するという目標に直接合致しています。
ビジネスへの影響:チェックアウトプロセスは、eコマースWebサイトの重要な部分であり、売上と収益に直接影響します。目標は、失敗したトランザクションの数を最小限に抑えることです。完了したチェックアウト要求の割合を測定することで、問題のビジネスへの影響を直接評価できます。
問題との相関関係:このシナリオでは、支払いゲートウェイからの応答時間の遅延とチェックアウトの失敗の相関関係が強調されています。完了したチェックアウトリクエストの割合を測定することで、応答時間の遅延の影響を間接的に把握できます。応答時間が長くなり、チェックアウトの試行が失敗した場合、このSLIは可用性の低下を正確に反映します。
定量的で理解しやすい:完了したチェックアウトリクエストの割合は、明確で理解しやすい指標です。これは、チェックアウトプロセスの可用性を評価するためのシンプルで定量化可能な方法を提供します。これにより、技術関係者と非技術者の両方が把握し、監視することが容易になります。
実用的なインサイト:完了したチェックアウトリクエストの割合を監視することで、トラブルシューティングと改善のための実用的な洞察を得ることができます。割合が大幅に低下した場合は、支払いゲートウェイまたはその他の関連コンポーネントに潜在的な問題があることを示しており、タイムリーな調査と解決を促します。
対照的に、オプションA、C、およびDは、チェックアウトプロセスの可用性の直接的またはユーザー中心の測定値を提供しません。
オプションA:平均応答時間は、必ずしもユーザーへの影響を捉えているわけではありません。一部のトランザクションで応答時間が極端に長くなり、障害につながる状況を見逃す可能性があります。
オプションC:支払いゲートウェイの問題が原因で失敗したチェックアウトリクエストのみをカウントすると、チェックアウトプロセス全体の可用性が反映されない場合があります。障害に影響を与える他の要因が見落とされる可能性があります。
オプションD:しきい値を超える応答時間を測定すると、可用性が正確に表されない場合があります。応答時間が長いとチェックアウトの失敗につながるとは限らず、応答時間が短い場合でも、他の問題による失敗につながる可能性があります。
したがって、最良の選択は、チェックアウトプロセスの可用性に関する全体的かつユーザー中心の視点を提供し、ビジネス目標とユーザーエクスペリエンスの目標とうまく連携するオプションBです。
<details><div>

### Q. 問題27: 未回答
医療アプリケーションは、医療記録や個人情報などの機密性の高い患者データを処理して保存します。責任の一環として、このデータを保護するための暗号化要件に対処する必要があります。
機密データは、保存時と転送時の両方で暗号化する必要があります。アプリケーションは、仮想プライベートクラウド (VPC) 内の複数のインスタンスで実行されます。セキュリティ侵害が発生し、攻撃者が基盤となるインフラストラクチャにアクセスした場合でも、暗号化されたデータは安全に保たれ、暗号化キーなしでは使用できないようにする必要があります。
このシナリオでは、暗号化キーを効果的に管理し、漏洩のリスクを最小限に抑えるために、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
このシナリオでは、オプション C が適切なアプローチです。
説明：
暗号化キーをアプリケーションのソースコードリポジトリ内のプレーンテキストファイルに保存すること(オプションA)は、安全な方法ではありません。キーをプレーンテキストで保存すると、リポジトリにアクセスできる人なら誰でも簡単にキーにアクセスできるため、漏洩のリスクが高まります。
暗号化キーの管理にサード・パーティーの認証サービスを使用すること (オプション B) は、暗号化キーの管理とは直接関係がない場合があります。認証サービスは、通常、暗号化キーの保護ではなく、ユーザー アクセスと ID 管理に使用されます。
Cloud Key Management Service(KMS)に暗号鍵を保存し、鍵のローテーション(オプション C)を実装することをおすすめします。Cloud KMS は、暗号鍵を安全かつ一元的に管理する方法を提供します。キーを定期的にローテーションすることで、侵害が発生した場合でも、キーが頻繁に変更されるため、1 つのキーの公開による影響は限定的です。
暗号化キーを必要とせずに単純なアルゴリズムを使用して機密データを暗号化すると (オプション D)、暗号化が脆弱になり、簡単に侵害される可能性があるため、安全ではありません。
したがって、暗号鍵を効果的に管理し、漏洩のリスクを最小限に抑えるには、暗号鍵を Cloud KMS に保存し、鍵ローテーション(オプション C)を実装するのが最善の選択です。
<details><div>

### Q. 問題28: 未回答
あなたは、重要なeコマースアプリケーションの管理を担当するDevOpsチームの一員です。アプリケーションは、新機能や拡張機能を導入するために頻繁にデプロイされます。ビジネス関係者は、アプリケーションのアップタイムとパフォーマンスに関するサービス レベル目標 (SLO) を設定しています。
ベスト プラクティスに合わせるために、チームはインシデントを処理し、必要に応じて信頼性を優先するための明確なプロセスを確立したいと考えています。エラー予算ポリシーを定義することで、このバランスを達成できると考えています。エラーバジェットポリシーを実装する前に何をする必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。ある
説明：
重大なインシデントが発生する前に、すべてのサービス関係者と協力して適切なエラー予算ポリシーを確立することが重要です。エラーバジェットは、サイト信頼性エンジニアリング (SRE) の重要な概念であり、チームが機能開発から信頼性の向上に焦点を移す前に発生する可能性のある信頼性低下の許容レベルを定義します。
このアプローチが最も適している理由は次のとおりです。
ステークホルダーとの協働:エラー予算ポリシーの策定には、開発者、プロダクトマネージャー、ビジネスチームなど、さまざまな関係者とのコラボレーションが必要です。これにより、機能開発と信頼性のバランスについて全員が認識を一致させることができます。
定量的測定:エラーバジェットは、合意されたサービスレベル目標(SLO)を満たしながら、どの程度のダウンタイムまたは劣化が許容されるかを定量的に測定します。これにより、新機能と信頼性のトレードオフを明確に理解できます。
明確な優先順位付け:インシデントが発生し、エラー予算を超えた場合、開発チームは機能リリースよりも信頼性を優先するように明確なシグナルを発します。これにより、主観的な意思決定を回避し、データドリブンなアプローチを実現します。
バランスの取れたアプローチ:エラー予算ポリシーを設定しても、機能開発を完全に停止するわけではありません。その代わりに、イノベーションの必要性と信頼性の高いサービスの必要性の両方を考慮したバランスの取れたアプローチを確立しています。
継続的改善:エラーバジェットポリシーは、継続的な改善の文化を奨励します。チームは、信頼性の問題に対処し、システムの回復力を高めて将来のインシデントを防ぐことに意欲的です。
対照的に、他のオプションはそれほど効果的ではありません。
オプションB:新機能よりも常にサービスの信頼性を優先するように製品チームと交渉すると、異なるチーム間の競合や連携の欠如につながる可能性があります。客観的に意思決定を行うためには、エラーバジェットポリシーのようなデータドリブンなアプローチが不可欠です。
オプションC:リリース頻度を減らすと、俊敏性とユーザーに迅速に価値を提供する能力が妨げられる可能性があります。リリース頻度と信頼性の向上のバランスを取ることが重要です。
オプションD:インシデント発生時に新しいリリースを防ぐために Jenkins パイプラインにプラグインを追加しても、信頼性を管理するための全体的な戦略には対応できず、フォーカスをシフトするタイミングを明確に測定することはできません。
要約すると、エラーバジェットポリシーを設定することで、機能開発とサービスの信頼性のバランスを取るための適切に構造化された協調的なアプローチが保証され、チームはデータと合意されたしきい値に基づいて情報に基づいた意思決定を行うことができます。
<details><div>

### Q. 問題29: 未回答
お客様は、同じ Google Cloud Platform(GCP)プロジェクト内の Compute Engine インスタンスで実行されている複数の本番環境システムを管理する責任があります。各システムには、特定のワークロードを処理するための専用インスタンスの独自のセットがあります。責任の一環として、予算編成と最適化の目的で、これらの各システムの実行コストを正確に評価する必要があります。
これを実現するための推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. GCP の Cost Management ツールを利用して、請求レポートとコスト配分タグを有効にします。所属するシステムを表す各インスタンスに適切なタグを適用します。請求レポートのタグ別に費用の内訳を分析します。
説明：
このシナリオでは、同じ GCP プロジェクト内で複数の本番システムを管理しており、各システムの運用コストを正確に評価する必要があります。これを実現するには、GCP の Cost Management ツール(コスト配分タグなど)を使用することをお勧めします。
このアプローチのしくみは次のとおりです。
請求レポート: GCP は、ラベルやタグなどのさまざまなパラメータに基づいて費用を分類する詳細な請求レポートを提供します。請求レポートを有効にすることで、プロジェクト内のリソースの使用状況と関連コストに関する情報を収集できます。
コスト配分タグ: コスト配分タグを使用すると、リソースが属するシステムまたはアプリケーションを表す特定の識別子でリソースにラベルを付けることができます。Compute Engine インスタンスごとに、関連付けられている本番環境システムを示すタグを割り当てることができます。
適切なタグ付け: サービスを提供する本番環境システムに応じて、関連するコスト配分タグを各 Compute Engine インスタンスに適用します。このタグ付けは、コストを正確に帰属させるのに役立ちます。
タグで分析する: リソースにタグを付けたら、課金レポートでタグ別のコスト内訳を分析できます。これにより、各実稼働システムに関連するコストを明確に把握できます。
このアプローチにより、すべてのシステムを同じGCPプロジェクト内で維持し、管理とリソース割り当てを合理化できます。同時に、各システムのコストを個別に効果的に追跡および監視できるため、予算編成、最適化、およびリソース割り当ての決定に役立ちます。
オプションB、C、およびDは、このユースケースには適していません。オプションBは、個別のプロジェクトを作成することを提案していますが、これは複雑さと分離の課題につながる可能性があります。オプション C では、ログと BigQuery を使用しますが、費用配分タグを使用する場合ほど直接的で正確な費用の内訳が得られない可能性があります。オプション D は、エージェントとリソース使用量の監視に重点を置いていますが、これは各システムのコスト割り当てに直接変換されない場合があります。
したがって、オプション A は、同じ GCP プロジェクト内で複数の本番システムを実行するコストを効果的に追跡および管理するための推奨されるアプローチです。
<details><div>

### Q. 問題30: 未回答
あなたは、最近アプリケーションをGoogle Cloud Platform(GCP)に移行したeコマース企業のDevOpsリーダーです。ホリデー シーズンが近づいており、トラフィックの急増に対するアプリケーションの準備が整っていることを確認する必要があります。これからの繁忙期に備えるための最初のステップは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
来たる繁忙期に向けて電子商取引アプリケーションを準備するには、まず、アプリケーションのロード テストを行い、スケーリングのパフォーマンスをプロファイリングする必要があります。
説明：負荷テストでは、アプリケーション上での現実的なユーザー トラフィックと対話をシミュレートして、さまざまなレベルの負荷下でのパフォーマンスを評価します。徹底的な負荷テストを実施することで、次のことを実現できます。
パフォーマンスのボトルネックを特定します。ロード テストは、低速なデータベース クエリ、非効率的なコード、リソースの制限など、トラフィックのピーク時にアプリケーションの応答性に影響を与える可能性のあるパフォーマンスのボトルネックを明らかにするのに役立ちます。
リソース割り当ての最適化:ロード テストは、リソース使用率のパターンに関する分析情報を提供し、Web サーバー、データベース、キャッシュ レイヤーなど、アプリケーションのさまざまなコンポーネントのリソース割り当てを最適化するのに役立ちます。
スケーラビリティの確保:ロード テストでは、インスタンスを追加することで、アプリケーションの水平方向のスケーリングをどの程度適切に行うかをテストできます。これにより、インフラストラクチャがパフォーマンスを損なうことなくトラフィックの増加を処理できるかどうかを判断できます。
荷重公差の推定:負荷テストは、アプリケーションの負荷許容範囲と容量制限を理解するのに役立ちます。この情報は、予想されるトラフィックの急増に対応するためのリソースのプロビジョニングについて、十分な情報に基づいた決定を下すために重要です。
脆弱性の検出:負荷テストでは、競合状態や同期の問題など、通常の状態では表面化しない可能性のある脆弱性が明らかになり、トラフィックが多いときに障害が発生する前に修正できます。
徹底的な負荷テストを実施することで、パフォーマンスの課題に積極的に対処し、アプリケーションのインフラストラクチャを最適化し、繁忙期にシームレスなユーザーエクスペリエンスを確保できます。
<details><div>

### Q. 問題31: 未回答
シナリオ：
ロードバランサーを使用せずにHTTPリクエストを直接処理する重要なWebアプリケーションを管理する責任があります。最適なユーザーエクスペリエンスを確保し、レイテンシーを監視することは、チームにとって最優先事項です。そこで、Stackdriver Monitoring を活用して、ユーザーが経験した HTTP レイテンシに関する分析情報を取得することにしました。これを達成するには、どのような手順を踏む必要がありますか?
質問：
HTTP レイテンシを効果的にモニタリングするには、Stackdriver でどのような指標とグラフを使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解はオプションBです。
B. 指標: HTTP エンドポイントの Stackdriver Monitoring が提供する組み込みの指標を利用します。このメトリックは、受信リクエストのレイテンシを記録します。response_latencies
グラフ: 時間の経過に伴うメトリックを表示する折れ線グラフを生成します 。チャートをカスタマイズして、さまざまなパーセンタイル(50位、95位など)を表示し、レイテンシーの分布を把握します。response_latencies
説明：
Stackdriver Monitoring には、HTTP エンドポイントのレイテンシをモニタリングするために特別に設計された、事前定義された一連の指標が用意されています。response_latencies
このメトリクスは、受信 HTTP リクエストが処理および処理されるまでにかかる時間をキャプチャします。response_latencies
時間の経過に伴うメトリックを表示する折れ線グラフを作成することで、さまざまな時間間隔でレイテンシーがどのように変化するかを視覚化できます。response_latencies
パーセンタイル(50位、95位など)を表示するようにチャートをカスタマイズすると、レイテンシー値の分布を理解し、潜在的な外れ値やスパイクを特定するのに役立ちます。
このアプローチにより、ユーザーが経験するHTTPレイテンシーを効果的に監視し、情報に基づいた意思決定を行ってユーザーエクスペリエンスを最適化できます。
<details><div>

### Q. 問題32: 未回答
あなたは、複数の地域で運営されている大規模な e コマース プラットフォームの主任管理者です。このプラットフォームは、Google Cloud Platform(GCP)を使用してアプリケーションとデータベースをホストします。最近、Webサービスの応答時間が変動し、ユーザーエクスペリエンスに影響を与えていることに気付きました。この問題は、異なる GCP リージョン間のネットワーク遅延に関連している可能性があります。この問題を調査し、ユーザーに最適なネットワーク パフォーマンスを確保する必要があります。
GCPリージョン間の潜在的なネットワーク遅延の問題を診断して対処するにはどうすればよいですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
A. GCP の Network Intelligence Center を使用して、異なる GCP リージョン間でネットワーク パフォーマンス テストを実行します。テスト結果を分析して、待機時間のボトルネックを特定し、適切なアクションを実行します。
説明：GCP の Network Intelligence Center は、GCP リージョン内およびリージョン間でのレイテンシなどのネットワークの問題を診断するためのツールと分析情報を提供します。このツールを使用してネットワーク パフォーマンス テストを実行することで、異なるリージョン間の待機時間に関する正確でリアルタイムのデータを収集できます。テスト結果を分析すると、待機時間が長くなっている特定のリージョンまたはルートを特定するのに役立ち、問題に対処するための的を絞ったアクションを実行できます。このアプローチでは、GCP の組み込み機能を活用し、ネットワーク パフォーマンスを最適化するための実用的な分析情報を提供します。
オプション B では、仮想マシン インスタンスの容量を増やすことが提案されていますが、リージョン間のネットワーク待機時間の根本原因に直接対処できない可能性があります。
オプション C では、GCP の外部に別の監視ツールを設定することを提案していますが、複雑さと追加のオーバーヘッドが生じる可能性があります。
オプションDは、より大きなペイロードを使用するようにアプリケーションコードを変更することを提案していますが、これは必ずしもネットワーク遅延の影響を軽減せず、アプリケーションを不必要に複雑にする可能性があります。
したがって、オプションAは、GCPリージョン間のネットワーク遅延の問題を診断して対処するための最適な選択肢です。
<details><div>

### Q. 問題33: 未回答
シナリオ：
Compute Engine インスタンスを使用して、Google Cloud Platform(GCP)上のリアルタイム ゲーム プラットフォームの運用を監督します。このプラットフォームには、プレイヤー向けの本番環境と、個別のテスト環境の両方が含まれます。各環境は、専用の仮想プライベートクラウド(VPC)ネットワーク内で動作し、フロントエンドサーバーとバックエンドサーバー用に個別のサブネットがあります。最近のセキュリティ上の懸念により、運用フロントエンド サーバー内での不正な通信の疑いが生じています。これに対処するには、ネットワーク トラフィックを分析して、悪意のあるアクティビティの兆候がないか確認する必要があります。
質問：
ここで説明したコンテキストでは、リアルタイムゲームプラットフォームの運用フロントエンドサーバー内の潜在的な悪意のあるアクティビティについてネットワークトラフィックを監視するタスクにどのようにアプローチする必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。C
説明：
ある。このオプションでは、本番稼働 VPC ネットワークでのみ、サンプルボリュームスケールが 0.5 の VPC フローログを有効にすることを提案します。本番環境のログはキャプチャされますが、テストは含まれていないため、テスト環境での潜在的な悪意のあるアクティビティを見逃す可能性があります。
B.このオプションでは、本番稼働 VPC ネットワークでのみ、サンプルボリュームスケールが 1.0 の VPC フローログを有効にすることをお勧めします。すべてのトラフィックをキャプチャすることは包括的に見えるかもしれませんが、過剰なデータキャプチャ、ストレージコスト、および潜在的なパフォーマンスオーバーヘッドにつながる可能性があります。
C. このオプションでは、ボリュームスケールが 0.5 のテストと本番の両方の VPC ネットワークフロントエンドおよびバックエンドサブネットで VPC フローログを有効にすることを提案します。このアプローチは、両方の環境からのトラフィックの代表的なサンプルをキャプチャし、ロギング システムを圧迫することなくパターンや異常を特定するのに役立ちます。運用前にテストに変更を適用することで、影響を確実に検証できます。
D. このオプションでは、テスト VPC ネットワークと本番環境の両方の VPC ネットワークで、サンプルボリュームスケール 1.0 の VPC フローログを有効にすることを提案します。オプション B と同様に、過剰なデータ キャプチャとリソースの使用につながる可能性がありますが、オプション C のよりバランスの取れたアプローチよりも大きな利点はありません。
結論：オプション C は、0.5 の妥当なサンプル量スケールを使用しながら、テスト環境と運用環境の両方からネットワーク トラフィック ログをキャプチャするためのバランスの取れたアプローチを提供するため、最も適した選択肢です。これにより、効果的な監視、潜在的な悪意のある通信や不正な通信の特定、ロギングシステムの管理性と効率性を確保することができます。これはネットワーク トラフィックの一部しかキャプチャしませんが、ロギング システムを圧迫することなくパターンや異常を特定するのに役立つ代表的なサンプルを提供します。このアプローチでは、包括的な監視の必要性と、リソース使用に関する実用的な考慮事項とのバランスが取れています。
実際のシナリオでは、サンプル ボリューム スケール 1.0 ですべてのネットワーク トラフィックをキャプチャすると、すぐに管理が困難になり、リソースを大量に消費する可能性があります。したがって、オプションCは、ロギングシステムの効率性と有用性を維持しながら、効果的な監視を可能にするため、多くの場合好まれます。これは、セキュリティとトラブルシューティングの目的でネットワーク トラフィック データをキャプチャして分析するための、よりバランスの取れた実用的なアプローチです。
<details><div>

### Q. 問題34: 未回答
ユーザー向け Web アプリケーションの信頼性を監督し、過去 6 か月のエラー バジェット データを綿密に分析した結果、アプリケーションの使用率は各時間枠内で一貫して 5% を大きく下回っていることがわかりました。関連するビジネス関係者を巻き込んだ包括的な SLO レビューの後、現在の SLO の正確性をまとめて確認しました。しかし、アプリケーションで観測された信頼性に厳密に一致させるには、何を検討すべきでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
回答: B. 現在の SLO は、アプリケーションの実証済みの信頼性と既に一致しているため、維持します。
説明：
このシナリオでは、過去 6 か月間のアプリケーションのエラー バジェット分析は、どの時間枠内でも使用率が 5% を超えていないことを一貫して示しています。ビジネス関係者との最近のSLOレビューにより、現在のSLOの妥当性が確認されました。現在の SLO は、アプリケーションの観察された信頼性を正確に反映しているため、調整を行う必要はありません。既存の SLO を維持することは、SLO がアプリケーションのパフォーマンスと信頼性を正確に反映し続けるようにするための最も適切なアクションです。
<details><div>

### Q. 問題35: 未回答
シナリオ：あなたは、政府機関の重要なアプリケーションの管理を担当するチームの一員です。この機関は、規制遵守のために、アプリケーション ログを 7 年間保持するという厳しい要件を設けています。ここで行う作業は、Google Cloud Platform(GCP)が提供するログ記録およびモニタリング サービスである Stackdriver を構成して、この要件を満たしながら、ログの長期保持に関連するストレージ コストを最適化することです。
質問：ストレージ コストを最小限に抑えながらアプリケーション ログを 7 年間アーカイブするという政府機関の要件を遵守するには、Stackdriver を構成する際にどのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：B
説明：保持期間が 7 年の BigQuery にログをエクスポートするように Stackdriver を設定することは、ストレージ費用を最適化しながら政府機関の要件を満たすための最適なアプローチです。このオプションが最良の選択である理由は次のとおりです。
長期保存:BigQuery では、カスタムのデータ保持期間を定義できるため、必要な 7 年間のログ保持に適しています。
クエリと分析:BigQuery には強力なクエリ機能と分析機能が備わっており、必要に応じてアーカイブされたログに対して高度な分析とレポートを実行できます。
費用対効果:BigQuery は、オンデマンド料金や定額料金オプションなど、柔軟な料金モデルを提供しています。使用パターンに基づいて適切な価格モデルを選択することで、コストを最適化できます。
マネージドサービス:BigQuery は、フルマネージドのサーバーレス データ ウェアハウス サービスです。基盤となるインフラストラクチャは Google によって管理されるため、ユーザーは運用タスクではなくデータ分析に集中できます。
オプション A、C、および D は、特定のシナリオに最も適した選択肢ではありません。
オプション A(デフォルトの保持期間で Cloud Storage にエクスポート)では、特定の保持期間を柔軟に設定できず、時間の経過とともにストレージ コストが増加する可能性があります。
オプション C(カスタム保持を使用して Cloud Pub/Sub にエクスポート)には、7 年間のログ保持期間に必要なデータクエリと分析機能がありません。
オプション D(ライフサイクル管理を使用して Cloud Storage にエクスポート)では、BigQuery と比較してストレージ費用が高くなる可能性があり、BigQuery が提供する高度なクエリ機能は提供されません。
要約すると、7 年間の保持期間でログを BigQuery にエクスポートするように Stackdriver を設定することは、ストレージ コストを最適化し、必要なクエリ機能と分析機能を提供しながら、ログを長期アーカイブするという政府機関の要件に合致します。
<details><div>

### Q. 問題36: 未回答
シナリオ：Google Cloud Platform(GCP)でホストされているウェブ アプリケーションを App Engine を使用して管理するのは、お客様の責任です。このアプリケーションは、リアルタイムのコラボレーション機能を提供し、複数のユーザーが共有ドキュメントを同時に編集できるようにします。最近、コラボレーションイベントによるユーザーアクティビティが急増し、アプリケーションのトラフィックと使用が増加しました。このピーク期間中、ユーザーは共有ドキュメントを編集する際にネットワーク遅延と応答時間の遅延を経験しました。今後、同様のコラボレーションイベントでスムーズなユーザーエクスペリエンスを確保したいと考えています。
質問：Web アプリケーションでネットワーク遅延と応答時間の遅延を引き起こした最近のコラボレーション イベントの後、アプリケーションのパフォーマンスを最適化し、将来のコラボレーション イベントのためにネットワーク遅延を削減するために最も適切なアクションは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:アプリケーションのパフォーマンスを最適化し、将来のコラボレーション イベントのためにネットワーク レイテンシを短縮するための最適なアクションは、オプション B です。 追加のアイドル状態の App Engine インスタンスをデプロイする: App Engine の設定を変更して、アイドル状態のインスタンスの余剰を自動的に維持することで、トラフィックのピーク時の応答時間を短縮し、ネットワーク レイテンシを短縮します。
説明：アイドル状態の App Engine インスタンスを追加でデプロイすると、受信リクエストを処理する準備が整った事前ウォーミングされたインスタンスの予約が確保されるため、トラフィックのピーク時のネットワーク レイテンシに対処できます。これらのアイドル状態のインスタンスは、トラフィックが減少すると自動的にスケールダウンできるため、インスタンスのウォームアップ時間に関連する遅延なしに、コラボレーションイベント中にシームレスなユーザーエクスペリエンスを提供できます。アイドル状態のインスタンスを最適な数に維持することで、アプリケーションはユーザーの要求に迅速に応答し、ネットワーク待機時間を短縮し、ユーザー アクティビティが多い期間のパフォーマンスを向上させることができます。
他のオプションが最良の選択ではない理由:
ネットワーク帯域幅を増やします(オプションA)。ネットワーク帯域幅を増やすと、全体的な接続性が向上しますが、コラボレーションイベント中のインスタンスのウォームアップ時間によって引き起こされるレイテンシーの問題に直接対処できない場合があります。
データベースクエリを最適化します(オプションC)。データベースクエリの最適化はアプリケーション全体のパフォーマンスにとって重要ですが、トラフィックのピーク時のネットワーク遅延に直接影響しない場合があります。
静的アセットのキャッシュを実装する (オプション D):静的アセットをキャッシュすると、コンテンツ配信速度は向上しますが、インスタンスのウォームアップ時間とネットワーク通信に関連するレイテンシーの課題に対処できない場合があります。
要約すると、アイドル状態の App Engine インスタンスを追加でデプロイすることは、インスタンスのウォームアップ時間を最小限に抑え、応答時間を短縮することで、ネットワーク レイテンシを低減し、コラボレーション イベント中のスムーズなユーザー エクスペリエンスを確保するためのプロアクティブなアプローチです。
<details><div>

### Q. 問題37: 未回答
シナリオ：組織はグローバルな e コマース プラットフォームを運用しており、主要なサービスは Google Kubernetes Engine(GKE)クラスタ上で実行されています。本番環境の GKE クラスタは ap-southeast-1 リージョンにあり、開発システムとビルドシステムは us-east-1 リージョンにあります。デプロイ パイプラインを最適化して、コンテナ イメージをビルドシステムから本番環境の GKE クラスタに効率的に転送する役割を担っています。画像転送の帯域幅を最大化しながら、これを実現するにはどうすればよいでしょうか。
質問：本番環境の GKE クラスタとビルドシステムが地理的に分散している場合、us-east-1 リージョンから ap-southeast-1 リージョンへのコンテナイメージの転送を最適化し、帯域幅を効率的に利用するには、どのような戦略を採用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：B
説明：
Google Container Registry のマルチリージョン機能(オプション B)を利用することで、us-east-1 リージョンと ap-southeast-1 リージョンの両方への低レイテンシ アクセスを提供する場所にコンテナ イメージを保存できます。このアプローチでは、最も近い GCR マルチリージョン エンドポイントからコンテナ イメージを直接プルできるため、帯域幅の使用率が最適化されます。イメージは複数のリージョンに冗長的に保存されるため、ビルドシステムや GKE クラスタの地理的な場所に関係なく、高速で信頼性の高いアクセスが保証されます。これにより、帯域幅の効率が最大化されるだけでなく、画像転送中の遅延も最小限に抑えられます。
他のオプションが最良の選択ではない理由:
プライベートDockerレジストリをap-southeast-1リージョンにデプロイします(オプションA)。ap-southeast-1 リージョンにプライベート Docker レジストリを設定すると、イメージのアクセス速度は向上しますが、リージョン間の帯域幅の使用が完全に最適化されない場合があります。
画像のコンテンツ配信ネットワーク (CDN) を実装します (オプション C)。CDNはコンテンツ配信を強化できますが、静的アセットの提供に適しており、コンテナイメージの配布に最も効率的なソリューションではない可能性があります。
リージョン間の直接ネットワークリンクを確立します(オプションD)。専用のネットワーク リンクを確立すると接続性が向上する可能性がありますが、複雑になる可能性があり、複数リージョンのコンテナー レジストリと同じレベルのスケーラビリティと冗長性を提供できない可能性があります。
要約すると、Google Container Registry のマルチリージョン機能を利用することで、異なるリージョン間でビルドシステムと GKE クラスタ間の効率的かつ最適化されたコンテナ イメージ転送が保証され、帯域幅の使用率が最大化され、レイテンシが最小限に抑えられます。
<details><div>

### Q. 問題38: 未回答
シナリオ：サイト信頼性エンジニアリング(SRE)チームの一員として、Google Cloud Platform(GCP)でホストされている重要なウェブアプリケーションの信頼性を維持する責任を負っています。役割の一環として、アプリケーションのパフォーマンスと信頼性を測定および監視するための効果的なサービスレベル指標(SLI)を確立する必要があります。Google の SRE のベスト プラクティスに合わせるには、選択した SLI が有意義で関連性のあるものであることを確認する必要があります。Google の SRE が推奨するベスト プラクティスに従って SLI を選択する場合、どうすればよいですか?
質問：ウェブ アプリケーションのパフォーマンスと信頼性を測定および監視するためのサービス レベル 指標(SLI)を選択する場合、Google の SRE が推奨するベスト プラクティスに沿ったアプローチはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:Google のサイト信頼性エンジニアリング(SRE)が推奨するベスト プラクティスに従ってサービス レベル 指標(SLI)を選択するための正しいアプローチは、オプション C: シンプルさと関連性を優先して、ユーザーに表示される成果と影響を直接反映する SLI に焦点を当てることです。
答え：
説明：Google の SRE の理念では、ユーザー エクスペリエンスと、システムの動作がユーザーに与える影響を重視しています。SLIを選択する際には、ユーザーに表示される結果に関連し、アプリケーションの信頼性とパフォーマンスを直接反映する指標を選択することが重要です。SLIは、有意義で、簡単に理解でき、ユーザーにとって最も重要なことに焦点を当てている必要があります。シンプルで関連性が高く、ユーザーエクスペリエンスに沿ったSLIを選択することで、ユーザー中心の観点からシステムの健全性を効果的に測定できます。
他のオプションが正しい選択ではない理由:
技術的に複雑なSLI(オプションA):精度は重要ですが、広範なデータ収集を必要とする技術的に複雑なSLIを選択すると、運用上のオーバーヘッドが増加し、ユーザーエクスペリエンスに必ずしも適合しない可能性があります。
影響を最小限に抑えた測定が容易なSLI(オプションB):SLIを測定の容易さのみに基づいて選択すると、ユーザーエクスペリエンスへの影響が最小限であっても、システムの信頼性に意味をなさない、または示さないメトリックを監視する可能性があります。
システム内部に関する高レベルの洞察(オプションD):システム内部を理解することは重要ですが、ユーザーエクスペリエンスと相関しない高レベルの分析情報のみに焦点を当てると、ユーザーの視点からアプリケーションの信頼性を明確に把握できない可能性があります。
結論として、Google の SRE は、SLI を選択するためのベスト プラクティスとして、ユーザーに表示される成果と影響を直接反映する SLI に焦点を当て、シンプルさと関連性を優先して、アプリケーションの信頼性を正確に評価することを提案しています。
<details><div>

### Q. 問題39: 未回答
シナリオ：さまざまなソースからデータを取り込み、変換を実行し、処理されたデータを Google BigQuery に書き込む複雑なデータ処理パイプラインを管理する必要があります。パイプラインは、組織の分析と分析情報を生成するために重要です。パイプラインの信頼性とパフォーマンスを確保するために、サービスレベル指標(SLI)を実装して、その動作を監視し、潜在的な問題を特定しました。ただし、SLI の 1 つがパイプラインのパフォーマンスを正確にキャプチャしていない可能性があります。
質問：上記のデータ処理パイプラインのシナリオのコンテキストで、次のサービスレベル指標(SLI)のうち、正しく定義されていない可能性があるのはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:データ処理パイプラインのシナリオのコンテキストでは、正しく定義されていない可能性があるサービス レベル インジケーター (SLI) は、オプション C: データ処理タスクを実行しているサーバーの平均 CPU 使用率です。
説明：SLIは、サービスまたはシステムの動作を反映する定量的な測定値です。これらは、サービスのパフォーマンス、可用性、および品質を監視するのに役立ちます。オプション C は関連するメトリックのように見えますが、次の理由により、データ処理パイプラインの正常性とパフォーマンスを正確に反映していない可能性があります。
ワークロードのばらつき:データ処理タスクのワークロードは、時間によって異なるため、CPU 使用率が変動します。平均では、さまざまな負荷条件下でのパイプラインのパフォーマンスを包括的に理解できない場合があります。
複数のコンポーネント:パイプラインは、複数のコンポーネントとステージで構成されている可能性が高く、それぞれに独自のリソース要件があります。平均 CPU 使用率のみに注目すると、すべてのコンポーネントのパフォーマンス特性を把握できない場合があります。
変換の複雑さ:データ変換には、CPU、メモリ、I/O 操作が混在して含まれる場合があります。CPU 使用率だけでは、パイプラインの全体的な効率と応答性を正確に表していない場合があります。
他のオプションがより適切である理由:
A. 平均エンドツーエンド遅延 (オプション A):このSLIは、データがパイプライン全体を通過するのにかかる時間を測定します。これにより、パイプラインの応答性と効率に関する分析情報が提供されます。
B. 正常に処理されたレコードの割合 (オプション B):このSLIは、データ処理におけるパイプラインの精度と信頼性を追跡します。これは、パイプラインの正確性を示す重要な尺度です。
D. 報告されたインシデントの数 (オプション D):この SLI は、パイプラインに関連して報告されたインシデントと問題をキャプチャします。インシデントを追跡すると、潜在的な信頼性の問題を特定するのに役立ちます。
結論として、データ処理パイプラインのSLIを定義するときは、パイプラインのパフォーマンス、可用性、品質を正確に表す指標を選択し、その複雑さと特定の特性を考慮することが重要です。
<details><div>

### Q. 問題40: 未回答
シナリオ：標準環境で Google App Engine にデプロイされたウェブアプリケーションのパフォーマンスを監視および最適化する責任があります。アプリケーションのリソース消費に関する洞察を得て、潜在的なパフォーマンスのボトルネックを特定するために、CPU 使用率データを収集することにしました。CPU 使用率の情報が正確にキャプチャされ、分析と最適化に利用できるようにする必要があります。
質問：Google App Engine スタンダード環境にデプロイされたアプリケーションの CPU 使用率データを収集し、リソース消費量を正確に測定するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：B
説明：
CPU プロファイリングの構成: App Engine アプリケーションの構成ファイルを変更して、CPU プロファイリングを有効にします。これには、CPU 使用率の追跡をアクティブにするために必要な設定を指定することが含まれます。app.yamlapp.yaml
組み込みの App Engine ログ:App Engine スタンダード環境には、組み込みのログ機能が用意されています。CPU プロファイリングを有効にすると、App Engine のログに CPU 使用率データが自動的に取得され、アプリケーションのランタイム ログの一部として記録されます。
他のオプションが正しくない理由:
A. カスタムログステートメントを実装します。カスタム ロギング ステートメントで CPU 使用率データを取得できますが、カスタムログのみに頼っていると、特に App Engine のような動的で分散的な環境では、CPU 消費量に関する正確で包括的な分析情報が得られない可能性があります。
C. サードパーティの監視エージェントを展開します。App Engine には、パフォーマンスのモニタリングとプロファイリングのためのネイティブ ツールと機能が用意されています。サードパーティのエージェントに依存すると、複雑さが増し、GCP エコシステム内でのモニタリングのベスト プラクティスと一致しない可能性があります。
D. Cloud Profiler API を有効にします。Cloud Profiler API を有効にすることは、アプリケーションのパフォーマンスデータをキャプチャするための有効なオプションですが、このシナリオでは、Google Cloud Profiler サービスへの言及を明示的に避けています。代わりに、組み込みの App Engine 機能の利用に重点を置いています。
結論として、オプション B は、Google App Engine スタンダード環境にデプロイされたアプリケーションの CPU 使用率データを収集し、ファイルを構成し、組み込みの App Engine ログを CPU 使用率分析に利用することで、リソース消費を正確に測定するための正しいアプローチを概説しています。app.yaml
<details><div>

### Q. 問題41: 未回答
シナリオ：Google App Engine の標準環境にデプロイされたウェブ アプリケーションを管理しており、そのパフォーマンスを最適化したい。Google Cloud Profiler を使用して詳細な実行データを取得することにしました。コードにプロファイラーを実装するときは、正しいモジュールをインポートし、適切なメソッドを使用してプロファイラーを起動する必要があります。
質問：Google Cloud Profiler を起動し、Google App Engine の標準環境にデプロイされたウェブアプリケーションで詳細な実行データを取得するには、どのモジュールをインポートし、どの方法を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：B
説明：
モジュールのインポート:このモジュールは、Google Cloud Profiler と対話するために必要な関数とメソッドを提供します。google.cloud.profiler
メソッドの使用 : Google Cloud Profiler を起動するには、インポートしたモジュールのメソッドを使用する必要があります。このメソッドは、プロファイリング プロセスを開始し、詳細な実行データのキャプチャを開始します。start()start()
他のオプションが正しい選択ではない理由:
A. インポート モジュール: Google Cloud が提供するモジュールはありません。正しいモジュールは です。google-profilergoogle-profilergoogle.cloud.profiler
C. インポート モジュール: 同様に、Google Cloud Profiler には呼び出されるモジュールはありません。正しいモジュールは です。cloud-profilercloud-profilergoogle.cloud.profiler
D. モジュールのインポート : このモジュール構造は正確ではなく、正しいモジュールにはメソッドがありません。profiler.cloud.googlebegin()google.cloud.profiler
結論として、Google Cloud Profiler を起動し、Google App Engine の標準環境にデプロイされたウェブアプリケーションで詳細な実行データを取得するには、モジュールをインポートしてメソッドを使用する必要があります。google.cloud.profilerstart()
<details><div>

### Q. 問題42: 未回答
シナリオ：Google Cloud Platform(GCP)でマイクロサービスベースのアプリケーションを管理する責任があり、異なるサービス間のリクエストフローを追跡する機能を向上させたいと考えています。Google Cloud Trace を使用して、マイクロサービス間の相互作用とレイテンシに関する分析情報を取得することにしました。
質問：リクエストフローを追跡し、アプリケーション内のマイクロサービス間のレイテンシを分析するように Google Cloud Trace を構成します。これを達成するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：B
説明：
Google Cloud Trace API:GCP プロジェクトで Google Cloud Trace API を有効にする必要があります。これにより、要求フローと待機時間のトレースを収集して分析できます。
コードをインストゥルメントする:リクエストを自動的にトレースし、レイテンシ データを取得するには、Google Cloud Trace が提供する適切なクライアント ライブラリを使用してコードをインストルメント化する必要があります。これらのライブラリを使用すると、アプリケーションのさまざまな部分とマイクロサービス間の相互作用を表すトレースとスパンデータを作成できます。
自動トレース:コードがインストゥルメントされると、Google Cloud Trace はマイクロサービスを通過するリクエストのトレースを自動的にキャプチャします。これらのトレースには、待機時間情報と、さまざまなサービスによって要求が処理される順序が表示されます。
ログステートメント (オプション A):手動ログステートメントを追加すると、アプリケーションの特定の側面を追跡するのに役立つ場合がありますが、詳細なリクエストフローとレイテンシーデータをキャプチャするのに最適な方法ではありません。Google Cloud Trace は、より自動化された正確なトレース機能を提供します。
第三者による監視(オプションC):サードパーティのモニタリング ツールでも同様の機能が提供される場合がありますが、シームレスな統合と効率的な分析のためには、GCP エコシステム内で Google Cloud Trace を直接使用することをおすすめします。
カスタムメトリクス(オプションD):Google Cloud Monitoring のカスタム指標は、アプリケーションのさまざまな側面をモニタリングするのに役立ちますが、Google Cloud Trace が提供する詳細なリクエスト フローとレイテンシ データをキャプチャして視覚化するために特別に設計されたものではありません。
要約すると、リクエストフローを追跡し、アプリケーション内のマイクロサービス間のレイテンシを分析するには、Google Cloud Trace API を有効にし、自動トレース用の適切なライブラリを使用してコードをインストルメント化する必要があります。
<details><div>

### Q. 問題43: 未回答
シナリオ：組織内の Linux サーバーでホストされているアプリケーションのパフォーマンスを監視する責任があります。詳細なアプリケーション固有のパフォーマンス メトリックを収集するために、Collectd 監視エージェントを使用することにしました。アプリケーションは Apache Web サーバー上で実行されています。目標は、これらのメトリックを正確にキャプチャするように Collectd エージェントをインストールして構成することです。
質問：Apache Webサーバーを備えたLinuxサーバーにCollectdエージェントをインストールして構成し、アプリケーション固有のパフォーマンスメトリックをキャプチャするには、どのような手順に従う必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある
説明：
Collectdをインストールします。ディストリビューションに適したパッケージマネージャーを使用して、LinuxサーバーにCollectdパッケージをインストールします。
設定 : 設定ファイルを変更して、Apache メトリクスをキャプチャするための設定を含めます。アプリケーションのパフォーマンスに関連する目的のメトリックを指定します。この構成には、プラグインの有効化とその設定のカスタマイズが含まれる場合があります。collectd.confcollectd.confapache
collectdサービスを再起動します。ファイルを構成したら、Collectd サービスを再起動して変更を適用します。これにより、構成で定義されている Apache メトリック収集がアクティブ化されます。collectd.conf
オプションB:ソースから Collectd をコンパイルすることは、標準インストールでは必要ではなく、不必要に複雑になる可能性があります。
オプションC:Apache Exporter プラグインは、標準の Collectd 構成の一部ではありません。さらに、Apache Webサーバーを再起動しても、Collectdメトリックはアクティブになりません。Collectd サービスを再起動する必要があります。
オプションD:プラグイン構成ファイルの編集は有効ですが、Collectd メトリックを有効にするために Apache Web サーバーを再起動する必要はありません。代わりに、Collectd サービスを再起動して、メトリックが正確に収集および報告されるようにする必要があります。apache.conf
結論として、Apache Web サーバーを備えた Linux サーバーに Collectd エージェントをインストールして構成し、アプリケーション固有のパフォーマンス・メトリックをキャプチャーするには、Collectd パッケージをインストールし、構成ファイルを変更して Apache メトリックを指定してから、Collectd サービスを再起動する必要があります。collectd.conf
<details><div>

### Q. 問題44: 未回答
アプリケーション開発ワークフローの継続的インテグレーション (CI) パイプラインを設定しています。このパイプラインの一環として、アプリケーションのソースコードから生成されたコンテナイメージが Google Container Registry(GCR)にプッシュされ、他のビルドアーティファクトが Google Cloud Storage(GCS)に保存されるようにする必要があります。このイメージとアーティファクトのストレージの分離を実現するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:正解はオプションCです
説明：Google Cloud Build は、Google Container Registry(GCR)および Google Cloud Storage(GCS)とシームレスに統合して、ビルド出力を保存します。Cloud Build を使用して CI パイプラインを構成することで、カスタム スクリプトや複雑な設定を必要とせずに、コンテナ イメージとその他のビルド アーティファクトを簡単に分離できます。
オプションA:コマンドライン ツールを使用してコンテナ イメージを GCR にプッシュしたり、Cloud Storage クライアント ライブラリを使用してビルド アーティファクトを GCS にアップロードしたりできますが、Cloud Build を使用すると、この分離をより合理的で宣言的な方法で実現できます。gcloud
オプションB:ビルドの種類を決定するカスタム シェル スクリプトを設定し、その決定に基づいてさまざまなツールを使用すると、複雑さとメンテナンスのオーバーヘッドが発生する可能性があります。Cloud Build は、このシナリオを処理するためのより統合されたアプローチを提供します。
オプションD:Jenkins などのサードパーティの CI ツールを使用できますが、保存先ごとに個別のステージとプラグインを構成するには、追加の設定が必要になる場合があり、Cloud Build の組み込み機能を使用する場合に比べて複雑になる可能性があります。
結論として、Cloud Build を使用して CI パイプラインを構成し、ビルド構成で と セクションを指定することは、GCR と GCS の間でコンテナ イメージとその他のビルド アーティファクトを分離するための最も効果的で簡単な方法です。imagesartifacts
オプションCでこれを実現する方法:
Cloud Build 設定ファイルを設定します。ソースコードリポジトリには、アプリケーションのビルド方法とビルドプロセス中に何が起こるかを定義する構成ファイルがあります。cloudbuild.yaml
と セクションの指定: ファイル内で、 と の個別のセクションを定義できます。各セクションは、対応するビルド出力の宛先を指定します。imagesartifactscloudbuild.yamlimagesartifacts
steps:
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/your-project-id/your-image-name', '.']
images:
  - 'gcr.io/your-project-id/your-image-name'
artifacts:
  objects:
    location: 'gs://your-gcs-bucket/artifacts/'
この例では、ソースコードからビルドされたコンテナ イメージにタグが付けられ、指定したイメージ名を使用して GCR にプッシュされます。さらに、ビルド プロセス中に生成されたビルド アーティファクトは、指定された GCS バケットに格納されます。
ビルドとデプロイ:Cloud Build を使用してビルドをトリガーすると、構成ファイルが読み取られ、表示される手順に沿って操作されます。コンテナ イメージがビルドされ、セクションで指定されているように GCR にプッシュされます。また、セクションで指定されているように、他のビルド成果物を GCS バケットにアップロードします。cloudbuild.yamlimagesartifacts
利点：
簡略：Cloud Build は、コンテナ イメージを GCR にプッシュし、アーティファクトを GCS にアップロードする詳細を処理します。構成ファイルで宛先を指定するだけで済みます。
統合：Cloud Build は他の Google Cloud サービスとシームレスに統合され、デプロイ ワークフローのさまざまな部分間の円滑なコミュニケーションを確保します。
宣言型アプローチ:構成ファイルを使用すると、ビルド プロセスを宣言的な方法で定義できるため、理解と保守が容易になります。
Cloud Build と構成ファイルのセクションを使用することで、コンテナ イメージとその他のビルド アーティファクトを効果的に分離し、アプリケーションのビルド出力を明確かつ効率的に管理、保存できます。imagesartifactscloudbuild.yaml
<details><div>

### Q. 問題45: 未回答
あなたは、重要なWebアプリケーションの信頼性と可用性を担当するチームを率いています。インシデント対応を改善し、Google のサイト信頼性エンジニアリング(SRE)プラクティスとの整合性を確保するには、インシデント管理手順を作成します。Google の SRE ガイドラインに従ってこの手順を開発する際に必要でないオプションはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：C
説明：Google のサイト信頼性エンジニアリング(SRE)は、迅速な復旧と継続的な改善を実現するための効果的なインシデント管理に重点を置いています。オプション A、B、D は SRE の原則に従ったインシデント管理手順の重要な要素ですが、オプション C はいくつかの理由から推奨されるアプローチではありません。
タイムリーなコミュニケーション:インシデント対応シナリオでは、電子メール通知によって通信に遅延が生じる可能性があります。インシデントには、多くの場合、対応者間の即時の注意と調整が必要です。メール通知を待っていると、迅速な対応が妨げられる可能性があります。
リアルタイムコラボレーション:SRE は、リアルタイムのコラボレーションとインシデントへの迅速な対応を促進します。電子メール通知を使用すると、インシデント対応中に必要な即時のリアルタイムの対話のレベルが提供されない場合があります。
自動化とアラート:Google の SRE は、インシデント対応者に迅速に通知できる自動アラート システムを提唱しています。これらのシステムは、テキストメッセージ、Slack、その他のメッセージングツールなど、複数のチャネルを介してアラートを送信できます。メール通知だけでは、必要なレベルの可視性と緊急性が得られない可能性があります。
有利なコミュニケーションチャネル:SRE は、コラボレーションと迅速な情報交換に役立つコミュニケーション チャネルの使用を推奨しています。電子メールは一般的なコミュニケーション ツールですが、インシデント対応は多くの場合、よりインタラクティブで応答性の高いプラットフォームの恩恵を受けます。
したがって、Google の SRE によると、役割の定義、インシデント後の分析の文書化、インシデント対応チームの設立はすべて、効果的なインシデント管理手順の重要な要素ですが、インシデント指揮官への通知をメール通知のみに頼ることは、リアルタイムのコミュニケーション、コラボレーション、自動化の重視とは一致しません。
<details><div>

### Q. 問題46: 未回答
お客様は、Google Cloud Platform(GCP)でホストされている重要なアプリケーションの運用上の信頼性に責任を負います。モニタリング戦略の一環として、Stackdriver Monitoring で事前定義された特定の条件が満たされたときに SMS 通知を受け取る必要があります。
SMS通知を設定するには、どのような手順に従う必要がありますか?最も適切なオプションを選択してください。
1. 
2. 
3. 
4. 
<details><div>
答え：D
説明：
オプションA:このオプションを使用すると、SMS、E メール、その他の種類の通知など、コンソールから直接通知チャネルを設定できます。このオプションは、追加の統合なしでSMS通知を設定する有効な方法です。
オプションB:サードパーティの SMS サービス プロバイダを GCP サービスに統合することは可能ですが、Stackdriver Monitoring では、サードパーティの統合に依存することなく、SMS などの通知を設定するためのネイティブ サポートが提供されます。
オプションC:アラートをフェッチしてSMSメッセージを送信するカスタムスクリプトを作成することは可能ですが、SMS通知を設定するための推奨または最も効率的な方法ではありません。Stackdriver Monitoring は、設定と管理が簡単なネイティブ通知機能を提供します。
オプションD:これはすべての中で最良のオプションです。このオプションには、いくつかの利点を提供するより洗練されたアプローチが含まれます。
柔軟性：Pub/Sub を仲介として使用することで、アラートを通知メカニズムから切り離すことができます。つまり、通知メカニズムを他のサービスに簡単に拡張したり、アラートの処理方法をカスタマイズしたりすることもできます。
拡張性:Cloud Functions はサーバーレスであり、受信アラートを処理するように自動的にスケーリングされるため、アラート量が多い場合でも SMS 通知が確実に送信されます。
カスタマイズ：Cloud Functions を使用すると、アラートを転送するためのカスタム ロジックを柔軟に実装できます。たとえば、アラートを集約したり、フィルターを適用したり、特定の API を使用してサードパーティの SMS プロバイダーと統合したりできます。
統合：サービス デスクやインシデント管理プラットフォームなどの外部システムと統合する必要がある場合は、Cloud Functions を使用すると、統合プロセスをより詳細に制御できます。
要約すると、オプションDは、監視のニーズに合わせて拡張できる、より堅牢で適応性の高いソリューションを提供し、より優れたカスタマイズと統合機能を可能にします。オプション A は単純なセットアップに適しているかもしれませんが、オプション D にはスケーラビリティ、カスタマイズ、統合という利点があるため、Stackdriver Monitoring で SMS 通知を設定する場合に適しています。
<details><div>

### Q. 問題47: 未回答
チームは、Google Cloud Platform(GCP)の内外にデプロイされる新しいアプリケーションの設計に取り組んでいます。信頼性の高いパフォーマンスと効果的なトラブルシューティングを確保する一環として、アプリケーションのシステムリソース使用率などの詳細なメトリックを収集する一元化されたシステムを実装する必要があります。ただし、このメトリック収集システムの設定と保守に必要な労力も最小限に抑える必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
答え：
ウ. モニタリング エージェントを GCP インスタンスにのみデプロイし、Cloud Monitoring の外部モニタリング機能を使用して GCP 以外のサーバーから指標を収集することで、ハイブリッド アプローチを実装します。
説明：
オプションAでは、GCPの内部と外部の両方で、各サーバーインスタンスにスタンドアロンの監視エージェントをデプロイすることを提案しています。これはうまくいくかもしれませんが、環境間で異なる監視エージェントを管理および維持するのは複雑になる可能性があります。
オプションBでは、GCPインスタンスでGoogle Cloud Monitoringエージェントを使用し、外部サーバーでカスタムスクリプトを設定することを提案しています。このアプローチでは、余分な労力と外部サーバー用のカスタムスクリプトが必要であり、効率的ではない可能性があります。
オプション D では、外部サーバー用に別のサードパーティ監視ツールを使用することが推奨されていますが、これにより不必要な複雑さが生じ、ユーザー インターフェイスが異なる可能性があります。
オプション E では、GCP インスタンスと非 GCP インスタンスの両方について、Cloud Monitoring の外部モニタリング機能のみに依存することを提案しています。これにより、監視を一元化できますが、ハイブリッドアプローチほど効率的ではない可能性があります。
オプションCは、ハイブリッドアプローチを実装することで、バランスが取れています。モニタリング エージェントを GCP インスタンスにのみデプロイすることで、GCP リソースに対する Google Cloud Monitoring の組み込み機能を活用できます。GCP 以外のサーバーでは、Cloud Monitoring の外部モニタリング機能を使用して、個別のエージェントを必要とせずに外部サーバーから指標を収集できます。このアプローチは、複雑さと労力を最小限に抑えながら、メトリックの収集を一元化するのに役立ちます。
要約すると、オプションCは、GCPのネイティブモニタリング機能を活用し、GCPインスタンスと非GCPインスタンスの両方の指標収集を効率的に処理する実用的なアプローチを提供します。
<details><div>

### Q. 問題48: 未回答
あなたは、Google Cloud Platform(GCP)でホストされているウェブアプリケーションの保守を担当するDevOpsチームの一員です。最近、アプリケーションの要求応答時間のサービス レベル インジケーター (SLI) が、定義されたサービス レベル目標 (SLO) を常に下回っており、一部のユーザー要求がタイムアウトする原因になっていることを確認しました。このパフォーマンスの低下はユーザーエクスペリエンスに悪影響を及ぼしており、チームは状況を改善するための最適なソリューションを探しています。
Webアプリケーションの応答時間を向上させ、SLIをSLOに合わせるには、どのアプローチが最も効果的ですか?
1. 
2. 
3. 
4. 
<details><div>
答え： D
説明：
このシナリオでは、要求応答時間の SLI が常に SLO を下回っているため、ユーザー要求がタイムアウトします。Web アプリケーションのパフォーマンスを向上させ、SLI を SLO に合わせるには、次の理由から、異なるアベイラビリティーゾーンのインスタンスで自動スケーリングを構成するのが最も効果的なアプローチです。
可用性の向上: 異なるアベイラビリティーゾーンに追加のレプリカインスタンスを作成するように自動スケーリングを構成すると、アプリケーションの高可用性が確保されます。1 つのアベイラビリティーゾーンで問題が発生した場合、トラフィックを他のゾーンの正常なインスタンスに再ルーティングして、リクエストタイムアウトのリスクを軽減できます。
分散負荷: 自動スケーリングを使用すると、アプリケーションはインスタンスの数を動的に調整することで、さまざまなレベルのトラフィックを処理できます。複数のアベイラビリティーゾーンに負荷を分散すると、トラフィックが均等に分散され、個々のゾーンが過負荷になるのを防ぐことができます。
レイテンシーの短縮: 異なるアベイラビリティーゾーンのインスタンスは、ユーザーのリクエストにより迅速に応答できるため、レイテンシーが短縮され、応答時間が長くなり、ユーザーエクスペリエンスが向上します。
他のオプション (A、B、C) はパフォーマンスの最適化に寄与する可能性がありますが、この特定のシナリオでは、異なる可用性ゾーンに追加のレプリカ インスタンスを作成するように自動スケーリングを構成することが、アプリケーションの応答時間を向上させ、要求のタイムアウトを減らし、SLI を SLO に合わせるための最適なソリューションです。
<details><div>

### Q. 問題49: 未回答
サイト信頼性エンジニア (SRE) は、組織内の重要なアプリケーションの可用性と信頼性を維持する責任があります。最近、ダウンタイムが発生し、ユーザーエクスペリエンスが損なわれる重大なインシデントが発生しました。責任の一環として、インシデントを分析し、根本原因を特定し、今後同様のインシデントを防ぐための是正措置を提案するための事後分析レポートを作成する任務があります。
事後分析レポートに含めるべきアクションは、次のうちどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：B
説明：
事後分析レポートは、インシデントを理解し、そこから学び、将来の再発を防ぐための重要なツールです。包括的な事後分析レポートを作成する際には、ベストプラクティスに従うことが重要です。
詳細なインシデント タイムライン: 事後分析レポートに詳細なタイムラインを含めることが重要です。このタイムラインには、インシデントに至るまでのイベント、インシデント自体、対応の取り組み、および解決をカバーする必要があります。明確なタイムラインは、インシデントのコンテキストとその影響を全員が理解するのに役立ちます。
他のオプション (A、C、D、E) は、事後分析レポートを作成するためのベスト プラクティスと一致していません。
オプションA:責任の所在の割り当ては逆効果であり、Site Reliability Engineeringが推進する協力的で誰も責めない文化に反します。システムやプロセスを理解することに重点を置くべきであり、責任転嫁すべきではありません。
選択肢C:懲罰的措置の提案は、学習と改善の目標に貢献しない。根本原因を特定し、是正措置を実施することに重点を置く必要があります。
オプション D: 技術的な詳細を省略すると、インシデントの徹底的な分析を行う能力が妨げられます。何が起こったのか、なぜ起こったのかを理解するには、技術的な詳細が不可欠です。
オプション E: インシデントの影響を文書化することは重要ですが、それは事後分析の 1 つの側面にすぎません。また、根本的な原因を理解し、再発を防ぐための解決策を提案することにも重点を置く必要があります。
結論として、詳細なインシデントタイムラインは、事後分析レポートの重要な要素です。インシデントのコンテキスト、要因、対応作業の有効性に関する分析情報を提供し、チームがシステムの信頼性を学習して改善できるようにします。
<details><div>

### Q. 問題50: 未回答
Google Kubernetes Engine(GKE)クラスタにデプロイされた複雑なマイクロサービスベースのアプリケーションを管理している。各マイクロサービスは、クラスター内で個別のポッドとして実行されています。ログ収集を効率化し、アプリケーション全体のパフォーマンスに関する分析情報を得るには、マイクロサービスを実行しているすべてのポッドからログを収集するように Fluentd を DaemonSet として構成する必要があります。これを実現するためにFluentdをどのように設定できますか?
1. 
2. 
3. 
4. 
<details><div>
答え： B
説明：
マイクロサービスを実行しているすべてのポッドからログを収集するために Fluentd を DaemonSet として構成するには、Fluentd DaemonSet 構成ファイルを編集してマイクロサービス ポッドの名前空間を指定する必要があります。オプション A が正しい選択である理由は次のとおりです。
名前空間の分離: Fluentd 構成でマイクロサービス ポッドの名前空間を指定することで、ログが目的の名前空間からのみ収集されるようにすることができます。この分離により、他の名前空間のログに干渉されることなく、関連するログを収集できます。
一元的な収集: 特定の名前空間からログを収集するように Fluentd を構成すると、一元化されたログ収集メカニズムが提供されます。これにより、これらの名前空間内のすべてのポッドからログが収集され、分析のために適切な宛先に送信されます。
シンプルさ: 名前空間固有のログ収集を含めるように Fluentd 構成を編集するのは簡単な方法です。複数の DaemonSet をデプロイしたり、マイクロサービスのコードを変更したりする必要はありません。
スケーラビリティ: 指定された名前空間に新しいマイクロサービスがデプロイされると、構成された Fluentd DaemonSet は、新しいポッドからのログの収集を自動的に開始します。
オプション B、C、および D は、このシナリオでは推奨されません。
オプション B: 新しい Kubernetes ServiceAccount を作成して Fluentd DaemonSet に関連付けることは、特定の名前空間からログを収集するように Fluentd を構成することとは直接関係ありません。
オプション C: マイクロサービス ポッドごとに個別の Fluentd DaemonSet をデプロイすることは実用的ではなく、不必要な複雑さとリソース消費につながる可能性があります。
オプション D: GKE クラスタ ノードから直接ログをスクレイピングするように Fluentd を構成すると、ログの収集と管理のための Kubernetes の組み込みメカニズムがバイパスされます。
要約すると、Fluentd DaemonSet 構成ファイルを編集してマイクロサービス Pod の名前空間を指定することは、GKE クラスタ内でマイクロサービスを実行しているすべてのポッドからログを収集するように Fluentd を構成するための正しいアプローチです。
これを達成するために必要な手順。
Fluentd DaemonSet 構成ファイルを編集して、マイクロサービスポッドの名前空間を指定します。
Fluentd DaemonSet 構成ファイル () を見つけて編集します。fluentd-daemonset.yaml
構成ファイルで、収集するログとその送信先を定義するセクションを見つけます。
セクションを追加または変更して、ログを収集するマイクロサービス ポッドの名前空間を指定します。例えば：selector
selector:
  matchLabels:
    k8s-app: fluentd
    namespace: my-microservices-namespace
<details><div>

### Q. 問題51: 未回答
Google Cloud DevOps エンジニアとして、グローバルな金融機関向けのデータ処理パイプラインの設計と実装を担当します。このパイプラインは、機密性の高い顧客トランザクション データを処理し、厳格なコンプライアンス要件を満たす必要があります。システムは、データの損失や破損に対する堅牢な保護を提供する必要があります。適切な監視と測定を確立するには、パイプラインの信頼性とデータの整合性を確保するために、どのサービスレベル指標(SLI)に焦点を当てる必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：B) 可用性SLI
説明：
可用性 SLIは、グローバルな金融機関のデータ処理パイプラインにおけるデータの整合性と信頼性を確保するための最も適切な指標です。高可用性とは、システムが稼働していることを意味し、システム障害によるデータの損失や破損のリスクを軽減します。これは、機密性の高い顧客取引データの継続的かつ信頼性の高い処理を確保し、厳格なコンプライアンス要件を満たすために重要です。
他のオプションが正しくない理由:
A) レイテンシーSLI:
レイテンシーSLIは、データがパイプラインを通過するのにかかる時間を測定します。低レイテンシーは重要ですが、データの整合性や耐久性ではなく、主に速度と応答性に重点を置いています。データ保護とコンプライアンスのニーズに直接対応しているわけではありません。
C) スループット SLI:
スループット SLI は、処理された量の観点から、データを処理するパイプラインの容量を評価します。スループットは重要ですが、データの整合性や信頼性に直接対処するものではありません。重要なのは、効率と容量です。
d)コスト効率SLI:
コスト効率 SLIは、リソース利用の効率を測定します。コスト効率は不可欠ですが、データの整合性と信頼性を確保するための主要な指標ではありません。機密データの保護やコンプライアンスの確保よりも、コストの最適化に重点を置いています。
<details><div>

### Q. 問題52: 未回答
Google Cloud DevOps のシナリオでは、オンプレミスと複数のクラウド プロバイダにまたがる複雑で多様なアプリケーション ランドスケープを持つ企業は、さまざまな環境間でシームレスにデプロイ、監視、スケーリングできる、アプリケーションの一貫性のある統一された管理アプローチを実現したいと考えています。この課題に効果的に対処するには、どの Google Cloud ソリューションが推奨されますか?
1. 
2. 
3. 
4. 
<details><div>
答え：D)グーグルアンソス
説明：
Google Anthosは、同社のニーズに推奨されるソリューションです。Anthos は、オンプレミスや複数のクラウド プロバイダなど、さまざまな環境でアプリケーションを一貫して管理、デプロイできるプラットフォームを提供します。Google Kubernetes Engine(GKE)と Istio を活用して、アプリケーション管理のための統一されたアプローチを提供し、ハイブリッドおよびマルチクラウド機能を実現します。
他のオプションが正しくない理由:
A)Google Kubernetes Engine(GKE):
GKE は Kubernetes クラスタとコンテナ化されたアプリケーションを管理するための優れたソリューションですが、多様な環境でアプリケーションを管理するための統一されたアプローチの要件に直接対応しているわけではありません。GKE 上に構築された Anthos は、この目標を達成するためにこれらの機能を拡張します。
B) Google Cloud Functions:
Google Cloud Functions は、イベントドリブン関数向けに設計されたサーバーレス コンピューティング サービスで、主に軽量で単一目的の関数に使用されます。さまざまな環境で複雑なアプリケーションを管理するのには適していません。
C)Google Cloud Dataprep:
Google Cloud Dataprep は、データの準備とクリーニングのサービスです。これは、Google Anthos の焦点である、異なる環境間で一貫したアプリケーション管理を実現するというシナリオの要件とは関係ありません。
<details><div>

### Q. 問題53: 未回答
Docker イメージを Google Container Registry(GCR)にデプロイし、非コンテナ アーティファクトを Google Cloud Storage にデプロイする Google Cloud プロジェクトの CI / CD パイプラインを構成する必要があります。この構成を効率的に実現するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
C) 特定の構成を行うことで、CI/CD パイプラインを強化します。Cloud Build 構成ファイルに「images」フィールドを追加して、Google Container Registry にプッシュする Docker イメージを指定します。さらに、「artifacts」フィールドを含めて、Google Cloud Storage に保存するコンテナ以外のアーティファクトを指定します。
説明：
オプション C は、CI/CD パイプライン内で Docker イメージと非コンテナー成果物の両方を処理するための構造化された保守可能な構成を提供するため、推奨されるアプローチです。Cloud Build 構成ファイルで「イメージ」と「アーティファクト」に別々のフィールドを指定することで、デプロイ プロセスの管理を明確かつ効率的に行うことができます。
他のオプションが正しくない理由:
A) Docker イメージ用と非コンテナー アーティファクト用に 1 つずつ、個別の CI/CD パイプラインを実装します。
このオプションを使用すると、複雑さが増し、作業が重複し、Docker イメージと非コンテナー成果物の CI/CD パイプラインを個別に管理する際の課題が生じる可能性があります。
B)Dockerイメージと非コンテナアーティファクトのデプロイを処理するカスタムデプロイスクリプトを開発します。
このアプローチは実現可能ですが、複雑さが増し、特に CI/CD パイプラインが進化するにつれて、時間の経過とともに維持が困難になる可能性があります。
D)Google Cloud Functionsを利用して、Dockerイメージと非コンテナアーティファクトの両方のデプロイプロセスを管理します。
Google Cloud Functions はさまざまな目的に使用できますが、両方のタイプのアーティファクトのデプロイを管理するための最も簡単なソリューションではない可能性があり、オプション C のアプローチと同じレベルの構成と制御は提供されません。
<details><div>

### Q. 問題54: 未回答
Google Cloud 上のソフトウェア プロジェクトの CI / CD パイプラインを確立する責任があります。目的は、コードの変更がバージョン管理リポジトリ内の特定のブランチにプッシュされるたびに、ビルドとデプロイのプロセスを自動化することです。このレベルの自動化と応答性を実現するには、どのようなアプローチが推奨されますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
C)Cloud Build 構成ファイルをプロジェクト リポジトリに直接統合し、Google Cloud Build 内にトリガーを作成します。目的のブランチのイベントトリガーを「ブランチにプッシュ」として指定します。
説明：
オプション C は、高度な自動化と応答性を提供するため、推奨される選択肢です。プロジェクト リポジトリ内に Cloud Build 構成ファイルを含め、Google Cloud Build で「ブランチにプッシュ」イベントを指定したトリガーを作成することで、CI / CD パイプラインが自動化され、コードの変更に迅速に応答します。このアプローチにより、展開プロセスの効率と信頼性が向上します。
他のオプションが正しくない理由:
A)コードの変更がブランチにプッシュされるたびにビルドを手動でトリガーします。
オプション A は、効率的な CI/CD パイプラインには適さない手動の非自動化アプローチを提案します。手動トリガーは、遅延や自動化の低下につながり、コード変更に対する応答性が低下する可能性があります。
B) サードパーティの CI/CD ツールを検討すると、多くの場合、より広範なカスタマイズと自動化機能が提供されます。
サードパーティのツールは価値がありますが、このシナリオの主な焦点ではありません。Google Cloud Build は、Google Cloud サービスと緊密に統合された CI/CD のネイティブ ソリューションを提供し、堅牢な自動化を可能にします。
D) 一定の間隔でスケジュールされた定期的な時間ベースのビルドを設定して、コードの変更が一貫してビルドおよびデプロイされるようにします。
オプション D は、コードの変更に対する応答性が低い、スケジュールされた時間ベースのビルドを提案します。変更が頻繁に発生しない場合、不要なビルドが発生し、説明されているシナリオの効率が低下する可能性があります。
<details><div>

### Q. 問題55: 未回答
複雑な Google Cloud Platform(GCP)アーキテクチャにおいて、ある多国籍企業はソフトウェア開発にマイクロサービスベースのアプローチを採用しています。異なるプロジェクトで複数のクラウドソースリポジトリを保持し、それぞれが特定のマイクロサービスのコードを格納しています。開発者のチームは、これらのマイクロサービスの保守を担当し、最小特権の原則を遵守しながら、それぞれのリポジトリにコードをコミットするアクセス許可が必要です。
デベロッパーが他のリソースへのアクセスを最小限に抑えながら、適切なクラウドソース リポジトリにコードをコミットできるようにするには、どの権限とロールのセットを構成する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
複雑な GCP アーキテクチャで最小権限の原則を遵守しながら、デベロッパーが関連する Cloud Source Repositories にコードをコミットできるように権限を設定する正しいアプローチは次のとおりです。
D)プロジェクトレベルで開発者に「ソースリポジトリライター」ロールを割り当て、IAM条件を使用してアクセスする必要があるリポジトリを指定します。
説明：
オプション D は、リポジトリにコードをコントリビュートするために必要なアクセス許可を開発者に付与する、定義済みの "ソース リポジトリ ライター" ロールを使用するため、推奨される選択肢です。最小限の権限を確保するために、IAM 条件を使用してアクセスが必要なリポジトリをさらに指定します。このアプローチは、アクセス制御とシンプルさのバランスを取ります。
他のオプションが正しくない理由:
A) リポジトリが存在するプロジェクト レベルで各開発者に "編集者" ロールを付与し、リポジトリへのアクセスを "ライター" ロールで提供します。
オプションAは、広範な「編集者」ロールを付与し、過剰な権限を与え、最小権限の原則に違反する可能性があります。
B) "source.repos.update" と "source.repos.get" のアクセス許可を含むカスタム IAM ロールを作成し、このロールをプロジェクト レベルで開発者に割り当てます。
オプションBはカスタムロールを作成しますが、リポジトリレベルのアクセスを指定していないため、意図しないアクセスや特定のリポジトリへのアクセスの欠如につながる可能性があります。
C)Google Cloud Identity and Access Management(IAM)条件を使用して、特定のリポジトリとデベロッパーのメールアドレスに基づいて権限を付与します。
オプションCは、IAM条件と開発者のメールアドレスに依存することで複雑さが増し、組織やリポジトリの数が増えるにつれて管理が困難になる可能性があります。また、IAM条件(オプションD)と組み合わされた「ソースリポジトリライター」ロールの単純さと特異性に欠ける場合もあります。

## 2
### Q. 問題1: 未回答
サイト信頼性エンジニア (SRE) は、多数のユーザーが使用する重要な Web アプリケーションの信頼性と可用性を維持する責任があります。最近のインシデントでは、アプリケーションで大規模な停止が発生し、ユーザーアクセスに影響を与え、サービスの中断を引き起こしました。サイト信頼性エンジニアリングの推奨事項に従って、この重大なインシデントを報告する正しい方法は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. タイムライン、要因、軽減手順、提案されたフォローアップ アクションの徹底的な分析を含む、事後分析レポートにインシデントを文書化します。
説明：
特定のシナリオでは、サイト信頼性エンジニアリング (SRE) の推奨事項に従って、サイト信頼性エンジニア (SRE) が、ユーザー アクセスに影響を与え、サービスの中断を引き起こした重大な停止を経験した後に重大なインシデントを報告する正しい方法は、インシデントを事後分析レポートに文書化することです。
オプションCが正しい選択である理由は次のとおりです。
事後分析レポート: サイト信頼性エンジニアリングは、インシデントから学習して将来の発生を防ぐ文化を強調しています。事後分析レポートは、原因、影響、修復手順など、インシデントの詳細な分析です。何が悪かったのか、プロセスをどのように改善すべきかを理解するための貴重な洞察を提供します。
徹底的な分析: 事後分析レポートには、インシデントのタイムライン、根本原因、要因、および停止につながった一連のイベントの包括的な分析を含める必要があります。この分析は、根本的な問題と脆弱性を明らかにするのに役立ちます。
軽減策と推奨されるアクション: 事後分析レポートでは、インシデントを軽減し、サービスを復元し、将来の同様のインシデントを防ぐために実行した手順を概説する必要があります。また、根本原因に対処し、システムの回復力を向上させるためのフォローアップアクションを提案する必要があります。
オプション A (エンジニアリング チームへの電子メール) は、技術的な詳細を提供する場合がありますが、事後分析レポートの体系的な分析と包括的なアプローチが欠けています。
オプションB(上級管理職と経営幹部に通知する)はコミュニケーションにとって重要ですが、インシデントから学び、プロセスを改善するための詳細な分析を実施することに主眼を置く必要があります。
オプション D (内部メッセージング プラットフォームでのインシデント ステータスの更新) は、簡単な概要しか提供せず、効果的なインシデント対応と防止に必要な詳細な分析が不足しています。
結論として、オプションCは、タイムライン、要因、軽減手順、および提案されたフォローアップアクションの徹底的な分析を含む事後分析レポートにインシデントを文書化することを含むものであり、インシデントから学習し、システムの信頼性を向上させるためのサイト信頼性エンジニアリングの推奨事項と一致しています。
<details><div>

### Q. 問題2: 未回答
シナリオ：あなたは、複雑なマイクロサービス アプリケーションをホストする Google Kubernetes Engine(GKE)クラスタの管理を担当する DevOps エンジニアです。CPU とメモリの使用率が高いなど、クラスターのパフォーマンスの問題に気付きました。リソース割り当てを最適化するには、GKE クラスタ内で CPU とメモリを最も多く消費しているコンテナを特定する必要があります。
質問：Google Kubernetes Engine(GKE)クラスタで CPU とメモリのリソースを最大限活用しているコンテナを特定するには、どのようなアプローチやツールを使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B. GKE の組み込みリソース モニタリングを有効にし、Stackdriver Monitoring を利用して CPU とメモリの指標を可視化することで、リソース使用率が最も高いコンテナを特定できます。
説明：
オプション B では、Google Kubernetes Engine(GKE)クラスタで最大の CPU リソースとメモリ リソースを消費しているコンテナを特定するための最適なアプローチの概要を示します。
オプションBが正しい選択である理由は次のとおりです。
組み込みのリソース監視:GKE には、クラスタで実行されているコンテナの CPU とメモリの使用状況に関連する指標を収集して公開する組み込みのリソース モニタリング機能が用意されています。
Stackdriver モニタリング:Stackdriver Monitoring は Google Cloud が提供する強力なツールで、GKE クラスタとその中のコンテナの CPU やメモリの使用量など、さまざまな指標を収集、可視化、分析できます。
リアルタイムの視覚化:Stackdriver Monitoring を使用すると、クラスタ内の各コンテナの CPU とメモリの指標をリアルタイムで可視化できます。これにより、リソース消費量が多いコンテナをすばやく特定できます。
スケーラビリティとインサイト:Stackdriver Monitoring は、大規模なモニタリングを処理するためのスケーラビリティを提供し、時間の経過に伴うリソース使用率の傾向とパターンに関する分析情報を提供します。
オプションA、C、およびDには、潜在的な課題と制限があります。
オプション A では、Kubernetes Dashboard の使用を提案していますが、Stackdriver Monitoring などのツールで利用できる詳細な分析情報や履歴データは提供されない可能性があります。
オプション C では、手動による介入とコンテナーへの SSH 処理が伴いますが、これは大規模なデプロイの監視とトラブルシューティングには効率的ではありません。
オプション D では、すべてのコンテナーのリソース制限を増やすことが提案されていますが、これはリソース消費量が多い根本原因に対処できず、過剰プロビジョニングにつながる可能性があります。
結論として、オプション B は、GKE の組み込みのリソース モニタリング機能を活用し、Stackdriver Monitoring を利用して包括的な可視化と分析を行うことで、GKE クラスタで最大の CPU リソースとメモリ リソースを消費しているコンテナを特定する効果的かつ効率的な方法を提供します。
<details><div>

### Q. 問題3: 未回答
シナリオ：Google App Engine にデプロイされた広く使用されているウェブ アプリケーションを管理します。このアプリケーションは、さまざまな地域やデバイスのユーザーにサービスを提供します。最適なパフォーマンスとユーザーエクスペリエンスを確保するには、アプリケーションへの受信トラフィックを監視し、潜在的な接続の問題を特定する必要があります。そこで、Stackdriver Monitoring for App Engine を活用することにしました。
質問：Google App Engine でグローバルにアクセスされるウェブアプリケーションを監視する場合、着信接続の合計数を追跡するにはどの指標を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. flex/connections/current (フレックス/接続/カレント)
説明：
オプション A は、Google App Engine でグローバルにアクセス可能なウェブアプリケーションの着信接続の合計数を監視するのに最も適した指標です。
オプションAが正しい選択である理由は次のとおりです。
flex/connections/currentに追加します。この指標は、App Engine フレキシブル環境インスタンスへの現在の接続数を表します。さまざまな地域やデバイスを考慮して、Webアプリケーションへのアクティブな接続を直接測定します。
オプション B、C、および D は、説明されているシナリオで着信接続を監視するための最良の選択肢ではありません。
tcp_ssl_proxy/new_connections:このメトリクスは、TCP/SSL プロキシ ロードバランサに固有で、新しい接続の割合を測定します。App Engine アプリケーションへの着信接続の合計数を正確に把握できない場合があります。
tcp_ssl_proxy/open_connections:このメトリクスは、TCP/SSL プロキシー・ロード・バランサーのオープン接続の数を測定しますが、これは Web アプリケーションへの接続を直接反映していない可能性があります。
appengine.googleapis.com/http/server/response_count:この指標は、App Engine アプリケーション内の HTTP サーバー レスポンスの数をカウントしますが、受信接続を直接測定するものではありません。
要約すると、オプション A(flex/connections/current)は、Stackdriver Monitoring を使用して Google App Engine でグローバルにアクセスされるウェブアプリケーションの着信接続の合計数をモニタリングするための適切な指標です。
<details><div>

### Q. 問題4: 未回答
シナリオ：Cloud SQL データベースとやり取りする新しいバージョンのウェブ アプリケーションをデプロイすると、データベース接続タイムアウトのレポートが受信されるようになります。特に、アプリケーションの同時ユーザー数は変更されません。アプリケーションは、データベース バックエンドとして Cloud SQL を利用します。
質問：一貫性のある同時ユーザーで新しいアプリケーション バージョンをデプロイし、Cloud SQL を使用した後に報告されたデータベース接続タイムアウトのコンテキストで、これらの接続タイムアウトの原因として考えられるものと、問題を診断するために調査する必要がある具体的な側面は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. 接続プールの枯渇
説明：
オプション A は、新しいアプリケーション バージョンを同時ユーザー数でデプロイし、Cloud SQL を利用した後にデータベース接続のタイムアウトが報告される原因として最も可能性の高い例です。
オプションAが正しい選択である理由は次のとおりです。
接続プールの枯渇:接続プールの使用効率が悪いと、使用可能なデータベース接続が枯渇する可能性があります。新しいバージョンのアプリケーションが接続を適切に解放または管理していない場合、プールが枯渇すると接続がタイムアウトになる可能性があります。
調査する要因 (オプション A に固有):
接続プールの構成:アプリケーションの接続プール設定の構成を確認します。最大プール サイズ、接続タイムアウト、アイドル タイムアウトなどのパラメーターを確認します。
接続解除:アプリケーションが不要になったときにデータベース接続を解放していることを確認します。接続が閉じられていないと、プールが枯渇する可能性があります。
接続の使用パターン:新しいアプリケーションバージョンでのデータベース接続の使用パターンを分析します。以前のバージョンと比較した接続の使用状況の変化や異常を特定します。
接続プールのメトリック:接続プールのメトリックと使用パターンを監視して、接続使用率の高いスパイクまたはパターンを特定します。
オプション B、C、D、E は、原因の可能性が低く、報告されたデータベース接続タイムアウトを直接説明できない場合があります。
データベース・インスタンスのスケーリング:スケーリングの問題は全体的なパフォーマンスに影響を与える可能性がありますが、同時ユーザー数が一定であれば、突然のタイムアウトがインスタンスのスケーリングに関連している可能性は低くなります。
ネットワーク遅延:ネットワークの問題によって遅延が発生することがありますが、重大な接続の問題がない限り、接続タイムアウトが発生する可能性は低くなります。
クエリパフォーマンス:クエリのパフォーマンスの問題は応答時間に影響を与える可能性がありますが、通常は接続タイムアウトではなく、クエリの遅延として現れます。
認証と承認:認証と承認の問題により、データベースへのアクセスが妨げられる可能性がありますが、接続タイムアウトが発生する可能性は低くなります。
要約すると、同時ユーザー数が一定であることと、報告される接続タイムアウトを考慮すると、オプション A (接続プールの枯渇) が調査に最も適した原因です。接続プールを効率的に管理して利用することで、報告されたデータベース接続のタイムアウトを Cloud SQL のコンテキストで診断し、対処することができます。
<details><div>

### Q. 問題5: 未回答
シナリオ：Google Cloud Platform(GCP)でホストされているクラウドネイティブ アプリケーションを管理します。アプリケーションのソース・コード・リポジトリーは、バージョン・タグ付けを使用して、異なるリリースを示します。ソース・コード・リポジトリーでタグ付けされたリリース・バージョンに基づいて、特定のバージョンのアプリケーションをデプロイする必要があります。
質問：Google Cloud Platform(GCP)のソースコードリポジトリでタグ付けされたリリースバージョンに基づいて特定のバージョンのアプリケーションをデプロイする場合、このデプロイを実現するための推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. Cloud Build のトリガー
説明：
オプションAは、Google Cloud Platform(GCP)のソースコードリポジトリでタグ付けされたリリースバージョンに基づいて、特定のバージョンのアプリケーションをデプロイするための推奨されるアプローチです。
オプションAが正しい選択である理由は次のとおりです。
Cloud Build トリガー:Cloud Build は、Google Cloud が提供するフルマネージドの CI / CD プラットフォームです。Cloud Build トリガーを設定することで、リリース バージョン タグに基づいて、さまざまなバージョンのアプリケーションをビルドしてデプロイするプロセスを自動化できます。
考慮すべき要素(オプションAに固有):
自動化されたワークフロー:Cloud Build トリガーは、新しいリリース バージョン タグがソースコード リポジトリにプッシュされるたびに、ビルドとデプロイを自動的にトリガーするように設定できます。
バージョンタグの検出:Cloud Build は、リポジトリから特定のリリース バージョン タグを検出し、それを使用して対応するバージョンのアプリケーションをビルドするように設定できます。
ビルドとデプロイの手順:Cloud Build では、コードのコンパイル、テストの実行、デプロイ アーティファクトの作成など、ビルド ステップを定義できます。また、App Engine、Kubernetes Engine、その他の GCP サービスなどのデプロイ ターゲットを指定することもできます。
ソース管理との統合:Cloud Build は、GitHub や Bitbucket などの一般的なソースコード リポジトリとシームレスに統合されるため、バージョン タグに基づいてトリガーを簡単に設定できます。
オプション B、C、および D は、このシナリオにはあまり適していません。
Kubernetes のデプロイ:リリースバージョンごとにKubernetesのデプロイ構成を手動で更新すると、エラーが発生しやすく、時間がかかる場合があります。Cloud Build トリガーによる自動化は、より合理的で信頼性の高いアプローチを提供します。
スクリプトの手動実行:手動デプロイ スクリプトを記述して実行すると、複雑さと手動による介入が発生するため、自動化された反復可能なデプロイには適していません。
インスタンステンプレート:Google Compute Engine のインスタンス テンプレートは、バージョン タグに基づいてアプリケーション コードをデプロイするよりも、仮想マシン インスタンスの管理に適しています。
要約すると、オプションA(Cloud Build Triggers)は、Google Cloud Platform(GCP)のソースコードリポジトリでタグ付けされたリリースバージョンに基づいて、特定のバージョンのアプリケーションをデプロイするための効率的で自動化されたアプローチを提供します。このアプローチにより、デプロイ プロセスが合理化され、CI/CD ワークフローの一貫性と信頼性が確保されます。
<details><div>

### Q. 問題6: 未回答
シナリオ： クラウドベースのeコマースプラットフォームでは、フラッシュセールイベントによりトラフィックが急増します。ユーザーアクティビティの増加により、パフォーマンスが低下し、ページの読み込み時間が遅くなり、エラーが発生することがあります。インシデント指揮官 (IC) として、パフォーマンスの問題に対処するために、オペレーション リード (OL) とコミュニケーション リード (CL) を含むインシデント対応チームを編成しました。
質問：フラッシュセールイベント中にeコマースプラットフォームのパフォーマンス低下を管理する場合、サイト信頼性エンジニアリング(SRE)の推奨プラクティスは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. コミュニケーション リード (CL) と協力して、明確で透明性のあるコミュニケーションを通じてパフォーマンスの低下についてユーザーに通知します。
説明：
オプション C は、フラッシュ セール イベント中に e コマース プラットフォームのパフォーマンス低下を管理するために従うべき、推奨されるサイト信頼性エンジニアリング (SRE) プラクティスです。
オプションCが正しい選択である理由は次のとおりです。
透明性のあるコミュニケーション:透明性の高いコミュニケーションは、SRE の基本プラクティスです。フラッシュセールイベント中にプラットフォームのパフォーマンスが低下した場合、状況についてユーザーに明確かつタイムリーなコミュニケーションを提供することが重要です。これにより、ユーザーの期待を管理し、信頼を築くことができます。
ユーザーインパクト管理:パフォーマンスの低下についてユーザーに通知することで、ユーザーは情報に基づいた意思決定を行い、適切な期待値を設定し、緊急性のないアクションを遅らせることができます。ユーザーは、進行中の問題に関する透明性と理解を高く評価しています。
コミュニケーションリードとの調整:コミュニケーション リード (CL) と協力することで、コミュニケーション戦略が明確に定義され、正確で、インシデント対応の取り組みと一致していることが保証されます。この調整により、誤った情報を防ぎ、関係者に情報を提供し続けることができます。
オプション A、B、D は、このシナリオでは推奨される SRE プラクティスではありません。
追加リソースのプロビジョニング:リソースのスケールアップが必要な場合もありますが、適切な評価を行わずに即時プロビジョニングを行うと、リソースの非効率性につながったり、問題が悪化したりする可能性があります。透過的な通信は、リソースのスケーリングの前に優先されます。
利害関係者を巻き込まずにパフォーマンスを回復する:利害関係者を巻き込み、問題についてオープンにコミュニケーションをとることは、信頼を維持し、統一された対応努力を確実にするために不可欠です。利害関係者を分離することは、SRE の原則と一致しません。
根本原因分析の実施:根本原因分析の実施は重要ですが、パフォーマンスの問題が進行中のフラッシュセールイベントでは、透明性のあるコミュニケーションが優先されます。根本原因分析は、差し迫った状況に対処した後に行うことができます。
要約すると、フラッシュセールイベント中にeコマースプラットフォームのパフォーマンス低下を管理するコンテキストでは、コミュニケーションリード(CL)と協力して、明確で透明性の高いコミュニケーション(オプションC)を通じてパフォーマンスの低下についてユーザーに通知することが、サイト信頼性エンジニアリング(SRE)の推奨プラクティスです。
<details><div>

### Q. 問題7: 未回答
シナリオ：あなたは、人気のあるeコマースプラットフォームの顧客ポータルとして機能するWebアプリケーションの管理を担当します。アプリケーションは Compute Engine インスタンスの固定セットで実行され、安定したワークロードと一貫したリソース使用率を備えています。目標は、アプリケーションのパフォーマンスと可用性を損なうことなくコストを最適化することです。
質問：一貫したパフォーマンスと可用性を確保しながら、固定された Compute Engine インスタンス セットで実行される安定したウェブ アプリケーションのコスト最適化を実現するには、どのようなアプローチが最も適切でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. 確約利用割引を実装して、特定の量のリソースに割引料金で長期間コミットします。
説明：
オプション A は、一貫したパフォーマンスと可用性を確保しながら、Compute Engine インスタンスの固定セットで実行される安定したウェブアプリケーションのコスト最適化を実現するための推奨アプローチです。
オプションAが正しい選択である理由は次のとおりです。
確約利用割引:確約利用割引では、1 年または 3 年の期間、特定の量のリソース (CPU とメモリ) の使用を約束できます。使用量レベルにコミットすることで、それらのリソースのコストを大幅に割引できます。これは、長期間にわたって一定量のリソースを必要とする安定したワークロードに最適です。
安定したワークロード:このシナリオでは、Web アプリケーションには、一貫したリソース使用率で安定したワークロードがあります。確約利用割引は、予測可能で定常状態のワークロードに適しています。
オプション B、C、および D は、このシナリオにはあまり適していません。
オプションB:マネージド・インスタンス・グループへの移行は、自動スケーリングとローリング・アップデートのメリットをもたらす可能性がありますが、固定リソースによる安定したワークロードのコストを直接最適化するものではありません。
オプションC:インスタンスをプリエンプティブル仮想マシンに変換するとコストを削減できますが、プリエンプティブル VM の最大実行時間は 24 時間で、Google Compute Engine によっていつでも終了できます。これは、ビジネスクリティカルな Web アプリケーションには適していない可能性があります。
オプションD:アンマネージド・インスタンス・グループを作成しても、本質的にコストの最適化には対応していません。マネージド インスタンス グループには、自動修復やスケーリングなどの利点がありますが、固定リソースで安定したワークロードには必要ない場合があります。
要約すると、オプション A(確約利用割引の実装)は、一貫したパフォーマンスと可用性を維持しながら、固定の Compute Engine インスタンスで実行される安定したウェブ アプリケーションの費用最適化を実現するためのベスト プラクティスと一致しています。
<details><div>

### Q. 問題8: 未回答
シナリオ：あなたは、トラフィックの多いeコマースWebサイトの管理を担当するリードDevOpsエンジニアです。最近、新機能をカナリアリリースとしてユーザーのサブセットにデプロイしました。ただし、デプロイ後まもなく、ユーザーから報告される HTTP 500 エラーの数が大幅に増加し、パフォーマンス監視ツールによって応答時間の待機時間が長くなっていることが示されます。優先事項は、ユーザーへの悪影響をできるだけ早く最小限に抑えることです。
質問：このような状況で、エラーと待機時間の増加の問題に対処し、ユーザーへの悪影響を最小限に抑えるために、最初に実行する必要がある手順は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. カナリア リリースをロールバックする: 新機能のデプロイを以前の安定バージョンにすばやく戻して、ユーザーへの悪影響を排除します。
説明：
このシナリオでは、最も即時のアクションは、新機能のカナリア リリースをロールバックすることです。HTTP 500 エラーの急増とレイテンシーの増加は、新機能がユーザーに問題を引き起こしている可能性があることを示しています。リリースを以前の安定バージョンにロールバックすると、変更が効果的に元に戻され、アプリケーションが既知の動作状態に復元されます。これは、ユーザーへの悪影響を最小限に抑え、Webサイトのパフォーマンスと信頼性を確実に回復するための重要なステップです。
オプション B、C、D はロールバックに続く重要な手順ですが、この段階での主な焦点は、カナリア リリースを元に戻すことで、ユーザーへの直接的な影響に迅速に対処することです。状況が安定したら、さらなる調査と分析を実施して、問題の根本原因を特定し、将来の同様のインシデントを防ぐことができます。
<details><div>

### Q. 問題9: 未回答
シナリオ：あなたは、ユーザーの閲覧履歴に基づいてさまざまな製品の推奨事項をユーザーに表示する人気のある e コマース Web サイトを担当しています。製品レコメンデーションは、それぞれが異なるカテゴリの製品を担当する複数のレコメンデーションマイクロサービスによって生成されます。場合によっては、これらのマイクロサービスの一部で遅延やエラーが発生し、ユーザー向けの製品レコメンデーション リストが不完全になることがあります。ユーザーは、推奨事項をまったく表示しないよりも、いくつかの推奨事項を表示することを好みます。満足のいくユーザーエクスペリエンスを確保するには、製品レコメンデーションの可用性を測定するサービスレベル目標(SLO)を設定する必要があります。これを測定するには、どのサービスレベル指標(SLI)を使用する必要がありますか?
質問：eコマースWebサイトでの製品レコメンデーションの可用性を測定し、マイクロサービスの遅延やエラーが発生した場合でも満足のいくユーザーエクスペリエンスを確保するために、どのサービスレベル指標(SLI)を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は
A. 品質SLI:ユーザーセッションの合計に対する完全な製品レコメンデーションリストの比率。
説明：
このシナリオの目標は、一部のマイクロサービスで遅延やエラーが発生した場合でも、製品の推奨事項を提供することで、満足のいくユーザー エクスペリエンスを確保することです。品質SLIは、ユーザーセッションの合計数に対する完全な製品レコメンデーションリスト(少なくともいくつかのレコメンデーションを含む応答)の比率に焦点を当てています。このSLIは、ユーザーが少なくともいくつかの製品レコメンデーションを受け取る、成功したインタラクションまたは部分的に成功したインタラクションの割合を定量化することで、ユーザーエクスペリエンスをキャプチャします。
オプション B、C、D は、説明されている特定のユーザー エクスペリエンスをキャプチャするのに適していません。
オプション B (可用性 SLI) は、マイクロサービスの正常性を測定するもので、これは重要ですが、マイクロサービスの問題発生時でも一部のレコメンデーションを受け取るユーザー エクスペリエンスには直接対処しません。
オプション C (鮮度 SLI) は、レコメンデーションの更新の最新性を測定しますが、これはコンテンツの正確性に関連する可能性がありますが、少なくとも一部のレコメンデーションを受け取ったユーザー エクスペリエンスを直接反映するものではありません。
オプションD(レイテンシSLI)は、応答時間に焦点を当てており、これは重要ですが、マイクロサービスの遅延やエラー時に少なくともいくつかの推奨事項を提供するという側面を特に捉えていません。
要約すると、オプションAは、ユーザーセッションの合計に対する完全な製品レコメンデーションリストの比率を測定する高品質のSLIであり、時折発生するマイクロサービスの遅延やエラーが発生した場合でも、少なくともいくつかの製品レコメンデーションを提供することで、満足のいくユーザーエクスペリエンスを確保するという目標とよく一致しています。
<details><div>

### Q. 問題10: 未回答
あなたは、Google Cloud Platform(GCP)上で動作するクラウドベースのアプリケーションのDevOpsリーダーです。責任の一環として、アプリケーションのインフラストラクチャが効率的で安全であることを確認する必要があります。最近、チームはセキュリティの脆弱性を露呈する新しいバージョンのアプリケーションをデプロイし、その結果、機密性の高いユーザー データへの不正アクセスが発生しました。今後このようなインシデントを防ぐために、セキュリティ対策を組み込んだ堅牢な DevOps プラクティスを実装する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
ある。 自動化された脆弱性スキャンをCI/CDパイプラインに統合し、デプロイ前にコードベースのセキュリティ問題を特定して修正します。
説明：自動化された脆弱性スキャンをCI/CDパイプラインに統合することで、デプロイ前にコードベースでセキュリティチェックが実行されます。このプラクティスは、開発プロセスの早い段階でセキュリティの脆弱性を特定して対処し、機密性の高いユーザー データを公開するリスクを軽減するのに役立ちます。デプロイ前に問題をキャッチすることで、不正アクセスを防ぎ、アプリケーションの全体的なセキュリティ体制を向上させることができます。
オプションBは、コードベースのセキュリティ脆弱性への対処とは直接関係がなく、潜在的な脅威に対処するためのリソース割り当てに重点を置いています。
オプション C は、便利な展開戦略ですが、セキュリティ対策や脆弱性スキャンの実装には直接対応していません。
オプション D は、適切な制御を行わずに広範なアクセス許可を付与すると、不正アクセスやセキュリティ侵害のリスクが高まる可能性があるため、推奨されません。最小特権の原則に従い、適切なアクセス制御を実装することが重要です。
<details><div>

### Q. 問題11: 未回答
Google Kubernetes Engine(GKE)にデプロイされた重要なマイクロサービスの管理を担当します。このマイクロサービスはアプリケーションの機能において重要な役割を果たし、ダウンタイムが発生すると、ユーザーの不満や金銭的損失につながる可能性があります。リソースの枯渇やその他の問題が原因でマイクロサービスが応答しなくなり、通常の操作を復元するために手動による介入が必要になるインスタンスが発生しました。このマイクロサービスの信頼性を高め、サイト信頼性エンジニアリング (SRE) のベスト プラクティスに従って、手動の復旧タスクに費やす時間と労力を削減する必要があります。
このような状況を考えると、目標を達成するための最も効果的なアプローチは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
イ. Kubernetes Horizontal Pod Autoscalerを使用して、CPUとメモリの使用率に基づいて自動スケーリング・ポリシーを実装します。
説明：KubernetesのHorizontal Pod Autoscalerを活用することで、CPUとメモリの使用率に基づいてマイクロサービスポッドの数を動的に調整できます。このアプローチにより、マイクロサービスをスケールアウトして負荷の増加を処理し、需要の少ない期間に自動的にスケールインできるようになります。これにより、リソースの枯渇を防ぎ、応答性が向上し、リソース関連の問題時に手動による介入が発生する可能性が低くなります。これは、運用タスクを自動化し、サービスの信頼性を確保することで、サイト信頼性エンジニアリングの原則と一致しています。
オプション A の Pod Disruption Budget の作成は、リソースの枯渇に対処するよりも、更新およびメンテナンス アクティビティ中の中断の管理に重点を置いています。
オプション C (ポッドを再起動するように CronJob を設定する) は、リソースリークの回避策になる可能性がありますが、Horizontal Pod Autoscaler のプロアクティブなスケーリングおよび自己修復機能は提供されません。
StatefulSetを使用するオプションDは、ステートフルアプリケーションの管理に重点を置いていますが、これはこのマイクロサービスのシナリオでは必要ない可能性があり、リソース関連の問題に直接対処しません。
<details><div>

### Q. 問題12: 未回答
あなたの会社は、さまざまなマイクロサービスや外部依存関係を含む複雑で重要な分散システムを運用しています。突如、重大インシデントが発生し、広範囲にわたるサービス低下を招く。サイト信頼性エンジニアリング (SRE) の原則に従うインシデント指揮官は、影響を最小限に抑えるために効率的なコミュニケーションを確保する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は C. 複数のチャネルによる自動通知を含む事前定義されたインシデント コミュニケーション計画を使用して、すべての関係者に最新情報を提供します。
理由: サイト信頼性エンジニアリング (SRE) のコンテキストでは、インシデント発生時の効率的なコミュニケーションが、影響を最小限に抑え、解決を効果的に調整するために重要です。オプション C は、明確に定義されたインシデント管理プロセスを持つという SRE の原則に沿っているため、最良の選択です。
オプション C が推奨されるアプローチである理由は次のとおりです。
事前定義されたコミュニケーション計画:SRE は、インシデントを処理するためのプロセスと手順を事前に定義することの重要性を強調しています。事前定義されたインシデント コミュニケーション計画を使用すると、コミュニケーションの一貫性とタイムリーさが保証され、適切な利害関係者に的を絞ることができます。
自動通知:自動化は、インシデント発生時のコミュニケーションを合理化するのに役立ちます。複数のチャネル(電子メール、チャット、SMSなど)による自動通知により、すべての関係者にインシデントのステータス、更新、および解決に向けた進捗状況が迅速に通知されます。
手動オーバーヘッドの削減:重大なインシデントが発生すると、手作業によるコミュニケーション作業に時間がかかり、エラーが発生しやすくなります。自動通知により、インシデント指揮官は問題の診断と解決に集中する時間を確保できます。
拡張性と一貫性:自動通知により、同じ情報がすべての関係者と同時に共有されます。これにより、混乱や誤報につながる可能性のある誤解や不完全な更新を防ぐことができます。
オプション A、B、D には、SRE の原則に照らして欠点がある場合があります。
オプション A: すべての関係者に詳細なメールを送信すると、コミュニケーションが遅れたり、インシデントの影響をすぐに認識できなかったりする可能性があります。
オプション B: すべてのチームに対して電話会議を設定することは、特に分散した組織や大規模な組織では、調整が難しい場合があります。また、不必要な遅延が発生し、効率的なコミュニケーションが妨げられる可能性があります。
オプション D: 包括的なインシデント事後レポートの作成は不可欠ですが、インシデントが解決された後に行う必要があります。インシデント中のリアルタイムの通信ニーズには対応していません。
要約すると、自動通知を備えた事前定義されたインシデント コミュニケーション プランを使用すると、重大なインシデントの発生時にすべての関係者に効率的で一貫性のあるタイムリーなコミュニケーションが保証されるため、SRE の原則と一致します。
<details><div>

### Q. 問題13: 未回答
シナリオ：あなたは、人気のあるソーシャルメディアプラットフォームを動かす複雑な分散システムのリードDevOpsエンジニアです。このシステムは、異なるリージョンにまたがる Kubernetes クラスターで実行される複数のマイクロサービスで構成されています。メジャー リリース中に、予期しない構成変更が発生し、さまざまなマイクロサービスで断続的なサービスの中断とパフォーマンスの低下が発生します。ユーザーがプラットフォームにアクセスしようとすると、応答時間が遅くなり、時折エラーが発生します。優先事項は、問題の根本原因を特定し、できるだけ早く通常のサービス機能を復元することです。
質問：この重要なシナリオでは、分散マイクロサービス環境における断続的なサービスの中断とパフォーマンスの低下の根本原因を特定するために、どのような高度なトラブルシューティング手法とツールを活用できますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. Jaeger や Zipkin などのディストリビューティッド(分散)トレーシングツールを使用して、エンドツーエンドの要求フローを分析し、マイクロサービスの相互作用におけるレイテンシーのボトルネックを特定します
説明：
複雑な分散マイクロサービス環境で断続的なサービスの中断とパフォーマンスの低下に直面している特定のシナリオでは、問題の根本原因を特定し、通常のサービス機能を復元するために、高度なトラブルシューティング手法とツールを活用することが重要です。提供されているオプションの中で、JaegerやZipkinなどのディストリビューティッド(分散)トレーシングツールを使用するのが最も効果的なアプローチです。このオプションが最良の選択である理由の詳細な説明は次のとおりです。
ディストリビューティッド(分散)トレーシングツールを使用すると、システム内のさまざまなマイクロサービスを通過するリクエストのフローを追跡し、視覚化できます。トレース コードを使用してマイクロサービスをインストルメント化することで、要求と応答のタイミングと相互作用に関する詳細なデータを収集できます。この情報は、レイテンシーのボトルネックを特定し、パフォーマンスが低下している領域を特定し、断続的な中断の根本原因を明らかにするために重要です。
このシナリオでディストリビューティッド(分散)トレーシングツールを使用することが、どのように役立つかを次に示します。
マイクロサービスの相互作用の視覚化: ディストリビューティッド(分散)トレーシングツールは、異なるマイクロサービス間でリクエストがどのように移動するかを視覚的に表現します。この視覚化は、イベントのシーケンスを理解し、予期しない相互作用や問題のある相互作用を特定するのに役立ちます。
待機時間のボトルネックの特定: トレース データを分析することで、要求処理の遅延の原因となっているマイクロサービスまたはコンポーネントをすばやく特定できます。この情報は、調査に優先順位を付け、早急な対応が必要な領域に焦点を当てるのに役立ちます。
パフォーマンスと変更の相関関係: ディストリビューティッド(分散)トレーシングは、変更やデプロイをパフォーマンスの変化と関連付けるのに役立ちます。特定のデプロイ後に待機時間やエラーの急増に気付いた場合は、その特定のマイクロサービスまたは変更を調査できます。
ドリルダウン分析: 特定のトレースにドリルダウンして、各マイクロサービス内のどこで時間が費やされているかを確認できます。これは、速度低下の原因となっている特定の関数または呼び出しを特定するのに役立ちます。
リアルタイム監視:多くのディストリビューティッド(分散)トレーシングツールはリアルタイム監視を提供しており、進行中のリクエストを追跡し、異常が発生したときに検出できます。これにより、新しい問題や停止に迅速に対応できます。
対照的に、他のオプション (B、C、D) もトラブルシューティングの作業に役立つ可能性がありますが、マイクロサービスの相互作用とパフォーマンスに対する同じレベルの詳細な可視性は提供されません。オプションAは、分散システム向けに特別に設計された高度な監視および診断機能を活用し、直面している複雑な問題を効率的に診断および解決するための最適な選択肢となります。
目標は、問題の根本原因を迅速に特定し、ユーザーへの影響を最小限に抑え、通常のサービス機能を復元することです。ディストリビューティッド(分散)トレーシングツールを使用すると、これらの目的を効果的に達成するために必要な洞察が得られます。
<details><div>

### Q. 問題14: 未回答
シナリオ：
お客様は、Google Cloud Platform(GCP)にデプロイされた重要なウェブアプリケーションのパフォーマンスを監視および最適化する責任があります。アプリケーションは、応答時間を改善し、バックエンド サービスの負荷を軽減するために、キャッシュに大きく依存しています。アプリケーションからさまざまな指標とログを収集するように Stackdriver Monitoring を設定しました。最近、アプリケーションのパフォーマンスに影響を与える可能性のあるキャッシュミスがいくつかあることに気付きました。Stackdriver でキャッシュミス情報を視覚化して、この問題の範囲と影響をより深く理解する必要があります。
質問：
Stackdriver でキャッシュミス情報を可視化して、キャッシュミス率とウェブアプリケーションのパフォーマンスへの影響に関するインサイトを得るにはどうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
D. アプリケーションのインスタンスに Stackdriver Logging エージェントをデプロイし、適切なログフィルタを設定してキャッシュミスイベントをキャプチャし、ログビューアを使用してキャッシュミスログをクエリして可視化します。
説明：
このシナリオでは、Stackdriver でキャッシュミス情報を視覚化するのに最適な方法は、Stackdriver Logging エージェントを使用することです。オプションDが正しい選択である理由は次のとおりです。
Stackdriver Logging エージェント:Stackdriver Logging エージェントを使用すると、Compute Engine インスタンスで実行されているアプリケーションなど、さまざまなソースからログを収集、フィルタリング、エクスポートできます。
キャッシュ ミス イベントのキャプチャ:アプリケーションのインスタンスに Stackdriver Logging エージェントをデプロイすることで、キャッシュ ミス イベントを具体的にキャプチャするようにログ フィルタを構成できます。これらのログには、キャッシュミスと関連する詳細に関する情報が含まれます。
ログビューアとビジュアリゼーション:キャッシュミスイベントがログとしてキャプチャされたら、Stackdriver Logs Viewer を使用してキャッシュミスログをクエリして可視化できます。ログビューアには、強力なフィルタリングおよびクエリ機能があり、データを絞り込み、視覚化を作成するのに役立ちます。
洞察と分析:ログビューアを使用すると、チャート、グラフ、ビジュアライゼーションを作成して、時間の経過に伴うキャッシュミス率、パターン、および他のパフォーマンスメトリクスとの相関関係を示すことができます。これにより、キャッシュミスがアプリケーションのパフォーマンスに与える影響に関する洞察が得られます。
他のオプション(A、B、C)は、Stackdriver でキャッシュミス情報を視覚化するのに適していません。
ある。Stackdriver ではカスタム指標やグラフを作成できますが、専用のログ機能を使用する場合と比較して、キャッシュ ミス イベントをキャプチャして視覚化する最も効率的なアプローチではない可能性があります。
B. Stackdriver Profiler は、キャッシュ ミス イベントのリアルタイム監視ではなく、プロファイリングと最適化に重点を置いています。
C. データポータルはダッシュボードを作成するための強力なツールですが、Pub/Sub と統合してカスタム ダッシュボードを作成すると、Stackdriver の組み込みのログ機能や視覚化機能を使用する場合に比べて、不必要に複雑になる可能性があります。
<details><div>

### Q. 問題15: 未回答
ユーザーがアップロードした大きなビデオファイルを変換および圧縮するメディア処理プラットフォームのインフラストラクチャを管理する責任があります。プラットフォームでは、仮想マシン (VM) のフリートを使用して処理タスクを処理します。システムのアーキテクチャは、日中は変動する負荷を処理するように設計されており、夜間はピーク時に使用されます。
パフォーマンスを犠牲にせずにコストを最適化するには、プリエンプティブル VM インスタンスを使用してコスト効率の高いソリューションを実装することにしました。プリエンプティブルVMに最適なアプリケーションコンポーネントはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. オフピーク時にアップロードされたビデオを圧縮するバッチジョブ。
プリエンプティブルVMインスタンスは、有効期間が短く、低コストの仮想マシンインスタンスであり、柔軟な実行スケジュールを持つアプリケーションに適しており、中断を許容できます。これらは、複数のインスタンスに分散でき、継続的なアップタイムを必要としないワークロードに最適です。このシナリオでは、オフピーク時にアップロードされたビデオを圧縮するバッチ ジョブにプリエンプティブル VM インスタンスを使用すると、プリエンプティブル インスタンスの特性とよく一致し、コストを削減できます。
説明：
バッチ ジョブ: アップロードされたビデオの圧縮は、リソースが利用可能なオフピーク時に実行するようにスケジュールできる非リアルタイム タスクです。プリエンプティブルインスタンスは、並列に実行でき、時間的制約のないバッチ処理タスクに適しています。
オフピーク時間:オフピーク時にバッチジョブを実行すると、圧縮されたビデオが通常の使用時間中に使用できるようになります。プリエンプティブルインスタンスは、必要に応じて割り当てと割り当て解除を行うことができるため、定期的なタスクや重要でないタスクに効率的です。
中断の許容: プリエンプティブル インスタンスの最大有効期間は 24 時間で、他の場所でリソースが必要な場合は Google Cloud によってプリエンプト処理できます。ビデオ圧縮は小さなタスクに分割できるため、プリエンプトされたインスタンスを新しいインスタンスに置き換えることができ、大きな中断なしにバッチジョブを続行できます。
コスト効率: プリエンプティブルインスタンスは、通常のインスタンスよりも大幅に安価であるため、バッチ処理ワークロードの費用対効果の高い選択肢となります。ビデオ圧縮にプリエンプティブルインスタンスを利用することで、組織は必要なタスクを達成しながらコンピューティングコストを削減できます。
並列処理:ビデオ圧縮タスクを並列化して、複数のプリエンプティブルインスタンスを異なるビデオで同時に処理できます。これにより、利用可能なリソースが活用され、バッチ処理全体が高速化されます。
要約すると、オフピーク時にアップロードされたビデオを圧縮するバッチ ジョブにプリエンプティブル VM インスタンスを使用することは、費用対効果、中断の許容度、時間的制約のないワークロードへの適合性など、プリエンプティブル インスタンスの特性と一致します。この選択により、組織はコストとリソース使用率を最適化しながら、必要なビデオ圧縮を実現できます。
<details><div>

### Q. 問題16: 未回答
あなたは、Google Kubernetes Engine(GKE)上で動作する重要なアプリケーションのリード DevOps エンジニアです。組織でマイクロサービス アーキテクチャを使用しており、各マイクロサービスはコンテナ化され、Google Container Registry(GCR)に保存されます。セキュリティは最優先事項であり、特定の GCR リポジトリの信頼できるコンテナ イメージのみが GKE クラスタにデプロイされるようにする必要があります。
このセキュリティ目標を達成し、不正なコンテナ イメージが GKE クラスタにデプロイされるのを防ぐには、どうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
オプション D は、Binary Authorization を使用して、特定の Google Container Registry(GCR)から Google Kubernetes Engine(GKE)クラスタに信頼できるコンテナ イメージを確実にデプロイするプロセスを正確に記述しているため、正しいです。他のオプションが最良の選択ではない理由を詳しく見ていきましょう。
A. Cloud Build のカスタム ビルダーを作成して特定の GCR レジストリにイメージをプッシュすることは、不要で複雑なアプローチです。Binary Authorization には、イメージの検証とポリシー チェックを強制する組み込みメカニズムが既に用意されているため、このオプションは冗長になっています。
B. ホワイトリスト名パターンを含むバイナリ承認ポリシーを使用することは gcr.io/altostrat-images/ 良い概念ですが、バイナリ承認と同じレベルのセキュリティは提供されません。Binary Authorization が提供する重要なセキュリティ対策であるイメージの署名検証とポリシーベースのチェックは明示的に適用されません。
C. 特定の GCR レジストリのイメージのマニフェストをチェックするロジックをデプロイ パイプラインに追加することは、正しい方向への一歩ですが、バイナリ承認が提供する堅牢性と自動適用に欠けています。マニフェストを手動でチェックすると、エラーが発生したり、バイパスされたりする可能性があります。
D. (正しい)Binary Authorization は、コンテナ イメージが GKE クラスタにデプロイされる前に、その整合性とセキュリティを確保するために特別に設計されています。信頼できる GCR レジストリからのイメージに対して署名検証とポリシーチェックを適用するようにバイナリ認証を設定することで、承認されていないイメージや改ざんされたイメージがデプロイされるのを防ぐ強力なセキュリティメカニズムを確立します。このオプションは、セキュリティ、コンプライアンス、および信頼できるデプロイ パイプラインの維持に関するベスト プラクティスと一致しています。
要約すると、オプション D は、Binary Authorization の機能を活用してコンテナ イメージのポリシーベースのチェックと署名検証を実施し、信頼できるイメージを GKE クラスタに確実にデプロイするための包括的で自動化されたアプローチを提供するため、正しいです。他のオプションでは、不必要な複雑さが増すか、堅牢なセキュリティ適用が不足しているか、エラーが発生しやすい手動チェックに依存しています。
<details><div>

### Q. 問題17: 未回答
お客様は、Google Cloud PlatformでホストされているeコマースWebサイトの運営に責任を負います。このウェブサイトは、オンラインショッピングでそれに依存している多数の顧客にサービスを提供しています。開発チームは最近、新しい支払いゲートウェイ統合を導入した新しいアップデートをWebサイトに展開しました。デプロイ後まもなく、チェックアウトプロセス中のエラー率が大幅に増加し、顧客が不満を募らせて購入を完了できないことに気付きました。
問題に対処し、顧客への影響を最小限に抑えるために、当面の行動方針は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
この状況では、ユーザーへの影響を軽減するための最も効果的なアプローチは、最初の手順として最近のリリース (オプション A) をロールバックすることです。ロールバックには、新機能が導入される前のバージョンにアプリケーションを戻すことが含まれます。このアクションにより、ユーザーはすぐに安心し、問題をさらに調査する間、通常の機能が復元されます。
ロールバックの利点 (オプション A):
即時の救済:リリースをロールバックすると、レイテンシーの問題がすぐに軽減され、ユーザーの通常のログイン機能が復元されます。これは、フラストレーションを最小限に抑え、ポジティブなユーザーエクスペリエンスを確保するために重要です。
問題の切り分け:以前のバージョンに戻すことで、問題を最新のリリースに切り分けます。これにより、新機能で導入された変更に調査を絞り込むことができ、根本原因を簡単に特定できます。
焦点を絞った調査:影響に即座に対処することで、チームは継続的なユーザーからの苦情の緊急性なしに徹底的な調査を行うことができます。これにより、ログ、メトリクス、エラーデータをより包括的に分析し、レイテンシーの具体的な原因を特定できます。
予防策:根本原因が特定されたら、必要な手順を実行して問題を解決し、ソリューションを徹底的にテストし、機能を再デプロイできます。また、将来のリリースで同様の問題を回避するための予防策を実装する機会もあります。
ロールバック後の追加手順:
リリースをロールバックし、ユーザーログイン機能が復元されたことを確認したら、次の手順に進む必要があります。
究明：Stackdriver モニタリングやその他の関連ツールを使用して、最近のリリースに関連するログ、指標、パフォーマンス データを分析します。これにより、遅延の問題の正確な原因を特定できます。
根本原因分析:待機時間に寄与した可能性のある新機能に加えられた変更を特定します。ボトルネック、パフォーマンスの問題、または最近のリリースで発生したエラーを探します。
修正とテスト:根本原因が特定されたら、新機能の問題の修正に取り組みます。制御された環境で修正を徹底的にテストして、新しい問題が発生しないことを確認します。
通信：状況、ロールバック、調査と解決の進行状況について関係者に通知します。明確で透明性のあるコミュニケーションを提供し、信頼と透明性を維持します。
結論として、問題の根本原因を特定するには Stackdriver モニタリング(オプション B)のレビューが不可欠ですが、リリースのロールバック(オプション A)は、ユーザーへの影響を迅速に軽減し、集中的な調査に役立つ環境を提供するための最初のステップです。どちらのオプションも補完的であり、包括的なインシデント対応戦略において重要な役割を果たします。
<details><div>

### Q. 問題18: 未回答
Google Cloud Platform(GCP)にデプロイされた大規模な本番環境の管理者である。この環境は、複数の Google Kubernetes Engine(GKE)クラスタ上で動作する複雑なマイクロサービス アーキテクチャで構成されています。最近、本番環境の仮想プライベートクラウド(VPC)内のネットワークトラフィックの異常なパターンに気付きましたが、これは既知のプロセスやサービスに起因するものではありません。VPC 内で不正な通信または潜在的に悪意のあるアクティビティが発生している可能性がある。
この未知のネットワーク トラフィックの発生源を調査して特定するには、GCP のツールとサービスを使用して何をすべきでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解はAです。
VPC フローログを使用して、本番アプリケーションがデプロイされている Virtual Private Cloud (VPC) で不明な通信を特定するには、本番 VPC ネットワークと、異常なネットワークトラフィックが観察される特定のサブネットで VPC フローログを有効にする必要があります。メタデータとペイロード データを利用可能な最大ボリューム スケールでキャプチャするようにフロー ログを構成すると、ネットワーク トラフィックと通信パターンに関する包括的な情報を収集できます。
VPC フローログは、VPC 内のネットワークトラフィックをキャプチャして分析するための強力なツールです。これらは、プロトコル情報、ポート番号、その他のメタデータとともに、ネットワークトラフィックの送信元と宛先に関する洞察を提供します。VPC フローログを有効にし、大量のスケールを設定することで、不明な通信の送信元を特定し、不正または悪意のあるアクティビティを検出するのに役立つ詳細なデータを収集できます。
フローログを分析することで、サービスの通信パターンに関する貴重な洞察が得られ、異常の根本原因を特定し、セキュリティ上の懸念に対処するための適切なアクションを実行するのに役立ちます。
実行する手順の詳細な説明は次のとおりです。
VPC フローログを有効にする: VPC フローログを使用すると、VM インスタンスに出入りするネットワークトラフィックに関する情報をキャプチャできます。まず、アプリケーションがデプロイされている本番稼働 VPC ネットワークで VPC フローログを有効にする必要があります。これは、Google Cloud Console またはコマンドライン ツールを使用して実行できます。gcloud
サブネットの指定: 本番フロントエンド サーバーでの不明な通信が疑われるため、これらのサーバーが配置されている特定のサブネットで VPC フロー ログを有効にします。これにより、疑わしいエリアに関連するトラフィックのキャプチャに集中できます。
ログ形式の選択: VPC フローログには、「メタデータのみ」と「メタデータとペイロード」の 2 つのログ形式があります。不明な通信の性質を特定しようとしているため、「メタデータとペイロード」オプションを選択します。これにより、パケットペイロードデータなど、より詳細な情報が提供されます。
[Select High Volume Scale]: VPC フローログを設定するときに、1.0 や使用可能な最大値などの大容量スケールを選択します。これにより、包括的なデータセットを確実に取得し、詳細な分析を実行できます。
ログストレージ: VPC フローログを保存する場所を決定します。ログを Cloud Storage、BigQuery、Pub/Sub のいずれかにエクスポートできます。 ログを安全で簡単にアクセスできる場所に保存すると、分析が容易になります。
ログ分析: VPC フローログが有効になり、データをキャプチャしたら、ログの分析を開始できます。見覚えのない異常なパターンやコミュニケーションを探します。関連するサブネットと、不審なアクティビティが報告された期間に焦点を当てます。
データの関連付け: 不明な通信を特定するには、キャプチャされたネットワーク トラフィックを報告されたインシデントのタイムラインと関連付けます。送信元と宛先の IP アドレス、ポート、およびプロトコル情報に注意してください。ペイロードデータは、通信の性質に関する洞察を提供することができます。
さらに調査: 疑わしい可能性のあるトラフィックを特定したら、送信元と宛先のインスタンスをさらに調査します。通信が正当なものかどうか、または不正アクセスやセキュリティ違反を示しているかどうかを確認します。
アクションの実行: 結果に応じて、適切なアクションを実行します。これには、特定のIPアドレスのブロック、ファイアウォールルールの適用、不正アクセスを防ぐためのセキュリティ対策の実装などが含まれます。
これらのステップに従い、VPC フローログの機能を活用することで、VPC 内の不明な通信を効果的に特定して対処し、本番アプリケーションのセキュリティと信頼性を高めることができます。
<details><div>

### Q. 問題19: 未回答
シナリオ：あなたは、組織の重要なサービスに影響を与えた最近の停止の余波を管理する責任を持つインシデント指揮官です。停止は解決されましたが、根本原因に対処し、今後同様のインシデントを防ぐことが重要です。インシデント事後分析の一環として、アクションアイテムを作成し、さまざまなチーム メンバーに割り当てます。目標は、アクションアイテムが迅速かつ効果的に処理され、改善が促進され、再発を防ぐことです。
質問：停止の根本原因に効率的に対処し、アクション アイテムを管理するには、これらのアクション アイテムに所有者とコラボレーターを割り当てるためにどのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え:C。
説明：
所有者とコラボレーターを協力的かつ的を絞った方法でアクションアイテムに割り当てることは、停止の根本原因に効率的に対処し、改善を推進するために不可欠です。オプションCは、根本原因の特定と専門知識に基づくアクションアイテムの割り当てに、関連するチームメンバーを巻き込むもので、いくつかの理由から最も効果的なアプローチです。
専門知識と知識:根本原因に関連する専門知識と知識を持つチームメンバーを巻き込むことで、アクションアイテムがそれらに効果的に対処するのに最適な個人に割り当てられます。
コラボレーション：複数のチームメンバーが協力して関与することで、多様な視点と洞察が可能になり、根本的な問題に対処するためのより包括的で包括的なソリューションにつながります。
アカウンタビリティ：所有者をアクションアイテムに割り当てることで、その解決に対する明確な説明責任が確立されます。コラボレーターは、必要に応じて所有者にサポートと支援を提供できます。
効果的な優先順位付け:特定と割り当てのプロセスにチームメンバーを参加させることで、重要度と潜在的な影響に基づいてアクションアイテムに優先順位を付けることができます。
オプション A、B、および D は、停止後のアクション アイテムを管理するための最良の選択肢ではありません。
オプション A (すべてのアクションアイテムに同じチームメンバーを所有者として割り当てる) は、チームの多様なスキルと専門知識を活用できない可能性があり、同じ個人が複数のアクションアイテムに圧倒された場合に潜在的なボトルネックになる可能性があります。
オプション B (可用性とワークロードに基づいて所有者を割り当てる) では、必要な専門知識を持たない可能性のある個人にアクション アイテムが割り当てられ、最適ではない解決策が得られる可能性があります。
オプション D (最も稼働率の低いチーム メンバーにアクション アイテムを割り当てる) は、必要な知識を持つ適切な担当者が根本原因に対処していることを必ずしも保証するものではなく、最も効果的な結果につながらない可能性があります。
要約すると、根本原因を特定し、専門知識とコラボレーションに基づいてアクションアイテムを割り当てるために、関連するチーム メンバーを関与させることが、停止の根本原因に効率的に対処し、インシデント後のシナリオで改善を推進するための推奨されるアプローチです。
<details><div>

### Q. 問題20: 未回答
シナリオ：組織の重要なサービスは Google Kubernetes Engine(GKE)クラスタでホストされ、本番クラスタは us-west1 リージョンにあります。チームは、デプロイ プロセスを最適化して、GKE クラスタへの更新をシームレスかつ効率的にロールアウトする任務を負っています。この取り組みの一環として、ダウンタイムを最小限に抑えながら、Kubernetes マニフェストが本番環境の GKE クラスタに確実に適用されるようにする必要があります。これを実現するには、どのコマンドを使用する必要がありますか?
質問：ダウンタイムを最小限に抑えながら本番環境の GKE クラスタの更新を最適化する場合、Kubernetes マニフェストを適用するにはどの Kubernetes コマンドを使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:ダウンタイムを最小限に抑えながら Kubernetes マニフェストを本番環境の GKE クラスタに適用するための正しい Kubernetes コマンドは、オプション A: です。kubectl apply -f <manifest-file> --record
説明：このコマンドは、Kubernetes マニフェストで定義されているリソースを適用および更新するために使用されます。このフラグは、リソース・アノテーションの一部としてコマンドを記録するために使用され、リソースに加えられた変更の履歴記録を維持するのに役立ちます。これは、事後分析を実行したり、時間の経過とともに発生した変更を分析したりする場合に特に役立ちます。kubectl apply--record
他のオプションが正しい選択ではない理由:
kubectl rollout apply -f <manifest-file> --strategy=rolling-update (オプションB):このコマンドは有効な Kubernetes コマンドではありません。また、このフラグは or コマンドで認識されるフラグではありません。rollout apply--strategy=rolling-updaterolloutapply
kubectl deploy -f <manifest-file> --update-strategy=rolling (オプションC):コマンドにはフラグがありません。このコマンドは、マニフェストを直接適用するのではなく、デプロイの作成に使用されます。kubectl deploy--update-strategydeploy
kubectl update -f <manifest-file> --strategy=rolling (オプションD):このコマンドはリソースの更新に使用されますが、フラグはサポートしていません。更新ストラテジーを指定するための正しいフラグは で、コマンドで使用されます。kubectl update--strategy--recordapply
要約すると、ダウンタイムを最小限に抑え、変更履歴を維持しながら Kubernetes マニフェストを GKE クラスタに適用するための推奨コマンドは です。kubectl apply -f <manifest-file> --record
<details><div>

### Q. 問題21: 未回答
シナリオ：あなたは、組織の重要なアプリケーションに影響を与えた最近の停止の事後分析を主導しています。インシデントはサービスの中断につながり、根本原因を特定し、将来の発生を防ぐための徹底的な調査を実施する責任があります。事後分析プロセスの一環として、インシデントとその要因を文書化する必要があります。ただし、ドキュメントは、不要な詳細を含めずに、関連性のある有意義な情報に焦点を当てていることを確認する必要があります。
質問：上記の停止シナリオの事後分析を実施するコンテキストで、事後分析ドキュメントに含めるべきではないものは次のうちどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:事後分析を実施する場合、事後分析ドキュメントに含めてはならない情報は、オプション C: 影響を受けるアプリケーションの作成に関与したすべてのソフトウェア開発者の一覧です。
説明：事後分析ドキュメントは、インシデントの明確かつ簡潔な分析、その要因、および将来の発生を防ぐための推奨事項を提供するように設計されています。説明責任とコラボレーションは重要ですが、影響を受けるアプリケーションの作成に関与したすべてのソフトウェア開発者のリストを含めることは、事後分析の目標に直接関係しない場合があります。このドキュメントでは、責任の所在を明らかにするのではなく、停止の原因となった技術的、運用的、およびプロセス関連の問題を特定することに重点を置く必要があります。
他のオプションがより適切である理由:
A. 詳細なタイムスタンプ (オプション A):インシデント対応アクションのタイムスタンプを提供すると、イベントのシーケンスを再構築し、インシデントのタイムラインを理解するのに役立ちます。
B. 影響の概要(オプションB):停止がユーザー、事業運営、収益に与える影響を説明することで、結果とビジネスへの影響を明確に理解できます。
D.即時アクションの説明(オプションD):停止を軽減し、サービスを復元するために実行された即時アクションを文書化すると、対応の有効性と問題に対処するために実行された手順を理解するのに役立ちます。
結論として、事後分析ドキュメントを作成する際には、関連する技術的な詳細、インシデント分析、改善のための推奨事項に焦点を当てると同時に、インシデントの根本原因の理解や将来の発生の防止に直接貢献しない不要な情報を避けることが重要です。
<details><div>

### Q. 問題22: 未回答
シナリオ：Google App Engine の標準環境にデプロイされた重要なウェブ アプリケーションを管理します。アプリケーションの応答時間に一貫性がなく、CPU 使用率のスパイクがパフォーマンスの問題を引き起こしていると思われます。CPU スパイクの原因となっているコードベースの部分を特定し、それらを最適化して全体的なパフォーマンスを向上させる必要があります。
質問：Google App Engine の標準環境にデプロイされたウェブアプリケーションのパフォーマンスを最適化する場合、CPU 使用率をキャプチャして分析し、パフォーマンスのボトルネックを特定するには、どのオプションが適していますか?
1. 
2. 
3. 
4. 
<details><div>
答え： C
説明：
Google Cloud Profiler:Google Cloud Profiler は、アプリケーションからきめ細かなパフォーマンス データをキャプチャするように設計されています。CPU 使用率のパターンに関する分析情報を提供し、CPU スパイクの原因となる特定の関数またはメソッドを特定します。
App Engine の統合:Google Cloud Profiler は Google App Engine の標準環境とシームレスに統合されているため、コードを大幅に変更することなくプロファイリングを有効にできます。
詳細な分析:プロファイラーは、CPU リソースを最も多く消費しているコードベースの領域を強調表示する CPU 使用率プロファイルを生成します。これにより、パフォーマンスのボトルネックを特定し、アプリケーションの特定の部分を最適化できます。
他のオプションが最良の選択ではない理由:
A. 手動によるパフォーマンス ログ:パフォーマンス ログ ステートメントを手動で挿入すると、手間がかかり、CPU スパイクの原因となる特定の関数を特定するために必要なきめ細かな分析情報が得られない場合があります。
B.サードパーティの監視ツール:サードパーティのモニタリング ツールでは CPU 指標を提供できますが、Google Cloud Profiler が提供する直接的な統合や詳細な分析には対応していない可能性があります。
D. 負荷テストと Cloud Monitoring:負荷テストと Cloud Monitoring は、アプリケーション全体の動作に関する貴重な分析情報を提供しますが、CPU のボトルネックを特定するための Google Cloud Profiler と同じレベルの粒度を提供しない場合があります。
結論として、オプション C は、CPU 使用率をキャプチャして分析し、Google App Engine の標準環境にデプロイされたウェブ アプリケーションのパフォーマンスのボトルネックを特定するための優れた選択肢です。Google Cloud Profiler は、アプリケーションのコードベースに固有の詳細な CPU 使用率プロファイルと分析情報を提供し、パフォーマンスを効果的に最適化するのに役立ちます。
<details><div>

### Q. 問題23: 未回答
シナリオ：グローバルにデプロイされ、Google Cloud Platform(GCP)上のHTTP(S)グローバルロードバランサを介してアクセスできる重要なウェブアプリケーションを管理します。高可用性を確保し、停止にタイムリーに対応することが重要です。停止が発生したときに迅速にアラートが送信されるように、効果的な監視と通知を設定する必要があります。停止を追跡し、通知を受け取るには、どのような手順を実行する必要がありますか?
質問：HTTP グローバル・ロード・バランサーの背後で実行されている Web アプリケーションの停止に関する通知を受け取るには、何を追跡する必要があり、そのようなイベントの通知をどのように設定できますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:HTTP Global Load Balancer の背後で実行されている Web アプリケーションの停止について通知を受け取る正しい手順は次のとおりです。
オプションA:ロードバランサ構成でバックエンド インスタンスの HTTP(S) ヘルスチェック ステータスを追跡し、ヘルスチェックの失敗に基づいて Stackdriver Monitoring アラートを設定します。
説明：
ヘルスチェックの追跡:グローバル・ロード・バランサ構成でバックエンド・インスタンスに適切なHTTP(S)ヘルス・チェックを設定します。ヘルスチェックは、リクエストを送信し、特定の応答を期待することで、インスタンスの正常性を定期的に評価します。
Stackdriver Monitoring アラート:Stackdriver Monitoring を構成して、ヘルスチェックの結果をモニタリングします。ヘルスチェックのステータスが障害を示している場合、またはバックエンドインスタンスグループのサービス容量が低下した場合にトリガーされるアラートポリシーを設定します。
アラート チャネル:Stackdriver Monitoring 内でアラート チャネル(メール通知、SMS メッセージ、サードパーティ統合など)を設定して、ヘルスチェックの失敗が発生したときにリアルタイムでアラートを受信します。
オプションB:ロードバランサーのスループットと帯域幅の使用状況を監視することは、容量計画にとって重要ですが、アプリケーションの停止を直接示すものではない場合があります。
オプションC:待機時間の監視は、アプリケーションの応答性を評価するために重要ですが、アプリケーションの停止を直接示すものではない場合があります。Cloud Scheduler を使用して応答しないインスタンスを定期的にチェックするのは複雑であり、停止通知に最適なアプローチではありません。
オプションD:Google Cloud Pub/Sub を使用してリアルタイム ログをサブスクライブし、アラート用に Cloud Functions の関数を設定することは実現可能なアプローチですが、停止通知のヘルスチェックを追跡するほど直接的で単純ではありません。
結論として、HTTP Global Load Balancer の背後で実行されているウェブアプリケーションの停止に関する通知を受け取るには、ロードバランサ構成でバックエンド インスタンスの HTTP(S) ヘルスチェック ステータスを追跡し、ヘルスチェックの失敗に基づいて Stackdriver Monitoring アラートを設定する必要があります。このアプローチにより、アプリケーションで停止につながる可能性のある問題が発生した場合に、タイムリーに通知されます。
<details><div>

### Q. 問題24: 未回答
重要な Web アプリケーションの開発およびデプロイ プロセスを管理する責任があります。開発を合理化し、継続的インテグレーション (CI) を確実に行うには、コードがソース コード リポジトリにプッシュされるたびに CI パイプラインを自動的にトリガーする必要があります。CI / CD ツールとして Google Cloud Build を使用している。これを実現するには、どのような手順に従う必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： A
説明：ソース コード リポジトリに Webhook を設定することは、コードがプッシュされるたびに CI パイプラインなどの自動アクションをトリガーするための一般的な方法です。このプロセスのしくみは次のとおりです。
リポジトリ設定へのアクセス:GitHub、GitLab、Bitbucketなどのソースコードリポジトリで、リポジトリ設定に移動します。
Webhook を追加します。新しい Webhook を追加するオプションを探します。ここで、通知を受信するエンドポイントを構成します。
ペイロード URL を設定します。Google Cloud Build の場合は、Cloud Build Webhook の URL を指定します。この URL は Cloud Build によって自動的に生成されます。
[イベント] を選択します。Webhook をトリガーするイベントを指定します。この場合、"push" イベントを選択して、コードがリポジトリにプッシュされたときに Webhook をトリガーする必要があることを示します。
シークレットの追加 (オプション):セキュリティを強化するために、Webhook がペイロードと共に送信するシークレット トークンを含めることができます。このトークンは、受信要求の信頼性を検証するために使用されます。
Webhook の保存:Webhook 設定を構成したら、変更を保存します。
これで、コードがリポジトリにプッシュされるたびに、リポジトリから Cloud Build Webhook に通知が送信されます。その後、Cloud Build によって CI パイプラインが自動的にトリガーされ、指定されたビルドとデプロイのステップが実行されます。
利点：
オートメーション：CIパイプラインは、コードがプッシュされるたびに手動の介入なしにトリガーされるため、継続的なインテグレーションと迅速なフィードバックが保証されます。
一貫性：自動化により、人為的ミスの可能性が減り、コード変更のたびにパイプラインが一貫して実行されるようになります。
効率：開発者は、ビルドを手動でトリガーするのではなく、コードの記述に集中できるため、効率と生産性が向上します。
コードのプッシュ時に Cloud Build に通知を送信するように Webhook を構成することで、CI パイプラインをシームレスに自動化し、開発ワークフローを強化できます。
<details><div>

### Q. 問題25: 未回答
あなたは、Google Cloud Platform(GCP)上で動作するマイクロサービスベースのアプリケーションを管理するDevOpsチームの一員です。このアプリケーションは、相互接続された複数のマイクロサービスで構成されており、連携してさまざまな機能を提供します。Google Cloud Trace を使用してリクエストフローをトレースおよびモニタリングすることで、マイクロサービスのオブザーバビリティとパフォーマンス分析を強化したいと考えています。
この取り組みの一環として、マイクロサービスをインストルメント化して Google Cloud Trace と統合し、トレース データを収集する必要があります。マイクロサービス間の相互作用を分析し、ボトルネックを特定し、アプリケーション全体のパフォーマンスを最適化できるようにする必要があります。
Google Cloud Trace を効果的に使用するためにマイクロサービスをインストルメント化するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： B
説明：
Google Cloud Trace は、マイクロサービス アーキテクチャでリクエスト フローをトレースおよびモニタリングするための包括的なソリューションを提供します。マイクロサービスを効果的にインストルメント化し、Google Cloud Trace と統合するために、OpenTelemetry ライブラリは標準化されたアプローチを提供します。
トレース コレクション: OpenTelemetry は分散トレース用に設計されており、複数のプログラミング言語用のライブラリを提供します。OpenTelemetry をマイクロサービスに統合することで、カスタムログステートメントを必要とせずにトレースデータを自動的に収集できます。
Google Cloud Trace との統合: OpenTelemetry は、トレースを Google Cloud Trace に直接エクスポートするように構成できます。このシームレスな統合により、トレースデータが Google Cloud Trace に送信され、マイクロサービス間の相互作用を分析できます。
自動コンテキスト伝達: OpenTelemetry は、マイクロサービス間のコンテキスト伝達を処理し、要求がさまざまなコンポーネントを通過するときにトレース情報が渡されるようにします。
スパンの可視性: Google Cloud Trace との統合により、マイクロサービス全体のリクエスト処理のさまざまな段階を表すスパンを視覚化できるため、ボトルネックやレイテンシの問題を簡単に特定できます。
他のオプション(A、C、D)では、Google Cloud Trace を使用したリクエストフローのトレースとモニタリングの必要性は特に考慮されていません。カスタムロギングステートメント(オプション A)を追加すると、基本的なトレースに役立つ可能性がありますが、Google Cloud Trace の包括的な機能には欠けています。Cloud Monitoring アラートの構成(オプション C)は、トレースではなく、リソース使用率のモニタリングに関するものです。追加のインスタンスのデプロイ (オプション D) は、トレース用のマイクロサービスのインストルメント化とは直接関係ありません。
結論として、OpenTelemetry ライブラリをマイクロサービスに統合し、トレースを Google Cloud Trace にエクスポートするように構成することで、マイクロサービスをインストルメント化してトレースとパフォーマンス分析を強化するための標準化された効率的な方法が提供されます。
<details><div>

### Q. 問題26: 未回答
マイクロサービスベースのアプリケーションをホストしている Google Kubernetes Engine(GKE)クラスタを管理しています。クラスターは複数のノードで構成され、それぞれが複数のポッドを実行します。効果的な監視とトラブルシューティングを確実に行うには、クラスター内のすべてのポッドからログを収集し、それらを一元化されたログシステムに転送する必要があります。これを達成するための最良の方法は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：C
説明：
Fluentdは、KubernetesクラスタにDaemonSetとしてデプロイできる、一般的なオープンソースのログ収集エージェントです。Fluentd を DaemonSet として GKE クラスタにデプロイすると、ログの収集と転送にいくつかのメリットがあります。
効率的な収集: GKE クラスタ内のすべてのノードで DaemonSet として実行される Fluentd により、各ノード上のすべてのポッドからログが確実に収集されます。
集中型転送: Fluentd は、さまざまなソースからのログを効率的に集約し、Google Cloud Logging などの集中型ログ システムに転送できます。
標準化: Fluentd を使用すると、すべてのポッドで一貫したログの収集と書式設定が保証され、トラブルシューティングと分析が容易になります。
スケーラビリティ: クラスターが拡張されると、新しいノードに Fluentd が自動的にデプロイされ、手動による介入なしで継続的なログ収集が保証されます。
柔軟性: Fluentd はさまざまなプラグインと構成をサポートしているため、アプリケーションのニーズに基づいてログ収集を調整できます。
一方、オプション A、B、D には制限があります。
オプション A: 各ポッドにスタンドアロンのログ記録アプリケーションをデプロイすると、複雑でリソースを大量に消費し、クラスターのスケーリングに伴う管理が困難になります。
オプション B: Google Cloud Logging エージェントはログ収集に使用できますが、パフォーマンスと複雑さの問題から、各ポッド内にデプロイすることは推奨されません。
オプション D: 各ノードに個別のログ収集スクリプトを設定すると、DaemonSet としての Fluentd が提供する一元管理、標準化、および柔軟性が欠けます。
結論として、Fluentd を GKE クラスタに DaemonSet としてデプロイすることは、すべてのポッドからログを収集し、一元化されたロギング システムに効率的かつ効果的に転送するための最良の方法です。
<details><div>

### Q. 問題27: 未回答
HTTP POSTリクエストを使用して、Google Cloud Platform(GCP)環境からサードパーティアプリケーションにビルド情報を送信する必要があるプロジェクトに取り組んでいます。サードパーティ アプリケーションには、統合の目的で特定のビルド関連データが必要です。これは、GCPセットアップ内で実現する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある
説明：
Cloud Functions の関数を使用して HTTP POST リクエストをトリガーすることは、GCP のサードパーティ アプリケーションにビルド情報を送信するのに適したアプローチです。Cloud Functions は、サーバーレスのイベント駆動型関数であり、HTTP リクエストなどのさまざまなイベントによってトリガーできます。このシナリオでは、ビルドが成功するたびにトリガーされるように Cloud Functions の関数を構成できます。Cloud Functions の関数コード内に、HTTP POST リクエストをサードパーティ アプリケーションの API エンドポイントに送信するロジック(リクエスト ペイロードの一部として必要なビルド情報を含む)を含めることができます。
他のオプションは、このユースケースには適していません。
B. HTTP POST リクエストを自動的に送信するように Google Cloud Build トリガーを構成すると、サードパーティ アプリケーションの API との統合に必要な柔軟性とカスタマイズ性が得られない場合があります。
C. Cloud Pub/Sub トピックと Cloud Run サービスを設定すると、単純な HTTP POST リクエストが不必要に複雑になります(特に、Cloud Functions で同じ結果をより直接的に実現できる場合)。
D. ソースコード リポジトリ内でカスタム スクリプトを記述することは、Cloud Functions などの専用のサーバーレス サービスを使用して統合を処理する場合ほどスケーラブルでなく、メンテナンスも容易ではない可能性があります。
オプション A は、Cloud Functions のイベントドリブン アーキテクチャを利用して、HTTP POST リクエストを使用して必要なビルド情報をサードパーティ アプリケーションに送信することで、目的の結果を得るための簡単で効率的な方法を提供します。
<details><div>

### Q. 問題28: 未回答
重要な電子商取引アプリケーションの信頼性は、お客様の責任です。最近、構成の変更によりアプリケーションが停止し、ユーザーの遅延とエラーが増加しました。インシデントは最終的に解決されましたが、アプリケーションの信頼性を高める必要があり、インシデントの対応と解決のプロセスを測定および監視するための一連の主要業績評価指標 (KPI) を確立することにしました。インシデント発生後にサービスを復元するのにかかる時間を計算し、ユーザーへの影響を最小限に抑えるには、どのような指標を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：D
説明：
MTTR(平均復旧時間)は、インシデントまたは停止後にサービスを通常の運用に復元するのにかかる平均時間を測定する重要な指標です。インシデント対応と解決プロセスの効率性と有効性に関する貴重な洞察を提供します。MTTRを追跡することで、チームは問題を迅速に検出、診断、軽減する能力を評価し、ユーザーとビジネスへの影響を最小限に抑えることができます。
MTTR は、すべてのインシデントのダウンタイム期間を合計し、その合計をインシデントの合計数で割ることによって計算されます。これにより、インシデント発生後にサービスの復元に通常かかる時間を明確に把握できるため、チームはインシデント管理の傾向、改善すべき領域、ベスト プラクティスを特定できます。
特定のシナリオでは、平均復旧時間 (MTTR) を短縮して、電子商取引アプリケーションに対する将来のインシデントの影響を最小限に抑えることが目標です。これは、インシデント対応と解決のプロセスを改善し、サービスの復旧を迅速化するという目的に沿ったものです。
他のオプション (A、B、C) は、インシデントからの復旧にかかる時間の測定とは直接関係ありません。これらは、アプリケーションのパフォーマンスと正常性のさまざまな側面を監視するのに役立つ場合がありますが、インシデント後にサービス機能を復元するのにかかる時間を計算するという特定のニーズには対応していません。
<details><div>

### Q. 問題29: 未回答
シナリオ：
Google Kubernetes Engine(GKE)クラスタでホストされている人気のモバイルゲームのバックエンド オペレーションを監督します。ゲームのアーキテクチャには、世界中のプレイヤーからの大量のHTTPリクエストを処理することが含まれます。ゲームの人気が高まるにつれ、これらのリクエストの管理と処理に関連するネットワークコストを削減するソリューションを見つける任務を負っています。
質問：
Google Kubernetes Engine(GKE)クラスタでモバイルゲームを実行する際のネットワーク コストの課題に対処するには、どのような対策を講じる必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：ある
説明：
CDN の実装は、GKE クラスタから世界中に戦略的に配置されたエッジ サーバーにトラフィックをオフロードすることで、ネットワーク コストを削減する効果的な戦略です。これにより、データの移動距離が最小限に抑えられ、レイテンシーが短縮され、画像、ビデオ、その他のアセットなどの静的コンテンツの配信が最適化されます。その結果、このアプローチはネットワークコストを削減するだけでなく、全体的なプレーヤーエクスペリエンスも向上させます。
<details><div>

### Q. 問題30: 未回答
シナリオ：
あなたは、マイクロサービス アーキテクチャを利用し、Google Cloud Platform(GCP)でホストされている急成長中の e コマース プラットフォームの DevOps リーダーです。アプリケーションのコンテナ化されたサービスは、Google Container Registry(GCR)に保存されます。効率的な開発およびデプロイ プロセスを確保するために、チームは、GCR 内のコンテナー イメージに変更があるたびにトリガーされる自動デプロイ戦略の実装を検討しています。
質問：
Google Container Registry(GCR)内に保存されているコンテナイメージに変更が発生するたびに自動デプロイを実現するには、どの方法を採用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：ある
説明：
Cloud Build トリガーを使用すると、イメージ リポジトリとデプロイ パイプラインをシームレスに統合できます。この方法では、更新されたイメージをマイクロサービスにデプロイするプロセスが自動化され、新しいイメージ バージョンが GCR リポジトリにプッシュされるたびに、変更が環境全体に効率的に伝達されます。
<details><div>

### Q. 問題31: 未回答
シナリオ：
あなたは、コンテナ化されたアプリケーションを開発およびデプロイするソフトウェア会社のリードDevOpsエンジニアです。会社のコンテナー イメージはプライベート コンテナー レジストリに格納され、これらのイメージのセキュリティと整合性を維持することにコミットします。セキュリティ対策の一環として、コンテナイメージの脆弱性スキャンプロセスを実装して、潜在的なセキュリティ脆弱性を特定して対処する必要があります。
質問：
DevOps リーダーとしての役割では、プライベート レジストリに格納されているコンテナー イメージの脆弱性スキャン プロセスを確立する必要があります。これを達成するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：B
説明：
コンテナスキャンツールをデプロイプロセスに統合することで、コンテナイメージ内のセキュリティの脆弱性を特定して対処するための体系的なアプローチを確立できます。この予防的な対策は、運用環境にデプロイされるアプリケーションのセキュリティと整合性を確保するのに役立ちます。
コンテナイメージには、攻撃者に悪用される可能性のある既知の脆弱性を持つソフトウェアコンポーネントが含まれている場合があります。これに対処するには、コンテナスキャンツールをワークフローに統合することが不可欠です。このアプローチのしくみは次のとおりです。
コンテナスキャンツールを選択します。プライベート コンテナー レジストリと互換性のあるコンテナー スキャン ツールを選択します。人気のあるオプションには、Clair、Anchore、Trivy などがあります。これらのツールは、ソフトウェアコンポーネントをセキュリティデータベースと比較することで、既知の脆弱性のイメージを分析します。
スキャンを構成します。選択したツールをコンテナー レジストリと統合します。ビルド プロセス中やデプロイ前など、指定された間隔でイメージの脆弱性をチェックする定期的なスキャン スケジュールを設定します。
脆弱性データベースの比較:スキャンツールは、イメージ内のコンポーネントを、National Vulnerability Database (NVD) などの既知の脆弱性のデータベースと比較します。脆弱性が報告されたコンポーネントを特定し、重大度レベルを割り当てます。
通知設定:重大な脆弱性を持つイメージの通知を送信するようにスキャンツールを設定します。このアラートシステムにより、潜在的なセキュリティリスクについてチームに迅速に通知されます。
自動応答:脆弱性に自動的に対処するプロセスを実装します。重大度に応じて、重大な脆弱性が検出されたときにビルドを失敗させたり、デプロイをブロックしたりするようにツールを構成できます。また、影響を受けるイメージを検疫または更新する自動アクションを設定することもできます。
継続的改善:スキャン結果を定期的に確認し、イメージのソフトウェアコンポーネントを更新して脆弱性にパッチを適用します。これには、パッケージの更新や、脆弱性のない代替コンポーネントの選択が含まれる場合があります。
このアプローチに従うことで、コンテナイメージをデプロイする前にセキュリティの脆弱性をプロアクティブに特定して対処し、悪用のリスクを最小限に抑え、アプリケーションの整合性を維持できます。プロセスの自動化により、レジストリ内のすべてのイメージで一貫性のあるタイムリーな脆弱性評価が保証されます。
<details><div>

### Q. 問題32: 未回答
あなたは、マイクロサービスベースのeコマースアプリケーションをホストするKubernetesクラスタの管理を担当するDevOpsエンジニアです。アプリケーションのサービスは、セキュリティと信頼性を維持しながら、インターネットからアクセスできる必要があります。チームは、外部トラフィックが異なるサービスやバージョン間で均等に分散され、SSL ターミネーションが効率的に処理されるようにしたいと考えています。これらの目標を達成するには、どのKubernetesリソースを実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
d) イングレス
以下はその説明です。
イングレス：Ingressは、クラスタ内のサービスへの外部アクセスを提供するKubernetesリソースです。これにより、ホスト名、パス、またはその他の基準に基づいてトラフィックをさまざまなサービスに転送するルーティングルールを定義できます。Ingress は SSL ターミネーションもサポートしており、アプリケーションとの安全な通信を可能にします。これは、外部アクセスの管理、負荷分散、およびアプリケーションのさまざまなサービスやバージョンへのトラフィックのルーティングのための汎用性の高いリソースです。
オプション a) ClusterIP、b) NodePort、c) LoadBalancer は、主にシナリオで説明した要件に合わせて設計されていません。ClusterIP はクラスター内の通信用の内部 IP アドレスを提供し、NodePort は各ノードの IP の特定のポートでサービスを公開し、LoadBalancer はクラウド ロード バランサーを介してサービスを外部に公開します。イングレスは、記載された目標を達成するための最も適切なリソースです。
<details><div>

### Q. 問題33: 未回答
シナリオ：
あなたは、機密性の高い顧客データを扱う金融サービス会社のリード DevOps エンジニアです。同社のアプリケーションとサービスは Google Cloud Platform(GCP)でホストされており、インフラストラクチャのセキュリティを確保する責任はお客様にあります。重要な側面の 1 つは、GCP 環境内のさまざまなサービスで使用される安全なキーと認証情報の管理です。
質問：
リードDevOpsエンジニアは、Google Cloud Platform(GCP)環境で安全な鍵を管理する任務を負っています。キーと資格情報のセキュリティを確保するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：ある
説明：
Google Cloud KMS は、GCP 環境で暗号鍵を安全かつ一元的に管理できるように設計されています。KMS にキーを保存すると、適切なアクセス制御、監査、および不正アクセスに対する保護が保証されます。特に機密データや暗号化の要件を扱う場合は、安全なキー管理のために KMS などの専用サービスを使用するのがベスト プラクティスです。
<details><div>

### Q. 問題34: 未回答
シナリオ：
あなたは、顧客の支払い情報を処理する e コマース プラットフォームの DevOps リーダーです。会社のアプリケーションやサービスは Google Cloud Platform(GCP)上で実行されており、機密性の高い顧客データの保護に専念しています。セキュリティ対策の一環として、GCP 環境内で Google Cloud Key Management Service(KMS)を使用してセキュア キーを管理するための適切な方法を選択する必要があります。
質問：
DevOps リーダーは、GCP 環境で Google Cloud Key Management Service(KMS)を使用してセキュアな鍵を管理する責任があります。KMS で使用可能な方法の中で、最高レベルのセキュリティとキー管理を確保するために推奨されるアプローチはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
推奨されるアプローチ:
B. 非対称キー暗号化:
非対称暗号化では、暗号化用の公開鍵と復号化用の秘密鍵のキーペアを使用します。公開鍵はオープンに共有できるため、誰でもデータを暗号化できますが、秘密鍵は安全なままで、復号化に使用されます。この方法は、暗号化と復号化のプロセスにさまざまな関係者が関与している場合に適しており、セキュリティを強化します。
顧客の支払い情報を保護するコンテキストでは、非対称キーを使用すると、機密データを外部の関係者(支払いの詳細を送信する顧客など)によって暗号化され、プラットフォームでのみ復号化できます。このキーの分離により、セキュリティが強化され、機密データへの不正アクセスが防止されます。
他のオプションが正しくない理由:
A. 対称キー暗号化:
対称暗号化では、暗号化と復号化の両方に同じキーが使用されます。これは効率的ですが、暗号化と暗号化解除にさまざまな関係者が関与するシナリオには適していない場合があります。対称キーはパーティ間で安全に共有する必要があるため、共有キーの管理と保護に課題が生じる可能性があります。
C.キーローテーション:
キーローテーションはセキュリティに不可欠なプラクティスですが、それ自体はメソッドではありません。これは、新しいキーを定期的に生成し、古いキーを廃止して、長期的な露出を軽減するプロセスです。キーローテーションは、対称キー暗号化方式と非対称キー暗号化方式の両方と組み合わせることができます。重要ですが、キーローテーションだけでは必要な暗号化メカニズムは提供されません。
D. キーの直接埋め込み:
セキュリティ上の理由から、アプリケーション コードまたは構成ファイルに暗号化キーを直接埋め込むことはお勧めしません。これにより、コードベースにアクセスできる潜在的な攻撃者にキーが公開されます。さらに、Google Cloud KMS などの特殊な鍵管理サービスが提供する必要なアクセス制御と管理機能は提供されません。
要約すると、オプションB(非対称キー暗号化)は、特に機密性の高い顧客の支払い情報を扱う場合に、安全なキー管理と暗号化のベストプラクティスと一致するため、推奨されるアプローチです。これにより、復号化キーの機密性が確保され、外部関係者による安全な暗号化が可能になります。
<details><div>

### Q. 問題35: 未回答
シナリオ：
Google Cloud Platform(GCP)でホストされているウェブアプリケーションを開発するソフトウェア会社のDevOps運用を監督しています。アプリケーション イメージは Cloud Build を使用して構築され、Google Container Registry(GCR)に保存されます。デプロイ戦略の一環として、アプリケーションの特定のバージョンがソース管理リポジトリのリリースバージョンタグに基づいてデプロイされるようにする必要があります。
質問：
ソース管理リポジトリのリリース バージョン タグに基づいてアプリケーションのバージョン固有のデプロイを実現するには、イメージを Google Container Registry(GCR)にプッシュするときに何を実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
推奨されるアプローチ:
A. タグ付け規則を実装します。
Cloud Build パイプラインを構成して、アプリケーション イメージがビルドされて Google Container Registry(GCR)にプッシュされるときに、特定のリリース バージョン タグが自動的に適用されます。
ソース管理リポジトリで使用可能なバージョン情報を使用して、タグを動的に生成します。これは、環境変数またはその他のメカニズムを使用して実現できます。
たとえば、セマンティック バージョニング スキーム(「v1.2.3」など)に従っている場合、Cloud Build パイプラインはソース管理のコミットからバージョン タグを抽出し、イメージに適用できます。
このタグ付け規則を実装することで、ソース管理リポジトリのバージョン情報が、GCR に保存されているアプリケーション イメージに一貫して反映されます。これにより、これらのタグ付きイメージに基づくバージョン固有のデプロイのプロセスが簡素化されます。
他のオプションが理想的ではない理由:
B. Google Cloud Deployment Manager を使用します。
Google Cloud Deployment Managerは、テンプレートを使用してクラウドリソースを作成および管理するためのサービスです。インフラストラクチャのプロビジョニングには強力なツールですが、バージョン固有のアプリケーション展開には複雑すぎるソリューションになる可能性があります。このシナリオには、インフラストラクチャ全体ではなく、アプリケーションのデプロイ バージョンが含まれます。
c. Kubernetes 構成を手動で更新します。
イメージのプッシュ後に Kubernetes 構成ファイルを手動で更新しても、一貫性と自動化は保証されません。このアプローチでは、人為的ミスの余地があり、自動化と再現性に関するDevOpsの原則と一致しません。
D. Kubernetes のトリガーを作成します。
Kubernetes をトリガーして新しいイメージをデプロイすることもできますが、ビルドとデプロイのパイプライン内で一貫したバージョンタグ付けアプローチを使用する方が実用的です。これにより、ビルド パイプラインがバージョンのタグ付けを処理し、デプロイ パイプラインがデプロイ プロセスのオーケストレーションに重点を置くことで、懸念事項を明確に分離できます。
要約すると、オプション A(タグ付け規則の実装)は、アプリケーションイメージがビルドされて GCR にプッシュされるときに、バージョン情報をアプリケーション イメージに関連付けるための合理化および自動化されたプロセスを保証するため、推奨されるアプローチです。この方法により、特定のバージョンのデプロイが簡素化され、アプリケーション イメージの明確なバージョン履歴を維持できます。
<details><div>

### Q. 問題36: 未回答
シナリオ：
あなたは、新しいモバイルアプリケーションを急速に開発しているスタートアップ企業のリードDevOpsエンジニアです。このアプリは、頻繁な更新と開発チーム間のコラボレーションを必要とするリアルタイムメッセージングプラットフォームです。同社は、クラウドインフラストラクチャにGoogle Cloud Platform(GCP)を選択しました。開発チーム向けのバージョン管理ツールとコラボレーション ツールを設定する責任があります。
質問：
リードDevOpsエンジニアは、スタートアップのモバイルアプリケーション開発に適したバージョン管理およびコラボレーションツールを選択する必要があります。アプリケーションの性質とGoogle Cloud Platform(GCP)の使用を考えると、どのオプションが最適ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
このシナリオでは、最も適切な答えは次のとおりです。
A. クラウド ソース リポジトリの使用:
Cloud Source Repositories は、Google Cloud Platform(GCP)とシームレスに統合し、スタートアップのクラウド インフラストラクチャと連携するマネージド バージョン管理システムを提供するため、最適な選択肢です。バージョン管理、コラボレーション機能、合理化された CI/CD プロセスを提供するため、頻繁な更新とチームワークを必要とするリアルタイム メッセージング プラットフォームに適しています。
<details><div>

### Q. 問題37: 未回答
あなたは DevOps エンジニアで、Google App Engine Standard にデプロイされたアプリケーションの管理を担当しています。アプリケーションが大量のログを生成しており、問題のトラブルシューティングを行うには、エラーログにすばやくアクセスする必要があります。アプリケーションによって生成されたエラーログに効率的にアクセスするには、どのアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
b)Cloud Logging に移動し、ログをフィルタリングして、エラーログを表示します。
以下はその説明です。
Google App Engine Standard にデプロイされたアプリケーションによって生成されたエラーログにすばやくアクセスする必要があるシナリオでは、Cloud Logging を使用するのが最も効率的な方法です。Cloud Logging を使用すると、App Engine を含むさまざまな Google Cloud サービスからログを収集、表示、分析できます。ログの重大度レベルなどの特定の基準に基づいてログをフィルタリングすることで、エラーログにすばやくアクセスし、問題のトラブルシューティングを行うことができます。
オプション a)Cloud Console の [アクティビティ] タブに移動してエラーログを表示することは、特定のエラーログにアクセスする最も直接的な方法ではない可能性があります。
オプション c) Cloud Error Reporting に移動し、発生ごとに集計されたエラーログを表示すると、個々のエラーログにすばやくアクセスするのではなく、エラーを分析して分析情報を生成することに重点が置かれます。
オプション d) Cloud Monitoring に移動し、ログをフィルタリングしてエラーログを表示するは、主に指標とアラートのモニタリングに重点を置いており、エラーログにすばやくアクセスする最も効率的な方法ではない可能性があります。
説明されているシナリオのコンテキストでエラーログに効率的にアクセスするには、オプションbが最適な選択肢です。
<details><div>

### Q. 問題38: 未回答
あなたは、毎日何百万人ものユーザーにサービスを提供する人気のソーシャルネットワーキングプラットフォームのDevOpsリーダーです。シームレスなユーザーエクスペリエンスを確保することは重要であり、サービスの信頼性を正確に測定する必要があります。サービスの信頼性を測定するのに最適な方法は、次のうちどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：B(A 品質SLI:全回答数に対する劣化していない回答の比率)
説明：
サービスレベル指標(SLI)は、サービスのパフォーマンスと信頼性を測定するために使用される指標です。SLI は、サービスが定義されたパフォーマンス目標をどの程度達成しているかを評価するのに役立ちます。シームレスなユーザーエクスペリエンスが重要なソーシャルネットワーキングプラットフォームのコンテキストでは、高品質のSLIを使用することは、サービスの信頼性を測定するための最も適切な方法です。その理由は次のとおりです。
劣化していない応答:品質SLIは、総応答数に対する劣化していない応答の比率を測定することに重点を置いています。これは、期待されるパフォーマンス基準を満たす応答の割合を追跡していることを意味します。応答が低下している、または遅いと見なされると、必要なレベルのユーザーエクスペリエンスを満たしていない可能性があります。
ユーザーエクスペリエンスの正確性:劣化していない応答の比率を測定することで、ユーザーが望ましいレベルのサービス品質を体験している頻度を直接把握できます。これは、ユーザーが高速で信頼性の高いインタラクションを期待するソーシャルネットワーキングプラットフォームにとって不可欠です。
実際のユーザーへの影響:このSLIは、実際のユーザーインタラクションとそのエクスペリエンスを考慮に入れています。サーバーの稼働時間や技術的な指標だけではありません。これは、ユーザーが認識するサービスの品質を直接反映します。
サービス目標との整合性:高品質のSLIは、シームレスなユーザーエクスペリエンスを提供するというサービスの目的と一致しています。これにより、サービスがパフォーマンス目標を達成していない期間を特定し、迅速な調査と解決が可能になります。
継続的改善:総応答数に対する劣化していない応答の比率を継続的に監視することで、DevOps チームは時間の経過とともにサービスの信頼性の向上に取り組むことができます。この比率が低下すると、パフォーマンスを向上させるための積極的な対策が促されます。
他のオプションがあまり適していない理由:
A. 各ソフトウェア リリースで追加された新機能の数を数えます。このメトリックは、サービスの信頼性ではなく、機能開発とソフトウェアリリースに関連しています。重要ですが、ユーザーエクスペリエンスの質を直接測定するものではありません。
C. 応答時間を考慮せずにサーバーの稼働時間を監視する:サーバーの稼働時間は可用性に関連していますが、応答時間やユーザー エクスペリエンスは考慮されません。サービスは稼働していても、パフォーマンスが低下する可能性があります。
D. 1 日あたりのユーザー操作の総数を追跡します。ユーザーインタラクションは関連性がありますが、この指標だけでは、それらのインタラクションの品質に関する情報を提供することはできません。満足のいくエクスペリエンスと不十分なエクスペリエンスを区別しません。
要約すると、選択肢B(A quality SLI:総応答数に対する劣化していない応答の比率)は、ソーシャルネットワーキングプラットフォーム上のサービスの信頼性を測定するための最も適切な方法です。これは、サービスのパフォーマンスを直接、ユーザー中心に表示し、シームレスなユーザーエクスペリエンスを提供するという目標と一致しています。
<details><div>

### Q. 問題39: 未回答
あなたは、季節的なセール中にトラフィックが多くなる大規模なeコマースプラットフォームのDevOpsリーダーです。アプリケーション・リソースの可用性と最適なパフォーマンスを確保するには、システムの応答性をプロアクティブにモニターする必要があります。この目標を達成するには、どのアプローチがより効果的でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
最も適切な答えは次のとおりです。
B. カスタム クライアントを使用してシミュレートされた要求を送信する:
ユーザー要求をシミュレートし、さまざまなアプリケーション リソースの応答性を測定するカスタム クライアント アプリケーションを開発することは、アプリケーション リソースの可用性と最適なパフォーマンスをプロアクティブに監視するという目標を達成するための優れたアプローチです。
その理由は次のとおりです。
プロアクティブな監視:カスタム クライアントを使用してユーザー要求をシミュレートすることで、実際のユーザー操作を模倣して、実際のシナリオでアプリケーションの応答性を事前に監視できます。これにより、実際のユーザーに影響を与える前に、潜在的なパフォーマンスの問題を検出できます。
カスタマイズされたテスト:カスタム・クライアントを開発すると、重要なAPIエンドポイント、データベース・クエリー、ユーザー・フローなど、テストするアプリケーションの特定の部分に合わせてリクエストを柔軟に調整できます。このターゲットを絞ったテストは、リソースのボトルネックとパフォーマンスの低下を正確に特定するのに役立ちます。
リアリスティックシミュレーション:カスタム クライアントは、参照、検索、カートへのアイテムの追加、チェックアウトなど、さまざまな種類のユーザー操作をシミュレートできます。これにより、さまざまなアプリケーション リソースがさまざまな負荷とユーザー シナリオでどのように応答するかを包括的に把握できます。
データドリブンなインサイト:カスタム・クライアントは、応答時間、エラー率、およびリソース使用率に関する詳細なメトリックを収集できます。このデータ駆動型のアプローチは、時間の経過に伴うパターン、傾向、およびパフォーマンスの低下を特定するのに役立ちます。
オートメーション：カスタム クライアントは、特定の間隔で実行するように自動化できるため、ピーク時以外の時間でも、アプリケーションの正常性と応答性を継続的に監視できます。
オプションBは、現実的なシミュレーションとターゲットを絞ったテストを組み合わせて、パフォーマンスの問題をプロアクティブに特定して対処できるという点で際立っています。他のオプションでは、アプリケーションの可用性と応答性に関する包括的で自動化された現実世界の分析情報を提供するには不十分な場合があります。
<details><div>

### Q. 問題40: 未回答
あなたは、複数のリージョンにまたがるマルチクラウドインフラストラクチャの管理を担当するDevOpsエンジニアです。チームは、Google Cloud Platform(GCP)に新しい仮想マシン(VM)のセットをデプロイする任務を負っています。デプロイを成功させ、サービスの中断を回避するには、デプロイを開始する前にどのプラクティスを優先する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：ある
説明：
リソース制限コンプライアンスの確保:Google Cloud Platform(GCP)で新しい仮想マシン(VM)のデプロイを開始する前に、VM の意図したリソース要件が、選択したリージョンで使用可能な割り当て制限と一致していることを確認することが重要です。クォータ制限は、リソースの過剰使用やサービスの中断を防ぐために設定されています。デプロイが使用可能なクォータを超えると、VM のプロビジョニングが失敗したり、予期しないサービスが中断されたりする可能性があります。
ネットワーク遅延の監視:ネットワーク待機時間の監視は全体的なパフォーマンスにとって重要ですが、クォータ内のリソース制限の検証には直接関係ありません。ネットワーク待機時間の問題は、アプリケーションのパフォーマンスに影響を与える可能性がありますが、特定のクォータ境界内で VM を正常にプロビジョニングする場合の主な懸念事項ではありません。
アプリケーション・スケーリングの構成:自動スケーリング構成はアプリケーションのパフォーマンスの維持に関連しますが、このオプションでは、計画されたデプロイがリソースクォータに準拠していることを確認する最初の手順には特に対応していません。
セキュリティ ポリシーの適用:セキュリティ ポリシーの適用は、機密データを保護するために不可欠ですが、クォータ関連のデプロイの問題に対処する際の主要な考慮事項ではありません。
このシナリオでは、計画された VM デプロイのリソース要件が、選択したリージョンで使用可能なクォータ制限を超えないようにすることが最優先事項です。このプロアクティブなアプローチは、リソースの制限によるデプロイの失敗を防ぎ、デプロイ プロセスの成功を促進するのに役立ちます。
<details><div>

### Q. 問題41: 未回答
あなたは、Google Cloud Platform(GCP)でホストされている医療アプリケーションの管理を担当するリード DevOps エンジニアです。このアプリケーションは、医療記録や個人情報などの機密性の高い患者データを含むログを生成します。厳格なコンプライアンス要件のため、これらの機密性の高いログをフィルタリングして患者データを削除し、Google Cloud Storage(GCS)に安全に保存する必要があります。これをどのように実現する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
最も適切な答えは次のとおりです。
A. Fluentd フィルターと出力の構成:
Fluentdフィルターを設定して、事前定義されたパターンまたはカスタム正規表現を使用して機密性の高い患者データを識別して編集します。
フィルタリングされたログを Google Cloud Storage(GCS)に転送して安全なストレージを確保するように Fluentd を構成します。
以下はその説明です。
Fluentd フィルターの構成:Fluentdは、強力なログ収集および転送ツールです。Fluentd フィルターを設定することで、ログ内の機密性の高い患者データを識別するためのルールまたはパターンを定義できます。これには、医療記録、個人情報、その他の機密データが含まれます。
機密データのフィルタリング:フィルターを使用すると、機密性の高い患者データをログから編集またはマスキングしてから、さらに処理することができます。これにより、データ保護規制へのコンプライアンスが保証され、患者のプライバシーが保護されます。
Google Cloud Storage(GCS)への転送:機密性の高い患者データを削除したら、サニタイズされたログを Google Cloud Storage(GCS)に転送するように Fluentd を構成できます。GCS は、暗号化とアクセス制御を備えた安全なストレージを提供し、フィルタリングされたログを確実に保護します。
オプションB、C、Dには潜在的な解決策が含まれますが、ログから機密性の高い患者データをサニタイズし、GCSに安全に保存するという目標とはあまり一致していません。オプションAは、コンプライアンスとデータプライバシーを維持しながら、機密性の高い医療データログを処理するための現実的で効果的なアプローチを提供します。
<details><div>

### Q. 問題42: 未回答
あなたは DevOps エンジニアで、Google Cloud Build を使用して重要なアプリケーション デプロイ パイプラインを監督しています。最近、Git リポジトリに保存されている Cloud Build 構成ファイルを更新して、ビルド効率を向上させました。予期せず、これらの変更後、パイプラインでビルド プロセス中に繰り返しエラーが発生します。表示されるエラーメッセージは、問題の根本原因に関する明確な洞察を提供していません。リーダーとして、この問題のトラブルシューティングと対処のためにどのような手順を実行しますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
D. Git のバージョンを比較します。
Cloud Build 構成ファイルの最後に動作確認されたバージョンと、障害の原因となった最新バージョンを Git で比較します。問題を引き起こしている可能性のある特定の変更を特定します。
以下はその説明です。
Gitのバージョンを比較します。Git リポジトリに保存されている構成ファイルに変更を加えた後に予期しないエラーが発生した場合は、体系的なアプローチとして、ファイルの最後に確認された動作バージョンと最新の変更バージョンを比較します。この比較は、導入され、障害の原因となる可能性のある特定の変更を特定するのに役立ちます。
問題のある変更を特定します。2 つのバージョンの違いを分析することで、エラーの原因となっている可能性のある新しい構成、設定、または構文エラーを特定できます。この対象を絞ったアプローチにより、デバッグ プロセスが高速化され、問題に関連する可能性が最も高い変更に集中できます。
オプション A、B、C も有効なトラブルシューティング戦略ですが、オプションの D ほど正確で直接的な問題の根本原因に関する洞察が得られない場合があります。 エラーログの確認(オプション A)と変更の取り消し(オプション B)は一般的なアプローチですが、GCP サポート(オプション C)を利用すると専門家の支援を受けることができますが、追加の待ち時間が発生する可能性があります。Git バージョンの比較 (オプション D) は、問題のある変更をすばやく特定する簡単な実践的な方法であり、このシナリオで最も効果的なアプローチです。
<details><div>

### Q. 問題43: 未回答
あなたは DevOps エンジニアで、Google Kubernetes Engine(GKE)クラスタにデプロイされた重要なマイクロサービス アプリケーションを管理しています。アプリケーションのコンポーネントは、高可用性を維持するために、トラフィックの需要に基づいて動的にスケーリングする必要があります。責任の一環として、アプリケーションの可用性とパフォーマンスを最適化することに重点を置いています。新しいインスタンスが完全に動作し、トラフィックを処理する準備ができている場合にのみロードバランサーに追加されるようにするには、どのアプローチを優先する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. Readiness Probe の実行:
マイクロサービス内のコンテナーごとに準備プローブを構成します。
準備プローブは、新しいインスタンスが完全に動作し、トラフィックを処理する準備ができている場合にのみ、ロードバランサーに追加されるようにします。
以下はその説明です。
Readiness Probe の実行:Kubernetes 環境では、readiness probe を使用して、コンテナーがトラフィックの受信を開始する準備ができているかどうかを判断します。readiness probe はコンテナのステータスを定期的にチェックし、プローブが失敗した場合、コンテナは準備ができていないと見なされ、トラフィックの受信から除外されます。これにより、完全に動作可能なインスタンスのみがロードバランサーに追加され、トラフィックを処理できないインスタンスにユーザーが誘導されるのを防ぎます。
オプション A (Liveness Probe の実装) では、コンテナーの正常性と応答しないコンテナーの再起動に重点が置かれていますが、トラフィックを処理するための準備状況については特に説明していません。
オプションB(ポッドの水平自動スケーリングの構成)では、CPU使用率などのメトリックに基づいてレプリカの数を動的に調整しますが、ロード・バランサーに新しいインスタンスを追加する前の準備は保証されません。
オプション D (ネットワーク ポリシーの実装) は、ポッド通信のセキュリティと分離に重点を置き、ロード バランサーにインスタンスを追加する前に準備が整っていることを確認することには関係ありません。
<details><div>

### Q. 問題44: 未回答
あなたは、顧客にさまざまな API を提供する一般的な e コマース プラットフォームを管理する責任があります。これらの API は、JSON、XML、CSV などのさまざまな形式でデータを配信します。分析作業の一環として、これらの API の中で最もよく要求される形式を特定して、さらに分析する必要があります。この目標を効果的に達成するには、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： B
説明：
カスタム監視メトリックを作成します。API 形式ごとにカスタム監視メトリックを作成することで、各形式で行われた要求の数を追跡できます。
メトリックを使用して要求数を追跡します。メトリックを経時的に監視して、各 API 形式の要求の頻度を記録します。
最も一般的な形式を特定します。メトリックを分析して、要求数が最も多い API 形式を特定します。このアプローチにより、最も一般的に要求される形式を正確に識別できます。
オプション a) では、形式ごとに個別のログ コレクターをデプロイすることを提案していますが、これにより複雑さが増し、分析プロセスが遅れる可能性があります。
レート制限を実装するオプション c) は、最も一般的に要求される形式を特定する必要性に直接対処しません。
オプション d) にはサードパーティの分析が含まれるため、追加の依存関係が発生する可能性があり、カスタム監視メトリックを使用するほど直接的ではない可能性があります。
最も一般的に要求される API 応答形式を決定するコンテキストでは、各形式の要求数を効率的に追跡および分析できるため、オプション b が最も適切なアプローチです。
<details><div>

### Q. 問題45: 未回答
あなたは、重要なワークロードを実行するKubernetesクラスターの堅牢な監視ソリューションを設定する任務を負っています。目標は、クラスター内のすべてのノードを効率的に監視できるサードパーティの監視アプリケーションを展開することです。チームは、監視アプリケーションが各ノードで実行されていることを確認して、クラスターの正常性とパフォーマンスに関する包括的な分析情報を提供する必要があります。
効率とカバレッジを最大化しながらこの目標を達成するために、次の展開戦略のうち最も適切なのはどれですか。
1. 
2. 
3. 
4. 
<details><div>
答え： B
説明：
デーモンセット:DaemonSetは、Kubernetesクラスタに監視アプリケーションをデプロイするのに最も適したオプションです。これにより、監視アプリケーション Pod のコピーがクラスター内のすべてのノードにデプロイされます。このアプローチは、各ノードの正常性とパフォーマンスを監視するための包括的なカバレッジを提供します。
ReplicaSet を使用するオプション a) は、クラスター全体にポッドを分散しますが、すべてのノードが監視アプリケーションのインスタンスを持つことは保証されません。
StatefulSetを使用するオプションc)は、一意のIDを持つステートフルアプリケーションに適していますが、監視アプリケーションには不必要な複雑さをもたらす可能性があります。
LoadBalancerサービスを使用するオプションd)は、外部トラフィックを分散するためのものであり、クラスタ内の監視アプリケーションのデプロイには関係ありません。
サードパーティの監視アプリケーションをデプロイしてKubernetesクラスタ内のすべてのノードを効率的に監視するコンテキストでは、DaemonSetを使用するオプションb)が最適な選択肢です。
<details><div>

### Q. 問題46: 未回答
ある金融サービス会社は、機密性の高い顧客データを収集しており、保存データの暗号化キーを安全に管理する必要があります。この目標を達成するために、Google Cloud Key Management Service(KMS)の使用を検討しています。同社は、暗号化キーを管理し、適切なアクセス制御を確保するための一元化された組織的な方法を求めています。
Google Cloud KMS で安全で整理された鍵管理を実現するには、どのオプションが最適ですか?
1. 
2. 
3. 
4. 
<details><div>
答え： C
説明：
このシナリオでは、金融サービス会社は、保存されている機密性の高い顧客データの暗号化キーを安全に管理する必要があります。 Google Cloud Key Management Service(KMS)は、暗号鍵を管理するための堅牢なソリューションを提供しており、キーリングの使用は、会社の目標を達成するための最適な選択肢です。
キーリング:Google Cloud KMS のキーリングを使用すると、関連する暗号鍵を論理的にグループ化できます。この組織は、キーを管理するための構造化された一元化された方法を提供することで、キー管理を簡素化します。各キーリングは複数のキーのコンテナとして機能し、アクセス制御、キーローテーション、および管理タスクを容易にします。
データの種類ごとに暗号化キーを個別に管理するオプション a) は、キー管理の断片化と複雑さにつながる可能性があります。
暗号化鍵を共有 Google Cloud Storage バケットに保存するオプション b) は、Google Cloud KMS が提供する高度な機能やアクセス制御がないため、安全な鍵管理には推奨されません。
オプションd)は、複数のプロジェクト間で暗号化キーを共有することで、セキュリティと懸念事項の分離を損なう可能性があります。
安全で整理された一元化された鍵管理という会社の目標には、Google Cloud KMS Key Rings を利用するオプション c) が最も適切で最良の選択です。
<details><div>

### Q. 問題47: 未回答
クラウドサービスプロバイダーは、顧客にオンラインファイルストレージサービスを提供しています。サービスレベル目標 (SLO) を定義し、毎月の期間でサービスの可用性が 99.9% 以上である必要があると述べています。SLO では、可用性は、その月にサービスが運用されている時間の割合として測定されることが指定されています。
次のサービス レベル インジケーター (SLI) のうち、可用性に関して定義された SLO のコンプライアンスを追跡し、確実にするための指標を正しく表しているのはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え： D
説明：
このシナリオでは、定義されたサービス レベル目標 (SLO) は、サービスの可用性が月間で 99.9% 以上である必要があるということです。SLO では、可用性は、その月にサービスが運用されている時間の割合として測定されることが指定されています。したがって、このSLOのコンプライアンスを追跡して確認するための適切なサービスレベル指標(SLI)は、サービスの運用時間を直接測定するものです。
平均応答時間を測定するオプション a) は、可用性 SLO と直接一致しません。
失敗した要求の総数を合計要求数の割合として測定するオプション b) では、操作時間を直接測定するものではありません。
オプション c) では、要求総数に対する成功した要求の比率を測定しても、可用性の観点から運用時間を直接測定するものではありません。
サービスの運用時間をパーセンテージで測定するオプション d) は、可用性について定義された SLO と直接一致し、サービスが運用可能であり、月中の少なくとも 99.9% の時間利用可能であることを保証します。
<details><div>

### Q. 問題48: 未回答
ある組織は、Google Kubernetes Engine(GKE)を使用してコンテナ化されたアプリケーションを管理しています。彼らは、ダウンタイムを最小限に抑えながらシームレスな更新を保証するために、ブルー/グリーンデプロイ戦略を実装したいと考えています。同社は、Spinnaker を使用して展開プロセスを調整することを検討しています。
Spinnaker と GKE を使用してブルー/グリーン デプロイを実現するには、組織はどのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： B
説明：
ブルー/グリーンデプロイでは、既存のバージョンと一緒に新しいバージョンのアプリケーションをデプロイし、トラフィックを新しいバージョンに徐々に切り替えます。Spinnaker は、デプロイ パイプラインの管理に役立つツールであり、ブルー/グリーン デプロイに使用できます。
Google Kubernetes Engine(GKE)のコンテキストでは、アプリケーションのライフサイクルを管理するために Kubernetes Deployment を使用するのが最も適切なアプローチです。Kubernetes デプロイメントを使用すると、アプリケーションの望ましい状態を宣言的に管理し、ローリング アップデートやバージョン管理などの機能を提供できます。
GKE での Blue/Green デプロイに Spinnaker を使用する場合、Kubernetes デプロイを管理するように Spinnaker を構成できます。Spinnaker は、更新されたバージョンで新しいレプリカを作成し、定義された展開戦略に基づいてトラフィックを新しいバージョンに段階的に移行できます。
オプションa)、c)、d)には、ReplicaSets、DaemonSets、StatefulSetなどの他のタイプのKubernetesコントローラーが含まれます。これらのコントローラには独自のユースケースがありますが、Spinnakerを使用したブルー/グリーンデプロイメントでは、Kubernetesデプロイメントを使用するのが最も適切な選択です。
したがって、このシナリオではオプションb)が正解です。
<details><div>

### Q. 問題49: 未回答
Lisa は、急成長中のオンライン ゲーム会社である Spark Gaming の IT 管理者です。同社は、インフラストラクチャをクラウドに移行し、Compute Engine などのさまざまなクラウド サービスを利用することを決定しました。Lisa は、移行中のセキュリティとコンプライアンスを確保する任務を負っています。
要件: Lisa は、クラウド サービス全体の管理アクティビティと変更を追跡するために、適切な監査およびログ記録メカニズムを確立する必要があります。
この要件に対処するために、Lisa はどのオプションを選択する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある
説明：
クラウド サービスの管理アクティビティ監査ログを有効にすることは、管理アクティビティと変更を追跡するための適切な監査およびログ記録メカニズムを確立するという要件を満たすための最適なオプションです。管理アクティビティ ログは、サービス構成またはメタデータを変更するアクションをキャプチャし、管理アクションの包括的な記録を提供します。これは、管理者が行った変更を追跡し、説明責任を確保するために重要です。
オプション B、C、D は、セキュリティとコンプライアンスのさまざまな目的に関連しますが、管理アクティビティと変更の追跡に特に重点を置いていません。
B. システム イベント監査ログを有効にします。システム・イベント・ログは、VMインスタンスの起動や停止など、リソースのライフサイクルに対する変更をキャプチャします。リソースの正常性を監視するために重要ですが、すべての管理アクションをカバーしているわけではありません。
C. 使用するサービスのデータ アクセス監査ログを有効にします。データアクセスログは、ストレージ内のデータへのアクセスや変更など、ユーザー提供のデータの読み取りまたは変更を行う操作をキャプチャします。データアクセスの追跡には重要ですが、すべての管理上の変更をカバーできるわけではありません。
D. アクセスの透明性ログを有効にします。アクセスの透明性ログは、クラウドプロバイダーのサポートチームとエンジニアリングチームがリソースを操作するときに実行したアクションを可視化します。透明性は重要ですが、すべての管理アクションを包括的に把握できるわけではありません。
したがって、クラウド サービス内の管理アクティビティと変更の適切な監査ログを確立するという Lisa の要件では、A. サービスの管理アクティビティ監査ログを有効にするのが最善の選択です。
<details><div>

### Q. 問題50: 未回答
SREチームは、最近新しいパブリッククラウドプロバイダーを採用したクラウド環境を管理しています。このプロバイダーは大きな人気を博しており、その結果、チームでは、リソース消費クォータの引き上げのリクエストに関連する大量のチケットが発生しています。これらのクォータ増加要求は、チームの効率と応答性に影響を与えています。
Google の SRE のベスト プラクティスに従って、この状況を改善し、割り当ての引き上げリクエストの処理を改善するにはどうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え： D
説明：
Google の SRE のベスト プラクティスに従って、割り当ての収集、検証、更新を自動化することは、大量の割り当て引き上げリクエストを処理する状況を改善するためのプロアクティブなアプローチです。これらのプロセスを自動化することで、手作業による介入を減らし、エラーを最小限に抑え、要求への迅速な対応を実現します。自動化されたツールやスクリプトを導入することで、SRE チームはリクエストを効率的に検証し、組織のポリシーとの整合性を確保し、必要に応じてクォータを自動的に調整することができます。
オプションA、B、Cは、差し迫った懸念に対処するかもしれませんが、持続可能な解決策を提供しない可能性があります。
A. 専任のスタッフを割り当てます。専任のスタッフを配置することで一時的には安心できるかもしれませんが、問題の根本原因には対処できず、リソースの非効率につながる可能性があります。
B. 現在のクォータを増やす:すべてのユーザーのクォータを増やすと、過剰なプロビジョニングや非効率的なリソース割り当てにつながる可能性があります。
C. 積極的なコミュニケーション戦略を実施する:コミュニケーションは重要ですが、クォータの引き上げ要求を処理するワークロードに直接対処するものではありません。
したがって、SRE の原則に沿った最善の方法は、D. クォータの収集、検証、および更新を自動化する時間を確保することです。これにより、クォータの引き上げ要求を管理する際の効率、正確性、応答性が向上します。
<details><div>

### Q. 問題51: 未回答
シナリオ:
あなたはDevOpsエンジニアであり、サービスメッシュ管理にIstioを使用してKubernetesにデプロイされた複雑なマイクロサービスアーキテクチャの管理を担当しています。チームは、マイクロサービスの 1 つで断続的な障害の報告を受けました。これらのエラーはネットワーク待機時間が原因のようですが、特定の問題を特定し、マイクロサービスがこれらの状況をどのように処理するかを評価する必要があります。
質問:問題のあるマイクロサービスで断続的な障害を診断してシミュレートするには、次のうちどれを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
D. Istio のフォールト挿入は、障害のある動作をシミュレートする特定のマイクロサービスで使用します。
説明：特定のマイクロサービスで断続的な障害を処理するこのシナリオでは、最も効果的なアプローチは、特にそのマイクロサービスで Istio のフォールト挿入を使用することです。これが正しい選択である理由は次のとおりです。
Istio のフォールト挿入機能を使用すると、遅延、エラー、高レイテンシーなど、制御されターゲットを絞ったフォールトを、特定のマイクロサービスに関連するインタラクションに導入できます。これにより、その特定のマイクロサービスで発生している断続的な障害をシミュレートして診断できます。問題のあるマイクロサービスにテストを分離することで、悪条件の処理方法に関する分析情報を得ることができ、断続的な障害の根本原因を特定し、回復性と信頼性を向上させるための戦略を策定できます。
他のオプションが正しくない理由:
A. すべてのマイクロサービスで Istio テレメトリ データを分析して、潜在的なネットワークの問題を特定します。
テレメトリ データの分析は、問題を特定するための貴重なプラクティスですが、1 つのマイクロサービスで特定の障害をシミュレートまたは診断するのには役立ちません。これにより、システムパフォーマンスの概要が表示されます。
B. Istio のトラフィック ルーティング ルールを使用して、問題のあるマイクロサービスからトラフィックを一時的に迂回させます。
トラフィックを迂回させることで、問題のあるマイクロサービスの負荷を軽減できますが、断続的な障害の根本原因に対処したり、診断に役立てたりすることはありません。これは、どちらかというと緩和手法です。
C. Istio 内にグローバル フォールト挿入ポリシーを実装して、すべてのマイクロサービスに制御されたフォールトを導入します。
すべてのマイクロサービスにグローバル障害を実装すると、中断が発生する可能性があり、1 つのマイクロサービスで問題を診断することに特に関心がある場合は、必要または望ましくない場合があります。ターゲットを絞ったテストや診断はできません。
特定のマイクロサービスで断続的な障害を診断してシミュレートするコンテキストでは、問題が発生しているマイクロサービスに Istio のフォールト挿入を使用するオプション D は、問題が発生している特定のマイクロサービスに集中し、障害の根本原因を特定できるため、最適な選択肢です。
<details><div>

### Q. 問題52: 未回答
Google Cloud DevOps の複雑なシナリオでは、ある多国籍 e コマース企業が、オンライン プラットフォームをサポートするために Apache ウェブサーバーを実行する仮想マシン(VM)インスタンスの大規模なフリートを管理しています。同社は、これらのVMで実行されているApacheアプリケーションのパフォーマンスに関する包括的な洞察を得たいと考えており、リクエスト、エラー、レイテンシーに関連するメトリックを収集する予定です。そのために、Google Cloud Monitoring and Logging サービスを利用することにしました。
Apacheアプリケーションに関連する必要なメトリックをキャプチャし、正確で信頼性の高いパフォーマンス監視を保証するには、どのような手順と構成が必要ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
A)ConnectDエージェントを各VMインスタンスにデプロイし、Apacheアプリケーションとの互換性を確保し、適切な認証情報を使用してApacheメトリックを収集してGoogle Cloud Monitoringに転送するように構成します。
説明：
オプション A が推奨される方法です。ConnectDエージェントを各VMインスタンスにデプロイし、Apacheアプリケーションとの互換性を確保し、適切な認証情報を使用してApacheメトリックを収集してGoogle Cloud Monitoringに転送するように構成することを提案しています。このアプローチは信頼性が高く、モニタリングのために Google Cloud サービスと直接統合できます。
他のオプションが正しくない理由:
B)VM インスタンス上の Apache アプリケーションをモニタリングするカスタム スクリプトを作成し、指標を収集してから、その指標を Google Cloud Monitoring and Logging サービスに送信するように設定します。
オプションBでは、カスタムスクリプトの作成が複雑になり、実績のある監視ツールであるConnectDエージェントについては明示的に言及されていません。
C)Apache アプリケーション用の Google Cloud の組み込みモニタリング エージェントを使用して指標をキャプチャし、追加の構成を必要とせずに Google Cloud Monitoring and Logging サービスに転送します。
オプション C は、Apache アプリケーション用の組み込みモニタリング エージェントが存在することを前提としていますが、これは Google Cloud エコシステムでは利用できない可能性があります。信頼性のために、ConnectDのような公式エージェントを使用することを常にお勧めします。
D)ConnectD エージェントを中央コレクタとして 1 つの VM インスタンスにインストールし、サードパーティのモニタリング ツールを使用して、すべての VM インスタンスで実行されている Apache アプリケーションからメトリクスを収集して Google Cloud Monitoring and Logging サービスに送信します。
オプションDは、中央コレクターとサードパーティツールで不必要な複雑さをもたらします。各VMインスタンスでConnectDを直接使用することは、より合理的なアプローチです。
<details><div>

### Q. 問題53: 未回答
Google Cloud DevOps シナリオでは、Google Cloud Platform (GCP) でホストされている Web アプリケーションの高可用性と信頼性を確保する責任があります。アプリケーションは、ロードバランサーの背後にある複数のVMインスタンスに分散されます。目的は、アプリケーションの可用性を監視し、中断が発生した場合にタイムリーな通知を受け取ることです。
アプリケーションのアップタイムと信頼性を維持するために、これを実現するための推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
C) ロードバランサーの IP アドレスとアラートポリシーを使用して稼働時間チェックを作成し、通知をトリガーします。
説明：
オプション C が推奨される選択肢です。これには、ロードバランサーのIPアドレスを監視するアップタイムチェックの作成と、中断が発生したときに通知をトリガーするアラートポリシーの構成が含まれます。このアプローチは、アプリケーションの可用性を監視するための信頼性の高い自動化されたソリューションを提供します。
他のオプションが正しくない理由:
A) ロードバランサーの背後にある個々のVMインスタンスのステータスを定期的に確認し、問題が発生した場合は手動でチームに通知します。
オプション A は、特に DevOps シナリオで、アプリケーションのアップタイムと信頼性を維持するのに適していない、エラーが発生しやすい手動のアプローチを提案します。
B)複雑なカスタム監視スクリプトを実装して、ロードバランサーを監視し、通知を送信します。
オプション B では、カスタム監視スクリプトが必要になるため、保守が困難になり、組み込みソリューションほど効率的ではない可能性があります。
D)Google Cloud のデフォルトのモニタリングおよびアラート システムを利用して、稼働時間のチェックと通知を自動的に処理します。
オプション D は、必要な構成を指定せずに既定の設定に依存することを前提としているため、特定のシナリオに必要なレベルの制御とカスタマイズが提供されない可能性があります。
<details><div>

### Q. 問題54: 未回答
シナリオ：
あなたは「TechSprint Innovations」の DevOps エンジニアで、最近 Google Cloud App Engine に重要なアプリケーションをデプロイしたダイナミックな技術系スタートアップです。このアプリケーションは、リアルタイムデータを処理し、最小限のレイテンシーで顧客にサービスを提供するために不可欠です。最近、パフォーマンスのボトルネックに気付き、根本原因を特定してアプリケーションを最適化し、スケーラビリティとユーザーエクスペリエンスを向上させたいと考えています。
質問：
TechSprint Innovations が Google Cloud App Engine にデプロイしたリアルタイム データ処理アプリケーションのパフォーマンス最適化の取り組みにおいて、CPU 使用率プロファイルを取得し、アプリケーションのパフォーマンスを分析するための推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
C. Google Cloud Profiler API を有効にし、それを使用して Google Cloud App Engine で実行されているアプリケーションの CPU 使用率プロファイルをキャプチャします。
説明：
Google Cloud App Engine で TechSprint Innovations のリアルタイム データ処理アプリケーションの CPU 使用率プロファイルを取得し、アプリケーション パフォーマンスを分析するには、Google Cloud Profiler API を有効にすることをお勧めします。これが最適な選択である理由は次のとおりです。
Google Cloud Profiler API:Google Cloud Profiler は、Google Cloud App Engine など、Google Cloud で実行されているアプリケーションの CPU とメモリの使用状況のプロファイルを取得できる強力なツールです。この API を有効にすると、プロファイリング機能が有効になり、パフォーマンスのボトルネックを特定し、アプリケーションを最適化してスケーラビリティを向上させることができます。CPU 使用率に関する分析情報を提供し、改善が必要な領域を特定するのに役立ちます。
他のオプションが正しくない理由:
A. App Engine インスタンスで Google Cloud Monitoring エージェントを有効にし、組み込みの Cloud Monitoring 機能を利用して CPU 使用率をキャプチャして分析します。
Google Cloud Monitoring は、リソースとアプリケーションのパフォーマンスをモニタリングするための便利なツールですが、Google Cloud Profiler が提供する詳細な CPU プロファイリング機能は提供していません。メトリックと監視に重点を置いています。
B. サードパーティのツールを使用してカスタム プロファイリング ソリューションを構成し、CPU プロファイルをキャプチャし、App Engine アプリケーションのアプリケーション パフォーマンスを分析します。
サードパーティのツールを使用してカスタム プロファイリング ソリューションを作成するのは複雑で、App Engine とシームレスに統合できない場合があります。Google Cloud Profiler は、セットアップと使用が簡単な、プロファイリングのためのネイティブな統合ソリューションを提供します。
D. Stackdriver Trace を実装してアプリケーションのパフォーマンスをモニタリングし、それを使用して App Engine アプリケーションの CPU 使用率プロファイルをキャプチャします。
Stackdriver Trace は、主に分散トレースとアプリケーションのパフォーマンスのモニタリングに重点を置いていますが、Google Cloud Profiler が提供する特定の CPU プロファイリング機能は提供していません。これは、分散システムに関連する待機時間とパフォーマンスのボトルネックを特定するのに適しています。
要約すると、Google Cloud Profiler API を有効にし、それを使用して CPU 使用率プロファイルをキャプチャすることは、Google Cloud App Engine 上のリアルタイム データ処理アプリケーションでアプリケーション パフォーマンスを分析するための推奨的かつ効率的なアプローチであり、TechSprint Innovations がアプリケーションを最適化してスケーラビリティとユーザー エクスペリエンスを向上させるのに役立ちます。
<details><div>

### Q. 問題55: 未回答
シナリオ：
あなたは、Google Cloud でホストされている重要な金融アプリケーションを管理する会社「Skyline Technologies」の DevOps エンジニアです。このアプリケーションはリアルタイムのトランザクションを処理し、最高レベルの信頼性と可用性を必要とします。
質問：
非常に重要な財務アプリケーションを管理するコンテキストで、効果的なインシデント管理のために推奨されないプラクティスは、次のうちどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
B. インシデント発生時の即時対応として、根本原因分析の実施に重点を置く。
非常に重要な財務アプリケーションでは、インシデント発生時の即時対応は、主に可能な限り迅速にサービスを復旧することに重点を置く必要があります。根本原因分析は、将来のインシデントを防止し、アプリケーションを改善するために重要ですが、アプリケーションがダウンした場合の即時対応であってはなりません。このようなシナリオでは、財務上の影響を最小限に抑え、ビジネスの継続性を確保するために、サービスの復元が優先されます。
他のオプションが正しくない理由:
A. インシデント発生時の主な目標としてサービスの復元を優先する: これは、特にミッション クリティカルなアプリケーションにおいて、効果的なインシデント管理を行うために推奨される方法です。
C. インシデント管理手順の包括的なドキュメントを維持する: ドキュメントは、効果的なインシデント管理に不可欠です。これは、チームが確立された手順に従うのに役立ちますが、ベストプラクティスと矛盾することはありません。
D. すべてのチーム メンバーがインシデント管理プロセスにおける各役割を明確に理解していることを確認する: チーム メンバーが自分の役割を確実に理解することは、インシデント管理のベスト プラクティスです。それは効果的な管理と矛盾しませんが、それを補完します。
したがって、オプション B は、インシデントへの即時対応において最も推奨されないプラクティスであり、このコンテキストでは正しい答えになります。

## 3
### Q. 問題1: 未回答
あなたは Google Cloud DevOps エンジニアで、大規模アプリケーションのデプロイ パイプラインの管理を担当しています。このアプリケーションは、Google Cloud Build と Google Artifact Registry を使用して、開発とデプロイのプロセスを合理化します。チームは現在、複数のDockerイメージをビルドしてArtifact Registryに保存する必要がある新機能を実装しています。デプロイメント・パイプラインの一部として、これらのDockerイメージを効率的に管理するために、アーティファクト・レジストリの使用を最適化する必要があります。
このシナリオで Google Artifact Registry の使用を強化するには、次のうちどれを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. アーティファクト・レジストリでコンテンツ・ベースのイメージ保持ポリシーを構成します。

説明：Google Artifact Registry では、バージョニング、脆弱性スキャン、コンテンツベースの保持ポリシーなどの機能を提供することで、Docker イメージを効率的に管理できます。このシナリオでは、コンテンツベースの画像保持ポリシーを構成するのが最も適切なアクションです。これにより、タグ、ダイジェスト、時間などの特定の基準に基づいて保持ルールを定義し、必要なDockerイメージのみがアーティファクト・レジストリに保持されるようにすることができます。これらのポリシーを設定することで、ストレージの使用を最適化し、古いイメージや不要なイメージが自動的に削除されるようにして、ストレージをコスト効率よく管理できます。
正しくないオプション:
A. Docker イメージの保存には、Artifact Registry の代わりに Google Cloud Storage を使用します。
Google Cloud Storageは汎用のオブジェクトストレージソリューションであり、Dockerイメージを保存できますが、コンテナイメージを管理するためにArtifact Registryが提供する特定の機能がありません。アーティファクト・レジストリは、コンテナ・イメージ・ストレージ用に設計されており、Dockerレジストリ機能、バージョン管理、脆弱性スキャンなどの機能が含まれています。
C. アーティファクト・レジストリでDockerイメージのバージョニングを無効にします。
アーティファクト・レジストリのバージョニングは、コンテナ・イメージへの変更を経時的に追跡できる貴重な機能です。バージョン管理を無効にすると、この機能が削除され、変更の追跡、問題の特定、および必要に応じて以前のバージョンへのロールバックが困難になります。
D. Artifact Registry の Docker イメージごとに個別の Google Cloud プロジェクトを使用します。
分離のために別々のプロジェクトを使用することは良い方法ですが、このコンテキストでは最も効率的なソリューションではない可能性があります。Artifact Registry内でコンテンツベースの保持ポリシーを直接構成すると、Dockerイメージごとに個別のプロジェクトを作成することなく、イメージのライフサイクル管理をよりきめ細かく制御できます。このオプションを使用すると、不要な管理オーバーヘッドが発生し、複雑さが増す可能性があります。
<details><div>

### Q. 問題2: 未回答
あなたは Google Cloud DevOps エンジニアで、Google Kubernetes Engine(GKE)上のコンテナを使用してマイクロサービスをデプロイするプロジェクトに取り組んでいます。チームは Google Artifact Registry を使用してコンテナ イメージの保存と管理を行っています。デプロイ プロセスの一環として、コンテナーが効率的にデプロイされ、アプリケーションが需要に基づいてシームレスにスケーリングされるようにする必要があります。
このシナリオで展開プロセスを最適化するには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
正解: A. 新しいコンテナ バージョンをデプロイするためのローリング アップデート戦略を GKE に実装します。

説明：Google Kubernetes Engine(GKE)にローリング アップデート戦略を実装することで、新しいコンテナ バージョンがアプリケーション全体に段階的にロールアウトされ、ダウンタイムが最小限に抑えられ、サービスの中断が回避されます。この戦略により、古いバージョンから新しいバージョンへのスムーズな移行が可能になり、更新プロセス中もアプリケーションの可用性と応答性が維持されます。
正しくないオプション:
B. アーティファクト・レジストリの自動バージョニング機能を無効にして、よりクリーンなイメージ・リポジトリを維持します。
アーティファクト・レジストリの自動バージョン管理は、変更を追跡し、必要に応じて以前のバージョンにロールバックできる便利な機能です。この機能を無効にすると、展開を効果的に管理およびトラブルシューティングする機能が妨げられます。
C. すべてのマイクロサービスに 1 つの大きなコンテナー イメージを使用して、デプロイを簡略化します。
マイクロサービスを個別のコンテナイメージとしてデプロイすることで、スケーラビリティ、保守性、柔軟性が向上します。すべてのマイクロサービスを 1 つの大きなコンテナー イメージに結合すると、非効率性が生じ、スケーラビリティが妨げられ、デプロイ プロセスが複雑になる可能性があります。
D. 新しいコンテナ バージョンを GKE にデプロイする前に、手動承認プロセスを設定します。
手動の承認プロセスでは、追加の制御レイヤーを提供できますが、遅延が発生する可能性があり、展開プロセスの最適化には適していない場合があります。ローリングアップデートなどの自動化された戦略は、タイムリーで信頼性の高いデプロイを確保するためにより効率的です。
<details><div>

### Q. 問題3: 未回答
あなたは Google Cloud DevOps エンジニアで、Google Kubernetes Engine(GKE)と Google Artifact Registry を使用してコンテナ化されたアプリケーションのデプロイ パイプラインの管理を担当しています。チームはアプリケーションのスケーラビリティの向上に取り組んでおり、デプロイ プロセスでさまざまなワークロードを効率的に処理できるようにする必要があります。
このシナリオでは、展開プロセスのスケーラビリティの最適化に寄与するアクションはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. GKE に Horizontal Pod Autoscaling(HPA)を実装して、リソース使用量に基づいてレプリカ Pod の数を動的に調整します。

説明：Google Kubernetes Engine(GKE)に Horizontal Pod Autoscaling(HPA)を実装すると、システムは観測されたリソース使用率に基づいてレプリカポッドの数を自動的に調整できます。これにより、需要の変化に応じてアプリケーションをスケールアップまたはスケールダウンすることで、変動するワークロードを効率的に処理できます。これにより、アプリケーションはリソース要件の増減に自動的に適応し、パフォーマンスとコストの両方を最適化できます。
正しくないオプション:
A. 単一ノードの GKE クラスタを使用すると、リソースのオーバーヘッドが削減され、管理が簡素化されます。
単一ノード クラスタを使用すると、管理が簡素化されますが、スケーラビリティの最適化には貢献しません。スケーラビリティは、ワークロードの要求に基づいてリソースを動的に割り当てることができるマルチノード クラスターを使用することで、より適切に実現されます。
C. Google Artifact Registry でコンテナ イメージのキャッシュを無効にして、デプロイ中に常に最新のイメージがプルされるようにします。
コンテナー イメージのキャッシュでは、以前にプルされたイメージを再利用することで、デプロイ速度を向上させることができます。キャッシュを無効にすると、特に同じイメージを複数回使用する場合に、デプロイにかかる時間が長くなり、不要なリソース消費につながる可能性があります。
D. マイクロサービスごとにレプリカの数を固定して、アプリケーションの動作の一貫性を維持します。
一貫性を維持することは重要ですが、レプリカの数を固定すると、ワークロードの変化に基づいてアプリケーションを動的にスケーリングすることはできません。このアプローチでは、ワークロードに応じてリソースが過小または過剰に使用され、スケーラビリティが最適化されません。
<details><div>

### Q. 問題4: 未回答
医療アプリケーションの Google Cloud DevOps エンジニアは、Google Kubernetes Engine(GKE)上でコンテナ化されたマイクロサービスを安全にデプロイする役割を担っています。このアプリケーションは、Google Artifact Registry を使用してコンテナ イメージを安全に保存します。コンプライアンスとデータプライバシーは医療業界にとって重要であり、チームはこれらの要件を満たすことに重点を置いています。
医療アプリケーションのコンプライアンスとセキュリティのニーズを考慮すると、このシナリオで安全なコンテナイメージのデプロイを確保するために最も適切なアクションはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. GKE にバイナリ認証を実装して、署名および検証済みのコンテナ イメージのみをデプロイするポリシーを適用します。

説明：コンプライアンスとセキュリティが最優先される医療のコンテキストでは、GKE にバイナリ認証を実装することが重要です。Binary Authorization により、署名および検証済みのコンテナー イメージのみが運用環境にデプロイされます。これにより、コンテナイメージの整合性が維持され、不正または改ざんされたイメージのデプロイが防止され、セキュリティとコンプライアンスの懸念に対処できます。
正しくないオプション:
A. パブリック DockerHub レジストリを使用して、イメージの検証と更新に大規模なコミュニティを活用します。
パブリックレジストリを使用すると、機密性の高い医療データが公開され、セキュリティとプライバシーが侵害される可能性があります。さらに、医療業界の規制コンプライアンス要件と一致しない可能性があります。
C. Google Artifact Registry でコンテナ イメージのスキャンを無効にして、医療関連データの潜在的な露出を減らします。
コンテナイメージのスキャンは、イメージの脆弱性を特定するために不可欠です。無効にすると、セキュリティ上の問題のあるイメージがデプロイされる可能性があり、データのプライバシーとセキュリティが重要な医療アプリケーションでは特に危険です。
D. すべてのマイクロサービスで共通のサービス アカウントを共有して、GKE でのアクセス制御を簡素化します。
マイクロサービス間で共通のサービス アカウントを共有すると、アクセス制御が簡略化される可能性がありますが、医療アプリケーションのセキュリティとコンプライアンスのニーズに直接対応することはできません。各マイクロサービスには、最小特権の原則に従うために、独自のアクセス許可セットが必要です。
<details><div>

### Q. 問題5: 未回答
あなたは Google Cloud DevOps エンジニアで、機密性の高い顧客データを処理する金融アプリケーションのデプロイ パイプラインを担当しています。アプリケーションは Google Kubernetes Engine(GKE)にデプロイされたマイクロサービス アーキテクチャを使用し、コンテナ イメージは Google Artifact Registry に保存されます。セキュリティとコンプライアンスは、業界の規制を満たすために最も重要です。
財務アプリケーションのセキュリティとコンプライアンスの要件を考慮すると、このシナリオで安全なコンテナー イメージのデプロイを確保するために最も適切なアクションは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. GKE ワークロード ID を実装して、Kubernetes ポッドに Google Cloud サービスへのきめ細かなアクセス権を付与します。

説明: GKE ワークロード ID を実装すると、Kubernetes ポッドが Google Cloud Identity and Access Management(IAM)のロールを引き受け、きめ細かなアクセス制御が可能になります。これにより、各ポッドに Google Cloud サービスと安全にやり取りするために必要な権限が付与され、機密データを処理する金融アプリケーションのセキュリティとコンプライアンスの要件を満たすことができます。
正しくないオプション:
A. Google アーティファクト レジストリへの一般公開アクセスを許可して、外部パートナーとのコラボレーションを促進します。
アーティファクト・レジストリへのパブリック・アクセスを許可すると、機密性の高い財務データが公開され、セキュリティおよびコンプライアンス規制に違反する可能性があります。コンテナー イメージの機密性と整合性を維持するために、レジストリを非公開にしておくことが不可欠です。
C. Google Artifact Registry の Container Analysis を無効にして、財務データの潜在的な露出を減らします。
コンテナ分析は、コンテナイメージの脆弱性を特定するために重要です。これを無効にすると、セキュリティの問題を検出して対処する機能が妨げられ、侵害されたイメージが金融アプリケーションに展開されるリスクが高まります。
D. アクセス管理をシンプルにするために、すべての GKE クラスタで幅広い権限を持つ 1 つの Google Cloud サービス アカウントを使用します。
広範なアクセス許可を持つ 1 つのサービス アカウントを使用すると、不要なアクセスが提供されるため、セキュリティ リスクが発生します。最小特権の原則に違反し、不正アクセスや意図しないアクションにつながる可能性があります。特定のアクセス許可を持つ個々のサービス アカウントは、より安全な方法です。
<details><div>

### Q. 問題6: 未回答
あなたは Google Cloud DevOps エンジニアで、組織内の機密性の高いプロジェクトのデプロイ インフラストラクチャを管理する任務を負っています。このプロジェクトには機密データが含まれるため、チームは Google Kubernetes Engine(GKE)を使用してデプロイすることを決定しました。セキュリティとプライバシーは最優先事項であり、組織はデータやメタデータをプライベートネットワークから出さないように義務付けています。チームは、これらの厳格なセキュリティ要件を満たすために、プライベート GKE クラスタの使用を検討しています。
このシナリオでプライベート GKE クラスタをデプロイする主なメリットはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. Kubernetes APIサーバーへの外部アクセスを防止することで、アクセス制御とセキュリティを強化しました。

説明: プライベート GKE クラスタをデプロイすると、Kubernetes API サーバーへの外部アクセスが防止され、アクセス制御とセキュリティが強化されます。機密データが関係するこのシナリオでは、API サーバーへのアクセスをプライベート ネットワークに制限することで、セキュリティとプライバシーの要件を満たすことができます。プライベートネットワーク内の内部システムのみがKubernetes APIと対話できるため、追加の保護レイヤーが提供されます。
正しくないオプション:
A. 外部トラフィックのスケーラビリティとロード バランシングが向上しました。
一般に、GKE クラスタはスケーラビリティと負荷分散を提供しますが、プライベート クラスタのデプロイは、外部トラフィック管理ではなく、主にセキュリティを強化するために選択されます。
C. パブリック インターネット リソースへの直接アクセスによる簡素化された展開プロセス。
プライベート GKE クラスタをデプロイすると、パブリック インターネットへのアクセスが制限されるため、シナリオのセキュリティ要件に合致します。ただし、クラスタ内からパブリック インターネット リソースに直接アクセスすると、複雑さが生じる可能性があります。
D. 他組織との共有資源の活用によるコスト削減
プライベート GKE クラスタの使用は、本質的に他の組織との共有リソースを意味するものではありません。コスト削減は、リソースを効率的に使用することで実現できますが、プライベート GKE クラスタをデプロイする直接的なメリットではありません。
<details><div>

### Q. 問題7: 未回答
ミッションクリティカルなアプリケーションのインフラストラクチャの管理を担当する Google Cloud DevOps エンジニアは、堅牢なセキュリティとコンプライアンスのモニタリングを確保する必要があります。この組織は、監査目的での管理活動の追跡に重点を置いています。Google Cloud Platform(GCP)でログ設定を構成して、後で確認するために管理アクションに関する関連情報を取得しています。
GCP で管理アクティビティをトラッキングする目的に最も適した構成はどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. 包括的な可視性のために、データ アクセス ログを含めるように管理アクティビティ ログを構成します。

説明: GCP の管理アクティビティを包括的に把握するには、データアクセスログを含めるように管理アクティビティログを設定することが不可欠です。管理アクティビティログは管理アクション(リソースの作成、削除など)を記録し、データアクセスログは誰がどのリソースにどのようにアクセスしたかに関する詳細を提供します。この組み合わせ構成により、セキュリティとコンプライアンスの監視のためのより詳細な監査証跡が保証されます。
正しくないオプション:
A. Cloud Audit Logs を特定のサービスに対してのみ有効にして、ログの量を減らします。
ログの量は減る可能性がありますが、特定のサービスに対してクラウド監査ログを選択的に有効にすると、管理アクティビティの包括的な監査に必要な重要な情報が省略される可能性があります。
B. 管理アクティビティ ログを無効にして、ログ プロセスを合理化します。
管理者アクティビティ ログを無効にすると、管理アクションの追跡に必要なログ自体が失われ、堅牢なセキュリティとコンプライアンスの監視という目標に反します。
D. ログの保持期間を 24 時間に設定して、ログの迅速な分析とストレージ コストの削減を実現します。
ログの保持期間が短いと、特にコンプライアンスの目的で、組織が徹底的な監査を実施する能力が妨げられる可能性があります。通常、コンプライアンスと監査の要件には、ログの保持期間を長くすることをお勧めします。
<details><div>

### Q. 問題8: 未回答
Google Cloud DevOps エンジニアは、機密性の高い財務レポートを含む Google Cloud Storage(GCS)バケットに保存されているデータを保護する責任があります。セキュリティを強化するには、GCS バケットの権限とアクセス制御の変更を追跡する必要があります。組織では、アクセス権の変更を監視するために詳細な監査証跡が必要です。
Google Cloud Platform(GCP)でGCSバケットの権限変更を追跡するのに最適な設定は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. GCS バケットの管理者アクティビティログを有効にして、バケットレベルで権限の変更と変更をキャプチャします。

説明：GCS バケットの管理者アクティビティログを有効にすることは、権限の変更を追跡するのに最も適した設定です。管理アクティビティログには、バケットレベルでのアクセス制御やアクセス許可の変更など、管理アクションが記録されます。これにより、アクセス権に関連する変更を監視するための包括的な監査証跡が提供され、セキュリティとコンプライアンスが確保されます。
正しくないオプション:
A. オブジェクト変更ログを有効にして、GCS バケット内の個々のオブジェクトに対する変更をキャプチャします。
オブジェクト変更ログは、バケットレベルでのアクセス制御の変更ではなく、オブジェクトレベル(バケット内のファイル)での変更に重点を置くため、権限の変更の追跡には適していません。
B. Stackdriver Logging を利用して、オブジェクトレベルのイベントに基づいて権限の変更を手動で監視する。
Stackdriver Logging は強力なツールですが、権限の変更を手動によるモニタリングに頼ると効率が悪くなり、特に自動トラッキングが重要なシナリオでは見落としにつながる可能性があります。
C. アクセス制御の変更に関するリアルタイムの更新を受信するように GCS バケットの Pub/Sub 通知を設定する。
Pub/Sub 通知は、特定のイベントをリアルタイムで更新しますが、変更履歴を追跡するよりも、アクションをトリガーするのに適しています。管理アクティビティ ログは、アクセス許可の変更など、管理アクションのより包括的な履歴ビューを提供します。
<details><div>

### Q. 問題9: 未回答
重要な電子商取引アプリケーションの監視戦略を実装しており、インシデントを迅速に検出して対応するために適切な監視アプローチを選択する必要があります。ブラックボックス監視の特徴やインシデント対応への適合性について議論しています。
インシデント対応プロセスにおけるブラックボックス監視の使用を最もよく特徴付けるステートメントはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. ブラックボックス監視は、インシデント発生後のインシデントのページングに最もよく使用され、アプリケーションの外部動作に焦点を当てています。

説明: ブラックボックス監視は、アプリケーションの外部動作と応答性の評価に重点を置いているため、インシデント対応に特に効果的です。これは、エンドユーザーの視点から問題を検出し、インシデントが発生した後にページングする場合に最適です。このアプローチにより、アプリケーションの内部動作に関する知識に頼ることなく、ユーザーが直面する問題に迅速に対応できます。
正しくないオプション:
A. ブラックボックス監視は、開発中にアプリケーションのコードベースの潜在的な問題を特定するのに最も効果的です。
ブラックボックス監視は、主にアプリケーションの外部動作に焦点を当てており、開発中のコードベースの分析にはあまり適していません。
B. ブラックボックス監視は、CPU 使用率やメモリ使用率などの内部システム メトリックに関するリアルタイムの洞察を提供するように設計されています。
このステートメントでは、内部システムメトリックへのリアルタイムの分析情報に焦点を当てたホワイトボックス監視アプローチについて説明します。
D. ブラックボックス監視は、パフォーマンスのボトルネックの根本原因を診断するために、詳細なトランザクショントレースをキャプチャするのに適しています。
これは、内部診断のために詳細なトランザクショントレースをキャプチャするホワイトボックス監視アプローチの詳細を示しています。ブラックボックス監視は、外部の動作と応答性に重点を置いています。
<details><div>

### Q. 問題10: 未回答
Google Cloud DevOps エンジニアは、開発チームのコンテナ化されたアプリケーションのデプロイ プロセスを最適化する責任があります。チームは、Google Kubernetes Engine(GKE)を使用してコンテナ化されたワークロードをオーケストレーションし、Google Artifact Registry を使用してコンテナ イメージの保存と管理を行っています。目標は、デプロイ パイプラインのセキュリティと信頼性を強化することです。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: A. Google Artifact Registryで脆弱性スキャン用のContainer Analysisを設定します。

説明: Google Artifact Registryで脆弱性スキャン用にContainer Analysisを設定すると、デプロイメント・パイプラインのセキュリティが強化されます。この機能を使用すると、コンテナイメージの既知の脆弱性を自動的にスキャンし、潜在的なセキュリティリスクに関する洞察を得ることができます。開発ライフサイクルの早い段階で脆弱性を特定して対処することで、安全でないコンテナ化されたアプリケーションを展開するリスクを大幅に軽減できます。
正しくないオプション:
B. アーティファクト・レジストリに保存されているコンテナ・イメージの暗号化を無効にして、アクセス速度を向上させます。
コンテナー イメージの暗号化を無効にすると、イメージのセキュリティが損なわれます。暗号化により、保存された画像が安全に保護され、不正アクセスから保護されます。アクセス速度のために暗号化を犠牲にすることは、推奨されるセキュリティ対策ではありません。
C. コンテナー イメージの格納には、アーティファクト レジストリの代わりにパブリック DockerHub リポジトリを使用します。
Google Artifact Registry は、安全でプライベートなコンテナ イメージ レジストリを提供するように設計されています。DockerHubのパブリックリポジトリを利用すると、機密性の高いコンテナイメージが公開され、アプリケーションのプライバシーとセキュリティが損なわれる可能性があります。
D. アーティファクト・レジストリのDockerイメージ・サイズ制限を増やして、より大きなアプリケーションに対応します。
大規模なアプリケーションでは、画像サイズの制限を調整する必要がある場合がありますが、セキュリティや信頼性の向上には直接関係ありません。サイズ制限は、セキュリティ機能というよりは、容量計画の考慮事項です。セキュリティ対策に対処せずに制限を増やすと、脆弱性が発生する可能性があります。
<details><div>

### Q. 問題11: 未回答
DevOps エンジニアは、複雑な Web アプリケーションの堅牢な監視戦略を設計する責任があります。チームは、包括的なカバレッジを確保するために、さまざまな監視アプローチを検討しています。考慮事項の 1 つは、エンドユーザーの視点からアプリケーションの外部動作を評価するためのブラックボックス監視を実装することです。
Webアプリケーションのパフォーマンスと機能を評価するという文脈で、ブラックボックス監視を最もよく表す特性はどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. 外部ユーザーの視点からアプリケーションの動作と応答性を評価する。

説明：ブラックボックス監視は、外部ユーザーの視点からアプリケーションの動作と応答性を評価することに重点を置いています。これには、アプリケーションの内部動作を知らずにアプリケーションを監視および評価することが含まれます。このアプローチは、実際のシナリオでアプリケーションがどのように実行されるかを理解するのに役立ち、ユーザーが遭遇する可能性のある問題を特定するのに役立ちます。
正しくないオプション:
A. CPU 使用率やメモリ使用率などの内部システム メトリックを監視します。
ここでは、内部システムメトリックを監視し、アプリケーションの内部動作に関する洞察を得ることを含むホワイトボックス監視について説明します。
C. コードベースを分析し、潜在的なソフトウェアの脆弱性を特定する。
これはセキュリティ分析とコードレビューに関係しますが、特にブラックボックス監視とは関係ありません。ブラックボックス監視は、内部コード分析ではなく、外部動作に焦点を当てています。
D. 詳細なトランザクショントレースをキャプチャして、アプリケーションの内部ワークフローを理解する。
これは、ホワイトボックス監視に関連するインストルメンテーションまたはトレースアプローチの詳細を示しています。ブラックボックス監視は、エンドユーザーが経験するアプリケーションの外部動作に関係します。
<details><div>

### Q. 問題12: 未回答
大規模なアプリケーションに取り組む DevOps エンジニアは、ハイブリッド環境とマルチクラウド環境全体でコンテナを管理、オーケストレーションするための Anthos の導入を検討しています。お客様は、異なる環境間でアプリケーションの一貫性、スケーラビリティ、および管理の容易さを確保する展開戦略を実装する必要があります。
一貫性を維持し、さまざまな環境へのデプロイを可能にし、アプリケーションを管理するための統一された宣言型アプローチを提供する上で重要な役割を果たす Anthos コンポーネントはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解:A. Anthos Config Management

説明: Anthos Config Management は、アプリケーションを管理するための統一された宣言型のアプローチを提供することで、異なる環境間で一貫性を維持する上で重要な役割を果たします。これにより、DevOpsチームはクラスタ間で一貫したポリシーを定義して適用し、基盤となるインフラストラクチャに関係なく、アプリケーションが同じ構成と設定でデプロイされるようにすることができます。
正しくないオプション:
B. Anthos Identity サービス
Anthos Identity Service は、ID とアクセスの管理機能の提供に重点を置き、アプリケーションのデプロイの一貫性を維持することを主に担当しません。
C. Anthos サービス メッシュ
Anthos Service Mesh は、アプリケーション内のマイクロサービス通信を管理、保護するために設計されていますが、異なる環境間で一貫性を維持するための主要コンポーネントではありません。
D. Anthos Migrate (英語)
Anthos Migrate は、仮想マシン(VM)をコンテナや Kubernetes に移行することに重点を置いており、異なる環境間でのアプリケーション デプロイの一貫性を確保するための主要なコンポーネントではありません。
<details><div>

### Q. 問題13: 未回答
ハイブリッド環境とマルチクラウド環境全体でアプリケーションを管理、デプロイするために Anthos の導入を検討している。セキュリティチームは、統一されたセキュリティ体制を確保するために、すべてのクラスタで一貫したセキュリティポリシーとアクセス制御を維持することに特に関心を持っています。このイニシアチブを主導する DevOps エンジニアは、これらのセキュリティ上の懸念に対処する Anthos コンポーネントを特定する必要があります。
複数のクラスタに一貫したセキュリティ ポリシーとアクセス制御を適用し、一元化された統合セキュリティ管理ソリューションを提供するように特別に設計された Anthos コンポーネントはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解:D. Anthos Policy Controller

説明：Anthos Policy Controller は、Anthos の複数のクラスタに一貫したセキュリティ ポリシーとアクセス制御を適用するように特別に設計されています。一元化された統合セキュリティ管理ソリューションを提供し、組織がハイブリッド環境とマルチクラウド環境全体で一貫したポリシーを定義、適用、監査できるようにします。
正しくないオプション:
A. Anthos Config の管理
Anthos Config Management は、クラスタ間で構成と設定の一貫性を維持する役割を担いますが、セキュリティ ポリシーとアクセス制御の適用には主眼を置いていません。
B. Anthos Identity サービス
Anthos Identity Service は ID 管理とアクセス管理の機能を提供しますが、クラスタ間で一貫したセキュリティ ポリシーを適用するための主要コンポーネントではありません。
C. Anthos サービス メッシュ
Anthos Service Mesh はマイクロサービス通信を管理するために設計されており、セキュリティ ポリシーとアクセス制御を複数のクラスタに適用するための主要コンポーネントではありません。
<details><div>

### Q. 問題14: 未回答
Google Cloud Platform(GCP)でのコンテナ化されたアプリケーションのデプロイと管理を担当するDevOpsエンジニアは、CI/CDパイプラインで使用されるコンテナイメージのセキュリティと整合性を確保したいと考えています。組織では、コンテナー イメージの脆弱性スキャンとライセンス コンプライアンス チェックに重点が置かれています。
脆弱性スキャンを実行し、Container Registryに格納されているコンテナイメージのライセンスコンプライアンスを確保するために特別に設計されたGCPサービスまたはツールはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解:C. コンテナ分析

説明：GCPのコンテナ分析は、脆弱性をスキャンし、コンテナレジストリに保存されているコンテナイメージのライセンスコンプライアンスを確保するために特別に設計されています。コンテナイメージのセキュリティステータスに関する洞察を提供し、脆弱性を特定して修復し、ライセンス要件へのコンプライアンスを確保できるようにします。
正しくないオプション:
A. Google Cloud セキュリティ コマンド センター
Security Command Center は全体的なセキュリティの可視性を提供しますが、Container Analysis はコンテナイメージの脆弱性スキャンとライセンスコンプライアンスのための専用ツールです。
B. クラウドビルド
Cloud Build は GCP の CI / CD サービスであり、コンテナ イメージのビルドとデプロイに使用できますが、脆弱性スキャンやライセンス コンプライアンスのための特定の機能は提供していません。
D. Google Kubernetes Engine(GKE)セキュリティ グループ
GKE セキュリティ グループは、Kubernetes クラスタのネットワークレベルのセキュリティ ポリシーに使用され、コンテナ イメージの脆弱性スキャンやライセンス コンプライアンスのために特別に設計されたものではありません。コンテナ分析は、この目的により適したツールです。
<details><div>

### Q. 問題15: 未回答
Google Cloud Platform(GCP)環境でログの管理を担当するGCP DevOpsエンジニアは、アプリケーションログに含まれる機密情報を処理する任務を負っています。この機密情報を除外して Cloud Logging コンソールに表示されないようにすると同時に、フィルタリングされたログが Google Cloud Storage(GCS)バケットに安全に保存されるようにするソリューションを実装する必要があります。
Cloud Logging コンソールでログから機密情報をフィルタリングし、フィルタリングしたログを GCS バケットに安全に保存するには、どのようなソリューションが最適ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正しいオプション: C. Cloud Logging のフィルタとシンクを使用します。

説明：オプション C は、Cloud Logging コンソールでログから機密情報をフィルタリングし、フィルタリングしたログを GCS バケットに安全に保存する場合に最も適したソリューションです。
Cloud Logging フィルタ:Cloud Logging フィルタを利用すると、機密情報を保存または表示する前にログから除外するルールを定義できます。これにより、フィルタリングされたログが Cloud Logging コンソールに表示されなくなります。
Cloud Logging シンク:Cloud Logging でシンクを設定すると、GCS バケットなどの他の宛先にログをエクスポートできます。フィルタとシンクを組み合わせることで、フィルタリングされたログを GCS バケットに送信して安全に保管できます。
他のオプションが正しくない理由:
A. Cloud Logging の除外を使用します。Cloud Logging の除外では、Cloud Logging コンソール内でログをフィルタリングできますが、フィルタリングされたログを GCS バケットなどの外部ストレージにエクスポートするための直接的なメカニズムは提供されません。除外は、コンソールに表示されるログを制御するのに適していますが、外部の宛先へのエクスポートには適していません。
B. アプリケーション コードを変更します。機密情報を除外するようにアプリケーション コードを変更することは有効なアプローチですが、Cloud Logging フィルタを使用する場合ほどスケーラブルで柔軟性がありません。また、フィルタリングされたログを含むすべてのログを GCS にエクスポートすると、慎重に管理しない限り、機密データが漏洩する可能性があります。
D. 機密性の高いログを手動で削除します。Cloud Logging コンソールで機密性の高いログを手動で確認して削除することは、効率的でも安全でもありません。自動化が不十分で、人為的ミスにつながる可能性があります。さらに、手動削除に依存していては、GCS で安全に保管するための体系的なアプローチは提供されません。
要約すると、オプション C は、Cloud Logging フィルタを使用して機密情報を除外し、シンクを使用してフィルタリングされたログを GCS バケットにエクスポートして安全に保管することで、包括的で自動化されたソリューションを提供します。
<details><div>

### Q. 問題16: 未回答
あなたは、Google Cloud Platform(GCP)上の仮想マシン(VM)のフリートの管理を担当するDevOpsエンジニアです。組織は最近アプリケーションを GCP に移行したため、モニタリングとトラブルシューティングの目的で一元化されたログ ソリューションを設定する必要があります。目標は、システム ログ、アプリケーション ログ、およびその他の関連情報を VM からキャプチャすることです。
GCP VM で一元的なログ記録を実現し、システムログとアプリケーション ログを分析のために効率的に収集するには、どのアクションが最も適切ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: A. 各 VM にログ エージェントをインストールして、ログを Cloud Storage にストリーミングします。

説明: 各 VM にログ エージェントをインストールすることは、GCP で集中ログ記録を実現するための最も適切な選択肢です。ロギング エージェントは、システムログやアプリケーション ログなどのログを各 VM から Cloud Logging にストリーミングします。これにより、ログが効率的に収集、保存され、一元的に分析できるようになります。通常、Cloud Storage はログのストリーミングに直接使用されません。代わりに、ログは Cloud Logging に直接ストリーミングされます。
正しくないオプション:
B. Cloud Monitoring の組み込みのログ記録機能を VM に利用します。
Cloud Monitoring はログ記録機能を提供しますが、直接ログを保存するよりも、ログに基づくモニタリングとアラートに適しています。推奨されるアプローチは、Cloud Logging を使用することです。
C. Stackdriver Error Reporting を使用して、ログを自動的にキャプチャして分析します。
Stackdriver Error Reporting は、汎用の集中ログ ソリューションとしてではなく、アプリケーション エラーを自動的に検出して分析することに重点を置いています。
D. 各 VM から Cloud Logging にログを手動でエクスポートして、一元的なストレージを確保します。
手動エクスポートは、特に動的な環境では、集中ログ記録の効率的またはスケーラブルなソリューションではありません。ロギング・エージェントを使用すると、自動化された合理化されたアプローチが提供されます。
<details><div>

### Q. 問題17: 未回答
Google Cloud Platform(GCP)で Google Compute Engine (GCE) インスタンスを管理する DevOps エンジニアは、仮想マシンの自動再起動操作を監視および分析する必要があります。組織は、システムの信頼性を高めるために、自動再起動の背後にある理由を理解することに特に関心を持っています。
Google Compute Engine インスタンスの自動再起動オペレーションに関する情報を記録する GCP のログタイプはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解:B. Google Cloud システム ログ

説明: Google Cloud システムログには、Google Compute Engine インスタンスの自動再起動など、システムレベルのイベントとオペレーションに関する情報が記録されます。これらのログは、メンテナンスイベントやシステム障害など、再起動の背後にある理由に関する詳細を提供します。Google Cloud システムログは、システムレベルのアクティビティを包括的に把握し、GCE インスタンスの全体的な健全性と信頼性に影響を与えるイベントをモニタリング、分析するために利用できます。
正しくないオプション:
A. Google Cloud 監査ログ
Google Cloud 監査ログは、自動再起動などのインスタンスレベルの運用イベントではなく、API オペレーションと管理アクティビティの記録に重点を置いています。
C. Google Compute Engine インスタンスのログ
インスタンスログにはインスタンスイベントに関する情報が含まれる場合がありますが、Google Cloud システムログは、再起動を含むシステムレベルのイベントに関する詳細情報の記録に特化しています。
D. Google Cloud モニタリング ログ
Google Cloud Monitoring ログは、主にモニタリング対象リソースから指標、時系列データ、ログをキャプチャします。Google Cloud システムログは、インスタンスを含むリソースの全体的な健全性に関する分析情報を提供しますが、自動再起動などの特定の運用イベントに関する詳細情報には、より適しています。
<details><div>

### Q. 問題18: 未回答
あなたは、ログの一元化と管理のための Stackdriver Logging など、Google Cloud Platform(GCP)サービスを広範に使用するプロジェクトの DevOps リーダーです。チームには、ログを外部システムにエクスポートするために、特定のプロジェクトのログシンクを作成、変更、管理する機能が必要です。
GCP で Stackdriver Logging のログシンクを管理するために必要な権限をチーム メンバーに付与するには、どのような操作が最も適切ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. 、、および権限 、、および、 を持つカスタム IAM ロールを作成し、このロールをチーム メンバーに付与します。logging.sinks.createlogging.sinks.updatelogging.sinks.delete

説明: Stackdriver Logging でログシンクを管理するために必要な権限をチームメンバーに付与するには、ログシンクの管理に必要な特定の権限を持つカスタム IAM ロールを作成する必要があります。アクセス許可、、および権限は、ログ シンクの作成、変更、および削除に不可欠です。logging.sinks.createlogging.sinks.updatelogging.sinks.delete
正しくないオプション:
A. チームメンバーに Cloud IAM レベルでの IAM ロールを付与します。logging.viewer
logging.viewerログへの読み取り専用アクセスを提供しますが、ログ シンクの管理に必要なアクセス許可は付与しません。
B. Access Context Manager を構成して、チーム メンバーがログ シンクを管理できるようにします。
Access Context Manager は、ログシンクのアクセス許可を管理するようには設計されていません。IAM ロールは、この目的のための標準的なアプローチです。
D. 組織ポリシーを使用して、指定されたチーム メンバーに対してのみログ シンク管理のアクセス許可を有効にします。
組織ポリシーは、通常、ログ シンク管理のようなきめ細かなアクセス許可の付与には使用されません。IAM ロールは、このユースケースにより適しています。
<details><div>

### Q. 問題19: 未回答
DevOps チームは、重要なマイクロサービスベースのアプリケーションを Google Kubernetes Engine(GKE)にデプロイしました。アプリケーションのフロントエンドは、HTTP(S) Google Cloud Load Balancer(GCLB)イングレスを介して外部ユーザーに公開されます。組織は、フロントエンドの最適なパフォーマンスと応答性を確保することに重点を置いており、関連するメトリックに基づいて自動スケーリングを実装する必要があります。
適切なサービスレベル指標(SLI)に基づいてアプリケーションのフロントエンドのデプロイをスケーリングするには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. Stackdriver カスタム メトリック アダプターをインストールし、GCLB によって提供されるリクエスト数を使用するようにポッドの水平オートスケーラーを構成します。

説明：Stackdriver カスタム メトリック アダプターをインストールすると、リクエスト数などの GCLB 指標を自動スケーリングの決定に活用できます。これは、実際のユーザートラフィックに基づいてアプリケーションのフロントエンドのパフォーマンスと応答性を最適化するための適切なアプローチです。
正しくないオプション:
A. Liveness プローブと Readiness プローブからの平均応答時間を使用するようにポッドの水平オートスケーラーを構成します。
応答時間は重要なメトリックですが、フロントエンドの負荷を直接表すとは限らず、自動スケーリングに使用すると予期しない動作が発生する可能性があります。
B. Prometheus と Grafana を使用してカスタム メトリック ソリューションを実装し、アプリケーションのフロントエンド パフォーマンスを測定します。
Prometheus と Grafana はモニタリングのための強力なツールですが、GKE の自動スケーリング機能とシームレスに統合されない可能性があり、カスタム ソリューションではより多くのメンテナンスが必要になる場合があります。
D. NGINX 統計エンドポイントを公開し、NGINX デプロイによって公開される要求メトリックを使用するように水平ポッド オートスケーラーを構成します。
NGINX の統計情報を公開することは有効なオプションですが、Stackdriver と統合することで、モニタリングと自動スケーリングに対するより統一された GCP ネイティブのアプローチが提供されます。また、さまざまなコンポーネント間で一貫したメトリック管理が可能になります。
<details><div>

### Q. 問題20: 未回答
Google Cloud Platform(GCP)でホストされている複雑なウェブアプリケーションのDevOpsの取り組みをリードしています。アプリケーションは複数のマイクロサービスで構成されており、組織は、運用環境でのバグや停止の可能性を減らすための堅牢なテスト戦略を強調しています。目標は、開発とテストのワークフローを最適化することです。
開発者とテスト担当者の多様なニーズを考慮して、効果的な開発およびテスト環境を構築するために、どのような包括的なアプローチを採用すべきですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. コード変更の記述とテスト用に特別に調整された開発環境と、新機能の構成、実験、および徹底的なロード テストのための個別のテスト環境を作成します。

説明：ベスト プラクティスは、コード開発と初期テスト専用の開発環境を用意し、開発者がコードの変更に独立して取り組めるようにすることです。さらに、新機能の包括的なテスト、構成、実験、および徹底的な負荷テストのために、別のテスト環境を確立する必要があります。この分離は、制御を維持し、問題を切り分け、テストが開発プロセスに悪影響を与えないようにするのに役立ちます。
正しくないオプション:
A. 開発とテストの両方に 1 つのステージング環境を確立し、すべてのチームが協力してリソースの競合を回避できるようにします。
環境が 1 つあると、リソースの競合が発生したり、問題の切り分けが困難になったりして、開発とテストの両方の有効性に影響を与える可能性があります。
B. 開発環境とテスト環境を別々に実装し、それぞれが運用環境を厳密にミラーリングして、ソフトウェア開発ライフサイクル全体で一貫性を確保します。
運用環境のミラーリングは有益ですが、開発用とテスト用に環境を分けることで、柔軟性と制御性が向上します。
C. コード開発に運用環境を活用し、機能フラグを使用して、テスト フェーズ中に新機能の公開を制御します。
開発に本番環境を使用することは、本番環境で直接問題を引き起こすリスクがあるため、一般的にはお勧めしません。
E. それぞれが特定の機能またはマイクロサービスを表す複数の分離された環境を設定して、開発とテストを並行して行い、俊敏性を高めます。
特定の機能の並列環境は俊敏性を高めることができますが、異なる分離された環境間の複雑さと依存関係が生じる可能性があります。このアプローチは、特定のシナリオに適している場合もありますが、普遍的に適用できるとは限りません。
<details><div>

### Q. 問題21: 未回答
あなたは、Google Cloud Platform(GCP)でホストされている重要な金融アプリケーションのリリース プロセスの監督を担当する DevOps エンジニアです。このアプリケーションには複雑な財務計算が含まれており、最近のソフトウェアリリースにより、これらの計算に関連する予期しないエラーが発生しました。組織は、将来のリリース、特に重要な財務計算に関連するエラーで、このようなエラーを防ぐことの重要性を強調しています。
ソフトウェアリリース時の財務計算エラーのリスクを軽減するために、どのような対策を講じる必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. このタイプのエラーをキャッチするテスト ケースが、新しいソフトウェア リリースの前に正常に実行されることを確認します。

説明：ソフトウェアのリリース中に財務計算でエラーが発生するリスクを軽減するには、これらの計算を対象とする包括的なテストケースを用意することが重要です。各リリースの前にこれらのテストケースを正常に実行することで、新しいソフトウェアバージョンが展開される前に、重要な財務計算に関連するエラーが特定され、対処されます。
正しくないオプション:
A. アプリケーションのコードベースに対して定期的なセキュリティ監査を実施し、各リリースの前に潜在的な脆弱性を特定して修正します。
セキュリティ監査は重要ですが、主に脆弱性の特定に重点が置かれており、財務計算に関連するエラーに直接対処できない場合があります。
B. 包括的な監視およびアラート システムを実装して、ソフトウェアのリリース中に財務計算の異常をリアルタイムで検出します。
監視およびアラートシステムは、異常をリアルタイムで検出するために役立ちますが、財務計算のリリース前の徹底的なテストの必要性に取って代わるものではありません。
D. ブルーグリーン展開戦略を活用して、異なるアプリケーションバージョンをシームレスに切り替え、ソフトウェアリリースの影響を最小限に抑えます。
ブルーグリーン展開では、リリース中のダウンタイムとリスクを最小限に抑えることに重点を置いていますが、財務計算のエラーの特定に直接対処できない場合があります。
E. 財務計算コンポーネントに特に焦点を当てた負荷テストを実施して、パフォーマンスのボトルネックを特定し、安定性を確保します。
負荷テストはパフォーマンスの検証には重要ですが、財務計算の精度に関連するエラーをキャッチするには不十分な場合があります。これは、これらの計算に重点を置いたターゲットを絞ったテストケースの直接的な代替ではありません。
<details><div>

### Q. 問題22: 未回答
あなたは、Google Cloud Platform(GCP)でホストされている重要なeコマースアプリケーションのパフォーマンス最適化を監督するDevOpsエンジニアです。最近、ユーザーはページの読み込み時間に時折遅延があり、全体的なユーザーエクスペリエンスに影響を与えていると報告しています。タスクは、パフォーマンスのボトルネックを特定して解決し、最適なアプリケーションの応答性を確保することです。
パフォーマンスの問題を効果的に分析して対処し、タイミング関連のデータを Stackdriver に送信して詳細な分析を行うには、どのようなアプローチを採用すべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. Stackdriver Profiler を使用してアプリケーションをインストルメント化し、関数レベルのタイミング データを自動的に収集し、遅延の原因となっている特定のコード ホットスポットを特定します。

説明: Stackdriver Profiler は、アプリケーション内の詳細な関数レベルのタイミング データを収集するように設計されており、遅延の原因となっている特定のコード ホットスポットを特定できます。Stackdriver Profiler でアプリケーションをインストルメント化すると、詳細な分析と最適化に不可欠なタイミング関連データの自動収集が可能になります。
正しくないオプション:
A. Cloud Profiler を導入して、CPU とメモリの使用状況データを自動的に収集して分析し、アプリケーションのパフォーマンスのボトルネックに関する洞察を提供します。
Cloud Profiler は CPU とメモリの使用状況データの収集に重点を置いていますが、Stackdriver Profiler と同じレベルの詳細な関数レベルのタイミング データは提供されない場合があります。
C. Cloud Monitoring を利用して、アプリケーションのレイテンシを追跡するためのカスタム指標を作成し、予想されるパフォーマンスからの逸脱をチームに通知するアラートを設定します。
Cloud Monitoring ではカスタム指標とアラートを提供できますが、Stackdriver Profiler が提供する詳細な関数レベルのプロファイリングは提供されない場合があります。
D. Cloud Debugger をデプロイして、パフォーマンス関連のインシデント発生時のアプリケーションのランタイム状態のスナップショットをキャプチャし、インシデント後の分析を容易にします。
Cloud Debugger は、デバッグ目的でアプリケーションのランタイム状態のスナップショットを取得することに重点を置いており、継続的なパフォーマンス分析と最適化には最適なツールではない可能性があります。
<details><div>

### Q. 問題23: 未回答
GCP DevOps エンジニアは、新しいマイクロサービスベースのアプリケーションを複数のリージョンにデプロイし、世界中のユーザーが低レイテンシでアクセスできるようにする任務を負っています。アプリケーションは、コンピューティング インスタンス、ストレージ、ネットワーク コンポーネントなど、さまざまな GCP リソースに依存しています。
DevOpsエンジニアが、アプリケーションを複数のリージョンにデプロイするための容量計画中に実行する推奨アクションは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. リソース割当て制限の確認: ターゲット・リージョンのリソース割当て制限を調べて、必要なコンピュート・インスタンス、ストレージおよびその他のリソースをリージョン割当て制限を超えずにプロビジョニングできることを確認します。

説明：アプリケーションを複数のリージョンにデプロイするためのキャパシティ プランニングでは、各ターゲット リージョンで GCP によって課されるリソース割り当て制限を考慮することが重要です。リソース割当て制限を確認することで、必要なコンピュート・インスタンス、ストレージおよびその他のリソースを制限なくプロビジョニングできます。このプロアクティブな手順により、デプロイの問題を回避し、インフラストラクチャが目的のワークロードをサポートできるようになります。
他のオプションが正しくない理由:
A. 帯域幅要件を見積もる: マイクロサービス間の通信で予想されるネットワーク帯域幅要件を決定し、選択したリージョンが十分なネットワーク容量を提供することを確認します。
帯域幅要件の見積もりは重要ですが、コンピュート・インスタンスおよびストレージのプロビジョニングに直接影響するリソース割当て制限の主な考慮事項ではありません。
B. 地域データセンターの見直し: 各ターゲット地域で利用可能な地域データセンターの数を評価して、アプリケーションコンポーネントを均等に分散し、最適なフォールトトレランスを実現します。
アプリケーション・コンポーネントの分散とフォールト・トレランスは有効な考慮事項ですが、各リージョンのコンピュート・インスタンスおよびストレージのリソース割当て制限には直接対応していません。
D. 待機時間メトリックの評価: ターゲット リージョン間の待機時間メトリックを測定して、待機時間が最も短いリージョンを特定し、重要なマイクロサービスをデプロイするためにそれらのリージョンを優先します。
レイテンシー評価は、パフォーマンスの最適化に重点を置いていますが、リソースクォータの制限には特に対応していません。これは、ユーザー エクスペリエンスを最適化するための重要な考慮事項ですが、リソースの可用性に関する容量計画には適していません。
<details><div>

### Q. 問題24: 未回答
GCP DevOps エンジニアは、Google Container Registry(GCR)でホストされているコンテナ化されたアプリケーションの継続的インテグレーションとデプロイのプロセスを合理化する任務を負っています。この組織は、新しいバージョンのアプリケーション イメージがレジストリにプッシュされるたびに、自動ビルド プロセスをトリガーすることを目指しています。目標は、効率的で自動化されたワークフローを実装することです。
アプリケーション イメージが Google Container Registry にプッシュされたら自動ビルド プロセスを開始し、シームレスな継続的インテグレーションとデプロイを実現するには、どのようなアプローチを採用すべきでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. 新しいイメージが Container Registry にプッシュされたときにイベントを発行する Cloud Pub/Sub トピックを統合し、自動デプロイのためにこのトピックをリッスンするように Spinnaker パイプラインを構成します。

説明: Cloud Pub/Sub トピックを Container Registry と統合すると、イベント駆動型モデルを作成できます。新しい画像がプッシュされたときにイベントを発行すると、Spinnaker とのシームレスな接続がトリガーされます。この Cloud Pub/Sub トピックをリッスンするように Spinnaker パイプラインを構成すると、リアルタイム イベントに応答して自動デプロイが開始され、継続的インテグレーションとデプロイの目標に沿っています。
正しくないオプション:
A. コンテナ レジストリの変更をモニタリングし、カスタム アラートを作成して Cloud Build をトリガーし、ビルド プロセスを自動的に開始するように Cloud Monitoring を構成します。
Cloud Monitoring アラートは便利ですが、ビルド プロセスに遅延が生じる可能性があり、カスタム アラートは、Spinnaker パイプラインをトリガーするための Cloud Pub/Sub を使用したイベントドリブン アプローチほど柔軟ではない可能性があります。
B. Container Registry の変更を検出し、イメージのプッシュ時に自動ビルド プロセスのために Cloud Build を直接呼び出す Cloud Functions の関数を実装します。
Cloud Functions はイベントドリブン ワークフローに使用できますが、Cloud Build を直接呼び出すことは、デプロイのトリガーに Pub/Sub トピックを使用する場合ほどスケーラブルで柔軟性がない可能性があります。
D. 定期的な Cloud Scheduler ジョブを設定して、Container Registry で新しいイメージを定期的にチェックし、事前定義された間隔で Cloud Build をトリガーしてビルド プロセスを開始します。
Cloud Scheduler ジョブでは、ビルド プロセスに遅延が発生する可能性があり、チェックの定期的な性質が、イメージのプッシュに応答してビルドを即座に開始するという目標と一致しない可能性があります。
詳細については、以下をお読みください
Pub/Sub メッセージでの Spinnaker のトリガー
<details><div>

### Q. 問題25: 未回答
GCP DevOps エンジニアは、Google Cloud Platform でホストされる重要なウェブ アプリケーションの信頼性を確保する責任があります。組織は、既存のコードベースに変更を加えることなく、アプリケーションの信頼性を測定したいと考えています。目標は、ユーザー エクスペリエンスに関する分析情報を提供し、改善の余地がある領域を特定するソリューションを実装することです。
アプリケーションコードに変更を加えることなくアプリケーションの信頼性を測定し、ユーザーエクスペリエンスを非侵入的に評価するには、どのようなアプローチを採用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. アプリケーションを使用してユーザージャーニーをシミュレートする新しい合成クライアントを作成し、パフォーマンス メトリックをキャプチャし、信頼性の非侵入型評価を提供します。

説明：合成クライアントを作成するには、アプリケーションとのユーザー操作をシミュレートして生成する必要があります。このアプローチにより、既存のコードベースを変更することなく、パフォーマンス メトリックを測定し、アプリケーションの信頼性を評価できます。模擬テストは、ユーザーエクスペリエンスを評価し、改善の余地がある領域を特定するための非侵入型のプロアクティブな方法を提供します。
正しくないオプション:
A. Cloud Monitoring を利用して稼働時間チェックとレイテンシ モニタリングを設定し、リアルタイムの指標をキャプチャしてアプリケーションの信頼性を評価します。
Cloud Monitoring はリアルタイムの指標を提供しますが、ユーザー ジャーニーをシミュレートしたり、ユーザー エクスペリエンスを非侵入的に評価したりすることはできません。
B. リクエスト トレースを分析してパフォーマンスのボトルネックを特定するように Cloud Trace を構成し、ユーザー ジャーニーと改善の余地のある領域に関する分析情報を提供します。
Cloud Trace はリクエスト トレースの分析に役立ちますが、ユーザー ジャーニーの全体像を把握していない可能性があり、完全に非侵入型であるとは限りません。
C. Cloud Logging を統合してアプリケーション ログとエラー レポートをキャプチャし、信頼性関連の問題を遡及的に分析できるようにします。
Cloud Logging はログやエラーのキャプチャに役立ちますが、レトロスペクティブ分析に重点が置かれており、アプリケーション コードを変更しない限り、ユーザー エクスペリエンスや信頼性に関するリアルタイムの分析情報が得られない場合があります。
<details><div>

### Q. 問題26: 未回答
あなたは、Google Cloud Platform(GCP)上の重要なウェブアプリケーションの運用を担当するDevOpsエンジニアです。アプリケーションの最近のアップグレード後、ユーザーエクスペリエンス全体に影響を与えるデータベースタイムアウトの問題がユーザーから報告されています。ここでのタスクは、アプリケーション・コードを大幅に変更することなく、データベース・タイムアウトの問題の根本原因を特定することです。
アプリケーションのアップグレード後にデータベースのタイムアウトの問題の根本原因を効率的に特定するには、どのようなアプローチを採用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. StackDriver Profiler を使用してアプリケーションのリソース使用率を確認する

説明: 開発者がアプリケーションのアップグレード時に接続を閉じるのを忘れたか、アプリケーションで構成された接続プールのサイズが大きすぎる可能性があります。これらのシナリオでは、特定の数の接続が確立された後にデータベース接続がタイムアウトする可能性があります。Cloud SQL インスタンスへの接続数は、Cloud Monitoring または Cloud SQL Admin API を使用して確認することもできます。
正しくないオプション:
A. Cloud Trace データを分析して、データベースとのやり取りに重点を置き、アプリケーションのリクエスト フローのレイテンシのボトルネックを特定します。
Cloud Trace はレイテンシのボトルネックを分析するのに便利ですが、特定のボトルネックを特定するために重要な StackDriver Profiler と同じ詳細データを提供しない場合があります。
B. Cloud Logging を統合してアプリケーション ログとエラー レポートをキャプチャし、アップグレード中にデータベース関連の問題を遡及的に分析できるようにします。
Cloud Logging はログやエラーのキャプチャに役立ちますが、遡及的分析に重点を置いています。StackDriver Profiler と同じリアルタイムの分析情報は提供されない場合があります。
C. アプリケーションがデプロイされているコンピューティング エンジン VM インスタンスのシリアル ポートを確認します。
この場合、シリアルポートのログは、アプリケーションの接続数に関連する適切な情報を提供しません。これらのログは、VM インスタンスにデプロイされたアプリケーションに関する情報ではなく、VM インスタンスに関する情報を提供します。
<details><div>

### Q. 問題27: 未回答
GCP DevOps エンジニアは、Google Kubernetes Engine(GKE)にデプロイされた大規模なマイクロサービス アーキテクチャの管理を担当します。GKE クラスタの効果的なモニタリングは、パフォーマンスの問題を特定して対処するために不可欠です。Kubernetes 環境の正常性とパフォーマンスに関する包括的な分析情報を確実に取得する必要があります。
DevOps エンジニアが GKE クラスタの堅牢な Kubernetes Engine Monitoring を実現するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. 組み込みの GKE モニタリングを利用する: ネイティブの Kubernetes 指標を提供する組み込みの GKE モニタリング機能を活用します。Stackdriver Monitoring を設定し、事前定義されたダッシュボードを使用して、GKE クラスタのパフォーマンスと健全性に関する分析情報を取得します。

説明：Google Kubernetes Engine(GKE)には、ネイティブの Kubernetes 指標を提供する組み込みのモニタリング機能が用意されています。これらの組み込み機能を活用することで、DevOpsエンジニアは、大規模なカスタム構成を必要とせずに、堅牢なKubernetesエンジン監視を実現できます。Stackdriver Monitoring を設定することで、エンジニアは事前定義されたダッシュボードを利用して、GKE クラスタのパフォーマンス、健全性、リソース使用率に関する分析情報を得ることができます。
他のオプションが正しくない理由:
A. カスタム Prometheus 構成を実装する: Prometheus を GKE と統合し、カスタマイズされた Prometheus 構成を作成して詳細な指標を収集します。Grafana を使用して、Kubernetes メトリックの視覚化と分析を行います。
Prometheus と Grafana は強力なモニタリング ツールですが、GKE はセットアップと使用が簡単なネイティブ モニタリング機能を提供します。
B. Kubernetes API サーバーログを有効にする: Kubernetes API サーバーのログ記録を有効にし、Stackdriver Logging を活用してログを分析します。ログエントリに基づいてアラートを設定し、GKE クラスタの異常を検出します。
Kubernetes APIサーバーログの監視は、リアルタイムのパフォーマンスとヘルスの監視ではなく、監査証跡とイベントに重点を置いています。GKE には、これらの目的のための専用の指標が用意されています。
C. ログ収集用に Fluentd をデプロイする: Fluentd を各ポッド内にログ コレクターとしてインストールし、ログを集約して Stackdriver Logging に転送します。Stackdriver Monitoring を利用して、Kubernetes の指標を可視化し、カスタム ダッシュボードを作成します。
ログの収集には Fluentd が一般的に使用されますが、GKE にはログ管理のための Stackdriver Logging との統合が組み込まれています。ここでは、ログではなくKubernetesのメトリクスに焦点を当てています。
組み込みの GKE モニタリング機能を利用することで、DevOps エンジニアはモニタリングの設定を合理化し、GKE クラスタのパフォーマンスと健全性に関する包括的な分析情報を得ることができます。
<details><div>

### Q. 問題28: 未回答
GCP DevOps エンジニアとして、Google App Engine でホストされているグローバルにアクセス可能なアプリケーションのデプロイと運用を監督しています。アプリケーションは大量のユーザー トラフィックを処理するため、最適なパフォーマンスを確保し、潜在的な問題を特定するために、現在の接続数を監視する必要があります。目標は、適切な指標を選択して、この App Engine デプロイの現在の接続数をキャプチャすることです。
App Engine にデプロイされたグローバルにアクセス可能なアプリケーションの現在の接続数を把握するには、どの指標を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は D.appengine.googleapis.com/flex/connections/current

説明：App Engine にデプロイされたグローバルにアクセス可能なアプリケーションの現在の接続数をキャプチャするには、正しい指標は です。このメトリックは、現在の接続数に関するリアルタイムの情報を提供し、アプリケーションの接続ステータスを監視および分析できるようにします。appengine.googleapis.com/flex/connections/current
正しくないオプション:
ある。appengine.googleapis.com/system/flex/connections
このメトリック構造は正しくありません。現在の接続数をキャプチャするための正しい構造は、「system」プレフィックスを付けない必要があります。
B.appengine.googleapis.com/system/connections
このメトリック構造は正しくありません。現在の接続数をキャプチャするための正しい構造は、「フレックス」環境に固有である必要があります。
C.appengine.googleapis.com/global/flex/connections/current
このメトリック構造は正しくありません。「flex」環境で現在の接続数をキャプチャするための正しい構造には、「global」プレフィックスは含まれていません。正しい構造は です。appengine.googleapis.com/flex/connections/current
詳細については、以下をお読みください
Google Cloud の指標
<details><div>

### Q. 問題29: 未回答
あなたは、Google Cloud Platform(GCP)上で高度に動的で分散されたマイクロサービス アーキテクチャを管理する DevOps エンジニアです。最近のインシデントで、チームはマイクロサービスの 1 つでパフォーマンスの問題に遭遇し、インシデント発生時のアプリケーションの状態を分析する必要があります。この組織は、効率的なインシデント解決とダウンタイムの最小化に重点を置いています。
インシデント発生時のパフォーマンス問題の分析に役立てるために、変数やコールスタック情報など、アプリケーションの状態の詳細なスナップショットをキャプチャできる GCP 機能はどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解:A. Stackdriver Debugger

説明: GCP の Stackdriver Debugger では、実行時のアプリケーションの状態の詳細なスナップショット(デバッグ スナップショット)をキャプチャできます。これには、変数、呼び出し履歴情報、およびその他の関連するコンテキストが含まれます。デバッグスナップショットは、アプリケーションの本番環境に影響を与えることなく、パフォーマンスの問題やその他のバグを分析および診断する場合に特に役立ちます。
正しくないオプション:
B. クラウドトレース
Cloud Trace は、分散トレースとパフォーマンス分析用に設計されていますが、変数とコールスタック情報を含むアプリケーションの状態の詳細なスナップショットは提供しません。
C. エラー報告
エラー報告は、エラーを自動的に検出して報告するのに役立ちますが、デバッグ目的でアプリケーションのランタイム状態の詳細なスナップショットをキャプチャすることはありません。
D. クラウド監視
Cloud Monitoring はモニタリング機能とアラート機能を提供しますが、デバッグ用の詳細なランタイム スナップショットはキャプチャしません。Stackdriver Debugger は、GCP でこの目的に特化したツールです。
<details><div>

### Q. 問題30: 未回答
Google Compute Engine(GCE)インスタンスの管理を担当する DevOps エンジニアは、監査とトラブルシューティングの目的でメタデータの変更に関連する情報をキャプチャするソリューションを実装する必要があります。組織では、インスタンス メタデータの変更を追跡するための包括的なログ記録アプローチが必要です。
Google Compute Engine インスタンスのメタデータの変更に関連する情報を取得し、監査と説明責任のための詳細なログを提供するのに最適な GCP の機能またはツールはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解:D. 管理者アクティビティログ

説明: GCP の管理者アクティビティ ログには、Google Compute Engine インスタンスとそのメタデータなど、リソースに加えられた変更の詳細なログが表示されます。管理アクティビティ ログは、管理者による変更の追跡、説明責任の確保、インスタンス メタデータの変更に対する包括的な監査証跡の提供に適しています。
正しくないオプション:
A. クラウド監査ログ
Cloud Audit Logs は GCP のさまざまなアクティビティをキャプチャしますが、管理者アクティビティ ログは管理者による変更に特化しているため、インスタンス メタデータの変更を追跡するのに適しています。
B. Stackdriver デバッガ
Stackdriver Debugger は、デバッグ目的で実行時にアプリケーションの状態のスナップショットをキャプチャするように設計されており、インスタンス メタデータの変更の追跡には重点が置かれていません。
C. Cloud Monitoring でのインスタンス変更の追跡
このオプションは妥当に思えるかもしれませんが、2022 年 1 月の私の知識の締め切りの時点では、Cloud Monitoring には特定の「インスタンス変更追跡」機能はありません。ただし、管理アクティビティログは、インスタンスメタデータに加えられた変更に必要な可視性を提供します。
<details><div>

### Q. 問題31: 未回答
あなたは、Google Cloud Platform(GCP)にデプロイされたトラフィックの多いウェブアプリケーションの管理を担当する Google Cloud DevOps エンジニアです。責任の一環として、アプリケーションの動作とパフォーマンスを監視するための堅牢なログ記録および分析システムを確立する必要があります。この組織は、コンプライアンスのために一元化されたログ分析と長期保存の必要性を強調しています。
ログを一元化し、GCP での分析のための長期保存を可能にするという目標に最も適した構成はどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. Cloud Logging から BigQuery にログをエクスポートして一元的な分析と長期保存を行うようにログシンクを構成する。

説明: Cloud Logging から BigQuery にログをエクスポートするようにログシンクを構成することは、ログを一元化し、長期保存を有効にするのに最も適した選択肢です。この設定により、ログを BigQuery に保存できるため、長期間にわたる分析、クエリ、コンプライアンス レポート作成のためのスケーラブルでフルマネージド ソリューションが提供されます。
正しくないオプション:
A. Cloud Monitoring を使用してログをリアルタイムで表示し、問題が発生したときに特定します。
Cloud Monitoring はリアルタイムの可視性を提供しますが、長期保存や広範なログ分析用には設計されていません。問題の即時検出と解決に重点を置いています。
B. Cloud Logging にログベースの指標を実装して、カスタムログの分析と可視化を行う。
Cloud Logging のログベースの指標は、ログエントリに基づいてカスタム指標を作成するのに便利ですが、BigQuery へのログのエクスポートと同じ長期保存機能や分析機能は提供されません。
D. Stackdriver Error Reporting を有効にして、アプリケーション エラーを自動的に検出して分析する。
Stackdriver Error Reporting は、アプリケーションのエラーを自動的に検出して分析するように設計されていますが、長期保存や包括的なログ分析には適していません。ログを BigQuery にエクスポートすると、このような要件に対する柔軟性が高まります。
<details><div>

### Q. 問題32: 未回答
Google Cloud DevOps エンジニアは、Google Cloud Platform(GCP)にデプロイされた多層アプリケーションのネットワーク コストを最適化する任務を負っています。このアプリケーションは、内部で通信し、外部ユーザーにサービスを提供するマイクロサービスで構成されます。組織は、パフォーマンスや信頼性を損なうことなく、費用対効果の高いソリューションを求めています。
効率的な通信を確保し、費用を最小限に抑えながら、アプリケーションのネットワークコストを削減するには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. ネットワーク サービスには Standard レベルを選択し、信頼性とパフォーマンスの高いネットワーク通信のための費用対効果の高いオプションを提供します。

説明: Google Cloud のネットワーク サービスに Standard レベルを選択すると、信頼性とパフォーマンスに優れたネットワーク通信を実現する費用対効果の高いオプションが提供されます。Standard レベルは、Premium レベルと比較してネットワークのエグレス コストが低いため、必要なレベルのパフォーマンスを維持しながらネットワーク費用を最適化したいと考えている組織に適しています。
正しくないオプション:
A. VPN 接続を実装してオンプレミスのデータ センターに接続し、Dedicated Interconnect を活用してコスト効率の高いネットワーク トラフィックを実現します。
VPN と Dedicated Interconnect は安全で信頼性の高い接続を提供しますが、GCP 環境内の内部通信のネットワーク コストを削減するという目標に直接対応できない場合があります。
B. VPC 内の通信にのみ内部 IP アドレスを使用するように Kubernetes Engine ノードを構成します。
内部通信に内部 IP アドレスを使用することは良い方法ですが、ネットワーク コストに大きな影響を与えない可能性があり、ネットワーク トラフィックのより広範なコストに関する考慮事項には対応していません。
C. Google Cloud CDN を利用してコンテンツをキャッシュして提供し、繰り返しデータ転送の必要性を減らします。
Google Cloud CDN は、VPC 内のマイクロサービス間の内部通信に直接関係しないコンテンツの配信とキャッシュに適しています。内部ネットワークのコストを削減するという具体的な目標には対応していない可能性があります。
<details><div>

### Q. 問題33: 未回答
経験豊富なGCP DevOpsエンジニアは、組織の運用に不可欠なグローバルに分散されたマイクロサービスベースのアプリケーションのアーキテクチャとデプロイを任されています。サイト信頼性エンジニアリング (SRE) の原則を採用して、高可用性、スケーラビリティ、回復性を実現することを目標としています。アプリケーションは、異なるリージョン間で効率的かつ一貫してデプロイする必要がある複数のサービスで構成されています。目的は、重要なマイクロサービスの 1 つに仮想マシン インスタンスをデプロイするための最も適切な方法を決定することです。
仮想マシン インスタンスをデプロイするには、可用性と拡張性に優れたマイクロサービスのための高度な SRE のベスト プラクティスに従って、どのようなアプローチを採用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. インスタンス テンプレートを使用して Managed Instance グループを作成し、事前定義された構成に基づいて VM インスタンスの自動スケーリング、分散、およびリージョン デプロイを有効にします。

説明：グローバルに分散されたマイクロサービス アプリケーションと高度な SRE プラクティスのコンテキストでは、インスタンス テンプレートを使用してマネージド インスタンス グループを作成することが推奨されるアプローチです。これにより、自動スケーリング、リージョン間の分散が容易になり、構成の一貫性が確保されます。マネージド インスタンス グループは、リージョン デプロイ機能と、大規模なインスタンスを効率的に管理する機能を提供することで、高度な SRE の原則に沿っています。
正しくないオプション:
A. Terraform を活用して、カスタマイズされた構成で個々の VM インスタンスを定義およびプロビジョニングし、各インスタンスをきめ細かく制御できるようにします。
Terraform は強力なコードとしてのインフラストラクチャ ツールですが、SRE のベスト プラクティスのコンテキストでは、マネージド インスタンス グループと同じレベルの自動化、スケーラビリティ、およびリージョン デプロイ機能を提供しない場合があります。
B. Deployment Manager を使用して、特定の構成を持つ一連の VM インスタンスのデプロイ テンプレートを作成し、自動化されたバージョン管理されたデプロイを容易にします。
Deployment Manager は自動化に適したツールですが、マネージド インスタンス グループには、自動スケーリングやリージョン デプロイなどの追加の利点があり、高度な SRE プラクティスとの整合性が高まります。
C. 単一の VM インスタンスを実装し、需要に基づいて自動スケーリングを構成し、簡素化されたデプロイ モデルを維持しながら、インスタンスの数を動的に調整します。
自動スケーリングは有益ですが、単一のVMインスタンスに依存すると、可用性が高くグローバルに分散されたマイクロサービスアプリケーションに必要なリージョンの冗長性と回復力が得られない可能性があるため、このオプションは高度なSREプラクティスとの整合性が低下します。
<details><div>

### Q. 問題34: 未回答
あなたは、マイクロサービス・アーキテクチャを利用したクラウドベースのドキュメント・コラボレーション・プラットフォームのDevOpsリーダーです。このプラットフォームには、ドキュメント編集、ファイルストレージ、リアルタイムコラボレーションなどのさまざまなモジュールが含まれています。各モジュールは、特定の機能の提供を担当する専用のマイクロサービスによって強化されています。これらのマイクロサービスで一時的な障害が発生することがありますが、プラットフォームは、ユーザーが使用可能な機能を引き続き使用できるようにすることで、これらのインスタンスを適切に処理します。
信頼性が高くユーザーフレンドリーなエクスペリエンスを確保するために、サービス レベル目標 (SLO) を確立し、マイクロサービスの正常性を測定するための最適なサービス レベル インジケーター (SLI) を特定する必要があります。
マイクロサービスの健全性を評価し、ドキュメントコラボレーションプラットフォームの堅牢なSLOを確立するのに最も適したSLIはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：

説明：
正解:B. マイクロサービスの稼働時間 SLI: マイクロサービスの総数に対する正常なマイクロサービスの比率
一時的な障害が発生する可能性のあるドキュメント コラボレーション プラットフォームのコンテキストでは、マイクロサービスの稼働時間と正常性を測定することが重要です。マイクロサービスの総数に対する正常なマイクロサービスの比率として定義されるマイクロサービス稼働時間 SLI は、最も適切な選択肢です。
他のオプションが正しくない理由:
A. 機能可用性 SLI: モジュールの総数に対する使用可能な機能モジュールの比率
機能の可用性は重要ですが、個々のマイクロサービスの正常性を直接反映しない場合があります。一部のモジュールは、基盤となるマイクロサービスで問題が発生している場合でも使用できる場合があります。
C. ユーザーインタラクションレイテンシーSLI:プラットフォームとのユーザーインタラクションの平均応答時間
ユーザー操作の待機時間は、応答時間とユーザー エクスペリエンスに重点を置いていますが、一時的な障害時のマイクロサービスの正常性を直接測定できない場合があります。このシナリオでは、稼働時間がより適切です。
D. データ整合性 SLI: データ同期の試行回数の合計に対する、マイクロサービス間で正常に同期されたデータの比率
データの整合性は重要ですが、マイクロサービス全体の正常性ではなく、同期プロセスに固有です。稼働時間は、このコンテキストに対するより広範で一般化可能な尺度です。
<details><div>

### Q. 問題35: 未回答
「CloudOps Innovations」の GCP DevOps エンジニアは、Google Cloud Platform(GCP)で大規模なデータ処理ワークロードを実行するコストを最適化する任務を負っています。ワークロードには、データ処理のための仮想マシン (VM) の一貫した使用が含まれており、財務チームはコスト削減を実現する方法を模索しています。
質問：このシナリオを前提として、GCP DevOps エンジニアがデータ処理ワークロードの VM 使用量を安定して予測可能に保ちながらコスト削減を実現するには、どのようなアプローチが推奨されるでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. 確約利用割引は、一定期間の VM 使用量にコミットし、オンデマンド価格と比較して割引料金を提供することで使用します。

説明：
VM を一貫して使用して大規模なデータ処理ワークロードのコストを最適化するシナリオでは、確約利用割引を活用することをお勧めします。その理由は次のとおりです。
確約利用割引:
一定期間 (1 年または 3 年など) の VM 使用量を一定量にコミットすることで、標準のオンデマンド価格と比較して割引料金の恩恵を受けることができます。
予測可能なワークロード:
確約利用割引は、時間の経過に伴う使用量が予測可能で安定したワークロードに最適です。これは、継続的な VM インスタンスを必要とするデータ処理ワークロードに合わせて調整されます。
有期契約:
有期のコミットメントにより、より良い財務計画と予算編成が可能になります。これにより、指定した期間のコストを予測できます。
他のオプションが正しくない理由:
A. プリエンプティブル VM インスタンスを実装します。
プリエンプティブルVMインスタンスはコストが低くなりますが、中断される可能性があるため、継続的で予測可能な処理を必要とするワークロードには適していません。
B. 継続利用割引を活用する:
継続利用割引は、VM インスタンスの継続使用に基づく自動割引を提供しますが、一定期間の確約利用割引と同じレベルのコスト削減は提供されない場合があります。
D. Cloud Functions を使用してサーバーレス アーキテクチャを実装します。
サーバーレス アーキテクチャはコストを最適化できますが、すべてのワークロードに適しているとは限りません。前述のデータ処理ワークロードでは、継続的な VM インスタンスが必要になる可能性が高く、サーバーレス アプローチはあまり実用的ではありません。
オプション C は、説明されているシナリオで安定した予測可能な VM 使用量を維持しながらコスト削減を実現するための最も推奨される選択肢です。
<details><div>

### Q. 問題36: 未回答
GCP DevOpsエンジニアは、Google Cloud Platform(GCP)でホストされているウェブアプリケーションのパフォーマンスを最適化する任務を負っています。アプリケーションは複数のマイクロサービスで構成されており、システム全体の応答性を測定および監視するには、最も適切なサービスレベル指標(SLI)を選択する必要があります。
GCP 上のウェブ アプリケーションのマイクロサービス アーキテクチャの応答性を測定およびモニタリングするのに最も適したサービス レベル 指標(SLI)は、次のうちどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正しいオプション: A. レイテンシー

説明：レイテンシーは、Webアプリケーションのマイクロサービスアーキテクチャの応答性を測定および監視するための重要なサービスレベル指標(SLI)です。具体的には、要求がクライアントからサーバーに移動し、戻ってくるのにかかる時間を定量化します。GCP DevOps エンジニアは、レイテンシをモニタリングすることで、システムがユーザーのリクエストにどれだけ迅速に応答しているかを直接把握できます。待機時間の値が小さいほど応答性が高いことを示し、値が高いほどパフォーマンスの問題または遅延を示す可能性があります。
他のオプションが正しくない理由:
イ. CPU 使用率: CPU 使用率の監視は、システム全体の正常性を理解するために不可欠ですが、Web アプリケーション内の個々のマイクロサービスの応答性を示す最も直接的な指標ではない場合があります。CPU 使用率が高い場合は、リソースの制約を示している可能性がありますが、待機時間や応答性の問題の根本原因を特定できない可能性があります。
ウ. ネットワークスループット:ネットワークスループットは、ネットワーク上で転送されるデータの量を測定し、ネットワークパフォーマンスの評価に関連します。ただし、マイクロサービスの応答性を直接表すものではない場合があります。ネットワークのスループットは高くても、ネットワークの輻輳やパケット損失などのさまざまな要因により、遅延の問題が発生する可能性があります。
D. エラー率:エラー率の監視は、システムの信頼性を確保するために重要ですが、応答性の最も直接的な指標ではない可能性があります。エラー率が低いアプリケーションでも遅延が発生する可能性があり、エラー率のみに焦点を当てると、ユーザーエクスペリエンスと応答性の全体像を把握できない可能性があります。
要約すると、CPU使用率、ネットワークスループット、エラー率は、システムパフォーマンスのさまざまな側面を示す貴重な指標ですが、レイテンシーは、Webアプリケーションのマイクロサービスアーキテクチャの応答性を直接評価するための最も適したSLIです。
<details><div>

### Q. 問題37: 未回答
あなたは Google Cloud DevOps エンジニアで、組織にとって重要な財務分析アプリケーションを Google Kubernetes Engine(GKE)にデプロイする任務を負っています。関係する財務データは非常に機密性が高いため、組織はセキュリティとプライバシーを懸念しています。デプロイ戦略の一環として、チームはプライベート GKE クラスタの使用を検討しています。
このシナリオでプライベート GKE クラスタに通常関連付けられる主要なセキュリティ構成は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. パブリック エンドポイント アクセスが無効になっています。制限付きアクセスが有効な許可されたネットワーク。

説明: プライベート GKE クラスタは、特に機密性の高い財務データを含むシナリオでは、通常、パブリック エンドポイント アクセスが無効になっている構成になっています。代わりに、Kubernetes APIサーバーへのアクセスは組織内の許可されたネットワークに制限され、より安全な環境が提供されます。これにより、外部からの不正アクセスを防止し、機密性の高い財務情報を処理するためのセキュリティのベストプラクティスに合わせることができます。
正しくないオプション:
A. Kubernetes APIサーバーのパブリックエンドポイントを有効にして、外部アクセスを容易にします。
パブリック エンドポイントを有効にすると、Kubernetes API サーバーがパブリック インターネットに公開され、特に機密性の高い財務データを扱う場合は、プライベート GKE クラスタのセキュリティ目標に反します。
B. 外部の金融データプロバイダーとのシームレスな通信を確保するために、無制限のネットワークアクセスを実装します。
無制限のネットワーク アクセスはセキュリティの脆弱性をもたらす可能性があり、プライベート GKE クラスタの一般的なセキュリティ構成ではありません。機密性の高い環境では、安全で制御された通信が優先されます。
C. 承認されたネットワークを無効にして、コラボレーションのために組織内でより広範なアクセスを許可します。
許可されたネットワークを無効にすると、プライベート GKE クラスタでアクセスを制限するという目標と矛盾します。許可されたネットワークへのアクセスを制限することは、機密性の高いシナリオで制御とプライバシーを維持するための重要なセキュリティ対策です。
<details><div>

### Q. 問題38: 未回答
GCP DevOps エンジニアは、アプリケーションを Google Kubernetes Engine(GKE)クラスタにデプロイするための継続的インテグレーション / 継続的デプロイ(CI / CD)パイプラインの実装を主導しています。Cloud Build は、ビルドとデプロイのプロセスを自動化するための CI / CD ツールとして選択されています。GKE クラスタで認証プロセスを保護することは、重要な考慮事項です。
DevOps エンジニアが CI / CD パイプラインで Cloud Build を使用してアプリケーションをデプロイする際に、GKE クラスタで認証するための推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. コンテナ開発者ロールの割り当て: Cloud Build Service アカウントに GKE クラスタのコンテナ開発者ロールを割り当てます。このロールは、アプリケーションのデプロイに必要なアクセス許可を付与し、デプロイ プロセス中にシームレスで安全な認証を保証します。

説明：Cloud Build Service アカウントにコンテナ デベロッパー ロールを割り当てることは、GKE クラスタでの安全な認証に推奨される方法です。このロールは、アプリケーションを GKE クラスタにデプロイするために必要な権限を提供し、最小権限の原則を維持し、安全な CI / CD パイプラインを確保します。
他のオプションが正しくない理由:
A. GCP サービス アカウント キーを生成する: 専用の GCP サービス アカウントを作成し、JSON キーを生成して、暗号化された変数として Cloud Build に安全に保存します。デプロイ プロセス中にこのキーを使用して GKE クラスタで認証するように Cloud Build を構成します。
このオプションは認証を提供しますが、機密性の高い資格情報の処理と保護が必要であり、困難な場合があります。ロールの割り当ては、より制御された安全なアプローチです。
C. GKE Workload Identity を使用する: Cloud Build と GKE クラスタの Workload Identity を有効にします。Workload Identity を活用して GKE クラスタでのセキュアで自動化された認証を行うように CI / CD パイプラインを構成します。
ワークロード ID は認証の有効なオプションですが、複雑になる可能性があります。多くの場合、ロールを直接割り当てる方が簡単で、特定のアクセス許可を付与するために広く使用されています。
D. Cloud Build API を有効にする: Cloud Build API を有効にし、GKE クラスタでの認証に API キーを利用するように CI / CD パイプラインを構成します。これにより、デプロイ中の Cloud Build と GKE クラスタ間の通信が安全に行われます。
Cloud Build API を有効にしても、GKE クラスタ認証のニーズに直接対応できない場合があります。API キーは通常、CI / CD パイプラインの GKE クラスタで認証するのではなく、API へのアクセスに使用されます。ロールを割り当てると、より的を絞ったソリューションが提供されます。
<details><div>

### Q. 問題39: 未回答
あなたは、Google Cloud Platform(GCP)でホストされている重要なeコマースアプリケーションのロギングインフラストラクチャの管理を担当するDevOpsエンジニアです。アプリケーションはカスタム Ubuntu VM で実行され、ログを収集して Stackdriver Logging に転送するための Stackdriver Logging エージェントをデプロイしました。
最近、VM の syslog ログが GCP Console の Stackdriver Logging Viewer の [すべてのログ] プルダウン リストに表示されないことに気付きました。アプリケーションは重要なイベントを記録するためにsyslogに大きく依存しており、効果的な監視とトラブルシューティングを確実に行うには、この問題に迅速に対処する必要があります。
質問：このシナリオでは、Stackdriver Logging Viewer の [すべてのログ] プルダウン リストに syslog ログが表示されない問題をトラブルシューティングして解決するために、最初に行うべき手順は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解：
D. VM の 1 つに SSH 接続し、コマンドを実行して fluentd プロセスのステータスを確認し、潜在的な問題を特定します。

説明：
Fluentdプロセスチェック:
Fluentd は、ログを収集して Stackdriver Logging に転送する役割を担っています。VM の 1 つで Fluentd プロセスの状態を確認すると、エージェントが問題なく実行されているかどうかを把握できます。
またはなどのコマンドを使用して、Fluentd がアクティブでエラーが発生していないかどうかを確認します。sudo systemctl status google-fluentdps aux | grep fluentd
ローカルセキュリティログ監視:
VM 上のローカル syslog ファイルを検査すると、syslog ログが生成されているかどうか、および VM レベルで問題があるかどうかを判断するのに役立ちます。
syslogファイル(またはなど)をチェックして、ログが生成されていることを確認し、Fluentdに関連するエラーメッセージを特定します。/var/log/syslog/var/log/messages
Stackdriver Logging への接続:
VM 上の Fluentd プロセスが Stackdriver Logging に正常に接続され、ログを転送していることを確認します。Fluentdログ(通常は「」)でエラー・メッセージまたは接続の問題を確認します。/var/log/google-fluentd/google-fluentd.log
このアプローチでは、VM 上のログ収集の状態を直接調べることができ、ソースでの潜在的な問題を特定して対処するのに役立ちます。
他のオプションが正しくない理由の説明:
A. ログビューアでエージェントのテストログエントリを確認し、syslogログ収集に関連するエラーや問題を確認します。
このオプションでは、テスト ログ エントリの確認に重点が置かれていますが、VM 上の Fluentd プロセスまたはローカル ログ生成の状態は直接調査されません。問題の根本原因に関する詳細な分析情報が提供されない場合があります。
B. すべての VM に最新バージョンの Stackdriver エージェントをインストールして、互換性を確保し、最新機能にアクセスできるようにします。
ソフトウェアを最新の状態に保つことは良い習慣ですが、やみくもにアップグレードしても、目前の問題にすぐに対処できない場合があります。アップグレードを実行する前に、まず現在の構成と状態を調査する方が効率的です。
C. VM に関連付けられているサービス アカウントを確認し、ログ記録に必要なアクセス許可を付与するための monitoring.write スコープがあることを確認します。
このオプションでは、サービス アカウントの権限を確認する必要がありますが、これは Stackdriver Logging の全体的な機能にとって重要です。ただし、特にログにアクセス許可に関連する明示的なエラーがない場合は、最初の手順ではない可能性があります。VM 上の Fluentd プロセスとローカル ログを直接調べることは、より迅速なトラブルシューティング手順です。
<details><div>

### Q. 問題40: 未回答
GCP DevOps エンジニアは、変動するバースト的なワークロードが発生するスケーラブルなウェブ アプリケーションのインフラストラクチャを最適化する任務を負っています。アプリケーション アーキテクチャにはマイクロサービスが含まれており、目標は、システムが動的な需要を処理できるようにしながら、インフラストラクチャをコスト効率の高いものにすることです。
DevOpsエンジニアは、最適なコスト効率を得るために、プリエンプティブルVMインスタンスの使用を検討する必要があるユースケースはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. バッチ処理: データ分析やレポート生成など、中断を許容できる定期的なバッチ処理タスクを実行します。

説明：プリエンプティブルVMインスタンスは、通常の(プリエンプティブルでない)インスタンスによってプリエンプトできるため、中断を許容できるワークロードに適しています。データ分析やレポート生成などのバッチ処理タスクは、多くの場合、定期的に行われるため、中断を適切に処理するように設計できます。このシナリオでプリエンプティブル VM を使用すると、プロセス全体を損なうことなく、インフラストラクチャのコストを大幅に削減できます。
他のオプションが正しくない理由:
A. データベースサーバー:重要なアプリケーションデータを格納するプライマリデータベースサーバーを展開します。
プリエンプティブルVMは、急な通知で終了する可能性があるため、プライマリデータベースサーバーなどの重要なサービスには適していません。データベース サーバーには、継続的な可用性が必要です。
C. 継続的インテグレーション サーバー: アプリケーション コードのビルドおよびテスト プロセスを処理する継続的インテグレーション サーバーを実行します。
継続的インテグレーション サーバーは、安定性と一貫した可用性の恩恵を受けます。中断の可能性があるプリエンプティブル VM は、コードの変更に迅速に対応する必要がある CI サーバーには適していない場合があります。
D. ハイパフォーマンス コンピューティング (HPC) クラスター: クラスター上で高いリソース要件を持つ計算負荷の高いタスクを実行します。
プリエンプティブル VM は、特定の種類の HPC ワークロードに対してコスト削減を提供する可能性がありますが、HPC タスクの性質上、多くの場合、一貫した可用性が必要であり、プリエンプティブル インスタンスはこのユース ケースにはあまり適していません。
<details><div>

### Q. 問題41: 未回答
GCP DevOps エンジニアは、Google Cloud Platform(GCP)にデプロイされた複雑なマイクロサービス アーキテクチャを管理しています。運用環境では、パフォーマンス、エラー、およびセキュリティを監視する必要がある大量のログが生成されます。あなたの仕事は、セキュリティのベストプラクティスを遵守しながら、特定のチームメンバーに本番ログへの制御されたアクセスを提供することです。
DevOpsエンジニアが、指定されたチームメンバーに対して可視性を制御して運用ログへのアクセス権を付与するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. 専用の Stackdriver ワークスペースを確立する: 本番環境のログ専用の新しい Stackdriver ワークスペースを作成します。運用プロジェクトをこのワークスペースにアタッチし、アクセス制御を構成して、関連するチーム メンバーにのみ読み取りアクセス権を付与します。

説明：専用の Stackdriver ワークスペースを作成すると、本番環境のログをモニタリングしてアクセスするための一元化された整理された環境が提供されます。運用プロジェクトをこのワークスペースにアタッチすることで、ワークスペース レベルでアクセスを制御し、指定されたチーム メンバーのみが運用ログへの読み取りアクセス権を持つようにすることができます。このアプローチは、セキュリティのベストプラクティスに従い、可視性を制御するための簡単なソリューションを提供します。
他のオプションが正しくない理由:
A. ログ用の Cloud Storage バケットの設定: 本番環境のログを Cloud Storage バケットにエクスポートします。バケットにアクセス制御を実装してアクセスを制限し、指定されたチームメンバーにのみ読み取り権限を付与します。
Cloud Storage バケットはログ ストレージに使用できますが、バケット レベルでアクセス制御を構成すると、特定のログの可視性を制御するために必要な粒度が得られない場合があります。
B. IAM ポリシーで Cloud Pub/Sub を利用する: 本番環境のログを Cloud Pub/Sub にエクスポートするようにログシンクを構成します。 Pub/Sub トピックの IAM ポリシーを定義してアクセスを制限し、指定されたチーム メンバーのみがログ メッセージをサブスクライブおよび受信できるようにします。
Pub/Sub はログのストリーミングに役立ちますが、IAM ポリシーでは、特定のログの可視性を制御するために必要なきめ細かな制御が提供されない場合があります。
C. ログベースの指標を実装する: 特定のログエントリに基づいて、Stackdriver Logging でログベースの指標を作成します。これらのメトリックへのアクセス権を指定されたチームメンバーに付与し、メトリックを使用して本番ログの可視性を制御します。
ログベースのメトリクスは、データの集計には便利ですが、個々のチームメンバーに必要な詳細なログアクセス制御を提供しない場合があります。
専用の Stackdriver ワークスペースを作成することは、GCP でログデータを整理して保護し、指定されたチームメンバーに本番環境のログへのアクセスを制御するためのおすすめの方法です。
<details><div>

### Q. 問題42: 未回答
あなたは「CloudTech Solutions」のDevOpsエンジニアであり、Google Cloud Platform(GCP)上のミッションクリティカルなeコマースプラットフォームのインフラストラクチャとデプロイパイプラインの管理を担当しています。大規模なショッピングイベント中にサービス停止が発生し、プラットフォームへのユーザーアクセスに数時間影響を与えました。インシデントは正常に解決され、サービスは通常運用に復元されました。
質問：サービス停止の余波で、サイト信頼性エンジニアリング (SRE) のベスト プラクティスに従って、DevOps エンジニアが効果的なコミュニケーションとインシデントからの学習を確実に行うために推奨される行動方針は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. インシデントの根本原因、影響、解決手順、予防策を概説した詳細な事後分析ドキュメントを作成し、関連する利害関係者に配布します。

説明：
サービス停止の余波で、サイト信頼性エンジニアリング (SRE) のベスト プラクティスに従って、DevOps エンジニアに推奨される一連のアクションは、詳細な事後分析を作成することです。その理由は次のとおりです。
インシデントの分析:
事後分析では、根本原因の特定、ユーザーへの影響、問題を解決するために実行された手順、イベントのタイムラインなど、インシデントの徹底的な分析が提供されます。
調査結果の文書化:
インシデントに関連する主な調査結果、要因、および注意が必要なシステムの脆弱性や弱点を文書化します。
再発防止策
事後分析には、今後同様のインシデントを防ぐための実行可能な手順と推奨事項が含まれています。システムの信頼性の向上と強化に焦点を当てています。
ステークホルダーへのコミュニケーション:
事後分析を関係者に配布することで、透明性のある効果的なコミュニケーションが確保されます。経営陣、開発者、運用チームなどの利害関係者は、インシデントに関する分析情報を取得し、それに対処するために取られた対策を理解できます。
他のオプションが正しくない理由:
A. 直ちに個々の関係者との緊急会議を招集する。
コミュニケーションは重要ですが、個々の関係者に電話をかけても包括的な概要が得られない場合があり、事後分析に見られる詳細な分析と文書化が不足しています。
C. 障害の影響を受けたすべてのユーザーに謝罪メールを送信します。
ユーザーへの影響を認識することは重要ですが、謝罪メールを送信するだけでは、事後分析によって提供される詳細な分析と予防策の必要性に対処できない可能性があります。
D. チームリーダーに概要レポートの作成を依頼します。
チーム リーダーに概要レポートを要求すると、インシデントから学習するために必要な詳細な分析が不足している可能性があります。また、インシデント対応チーム全体を文書化プロセスに関与させる機会を逃す可能性もあります。
オプション B は、インシデントから学び、透明性を高め、包括的な文書化とコミュニケーションを通じてシステムの信頼性を継続的に向上させる文化を促進することで、SRE のベスト プラクティスと一致します。
<details><div>

### Q. 問題43: 未回答
Google Cloud Platform(GCP)上の重要なアプリケーションの信頼性を監督する GCP DevOps エンジニアは、重大なインシデントに迅速に対応する必要があります。これを実現するには、Cloud Monitoring で重要なアラートの SMS ベースの通知を設定します。これにより、オンコール チームはモバイル デバイスですぐにアラートを受信できます。
DevOps エンジニアが Cloud Monitoring で重要なアラートに対して SMS ベースの通知を構成するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: D. Cloud Monitoring アラート ポリシーで SMS 通知チャネルを使用する: Cloud Monitoring アラート ポリシー内で新しい SMS 通知チャネルを構成します。このチャネルを重要なアラート条件に関連付けて、オンコール チームの指定された電話番号に SMS 通知を直接送信します。

説明：Cloud Monitoring は、アラート ポリシーを通じて SMS 通知をネイティブにサポートします。推奨される方法は、Cloud Monitoring アラート ポリシー内に新しい SMS 通知チャネルを作成することです。これにより、重要なアラートのSMS配信を直接構成できるため、オンコールチームへのタイムリーな通知が保証されます。
他のオプションが正しくない理由:
A. Twilio との統合: Cloud Monitoring と Twilio の統合を確立します。Cloud Monitoring でアラート ポリシーを構成して、重要なアラートについて Twilio の SMS サービス経由で通知をトリガーします。
Twilio との統合は有効なオプションですが、Cloud Monitoring 内でネイティブの SMS 通知チャネルを使用すると、構成が簡素化され、外部サービスが不要になります。
B. Google Cloud Pub/Sub と Cloud Functions を利用する: Cloud Monitoring を設定して、重要なアラート通知を Cloud Pub/Sub トピックに公開します。Pub/Sub メッセージによってトリガーされる Google Cloud Functions の関数を作成し、Twilio SMS API を使用して SMS 通知を送信します。
これには、Cloud Pub/Sub と Cloud Functions の複雑さが増します。Cloud Monitoring 内で SMS チャネルを直接構成する方が簡単です。
C. Email-to-SMS ゲートウェイを実装する: 重要なアラートのメール通知を送信するように Cloud Monitoring アラート ポリシーを構成します。電子メールからSMSへのゲートウェイサービスを利用して、これらの電子メールアラートをSMSメッセージに変換し、すぐに配信します。
Email-to-SMS ゲートウェイを使用すると、複雑さと依存関係がさらに増します。Cloud Monitoring の直接 SMS 通知チャネルは、より合理化されたアプローチです。
Cloud Monitoring アラート ポリシー内で SMS 通知チャネルを構成すると、オンコール チームのモバイル デバイスに重要なアラートを送信するための簡単でネイティブなソリューションが提供されます。
<details><div>

### Q. 問題44: 未回答
あなたは、主要なeコマースプラットフォームのサイト信頼性エンジニア(SRE)であり、顧客向けアプリケーションに影響を与える重大なインシデントが発生しました。コミュニケーションの担当者として、開発チームや経営陣などの内部関係者から、停止に関する最新情報を求めるメールと、懸念を表明し、注文の状態に関する情報を求めている顧客の両方からメールを受信します。インシデントは複雑で、解決までの推定時間はありません。
質問：このシナリオでは、コミュニケーションを担当する SRE が、サイト信頼性エンジニアリング (SRE) のプラクティスに従って、進行中のインシデント中に更新とコミュニケーションを処理するための最も効果的なアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. 社内の関係者と顧客の両方に定期的な更新を提供し、すべてのコミュニケーションで「次の更新」時間を約束します。

説明：
顧客向けアプリケーションに影響を与える重大なインシデントが発生した場合、コミュニケーションを担当する SRE にとって最も効果的なアプローチは、社内の関係者と顧客の両方に定期的な更新を提供し、すべてのコミュニケーションで "次の更新" 時間を約束することです。その理由は次のとおりです。
透明性とコミュニケーション:
SRE のプラクティスでは、インシデント発生時の透明性が重視されます。定期的な更新を提供することで、社内の利害関係者と顧客の両方に、インシデントの状態、進行中の取り組み、および変更について常に通知されます。
「次の更新」時間にコミットします。
すべてのコミュニケーションに「次の更新」時間へのコミットメントを含めることで、期待値が設定されます。これは、情報の流れを管理し、不確実性を減らし、利害関係者に情報を提供するための積極的なアプローチを示すのに役立ちます。
他のオプションが正しくない理由:
A. 主に社内の関係者からのメールへの返信に重点を置きます。
社内の関係者は重要ですが、社内コミュニケーションのみに焦点を当てると、最新情報が不足して顧客の不満につながる可能性があります。社内外の関係者は、インシデント発生時にタイムリーで透明性の高い情報を必要としています。
B. 社内の関係者のメールに返信するタスクを委任する:
社内コミュニケーションを委任すると、遅延が発生し、誤解が生じる可能性があります。コミュニケーションの担当者は、インシデント発生時に内部と外部の両方のコミュニケーションを積極的に管理する必要があります。
D. すべての内部関係者の電子メールをインシデント指揮官に転送します。
インシデント指揮官とのコラボレーションは重要ですが、社内の関係者のメールをすべて転送すると、コミュニケーションが遅れる可能性があります。コミュニケーション担当者は、すべての利害関係者に効果的かつタイムリーな更新を保証する上で極めて重要な役割を果たします。
オプション C は、インシデント発生時に社内外の関係者との明確かつ定期的なコミュニケーションを重視し、透明性を高め、確定した更新時間を通じて期待値を管理することで、SRE のベスト プラクティスと一致します。
<details><div>

### Q. 問題45: 未回答
GCP DevOps エンジニアは、重要なアプリケーションの継続的インテグレーション / 継続的デプロイ(CI / CD)パイプラインを保護する責任があります。この責任の一部には、デプロイ プロセス中に使用される API キーや資格情報などの機密情報の保護が含まれます。これらのシークレットを CI/CD パイプラインに格納してアクセスするための安全なアプローチを実装する必要があります。
DevOps エンジニアが CI/CD パイプライン内のシークレットのストレージと取得をセキュリティで保護するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. カスタマー マネージド キーで暗号化された Google Cloud Storage (GCS) にシークレットを保存する: シークレットを暗号化し、専用の Google Cloud Storage バケットに保存します。暗号化には、Cloud Key Management Service(KMS)に保存されているカスタマー マネージド キーを使用します。CI / CD パイプラインが IAM 経由で Cloud KMS にアクセスできるようにし、シークレットの鍵の安全な取得と復号を行います。

説明：Cloud Key Management Service(KMS)に保存されたカスタマー マネージド キーで暗号化されたシークレットを Google Cloud Storage(GCS)に保存すると、安全で一元化されたソリューションが提供されます。CI / CD パイプラインは、Identity and Access Management(IAM)を介して Cloud KMS にアクセスし、保存されているシークレットの安全な鍵の取得と復号を行うことができます。
他のオプションが正しくない理由:
A. 環境変数にシークレットを格納する: CI/CD パイプライン構成内の環境変数にシークレットを直接埋め込みます。CI/CD 構成ファイルへのアクセスを、許可された担当者に制限します。
シークレットを環境変数に直接埋め込むと、簡単に公開される可能性があるため、セキュリティ上のリスクが生じます。機密情報の一元管理や暗号化は提供されません。
B. HashiCorp Vault を利用する: HashiCorp Vault を実装して、シークレットを一元的に管理し、CI/CD パイプラインに配布します。デプロイ時に必要なシークレットを取得するために Vault で認証するようにパイプラインを設定します。
HashiCorp Vault は強力なシークレット管理ソリューションですが、このシナリオでは GCP ネイティブのアプローチについて具体的に言及しています。Cloud KMS と GCS の使用は、GCP のベスト プラクティスにより合致しています。
D. ソースコードにシークレットを埋め込む: シークレットをソースコードリポジトリに直接埋め込み、CI/CDプロセス中にアクセスできるようにします。アクセス制御を適用してリポジトリへのアクセスを制限し、変更を監視して不正アクセスを防止します。
ソース コードにシークレットを埋め込むことは、漏洩のリスクがあるため、推奨される方法ではありません。一元管理、暗号化、シークレットへのアクセスを個別にローテーションまたは取り消す機能がありません。
Cloud KMS のカスタマーマネージド鍵で暗号化したシークレットを GCS に保存することで、CI / CD パイプラインで機密情報を管理するための、安全でスケーラブルで監査可能なソリューションが確保されます。
<details><div>

### Q. 問題46: 未回答
重要な e コマース アプリケーションを管理する GCP DevOps エンジニアは、最適なパフォーマンスと可用性を確保するために監視機能を強化する必要があります。アプリケーションは、ユーザーインタラクション、注文処理、およびシステムイベントに関連するさまざまなログを生成します。監視の重要な側面の 1 つは、ユーザー エクスペリエンスに直接影響する特定の API エンドポイントの応答時間を追跡することです。
DevOps エンジニアが Cloud Logging 環境でログから派生したカスタム指標を作成、モニタリングするには、どのようなアプローチをお勧めしますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. Cloud Logging のログベースの指標を活用する: 対象の API エンドポイントの応答時間を表す特定のログエントリに基づいて、Cloud Logging でログベースの指標を作成します。このログベースの指標を利用して、Cloud Monitoring でカスタム ダッシュボードを作成し、平均応答時間を可視化してモニタリングします。

説明：Cloud Logging でログベースの指標を作成すると、ログエントリから直接カスタム指標を導き出すことができます。このシナリオでは、APIエンドポイントの応答時間をキャプチャするログエントリを定義し、これらのログを使用してログベースのメトリクスを作成できます。その後、Cloud Monitoring を活用してカスタム ダッシュボードを作成し、経時的な平均応答時間を可視化、モニタリングできます。
他のオプションが正しくない理由:
A. Cloud Logging Export to BigQuery を利用する: 関連するログを Cloud Logging から BigQuery にエクスポートし、SQL クエリを使用してログデータを BigQuery で直接集計、分析します。BigQuery でカスタム ダッシュボードを作成し、特定の API エンドポイントの平均レスポンス時間を経時的に可視化します。
BigQuery へのログのエクスポートは有効なオプションですが、追加の手順とツールが必要です。ログベースの指標を Cloud Logging と Cloud Monitoring で直接活用することで、より合理的なソリューションが実現します。
C. パフォーマンス モニタリングのための Stackdriver Trace の実装: Stackdriver Trace を構成して、特定の API エンドポイントのパフォーマンスの詳細なトレースをキャプチャします。Stackdriver Trace でトレースベースのカスタム指標を作成し、Stackdriver Trace インターフェース内のカスタム ダッシュボードで視覚化して、応答時間のパターンを監視します。
Stackdriver Trace は、ログベースの指標を作成するためではなく、分散トレース用に設計されています。これは、特定の API エンドポイントの応答時間を監視するための不必要な複雑さを追加する可能性があります。
D. Cloud Storage への Cloud Monitoring 指標のエクスポートを有効にする: API エンドポイントの応答時間に関連する Cloud Monitoring 指標を Cloud Storage バケットにエクスポートします。カスタムスクリプトを使用してメトリックデータを処理および分析し、平均応答時間を視覚化および監視するための別のWebサーバーでホストされるカスタムダッシュボードを作成します。
Cloud Storage への指標のエクスポートとカスタム ソリューションの作成には、より多くの手作業が必要であり、Cloud Logging や Cloud Monitoring でログベースの指標を利用するほど簡単ではない場合があります。
<details><div>

### Q. 問題47: 未回答
Google Kubernetes Engine(GKE)にグローバルにデプロイされたマイクロサービス アーキテクチャを監督する GCP DevOps エンジニアは、Google Cloud CDN を活用して、さまざまなリージョンのユーザーへのコンテンツ配信を最適化しています。高いサービス可用性を確保し、CDNからのキャッシュミス率を監視することは、アプリケーションのパフォーマンスにとって重要なSLIです。これらのSLIを正確に測定するには、堅牢な戦略が必要です。
GKE と CDN のコンテキストで、サービスの可用性や CDN からのキャッシュ ミスに関連する目的の SLI を DevOps エンジニアが効率的に測定するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. 合成クライアントのデプロイ: 特に CDN キャッシュ コンテンツをターゲットとする、アプリケーションとのエンド ユーザーの対話をシミュレートする合成クライアントを開発およびデプロイします。これらの合成クライアントは、制御されたトラフィックを生成できるため、実際のユーザーに影響を与えることなく、サービスの可用性とキャッシュのミス率を正確に測定できます。

説明：エンドユーザーとのやり取りをシミュレートするシンセティッククライアント、特にCDNキャッシュされたコンテンツをターゲットとするシンセティッククライアントの導入は、サービスの可用性とキャッシュミス率を正確に測定するための推奨されるアプローチです。この方法により、DevOpsエンジニアは制御されたトラフィックを生成し、実際のユーザーに影響を与えることなく、CDNがさまざまなシナリオを処理する方法に関する洞察を得ることができます。
他のオプションが正しくない理由:
A. Google Cloud Monitoring の指標を利用する: サービスの可用性とキャッシュのミス率に重点を置き、GKE クラスタと CDN から直接指標を収集するように Cloud Monitoring を構成します。収集されたメトリックを分析して、アプリケーションのパフォーマンスのSLIを導き出します。
Google Cloud Monitoring の指標はモニタリングに役立ちますが、このシナリオでは、CDN キャッシュされたコンテンツに関連する特定の SLI を測定するために、より制御されたターゲットを絞ったアプローチの必要性が強調されています。
C. Stackdriver Profiler を実装する: Stackdriver Profiler を有効にして、GKE クラスタで実行されているアプリケーションの詳細なパフォーマンス プロファイルをキャプチャし、キャッシュ関連のオペレーションを強調します。プロファイリングデータを分析してボトルネックを特定し、キャッシュミス率のSLIを導き出します。
Stackdriver Profiler は、アプリケーション コード内のパフォーマンスのボトルネックを特定するのに適しています。CDNからのキャッシュミス率を測定するために必要な特定の分析情報は提供されない場合があります。
D. クラウド負荷テストの活用:クラウド負荷テストサービスを利用して、合成トラフィックを生成し、エンドユーザーのリクエストをシミュレートし、CDNに負荷をかけます。負荷がかかった状態でのアプリケーションの応答を監視し、リアルタイムデータに基づいてサービスの可用性とキャッシュのミス率を測定します。
Cloud Load Testing は、ストレス テストと、負荷がかかった状態でのシステムの応答の評価に重点を置いています。CDNキャッシュされたコンテンツに関連する特定のSLIを正確に測定するために必要な制御された条件が提供されない場合があります。
<details><div>

### Q. 問題48: 未回答
大規模な e コマース プラットフォームの GCP DevOps エンジニアは、デプロイ パイプラインを管理し、ソース管理リポジトリでタグ付けされたリリース バージョンに基づいてアプリケーションの特定のバージョンがデプロイされるようにする責任があります。アプリケーション イメージは Cloud Build を使用してビルドされ、Google Container Registry(GCR)にプッシュされます。
この実際のシナリオで GCR にイメージをプッシュするときに、デプロイするアプリケーションの特定のバージョンをバージョン管理して指定するための推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: A. Cloud Build の置換を利用する: ビルド プロセス中にソース管理リポジトリのバージョンを自動的に置き換えるように Cloud Build を構成します。Docker イメージに代替バージョンをタグ付けし、GCR にプッシュします。

説明： Cloud Build Substitutions の使用は、ソース管理リポジトリからの情報に基づいてバージョン管理プロセスを自動化するための推奨されるアプローチです。Cloud Build は置換をサポートしており、ビルド プロセス中にバージョン番号などの動的な値を置き換えることができます。これにより、Dockerイメージに正しいバージョンが自動的にタグ付けされ、デプロイワークフローが合理化されます。
他のオプションが正しくない理由:
B. Docker イメージに手動でタグを付ける: Cloud Build プロセスの後、ソース管理リポジトリから取得したリリース バージョンで Docker イメージに手動でタグを付けます。 コマンドを使用して、タグ付けされたイメージを GCR にプッシュします。docker tag
画像に手動でタグを付けるとエラーが発生しやすく、人為的ミスのリスクが生じます。一貫性を確保し、バージョン管理のミスを避けるために、自動化が推奨されます。
C. 自動タグ付け用の Git フックを実装する: ソース管理リポジトリで新しいリリース バージョンがタグ付けされたときにタグ付けプロセスを自動的にトリガーするように Git フックを設定します。このプロセスを使用して Docker イメージにタグを付け、GCR にプッシュします。
Git フックは便利ですが、ローカル開発者のマシン上で動作します。CI/CD パイプラインの場合は、ビルド プロセス内でバージョン管理を一元化して、よりスケーラブルで管理しやすくすることをお勧めします。
D. コンテナ レジストリ Webhook を使用する: ソース管理リポジトリで新しいバージョンがタグ付けされるたびに Cloud Functions の関数をトリガーするようにコンテナ レジストリ Webhook を構成します。Cloud Functions の関数は Docker イメージに動的にタグを付け、GCR にプッシュします。
Webhook と Cloud Functions は自動化されたワークフローの一部にできますが、Cloud Build はビルド プロセスとより直接的に統合されています。Cloud Build Substitutions を使用すると、ビルド中にバージョニングを実現するための、より簡単でネイティブな方法になります。Webhook を使用すると、追加のコンポーネントと複雑さが生じます。
これを実現する方法:
Google Container Registry(GCR)にイメージをプッシュするときにソース管理でタグ付けされたリリース バージョンに基づいて、バージョン管理を行い、デプロイするアプリケーションの特定のバージョンを指定するには、Docker イメージをプッシュする前に、目的のバージョンでタグ付けする必要があります。従うことができる手順は次のとおりです。
Dockerイメージをビルドします。Cloud Build を使用して Docker イメージをビルドします。ビルド プロセス中に、バージョンを指定するか、環境変数を使用して、ソース管理情報に基づいてバージョンを動的に設定できます。
Docker イメージにバージョンをタグ付けします。ビルド後、Dockerイメージに目的のバージョンをタグ付けします。たとえば、バージョンが環境変数に格納されている場合、またはビルド コンテキストで使用できる場合は、それを使用してイメージにタグを付けることができます。次のようなコマンドを使用します。
docker tag [IMAGE_ID] gcr.io/[PROJECT_ID]/[IMAGE_NAME]:[VERSION]
Dockerイメージの実際のID、GCPプロジェクトID、Dockerイメージの名前、および目的のバージョンに置き換えます。[IMAGE_ID][PROJECT_ID][IMAGE_NAME][VERSION]
タグ付けされた画像を GCR にプッシュします。タグ付けされた Docker イメージを Google Container Registry にプッシュします。
docker push gcr.io/[PROJECT_ID]/[IMAGE_NAME]:[VERSION]
、、 を実際の GCP プロジェクト ID、Docker イメージ名、および目的のバージョンに置き換えてください。[PROJECT_ID][IMAGE_NAME][VERSION]
これらの手順に従うことで、ソース管理でタグ付けされたリリース バージョンに基づいて GCR の Docker イメージをバージョン管理し、必要に応じて特定のバージョンのアプリケーションをデプロイできます。
<details><div>

### Q. 問題49: 未回答
GCP DevOps エンジニアは、Google Kubernetes Engine(GKE)にデプロイされた複雑なマイクロサービス アーキテクチャを管理しています。マイクロサービスはコンテナー化され、アプリケーションの全体的なパフォーマンスにおいて重要な役割を果たします。潜在的な問題を特定して対処するには、リソース使用率の効率的な監視が不可欠です。
DevOps エンジニアが GKE クラスタにデプロイされたコンテナの CPU とメモリの消費パターンに関する分析情報を得るために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. GKE Pod Resource Metrics を活用する: Pod に組み込まれている GKE 指標を利用して、CPU とメモリの使用率をモニタリングします。事前定義されたしきい値に基づいてアラートを作成し、異常をプロアクティブに検出します。
答え：
説明：Google Kubernetes Engine(GKE)には、CPU やメモリの使用率など、ポッドをモニタリングするための組み込み指標が用意されています。これらの GKE 指標を活用することで、DevOps エンジニアは GKE クラスタ内のコンテナのリソース消費パターンを効率的にモニタリングできます。事前定義されたしきい値に基づいてアラートを設定することで、エンジニアは異常を事前に検出し、リソースの使用を最適化するために必要なアクションを実行できます。
他のオプションが正しくない理由:
A. Stackdriver Logging を有効にする: Stackdriver Logging を利用して、コンテナによって生成されたログを収集、分析し、CPU とメモリの使用状況に関連するパターンを特定します。
Stackdriver Logging は、CPU やメモリの使用率などのリソース指標をリアルタイムでモニタリングするよりも、ログ分析に適しています。
B. カスタム Prometheus 指標を実装する: Prometheus を GKE と統合し、CPU とメモリの使用状況を追跡するようにカスタム指標を構成します。Prometheus のクエリとダッシュボードを使用して、詳細な分析情報を得ることができます。
Prometheus は強力な監視ソリューションですが、カスタムメトリクスを設定するには追加の設定が必要です。GKE はネイティブの指標を提供するため、リソースの使用状況をより簡単にモニタリングできます。
D. OpenTelemetry エージェントをデプロイする: 各コンテナー内に OpenTelemetry エージェントをインストールして、詳細なパフォーマンス メトリックを収集します。OpenTelemetry 互換の監視ソリューションを使用して、包括的な分析を行います。
OpenTelemetry はオブザーバビリティのための貴重なツールですが、すべてのコンテナーにエージェントをデプロイすると、複雑さが増す可能性があります。GKE のネイティブ指標は、リソースの使用状況をモニタリングするためのよりシンプルで効果的なソリューションを提供します。
<details><div>

### Q. 問題50: 未回答
あなたは、大手クラウドサービスプロバイダーである「CloudOpsTech」のDevOpsエンジニアです。CloudOpsTech は、多数の仮想マシン (VM) がクライアントの重要なアプリケーションを提供する広大なインフラストラクチャを運用しています。現在、VM 使用率ログは Stackdriver に保存されており、包括的なモニタリングとレポート作成のソリューションを確立する必要があります。
目標は、簡単に共有でき、リアルタイムの更新を提供し、四半期ごとに集計された情報を含む対話型の VM 使用率ダッシュボードを作成することです。このダッシュボードは、社内の可視性を高めるだけでなく、利害関係者の主要業績評価指標としても機能します。
このようなユースケースに最適なオプションを選択してください。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: b. 四半期ごとの集計用に Cloud Scheduler ジョブを設定し、リアルタイム更新のために Dataflow for Streaming を使用します。

説明：このオプションは、リアルタイムで更新され、四半期ごとに集計された情報を含むインタラクティブな VM 使用率ダッシュボードを作成するのに最適です。その理由は次のとおりです。
Cloud Scheduler ジョブ:Cloud Scheduler ジョブをスケジュールして Dataflow パイプラインをトリガーすると、定期的な実行が可能になり、VM ログが四半期ごとに集計されます。このアプローチにより、集計プロセスの処理が自動化され、信頼性が高まります。
ストリーミングのデータフロー:ストリーミングに Dataflow を使用すると、VM ログがリアルタイムで処理および更新され、ダッシュボードにタイムリーで正確なデータが提供されます。結果は BigQuery に保存されるため、クエリや可視化が簡単になります。
データポータルの統合:データポータルは BigQuery の更新されたデータに直接接続できるため、リアルタイムの変更を反映し、四半期ごとに集計された情報を含むインタラクティブで動的なダッシュボードを作成できます。
他のオプションが正しくない理由:
a. VM ログを BigQuery にエクスポートし、四半期ごとに集計するためのスケジュールされたクエリを作成します。
VM ログを BigQuery にエクスポートすることは有効な方法ですが、四半期ごとの集計にスケジュールされたクエリを使用すると、遅延が発生し、ダッシュボードのリアルタイム更新が得られない可能性があります。
C. リアルタイム更新の Stackdriver Monitoring アラートを設定し、データポータルにエクスポートする:
Stackdriver Monitoring のアラートは、継続的なデータ更新よりも、リアルタイムのアラートに適しています。アラートを BigQuery にエクスポートしても、四半期ごとの集計に必要な包括的なデータが提供されない場合があります。
d. Cloud Functions を使用して VM ログを集約し、データポータルにプッシュします。
Cloud Functions はオンデマンドでタスクを実行できますが、四半期ごとの集計に使用すると、大規模なデータセットの処理が複雑になり、困難が生じる可能性があります。スケジュールされた Dataflow ジョブほど効率的ではない可能性があります。
要約すると、オプション B は、リアルタイムの更新と四半期ごとの集計情報を含む対話型の VM 使用率ダッシュボードを作成するための堅牢でスケーラブルなソリューションを提供します。
これを実現する方法:
オプション B では、四半期ごとの集計用に Cloud Scheduler ジョブを設定し、ストリーミングに Dataflow を使用しますが、Google Cloud Dataflow の継続的なストリーミング機能によってリアルタイムの更新が実現されます。リアルタイム更新がどのように行われるかを分析してみましょう。
Cloud Scheduler ジョブ:
Cloud Scheduler ジョブは定期的に実行するようにスケジュールされ、Dataflow パイプラインの実行がトリガーされます。このシナリオでは、ジョブは四半期ごとに実行するように構成され、目的の集計頻度に合わせて調整されます。
データフロー パイプライン:
トリガーされた Dataflow パイプラインは、VM 使用率ログをリアルタイムで処理するように設計されています。Dataflow は、バッチ処理とストリーム処理の両方をサポートしています。この場合、ストリーミング機能を利用して、生成されたログを処理し、継続的かつ最新のデータフローを確保します。
連続ストリーミング:
Dataflow はストリーミング データを処理する際に、集計結果をリアルタイムで継続的に更新します。つまり、新しい VM ログが生成され、パイプラインにフィードされると、集計されたデータが動的に更新され、最新の情報が反映されます。
BigQuery のストレージ:
集計された情報を含む Dataflow パイプラインの結果は、BigQuery に保存されます。BigQuery は、大規模なデータセットを処理し、高速なクエリ パフォーマンスを提供できる、スケーラブルでフルマネージドのデータ ウェアハウスです。
データポータルの統合:
強力な可視化ツールであるデータポータルは、集計データを保存する BigQuery テーブルに直接接続されています。そのため、BigQuery のデータに対する変更や更新は、データポータルのダッシュボードにすぐに反映されます。
リアルタイムダッシュボードの更新:
データポータルのダッシュボードにアクセスするユーザーには、新しいデータが Dataflow で処理され、集計され、BigQuery に保存されると、リアルタイムで更新情報を確認できます。これにより、ダッシュボードに VM 使用率のインタラクティブで継続的に更新されるビューが表示されます。
このソリューションでは、Dataflow のストリーミング機能を活用することで、VM 使用率ダッシュボードを四半期ごとに更新して分析情報を集約するだけでなく、新しいデータが利用可能になるとリアルタイムで継続的に更新されます。
<details><div>

### Q. 問題51: 未回答
急成長中のeコマース企業「CloudSolutions Corp」のGCP DevOpsエンジニアとして、フラッグシップアプリケーションのインフラストラクチャを管理する任務を負っています。アプリケーションは完全にコンテナ化され、3 つのゾーンにまたがる Standard リージョン クラスタの Google Kubernetes Engine(GKE)上で実行されており、さまざまなトラフィック パターンを経験します。同社は、今後6か月間でユーザートラフィックが前月比で10%増加すると予想しています。
最適なパフォーマンス、ゾーン障害に対する回復性、およびコスト効率を確保するには、予測される増加に対処するための戦略を実装する必要があります。
次のうち、戦略の一部にすべきアクションはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. GKE Cluster Autoscaler を有効にし、リソース要件に基づいてノード数を調整するように設定します。
GKE Cluster Autoscaler:
GKE Cluster Autoscaler は、クラスタで実行されているワークロードのリソース要件に基づいて GKE クラスタのサイズを自動的に調整するツールです。これは、リソースを効率的に管理し、需要に合わせてクラスターをスケーリングするのに役立ちます。
利点：
オートメーション：GKE Cluster Autoscaler は、クラスタ内のノード数を調整するプロセスを自動化するため、手動による介入が不要になります。
効率：オートスケーラーは、ワークロードの実際のリソース使用率に基づいてクラスター サイズを調整し、最適なリソース割り当てを確保します。
拡張性:これは、さまざまなトラフィック パターンと予想される成長を処理するのに適しており、スケーラブルなソリューションになります。
他のオプションが正しくない理由:
A. 予想されるユーザー数の増加に対応するために、毎月 GKE クラスタを手動でスケーリングします。
クラスターを毎月手動でスケーリングすることは、労力がかかり、エラーが発生しやすく、動的で急速に成長する環境には適していません。運用上のオーバーヘッドが発生し、非効率性につながる可能性があります。
C. シンプルさと費用対効果のためにシングルゾーンクラスタを実装します。
単一ゾーンのクラスターは、シンプルさと費用対効果を提供する可能性がありますが、ゾーンの障害に対する回復性に欠けています。本番環境では、高可用性を確保するために、マルチゾーンまたはリージョンクラスタが推奨されます。
D. Vertical Pod Autoscaling (VPA) を使用して、ポッド内の CPU およびメモリ要求を動的に調整します。
VPA は、実際の使用状況に基づいて個々のポッド内のリソース要求を調整することに重点を置いています。ポッド リソースの微調整には便利ですが、クラスター全体の容量計画とスケーリングを管理するための主要なツールではありません。
要約すると、オプションBは、今後6か月間に予測されるユーザートラフィックの増加を処理するのに最も適しています。GKE Cluster Autoscaler は、リソース要件に基づいてクラスタ サイズを調整し、最適なパフォーマンス、耐障害性、費用効率を確保するための自動化されたスケーラブルなソリューションを提供します。
<details><div>

### Q. 問題52: 未回答
グローバルなeコマースプラットフォームである「CloudCommerce Solutions」のGCP DevOpsエンジニアとして、高度に動的でスケーラブルなマイクロサービスアーキテクチャの管理を担当しています。1 つの重要なマイクロサービスは、顧客の注文とトランザクションの処理を担当します。プラットフォームでは、さまざまなトラフィックパターンが発生し、プロモーションや販売イベント中に定期的に急増します。
最近、注文処理に関する断続的な問題からユーザーから報告があり、これらの問題の根本原因を特定する必要があります。目標は、高可用性を確保し、リソース使用率を最適化し、トラフィックの急増時のユーザーエクスペリエンスへの影響を最小限に抑えることです。
質問：上記のシナリオを考慮して、報告された注文処理で断続的に発生する問題に対処し、トラフィックの急増時に最適なパフォーマンスを実現するために、GCP サービスとベスト プラクティスのどの組み合わせを実装しますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. マイクロサービスの前に Google Cloud CDN をデプロイして静的コンテンツをキャッシュし、レイテンシを低減すると同時に、Stackdriver Monitoring を使用してパフォーマンスの異常に関するアラートを設定します。
Google Cloud CDN(コンテンツ配信ネットワーク):
Google Cloud CDN は、静的コンテンツをユーザーの近くにキャッシュすることでコンテンツ配信を高速化する、グローバルに分散されたエッジ キャッシュ サービスです。マイクロサービスの前に Cloud CDN をデプロイすることで、画像やスタイルシートなどの静的コンテンツのレイテンシを大幅に削減できます。
Stackdriver モニタリング:
Stackdriver Monitoring は、アプリケーションのパフォーマンスと健全性を包括的に監視します。Stackdriver Monitoring でアラートを設定することで、パフォーマンスの異常をプロアクティブに検出し、主要な指標をモニタリングし、注文処理に影響を与える問題に関する通知を受け取ることができます。
他のオプションが正しくない理由:
A. サービス間通信用に Google Cloud Pub/Sub を実装し、Google Kubernetes Engine(GKE)に Horizontal Pod Autoscaling をデプロイして、さまざまなトラフィック負荷を処理します。
Google Cloud Pub/Sub はマイクロサービス間の非同期通信に役立ち、GKE の Horizontal Pod Autoscaling はさまざまなトラフィック負荷に対処しますが、注文処理に関する断続的な問題に直接対処できない場合があります。このオプションはスケーリングに重点を置いていますが、パフォーマンスが最適化されない可能性があります。
B. Cloud Functions を利用してサーバーレス環境で顧客の注文を処理し、分散トレースに Stackdriver Trace を活用してボトルネックを特定します。
Cloud Functions は、イベント駆動型のステートレスな関数に適していますが、顧客の注文をトランザクション方式で処理する場合には最適ではない場合があります。Stackdriver Trace はボトルネックの特定に役立ちますが、この場合、静的コンテンツ配信の最適化とパフォーマンスの異常のモニタリングがより重要になります。
D. DDoS 攻撃から保護するように Cloud Armor を構成し、トラフィックを地理的に分散するために複数のリージョン バックエンドを備えたグローバル ロードバランサを実装します。
Cloud Armor は DDoS 攻撃に対するセキュリティを提供し、グローバル ロードバランサは地理的な分散を可能にしますが、これらの対策は、断続的なパフォーマンスの問題に対処するのではなく、セキュリティと可用性に重点を置いています。
オプション C は、Google Cloud CDN を使用した静的コンテンツ配信の最適化と、Stackdriver Monitoring を使用したパフォーマンスのモニタリングの両方のニーズに対応するため、最適な組み合わせです。このアプローチは、さまざまなトラフィックパターンに対して応答性と信頼性の高い顧客体験を確保するのに役立ちます。
<details><div>

### Q. 問題53: 未回答
シナリオ：大手eコマースプラットフォーム「CloudRetail Solutions」のGCP DevOpsエンジニアとして、商品レコメンデーションを処理する重要なマイクロサービスの管理を担当しています。このマイクロサービスは、ロード バランサーを使用せずに HTTP エンドポイントを公開し、ユーザーの閲覧履歴に基づいてパーソナライズされたレコメンデーションをユーザーに提供します。
HTTPレスポンスのレイテンシーは、シームレスで応答性の高いユーザーエクスペリエンスを提供するために重要です。最近、ユーザーから製品のレコメンデーションの受信が遅れることが報告されており、ユーザーの満足度を高めるには、待機時間の問題を特定して対処する必要があります。
質問：説明されているシナリオを考慮して、製品レコメンデーション マイクロサービスの HTTP レイテンシを効果的に監視および最適化するために、GCP サービスとベスト プラクティスのどの組み合わせを実装しますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: A. 分散トレースに Stackdriver Trace を実装し、静的コンテンツのキャッシュに Cloud CDN を活用して、パーソナライズされたレコメンデーションのレイテンシを削減します。
Stackdriver トレース:
Stackdriver Trace は、アプリケーションの分散トレース機能を提供し、さまざまなマイクロサービスを通過するリクエストのエンドツーエンドのレイテンシを分析、可視化します。Stackdriver Trace を実装することで、プロダクト レコメンデーション マイクロサービスに対するリクエストのレイテンシを把握し、パフォーマンスのボトルネックを特定して対処できます。
クラウドCDN:
Cloud CDN は、静的コンテンツをユーザーの近くにキャッシュし、コンテンツ配信のレイテンシを低減するコンテンツ配信ネットワークです。このコンテキストでは、Cloud CDN を活用することで、パーソナライズされたレコメンデーションに関連付けられた静的コンテンツの配信を最適化し、より高速で応答性の高いユーザー エクスペリエンスを実現できます。
他のオプションが正しくない理由:
B. Google Cloud Armor をデプロイして DDoS 攻撃から保護し、カスタム レイテンシ指標を使用して Stackdriver Monitoring を構成してリアルタイムの分析情報を得る。
Google Cloud Armor は DDoS 攻撃に対するセキュリティに重点を置いており、レイテンシの最適化には直接対応していない場合があります。Stackdriver Monitoring は指標をキャプチャできますが、Stackdriver Trace が提供する詳細なトレース機能は提供されない場合があります。
C. Cloud Functions を使用して、サーバーレス環境でレコメンデーション リクエストを処理し、Cloud Monitoring をレイテンシしきい値のカスタム アラートと統合します。
Cloud Functions はステートレスなイベント駆動型関数向けに設計されているため、レコメンデーション リクエストの処理には適していない可能性があります。Cloud Monitoring は分析情報を提供できますが、Stackdriver Trace はマイクロサービスのレイテンシの追跡に適しています。
D. Stackdriver Profiler を活用してコードレベルのパフォーマンスを分析し、マイクロサービスの前に Google Cloud CDN を実装して動的コンテンツをキャッシュし、レイテンシを改善します。
Stackdriver Profiler はコードレベルのパフォーマンスの分析に使用され、動的コンテンツをキャッシュするためのレイテンシに直接対処できない場合があります。また、Cloud CDN は、動的コンテンツよりも静的コンテンツのキャッシュに適しています。
オプション A は、Stackdriver Trace の詳細なトレース機能と Cloud CDN のコンテンツ キャッシュの利点を組み合わせて、プロダクト レコメンデーション マイクロサービスの HTTP レイテンシを最適化するため、最も適切な組み合わせです。
<details><div>

### Q. 問題54: 未回答
シナリオ：あなたは、Google Cloud Platform(GCP)上でアプリケーションを開発およびデプロイする急成長中のテクノロジー企業である「CloudCo Solutions」のDevOpsエンジニアです。開発チームは複数の場所に分散しており、クラウドソースリポジトリを使用してコードを管理するための効率的なプロセスを確立する必要があります。
質問：上記のシナリオを考慮すると、GCP 環境で Cloud Source Repositories を介してコードを管理するための最も適切で効果的なプロセスは何でしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: B. プロジェクトごとに個別のリポジトリを確立して、コードの分離、スケーラビリティ、およびアクセス制御の管理の容易化を促進します。
説明：
「CloudCo Solutions」の分散チームのシナリオでは、GCP環境でクラウドソースリポジトリを使用してプロジェクトごとに個別のリポジトリを確立することが、いくつかの理由から最も適切で効果的なアプローチです。
コードの分離:
各プロジェクトには専用のリポジトリがあり、コードの分離が確保されます。これにより、異なるプロジェクト間の干渉を防ぎ、メンテナンスと更新を容易にします。
拡張性:
リポジトリを分離することで、プロジェクトの数が増えてもスケーラビリティを確保できます。チームは、他のチームに影響を与えることなく、プロジェクトを独立して管理および拡張できます。
アクセス制御:
アクセス制御の管理がよりきめ細かく、より簡単になります。権限はリポジトリレベルで設定でき、チームメンバーの役割と責任に基づいて安全なアクセスを提供します。
プロジェクト固有の構成:
各リポジトリには、CI/CD パイプライン、フック、プロジェクトの特定のニーズに合わせた設定など、独自の構成設定を含めることができます。
他のオプションが正しくない理由:
A. すべてのプロジェクトに対して 1 つのモノリシック リポジトリを作成します。
このアプローチでは、コードのもつれ、複雑さが増し、アクセス制御とスケーラビリティの管理が困難になる可能性があります。これにより、さまざまなプロジェクトチームの自律性が妨げられる可能性があります。
C. GitHub や Bitbucket などの外部バージョン管理システムを利用します。
Cloud Source Repositories は、GCP と統合されたフル機能のバージョン管理システムで、マルチプロジェクトのコード管理をサポートしています。外部システムを利用すると、不必要な複雑さが生じる可能性があります。
D. 各開発者の環境内に分散バージョン管理システムを実装します。
分散バージョン管理システムは柔軟性を提供しますが、各開発者の環境内でそれらを使用すると、一元管理、バージョン管理の一貫性、およびコラボレーション機能が不足する可能性があります。
オプション B の選択は、Cloud Source Repositories を使用した GCP 上の分散チーム環境でのコード管理のベスト プラクティスと一致しています。
<details><div>

### Q. 問題55: 未回答
あなたは、クラウドネイティブアプリケーションのリーディングカンパニーである「CloudWidgets」のDevOpsリードです。CloudWidgetsは、マイクロサービスアーキテクチャ上に構築された新しいeコマースプラットフォームをGoogle Kubernetes Engine(GKE)にデプロイしています。各マイクロサービスは、在庫管理、注文処理、ユーザー認証など、プラットフォームのさまざまな側面を処理します。シームレスなカスタマーエクスペリエンスを確保するには、アプリケーションのパフォーマンスと正常性のメトリックに対する堅牢な監視を確立する必要があります。
質問：CloudWidgets のシナリオを前提として、GKE でモニタリングするためにアプリケーションレベルの指標を一元的に収集して観察するための最も効率的で簡単なアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解: C. 各マイクロサービスに OpenTelemetry クライアント ライブラリをインストールし、指標のエクスポート先として Stackdriver を構成し、Stackdriver でアプリケーションの指標を確認します。
詳解：
OpenTelemetry の統合:
OpenTelemetry は、分散トレースとメトリックを収集するための標準 API とインストルメンテーション ライブラリを提供する、広く採用されているオブザーバビリティ フレームワークです。CloudWidgetsは、各マイクロサービスにOpenTelemetryクライアントライブラリをインストールすることで、アプリケーション全体で一貫性のある標準化されたメトリック収集を保証します。
Stackdriver にエクスポート:
Stackdriver を指標のエクスポート先として設定することで、CloudWidgets は Google Cloud のネイティブなモニタリングとオブザーバビリティのプラットフォームを活用できます。OpenTelemetry は Stackdriver とシームレスに統合され、指標を Stackdriver に自動エクスポートして、一元的な集計と分析を行うことができます。
Stackdriver による一元的な監視:
Stackdriver は、モニタリング、ログ記録、診断のための一元化されたプラットフォームを提供します。Stackdriver でアプリケーションの指標を観察することで、CloudWidgets はマイクロサービスのパフォーマンスを統一的かつ包括的に把握できます。これには、組み込みのダッシュボード、アラート、および視覚化ツールが含まれています。
標準化と管理の容易さ:
OpenTelemetry と Stackdriver を併用することで、マイクロサービス全体での指標収集方法の標準化が促進されます。この標準化により、管理が簡素化され、複雑さが軽減され、監視に対する一貫したアプローチが保証されます。
正しくないオプションに関するその他の注意事項:
A. Stackdriver Monitoring API のカスタム スクリプトを開発します。
カスタムスクリプトの開発にはメンテナンスの課題が生じ、標準化のベストプラクティスと一致しない可能性があります。OpenTelemetry は、よりモダンで標準化されたアプローチを提供します。
b. 指標の集計に Cloud Pub/Sub を使用します。
指標の集計に Cloud Pub/Sub を利用すると、不必要な複雑さが生じます。OpenTelemetry は、メトリックのエクスポートを簡素化し、追加のコンポーネントの必要性を減らします。
d. すべての指標をログメッセージとして Stackdriver Logging に出力する:
Stackdriver Logging で指標をログメッセージとして出力することは、指標の最適なアプローチではありません。ログと指標は目的が異なるため、パフォーマンス分析には Stackdriver Monitoring などの指標固有のツールを使用すると効果的です。
オプション C はベスト プラクティスに準拠しており、GKE で CloudWidgets アプリケーションをモニタリングするための合理化された標準化された効率的なソリューションを提供します。

## 4
### Q. 問題1: 未回答
Google Cloud Platform(GCP)で複雑なクラウドネイティブ アプリケーションを管理する DevOps エンジニアとして、強力な継続的デリバリー プラットフォームである Spinnaker を採用してデプロイを合理化しました。アプリケーションは複数のマイクロサービスで構成されており、それぞれがSpinnaker パイプラインを使用して個別にデプロイされます。マイクロサービスの新しいバージョンが、特定のパフォーマンスと安定性の基準に基づく厳密なカナリア分析に合格した場合にのみ運用環境に昇格されるようにする高度なデプロイ戦略を実装する必要があります。この高度な展開アプローチを実現するには、Spinnaker のどのようなメカニズムを活用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. 高度な機械学習モデルを備えた自動カナリア分析 (ACA) ステージで、履歴データとパフォーマンス メトリックのリアルタイム分析に基づいてカナリアの成功を判断します。

説明：厳密なカナリア分析に基づいてマイクロサービスの新しいバージョンを本番環境に昇格させるための高度なデプロイ戦略を実装する必要がある特定のシナリオでは、これを実現するための最も適切なメカニズムは、Spinnakerの「自動カナリア分析(ACA)」ステージです。
オプションAが正しい選択である理由は次のとおりです。
自動カナリア分析(ACA)ステージ:Spinnaker の ACA ステージでは、既存の製品バージョンに対して新しいマイクロサービス バージョンの自動データ駆動型カナリア分析を実行できます。この段階では、機械学習技術を活用して、履歴データとパフォーマンスメトリクスのリアルタイム分析に基づいてカナリアバージョンのパフォーマンスと安定性を評価します。
高度な機械学習モデル: ACA ステージでは、高度な機械学習モデルを使用して、カナリア バージョンと運用バージョンの間で、待機時間、エラー率、リソース使用率などのパフォーマンス メトリックを継続的に監視および比較します。これにより、カナリアの行動を動的かつインテリジェントに評価することができます。
履歴データとリアルタイム分析: ACA ステージでは、以前のカナリア デプロイの履歴パフォーマンス データが考慮され、比較のベースラインが提供されます。次に、Canary バージョンのメトリクスをリアルタイムで分析して異常やリグレッションを検出し、Canary を本番環境に昇格させる際のデータドリブンな意思決定を可能にします。
オプションB(Kubernetesクラスタ分析ステージ)は、Kubernetesクラスタ内のマイクロサービスの動作に関する洞察を提供する場合がありますが、自動化されたカナリア分析に特に焦点を当てたり、動的評価に機械学習手法を利用したりしません。
オプション C (カスタム スクリプトを使用したパイプライン トリガー) を使用して、特定の条件に基づいてデプロイをトリガーできますが、機械学習を使用した自動カナリア分析によって提供される洗練されたデータ駆動型のアプローチが欠けています。
オプション D (ブルー/グリーンデプロイと手動承認ゲートを使用したデプロイ戦略) には、カナリア バージョンを運用環境に昇格するための手動介入が含まれますが、これは、特定のパフォーマンス基準に基づく高度で自動化されたカナリア分析の要件と一致しません。
結論として、オプションAは、履歴データとリアルタイムのパフォーマンスメトリクスに基づく動的なカナリア評価のための高度な機械学習モデルを備えた自動カナリア分析(ACA)ステージを使用するもので、特定のシナリオで概説されている高度な展開アプローチを実現するためのSpinnakerの最も適切なメカニズムです。
<details><div>

### Q. 問題2: 未回答
シナリオ：Google Cloud Platform(GCP)でホストされている重要なウェブアプリケーションに取り組む Cloud DevOps エンジニアは、ダウンタイムを最小限に抑え、シームレスなリリース プロセスを保証するデプロイ戦略を計画する任務を負っています。アプリケーションのトラフィックが多く、新機能や拡張機能を導入するために頻繁な更新が必要です。これを実現するには、Blue-Green デプロイ アプローチを実装することにしました。GCP での Blue-Green デプロイを計画する際に考慮すべきこと
質問：特定のシナリオで、Google Cloud Platform(GCP)でブルーグリーンデプロイ戦略を計画する際に考慮すべき要素は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. トラフィック管理、ヘルスチェックとトラフィックの切り替えの自動化、カナリア分析によるアプリケーション動作の検証のために、Kubernetes ServiceオブジェクトでKubernetesクラスタを使用します。

説明：特定のシナリオで、Google Cloud Platform(GCP)でブルーグリーン デプロイ戦略を計画してダウンタイムを最小限に抑え、ウェブ アプリケーションのスムーズなリリース プロセスを確保する場合、考慮すべき最も関連性の高い要素は次のとおりです。
Kubernetesクラスタの使用:Kubernetesクラスタを実装すると、コンテナのオーケストレーションと管理機能が提供され、Webアプリケーションなどのコンテナ化されたアプリケーションのデプロイと管理が容易になります。
トラフィック管理のためのKubernetesサービスオブジェクト:Kubernetesサービスオブジェクトを利用すると、アプリケーションのポッド(コンテナ)のネットワークと負荷分散を抽象化できます。Kubernetes Service は、デプロイ プロセス中に、アプリケーションの Blue (現在) バージョンと Green (新しい) バージョン間のトラフィックを転送できます。
ヘルスチェックとトラフィックの切り替えの自動化: 自動ヘルスチェックを設定すると、Blue-Green デプロイメントプラットフォームが新しいグリーンバージョンのヘルスと可用性をモニタリングします。グリーンバージョンがヘルスチェックに合格すると、自動化はトラフィックをブルー環境からグリーン環境にシームレスに自動的に切り替えることができます。
カナリア分析によるアプリケーション動作の検証: カナリア分析では、新しいバージョン(グリーン)をユーザーまたはトラフィックのサブセットにリリースし、実際の条件でその動作とパフォーマンスを監視します。この検証は、完全なデプロイの前に問題を特定するのに役立ち、すべてのユーザーのダウンタイムのリスクを軽減します。
オプション A (セッションの永続化と手動ロールバック) は、特定の展開シナリオに関連する場合がありますが、Blue-Green 展開戦略と自動化には特に対応していません。
オプション B (手動トラフィック ルーティングと個別のデータベース インスタンスを備えた複数の VM インスタンス) は、従来のデプロイ アプローチとより整合しており、Blue-Green デプロイの利点を十分に活用していません。
オプション D(Cloud Functions、自動スケーリング、DNS 変更)は、ブルーグリーン デプロイ戦略とは直接関係ありません。Cloud Functions はサーバーレス機能であり、Blue-Green デプロイ シナリオには適していない場合があります。
結論として、オプションCは、トラフィック管理のためにKubernetes ServiceオブジェクトでKubernetesクラスタを使用し、ヘルスチェックとトラフィックスイッチングを自動化し、Canary分析でアプリケーションの動作を検証することを含むもので、Google Cloud Platform(GCP)でブルーグリーンデプロイ戦略を計画する際に考慮すべき最も関連性の高い要素を概説しています。このアプローチにより、ダウンタイムとリスクを最小限に抑えながら、効率的で自動化された展開が可能になります。
<details><div>

### Q. 問題3: 未回答
あなたは、Google Kubernetes Engine(GKE)でホストされているマイクロサービスベースのアプリケーションを管理する DevOps エンジニアです。アプリケーションのコンテナ イメージは、コードが変更されるたびに Google Container Registry(GCR)にビルドされ、保存されます。Spinnaker を使用して自動デプロイ プロセスを実装し、新しいコンテナ イメージがビルドされて GCR にプッシュされるたびに、アプリケーションが自動的に GKE にデプロイされるようにします。これを実現するための最良のアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. 新しいイメージがプッシュされたときに Cloud Pub/Sub メッセージをトリガーするように GCR を構成し、Pub/Sub トピックをリッスンしてメッセージの受信時にデプロイを開始するように Spinnaker パイプラインを設定します。
説明：
特定のシナリオでは、新しいコンテナ イメージがビルドされて Google Container Registry(GCR)にプッシュされるたびに、Spinnaker を使用してマイクロサービスベースのアプリケーションを Google Kubernetes Engine(GKE)に自動デプロイする場合、最適なアプローチはオプション A です。
オプションAが正しい選択である理由は次のとおりです。
GCR と Cloud Pub/Sub の統合:Google Container Registry(GCR)は、新しいコンテナ イメージがプッシュされるたびに Cloud Pub/Sub メッセージをトリガーするように設定できます。これにより、イメージの更新について他のサービスに通知するイベント駆動型のメカニズムが提供されます。
スピネーカーパイプライン:Spinnaker は、Cloud Pub/Sub メッセージをリッスンするように構成できる継続的デリバリー プラットフォームです。Pub/Sub トピックで新しいイメージ プッシュ イベントが検出されるたびにデプロイを開始するように Spinnaker パイプラインを設定できます。
自動化と統合:このアプローチにより、GCR、Cloud Pub/Sub、Spinnaker 間のシームレスな自動化と統合が実現します。新しいイメージがプッシュされると、Cloud Pub/Sub メッセージによって Spinnaker パイプラインがトリガーされ、GKE へのデプロイ プロセスが自動化されます。
オプションB(Spinnaker 展開構成の手動更新)では、手動の手順が導入され、イベント駆動型展開の即時かつ自動化された性質が欠けています。
オプション C(カスタム Webhook の作成)は機能しますが、既存の Google Cloud サービスを利用する場合と比較して、複雑さとメンテナンスの手間が増します。
オプション D(Spinnaker での定期的なタスクのスケジュール設定)は、Cloud Pub/Sub トリガーを使用するイベントドリブンなアプローチに比べて効率と動的性に劣ります。
結論として、新しいイメージがプッシュされたときに Cloud Pub/Sub メッセージをトリガーするように GCR を構成し、Pub/Sub トピックをリッスンしてメッセージの受信時にデプロイを開始するように Spinnaker パイプラインを設定するオプション A は、Spinnaker を使用して目的の自動デプロイ プロセスを実現するための最良のアプローチです。
<details><div>

### Q. 問題4: 未回答
シナリオ：あなたはインシデントコマンダー(IC)であり、人気のあるソーシャルメディアプラットフォームに重大なサービス中断を引き起こした重大なインシデントを監督しています。ユーザーレポートによると、プラットフォームの読み込みが遅く、一部の機能が期待どおりに機能していません。サイト信頼性エンジニアリング (SRE) の推奨事項の一環として、インシデント対応を開始し、オペレーション リード (OL) とコミュニケーション リード (CL) を含むチームを編成しました。
質問：インシデント対応の開始とインシデント対応チームの編成に続いて、サイト信頼性エンジニアリング (SRE) のベスト プラクティスに従って推奨される次のステップは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
D. インシデント対応チーム内に明確なコミュニケーション チャネルを確立し、コラボレーションを促進します。

説明：
オプション D は、ソーシャル メディア プラットフォームに影響を与える重大なインシデントを管理するためのサイト信頼性エンジニアリング (SRE) のベスト プラクティスに従って推奨される次のステップです。
オプションDが正しい選択である理由は次のとおりです。
明確なコミュニケーションチャネル:インシデント指揮官 (IC)、オペレーション リード (OL)、コミュニケーション リード (CL)、その他のチーム メンバーを含むインシデント対応チーム内に明確なコミュニケーション チャネルを確立することは、SRE の基本的なプラクティスです。効果的なコミュニケーションにより、すべてのチームメンバーが足並みを揃え、十分な情報を得て、インシデント対応中にシームレスにコラボレーションできるようになります。
コーディネーションとコラボレーション:明確なコミュニケーションチャネルにより、インシデント対応チームのメンバー間でのリアルタイムの更新、情報共有、意思決定が可能になります。これにより、全員が同じ認識を持ち、効率的に連携してサービスの中断に対処できます。
タイムリーなアクション:コミュニケーションチャネルを確立することで、チームはインサイトを迅速に交換し、進捗状況を共有し、アクションに優先順位を付けて、インシデントの影響を軽減し、通常のサービスを回復できます。
オプション A、B、C は、SRE のベスト プラクティスに従った、推奨される即時の次のステップではありません。
コード変更のロールバック:変更のロールバックが必要な場合もありますが、インシデント対応チーム内で明確なコミュニケーションとコラボレーションを行う必要があります。適切な調整を行わずにロールバックを急ぐと、さらなる問題が発生する可能性があります。
根本原因調査の実施:根本原因の調査は重要ですが、調査と分析が協調して行われるようにするには、通信チャネルの確立が前提条件です。
ユーザーへの通知:公式発表を通じてユーザーに通知することは重要なステップですが、社内のコミュニケーション チャネルが確立された後に行う必要があります。当面の焦点は、インシデント対応の調整です。
要約すると、サイト信頼性エンジニアリング (SRE) のベスト プラクティスによると、インシデント対応を開始してチームを編成した後の推奨される次のステップは、インシデント対応チーム内に明確なコミュニケーション チャネルを確立することです (オプション D)。これにより、重大インシデント管理プロセスにおける効果的なコラボレーションと調整が促進されます。
<details><div>

### Q. 問題5: 未回答
シナリオ：あなたは、Google Cloud Platform(GCP)にデプロイされた仮想マシン(VM)のフリートのパフォーマンスの監視と最適化を担当するDevOpsエンジニアです。VM 使用率ログは、各 VM の Stackdriver に保存されます。包括的なダッシュボードを作成して、すべての VM から集計された使用率データを視覚化し、それをそれぞれのチームと共有して、より優れた分析情報とコラボレーションを実現する必要があります。
質問：Stackdriver に保存されているすべての VM から集約された VM 使用率ログを視覚化するダッシュボードを作成し、それぞれのチームと共有するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
イ. 1.VM 使用率ログを Stackdriver から BigQuery にエクスポートして集計します。2. データポータルを使用してダッシュボードを設計し、BigQuery の集計データに接続します。3. データポータルで共有可能なリンクを生成して、ダッシュボードをそれぞれのチームと共有します。

説明：
オプション B は、Stackdriver に保存されているすべての VM から集約された VM 使用率ログを視覚化するダッシュボードを作成し、それぞれのチームと共有するための推奨される方法です。
オプションBが正しい選択である理由は次のとおりです。
BigQuery へのログのエクスポート:VM 使用率ログを Stackdriver から BigQuery にエクスポートすると、ログを集計してクエリ可能な形式で保存できます。BigQuery には、大規模なデータセットを分析するための強力なクエリ機能が用意されています。
データポータルのダッシュボードをデザインする:データポータルは、インタラクティブでカスタマイズ可能なダッシュボードを作成するためのユーザーフレンドリーなツールです。BigQuery で集計された使用率データに接続するダッシュボードを設計し、さまざまなグラフやウィジェットを使用して視覚化できます。
ダッシュボードの共有:データポータルでは、ダッシュボードの共有可能なリンクを生成できます。リンクを各チームと共有することで、リアルタイムで集約された VM 使用率ダッシュボードに直接アクセスできるようになります。
オプション A、C、および D は、このシナリオにはあまり適していません。
オプションA:Stackdriver ワークスペースを作成してカスタム指標を設定するのは有効なアプローチですが、Stackdriver のモニタリング ダッシュボード デザイナは、ログベースの指標を使用してカスタム ダッシュボードを作成するように特別に設計されていません。
オプションC:Stackdriver の組み込みダッシュボード テンプレートを利用すると、すべての VM の集計データを表示するようにダッシュボードをカスタマイズする柔軟性が得られない場合があります。 また、アラート ポリシーを構成しても、チームとのダッシュボードの共有には直接対応しません。
オプションD:ログを取得して処理するカスタム スクリプトを作成し、サードパーティのデータ可視化ツールを使用すると、複雑さが増し、BigQuery やデータポータルなどの Google Cloud のネイティブ ソリューションを利用するほど簡単ではありません。
要約すると、オプション B(VM 使用率ログを BigQuery にエクスポートし、データポータルでダッシュボードを設計し、ダッシュボードをそれぞれのチームと共有する)は、Google Cloud Platform ソリューションを使用して包括的な VM 使用率ダッシュボードを作成、共有するためのベスト プラクティスと一致しています。
<details><div>

### Q. 問題6: 未回答
シナリオ：あなたは、複雑なクラウドベースのアプリケーションのインフラストラクチャの管理を担当するDevOpsエンジニアのチームを率いています。2 人の新しいチーム メンバーが加わったため、コードの競合を防ぎ、コードの整合性を維持しながら、効率的なコラボレーションを確保するためのプロセスを確立する必要があります。インフラストラクチャは Terraform テンプレートを使用して定義されており、バージョン管理とコード管理を容易にするソリューションを採用する必要があります。
質問：効果的なコラボレーションを確保し、コードの競合を防ぎ、Terraformテンプレートのコード整合性を維持するには、コード管理プロセスをどのように設定する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B. Git ベースのバージョン管理システムを使用してコードを保存します。変更をメインブランチにマージする前に、ピアコードレビューと単体テストを含むプロセスを実装し、完全に統合されたコードが最新バージョンになるようにします。

説明：
オプション B は、コードのコラボレーション、バージョン管理、およびコードの整合性を管理するための最も効果的なアプローチを提供します。
Git ベースのバージョン管理システム:Git のようなバージョン管理システムを使用すると、複数の開発者が互いの変更を上書きすることなく、同じコードベースで共同作業を行うことができます。バージョン履歴、追跡、および分岐機能を提供します。
ピアコードレビューとユニットテスト:ピアによるコードレビューを組み込むことで、変更の品質と正確性が確実にレビューされます。単体テストでは、コード変更の機能を検証し、障害のあるコードの統合を防ぎます。
メインブランチへの変更のマージ:変更がレビューされ、テストされ、メイン ブランチに統合されるプロセスに従うことで、完全に検証され、統合されたコードのみが最新バージョンの一部になります。これにより、コードの整合性が維持され、エラーが発生するリスクが軽減されます。
オプションA、C、およびDには、オプションBと比較して制限があります。
オプションAは、Googleドライブと手動アーカイブを使用し、Gitのバージョン管理、履歴追跡、コラボレーション機能がありません。
オプション C は Cloud Storage を利用しますが、手動でのパッケージ化とバージョン管理では、バージョン管理とコードレビューのメリットがありません。
オプションDは、電子メールによる手動共有に依存しており、バージョン管理、コードレビュー、および一元管理の利点がありません。
要約すると、オプション B は、コードのコラボレーション、バージョン管理、およびコードの整合性を管理するための構造化された効率的なアプローチを提供し、特定のシナリオに最も適した選択肢となります。
<details><div>

### Q. 問題7: 未回答
サードパーティのコンテンツ配信ネットワーク(CDN)を使用して、世界中のユーザーにコンテンツを配信するビデオストリーミングサービスを管理しています。ユーザー要求は CDN に送信され、CDN はトラフィックをアプリケーション サーバーにルーティングします。可用性サービスレベルインジケータ (SLI) をアプリケーションサーバレベルで実装しておきます。ただし、CDNのパフォーマンスに関連する問題を検出するために、より包括的な監視アプローチを確保する必要があります。
この新しいSLIはどこで測定すべきでしょうか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
イ. CDN のパフォーマンスを監視するために、さまざまな地理的な場所からのユーザー要求をシミュレートする代理トランザクションを実装します。
説明：代理トランザクションを使用して、さまざまな地理的な場所からのユーザー要求をシミュレートすると、さまざまなリージョンからの CDN のパフォーマンスを直接測定できます。このアプローチにより、ユーザーエクスペリエンスに影響を与える可能性のあるCDNのパフォーマンスとグローバルネットワークに関連する問題を検出できます。
D. CDNのプラットフォーム上で直接、CDN固有の指標をキャプチャします。

説明：CDN固有の指標をCDNのプラットフォーム上で直接測定することで、そのパフォーマンスと健全性に関する洞察を得ることができます。このアプローチにより、CDN の動作を監視し、発生する可能性のある問題を検出し、コンテンツの配布が効率的かつ効果的であることを確認できます。
オプション A、C、および E では、CDN とグローバル ネットワークに関連する問題を包括的に監視できない場合があります。
オプション A (アプリケーションサーバーによって生成されたログを分析して CDN 関連の問題を特定する) は、アプリケーションサーバーに焦点を当てており、CDN 固有の問題に関する詳細な分析情報を提供しない場合があります。
オプション C (アプリケーション サーバーの正常性と応答時間の監視) は、アプリケーションのパフォーマンスにとって重要ですが、CDN 関連の問題を直接キャプチャしない場合があります。
オプションE(CDNとアプリケーションサーバー間のネットワークトラフィックとレイテンシーの分析)は、ネットワークパフォーマンスの監視に役立ちますが、CDN関連の問題の全体像を把握できない場合があります。
<details><div>

### Q. 問題8: 未回答
シナリオ：あなたは、Google Compute Engine(GCE)仮想マシンでホストされるビジネスクリティカルなアプリケーションの管理を担当する DevOps エンジニアです。アプリケーションでは、使用のピーク時にパフォーマンスの低下や速度低下が時折発生し、ユーザーエクスペリエンスに影響を与え、収益の損失を引き起こします。アプリケーションのパフォーマンスと信頼性を最適化しながら、効率的なリソース使用率と費用対効果を確保する必要があります。
このような状況の中で、Google Compute Engine でアプリケーションのパフォーマンスと信頼性を向上させるには、どのような対策を講じるべきでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
ある。 インスタンスグループと負荷分散を実装して、受信トラフィックを複数の仮想マシンに分散し、ピーク時の高可用性とパフォーマンスの向上を実現します。

説明：負荷分散を使用してインスタンスグループを実装すると、受信トラフィックが複数の仮想マシンインスタンスに分散され、リソース使用率と耐障害性が向上します。ピーク時の使用量では、ロードバランサーはリクエストを均等に分散できるため、1 つのインスタンスが過負荷になり、パフォーマンスが低下するのを防ぐことができます。また、このアプローチにより、インスタンスに障害が発生した場合にシームレスなフェイルオーバーが提供され、信頼性の向上に貢献します。
オプション B のプリエンプティブル仮想マシンへの移行は、コストを削減できる可能性がありますが、中断や不安定化につながる可能性があり、ビジネスクリティカルなアプリケーションには理想的ではありません。
オプション C は、CPU 使用率メトリックに基づいて自動スケーリングを構成することは有益ですが、リソースの競合やボトルネックが存在する場合に、それだけでは、ピーク使用時の時折のパフォーマンスの低下や速度低下に対処できない可能性があります。
オプション D は、カスタムマシンタイプと Stackdriver Monitoring を利用する場合、ピーク時のパフォーマンスと信頼性の問題に直接対処するのではなく、リソースの最適化とモニタリングに重点を置いています。
インスタンスグループと負荷分散の実装は、パフォーマンス、可用性、信頼性を向上させるためのベストプラクティスに沿っており、効率的なリソース使用率も確保します。
<details><div>

### Q. 問題9: 回答
CI/CD パイプラインのセキュリティを強化し、機密データを確実に保護するための継続的な取り組みの一環として、アプリケーション シークレットを管理し、効率的なシークレット ローテーションを可能にするための安全な方法を実装する必要があります。
CI/CD パイプラインでアプリケーション シークレットの安全な管理とシークレット ローテーションの合理化を実現するために推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
C. シークレットは、Cloud KMS の鍵で暗号化して Cloud Storage に保存します。CI / CD パイプラインに IAM 経由で Cloud KMS へのアクセスを提供します。

説明：Cloud KMS の鍵で暗号化されたシークレットを Cloud Storage に保存し、IAM を介して CI / CD パイプラインに Cloud KMS へのアクセス権を付与することで、アプリケーション シークレットを安全かつ制御して管理できます。Cloud KMS は鍵の管理と暗号化オペレーションを提供し、シークレットの安全な暗号化と復号を可能にします。このアプローチにより、セキュリティが強化され、セキュリティ侵害が発生した場合に効率的なシークレットローテーションが可能になります。ソースコードリポジトリ内にシークレットをプレーンテキストで保存したり (オプション A)、パブリックリポジトリでバージョン管理された外部構成ファイルを使用したり (オプション B)、パイプラインスクリプトにシークレットを埋め込んだり (オプション D) すると、機密情報が公開され、セキュリティの脆弱性につながる可能性があります。
<details><div>

### Q. 問題10: 未回答
シナリオ：
あなたは、Google Kubernetes Engine(GKE)クラスタでホストされる複雑なマイクロサービス アプリケーションの継続的インテグレーションと継続的デプロイ(CI / CD)パイプラインの管理を担当する DevOps エンジニアです。責任の一環として、CI / CD パイプラインが GKE クラスタで認証され、新しいデプロイを実行できるようにする必要があります。CI/CD パイプラインと GKE クラスタ間の安全な認証を有効にするには、どうすればよいですか?
質問：
GKE でマイクロサービス アプリケーションの CI / CD パイプラインを管理する DevOps エンジニアとして、新しいデプロイを実行するためにパイプラインと GKE クラスタ間の安全な認証を有効にするにはどうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。D

説明：
各回答の選択肢を分解し、詳細な説明を提供しましょう。
A. GKE クラスタの認証情報をパイプラインの環境変数に直接埋め込むことで、デプロイ中にシームレスな認証が可能になります。
GKE クラスタの認証情報などの機密性の高い認証情報を環境変数に直接埋め込むことは、安全な方法ではありません。環境変数は、パイプライン構成にアクセスできる人なら誰でも簡単にアクセスできるため、GKE クラスタへの不正アクセスにつながる可能性があります。
B. CI/CD パイプラインの SSH 鍵を設定し、認証目的でパイプラインからの SSH 接続を受け入れるように GKE ノードを構成します。
認証に SSH 鍵を使用することは、サーバーにアクセスするための一般的な方法ですが、GKE クラスタでの認証には推奨されません。GKE は、SSH 鍵ではなく、サービス アカウント トークンなどの Kubernetes 固有の認証メカニズムを使用します。
C. GKE クラスタの API サーバーをパブリック インターネットに直接公開し、CI / CD パイプラインが標準の API トークンを使用して認証できるようにします。
GKE クラスタの API サーバーをパブリック インターネットに直接公開することは、重大なセキュリティ リスクです。これにより、クラスターは潜在的な攻撃や不正アクセスにさらされます。また、認証に標準の API トークンを使用しても、GKE クラスタの管理に必要なレベルのセキュリティと制御は提供されません。
D. CI / CD パイプラインの Kubernetes サービス アカウントを作成し、GKE クラスタで適切な RBAC ロールを付与します。GKE API サーバーでの認証にサービス アカウントの認証情報を使用するようにパイプラインを構成します。
これは推奨される方法です。Kubernetesには、クラスターで実行されているアプリケーションに特定の権限を付与できるサービスアカウントの概念が用意されています。CI/CD パイプラインのサービス アカウントを作成し、ロールベースのアクセス制御 (RBAC) を使用して必要なロールを割り当てることで、クラスター内でパイプラインが実行できる操作をきめ細かく制御できます。その後、パイプラインはサービス アカウントの認証情報(トークン)を使用して、GKE API サーバーで安全に認証できます。
CI / CD パイプラインの Kubernetes サービス アカウントを作成し、GKE クラスタで適切な RBAC ロールを付与し、サービス アカウントの認証情報を使用するようにパイプラインを構成することが、新しいデプロイを実行するためにパイプラインと GKE クラスタ間の認証を有効にするための推奨かつ安全な方法です。このアプローチにより、GKE リソースを管理するためのセキュリティと適切なアクセス制御の両方が確保されます。
<details><div>

### Q. 問題11: 未回答
シナリオ: あなたは、人気のある e コマース プラットフォームのリード サイト信頼性エンジニア (SRE) です。チームは、プラットフォームの信頼性を維持し、サービス レベル目標 (SLO) を確実に満たす責任があります。ホリデーシーズンにはプラットフォームのトラフィックが多くなり、この時期に重大なインシデントが発生すると、ユーザーエクスペリエンスと収益に大きな影響を与える可能性があります。
質問: ホリデー シーズンのピーク時に機能開発と信頼性のバランスを効果的に管理するには、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
B. ホリデー シーズン中のダウンタイムまたは低下の許容レベルを定義するエラー バジェットを確立し、新機能と信頼性の向上の間のトレードオフを行うためのガイドとして使用します。

説明：
サイト信頼性エンジニアリング (SRE) の原則では、機能開発と信頼性のバランスを維持することの重要性が強調されています。「誤差バジェット」の概念は、このアプローチの重要な要素です。エラーバジェットは、サービスレベル目標(SLO)を満たしている間にサービスで発生する可能性のあるダウンタイムまたはパフォーマンス低下の許容量です。
ホリデーシーズンのピーク時には、eコマースプラットフォームはトラフィックが多くなり、ユーザーアクティビティが増加します。これらの重要な時期に機能開発と信頼性のバランスを効果的に管理するには、次の理由からエラー バジェットを確立することをお勧めします。
定量化可能なトレードオフ:エラーバジェットは、SREチームと開発チームが、新機能の導入と信頼性の維持の間で十分な情報に基づいたトレードオフを行うことを可能にする定量化可能な指標を提供します。これは、SLO に違反することなく、どの程度のダウンタイムや劣化を許容できるかについて、明確な期待値を設定するのに役立ちます。
優先順位付けされた意思決定:エラーバジェットが設定されている場合、新機能のデプロイやプラットフォームへの変更に関する決定は、信頼性への潜在的な影響を明確に理解した上で行われます。エラーバジェットは優先順位付けメカニズムとして機能し、加えられた変更がプラットフォームの信頼性目標と一致していることを確認します。
柔軟性と革新性:信頼性を維持することは重要ですが、特にトラフィックの多い時期には、革新を続け、ユーザーに価値を提供することも重要です。誤差バジェットは、定義された制限内で新機能、改善、または実験を導入する柔軟性を提供し、信頼性を損なうことなくイノベーションを促進します。
チーム間のコラボレーション:エラーバジェットのアプローチは、SREチームと開発チームの間のコラボレーションを促進します。両方のチームが協力して、プラットフォームのパフォーマンスを監視し、データ主導の意思決定を行い、リソースを効果的に割り当てて、エラー予算内に収めながらユーザーの要求を満たします。
継続的な学習と改善:ピークシーズンにエラーバジェットがどのように利用されているかを分析することで、チームは改善が必要な領域に関する洞察を得ることができます。これらの期間中に収集されたデータは、将来の計画、リソースの割り当て、および最適化の取り組みの指針となります。
要約すると、ホリデーシーズンのピーク時にエラーバジェットを確立することで、eコマースプラットフォームは新機能の提供と信頼性の維持のバランスを取ることができます。このアプローチにより、チームは情報に基づいた意思決定を行い、効果的にコラボレーションし、トラフィックの多い時間帯でもポジティブなユーザーエクスペリエンスを確保できます。
<details><div>

### Q. 問題12: 未回答
クラウドでホストされるデータベースとやり取りするウェブアプリケーションをデプロイするために、データベースの認証情報を Cloud Build パイプラインに安全に渡す必要があります。高レベルのセキュリティを維持しながらこれを実現するには、どのようなアプローチを取る必要がありますか?最適なオプションを選択してください。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解はBです。Google Cloud の Secret Manager の使用は、データベースの認証情報などの機密データを安全に管理し、Cloud Build パイプラインに渡すための推奨されるアプローチです。Secret Manager は、シークレットの一元管理、アクセス制御、自動ローテーション、監査証跡を提供し、デプロイ プロセス中に機密情報が保護されるようにします。
オプション A、C、D は、さまざまな方法で資格情報を公開し、アプリケーションとユーザー データのセキュリティを損なう可能性があるため、機密性の高い資格情報を安全に渡すにはお勧めしません。
<details><div>

### Q. 問題13: 未回答
あなたは、Google Kubernetes Engine(GKE)上で動作するマイクロサービスベースのアプリケーションの管理を担当する DevOps エンジニアです。アプリケーションは、シームレスなユーザーエクスペリエンスを提供するために相互に通信する複数のサービスで構成されています。ユーザー・トラフィックは、外部HTTP(S)ロード・バランサを使用して負荷分散されます。
最近、一部のマイクロサービスの応答時間がピーク使用時に増加し始めていることに気付きました。アプリケーションのフロントエンドは、優れたユーザーエクスペリエンスを維持しながら、増加したトラフィックを処理するために適切にスケーリングする必要があります。
この状況に対処し、アプリケーションのフロントエンドを効率的にスケーリングするための最良のアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
この状況に対処し、アプリケーションのフロントエンドを効率的にスケーリングするための正しいアプローチは次のとおりです。
C. Stackdriver カスタム メトリック アダプターをインストールし、Load Balancer への受信リクエストの数に基づいて水平ポッド オートスケーラーを構成します。

説明：
Stackdriver カスタム メトリック アダプター: Stackdriver カスタム メトリック アダプターをインストールすると、Load Balancer への受信リクエストの数など、特定のアプリケーションレベルのデータに基づいてカスタム指標を作成できます。
Horizontal Pod Autoscaler (HPA): HPA は、観測されたメトリックに基づいてデプロイ内のポッドの数を自動的に調整し、必要なパフォーマンス基準を満たすために必要に応じてアプリケーションをスケールアップまたはスケールダウンします。
受信要求の数: 受信要求の数に基づくスケーリングは、フロントエンド サービスの実用的なアプローチです。受信トラフィックが増加すると、HPA はフロントエンド ポッドを自動的に追加して負荷を分散し、最適な応答時間を維持します。
オプションA(リソース要求と制限の増加)は、フロントエンド自体のスケーリングではなく、ポッド内のリソース割り当てのみに対処するため、最適なソリューションではありません。
オプション B (CPU 使用率に基づくスケーリング) は、CPU 使用率がピーク使用時のフロントエンド パフォーマンスの直接的な指標ではない可能性があるため、最適な選択ではない可能性があります。
オプション D (垂直ポッド オートスケーラー) は、個々のポッドのリソース要求と制限の調整に重点を置いているため、トラフィックの増加を処理するためにフロントエンドをスケーリングする必要性に直接対処できない場合があります。
したがって、Stackdriver のカスタム指標を使用し、受信リクエストの指標に基づいて HPA を構成することで、フロントエンドを効率的にスケーリングしてさまざまなトラフィック負荷を処理できる、より効果的なソリューションが提供されます。
<details><div>

### Q. 問題14: 未回答
あなたは、Google Cloud の Virtual Private Cloud(VPC)で重要なワークロードを実行する大企業のネットワーク管理者です。VPC 内のネットワークトラフィックを監視して、セキュリティとコンプライアンスを確保したい。ただし、包括的な監視の必要性と、潜在的なコストおよびログ ストレージに関する考慮事項とのバランスを取る必要もあります。
VPC 内のネットワークトラフィックを監視するために VPC フローログを設定する場合、どのスケール設定を選択する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B. カバレッジとログ データのバランスを取るために、0.5 の中程度のボリューム スケール。

説明：
Virtual Private Cloud(VPC)内のネットワークトラフィックをモニタリングするように VPC フローログを設定する場合、ボリュームスケールによって、サンプリングおよびログに記録されるネットワークフローの割合が決まります。スケール設定は、包括的な監視の必要性と、潜在的なコストおよびログ ストレージ要件とのバランスを取ります。
0.5 の中規模ボリューム スケールを選択すると、ネットワーク フローの約 50% がサンプリングされ、ログに記録されます。これにより、カバレッジとログデータのバランスが取れ、ログストレージを圧迫したり、過剰なコストが発生したりすることなく、ネットワークトラフィックの代表的な部分を監視できます。
1.0 などのボリューム スケールが大きいほど、より包括的な監視が可能になりますが、大量のログ データが生成される可能性があり、ストレージ コストが増加します。一方、0.1 のようなボリュームスケールが低いと、重要なトラフィック パターンを見逃す可能性があります。
0.5 のミディアムボリュームスケールを選択することで、ネットワークフローの有意義なサブセットを確実にキャプチャし、コストとログストレージ要件を管理しながら、VPC 内のネットワークトラフィックを効果的にモニタリングおよび分析できます。
<details><div>

### Q. 問題15: 未回答
シナリオ：Compute Engine インスタンスを使用して、Google Cloud Platform(GCP)でデータ集約型の分析プラットフォームを管理している。このプラットフォームは、さまざまなソースからの大規模なデータセットを処理および分析し、組織に貴重な洞察を提供します。データ処理には複雑なクエリと変換が伴い、分析クエリのパフォーマンスはタイムリーな意思決定に不可欠です。プラットフォームが、使用のピーク時でも最適なパフォーマンスと応答性を維持する必要があります。
質問：シナリオを前提として、使用のピーク時に分析プラットフォームのパフォーマンスと応答性を最適化するには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。D

説明：
A. キャッシュ メカニズム (オプション A) の実装は、分析プラットフォームのパフォーマンスを最適化するための有効な戦略です。頻繁にアクセスされるデータセットをメモリにキャッシュすることで、クエリ処理に必要な時間を大幅に短縮できます。このアプローチは、同じデータが複数回クエリされるシナリオで特に効果的です。
B. プリエンプティブル仮想マシン (オプション B) の利用は、コスト効率の高いソリューションになる可能性がありますが、クエリのパフォーマンスを最適化する必要性に直接対処できない場合があります。プリエンプティブル VM は、中断を許容できるワークロードに適しており、バッチ処理や並列化可能なタスクによく使用されます。
C. 永続ディスクのサイズを増やす (オプション C) と、ストレージ容量は向上しますが、クエリのパフォーマンスを直接最適化できない場合があります。ストレージ容量は重要ですが、このシナリオではクエリ処理速度の向上に重点が置かれています。
D. 自動スケーリング構成 (オプション D) を調整して、ピーク時の使用量にリソースを追加すると、分析プラットフォームのパフォーマンスを最適化するのに役立ちます。必要に応じて Compute Engine インスタンスを動的に追加することで、処理負荷を分散し、需要の高い期間の応答性を維持できます。
したがって、ピーク使用時に分析プラットフォームのパフォーマンスと応答性を最適化するための最適なオプションは、オプション D: Compute Engine インスタンスの自動スケーリング構成を調整して、ピーク使用時に必要に応じてリソースを追加することです。このアプローチにより、プラットフォームはリソースを動的にスケーリングして、増加したワークロードを処理し、最適なクエリ パフォーマンスを維持できます。
<details><div>

### Q. 問題16: 未回答
あなたは DevOps エンジニアで、Docker コンテナと Google Cloud リソースを含むプロジェクトに取り組んでいます。チームは Google Container Registry を使用して Docker イメージを安全に保存しています。Dockerが適切な資格証明を使用してコンテナ・レジストリへのリクエストを認証できることを確認する必要があります。Google Container Registry を操作するときに gcloud で認証するように Docker を構成するには、どのコマンドを使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
b) gcloud auth configure-docker (英語)
以下はその説明です。
gcloud の認証情報を使用して Google Container Registry へのリクエストを認証するように Docker を構成するには、gcloud auth configure-docker コマンドを使用する必要があります。このコマンドは、gcloud によって管理される認証認証情報を使用するために必要な Docker 構成を設定し、Container Registry を安全に操作できるようにします。
a)gcloud auth configure docker、c)gcloud auth docker-configure、d)gcloud auth docker configure のオプションは無効なコマンドです。正しいコマンドは、オプション b に指定されています。
<details><div>

### Q. 問題17: 未回答
あなたは DevOps エンジニアで、重要なアプリケーションをホストするためのプライベートな Google Kubernetes Engine(GKE)クラスタを設定する任務を負っています。セキュリティ要件により、クラスターはインターネットに直接アクセスできないプライベート ネットワークで動作する必要があります。アプリケーションのコンテナは、プライベートネットワーク内の信頼できる安全なソースからDockerイメージをプルする必要があります。このシナリオでプライベート GKE クラスタが Docker イメージを安全にダウンロードできるようにするには、どの Google サービスを構成する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
d) コンテナレジストリ
以下はその説明です。
プライベート ネットワーク内で動作しているプライベート Google Kubernetes Engine(GKE)クラスタがあり、安全なイメージのプルが必要なシナリオでは、使用する適切な Google サービスは Container Registry です。Container Registryは、Dockerコンテナ・イメージを格納および管理するための管理された安全なプラットフォームを提供します。プライベート GKE クラスタがプライベート ネットワーク内から Docker イメージを安全にプルできるように構成して、セキュリティ要件への準拠を確保できます。
a)Cloud Build、b)Cloud Source Repository、c)Elastic Container Registry のオプションは、プライベート GKE クラスタ内で Docker イメージを安全にプルする特定の機能を提供するようには設計されていません。
<details><div>

### Q. 問題18: 未回答
シナリオ：最近のリリースによって引き起こされたメモリ リソースの枯渇が原因で発生したサービス停止のインシデント事後分析を主導しています。この停止は、リリースをロールバックすることで正常に軽減されました。サイト信頼性エンジニアリング (SRE) のプラクティスに従った包括的な事後分析を実施する取り組みの一環として、分析に最適なアプローチを検討しています。
質問：サイト信頼性エンジニアリングのプラクティスに従って、サービス停止の事後分析を実施する場合、主に何に焦点を当て、何にアプローチする必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
ある

説明：
サイト信頼性エンジニアリング (SRE) のプラクティスでは、インシデント後の分析に対する誰も責めないアプローチを重視しています。事後分析の主な目的は、インシデントの原因となったシステム上の問題を理解し、今後同様のインシデントが発生しないようにすることです。これは、特定の個人に責任を負わせるのではなく、原因を特定することに焦点を当てることによって達成されます。
他のオプションが最善のアプローチではない理由は次のとおりです。
新機能の開発 (オプション B):新機能の開発はサービスを強化するために重要ですが、インシデント後の事後分析の主な焦点ではありません。当面の関心事は、将来のインシデントを防ぐために根本原因を特定して対処することです。
個別ミーティング(オプションC):関係するエンジニアから意見を集めることは重要ですが、個別のミーティングを開催すると、インシデントの偏りや不完全な理解につながる可能性があります。包括的な分析には、協調的で部門横断的なアプローチが好まれます。
責任者を特定する(オプションD):責任の所在を明らかにすることは、SRE の非の打ちどころのない文化に反するものです。個人を名指しするのではなく、インシデントにつながった要因や体系的な問題を理解することに重点を置く必要があります。
要約すると、SRE プラクティスに従った事後分析では、システムの信頼性を高め、将来の同様のインシデントを防ぐために、インシデントの原因となる原因の特定を優先する必要があります。
<details><div>

### Q. 問題19: 未回答
シナリオ：あなたは、Google Cloud Platform(GCP)でコンテナ化されたアプリケーションの管理を担当するDevOpsエンジニアです。組織の本番環境サービスは、リージョンにある Google Kubernetes Engine(GKE)クラスタでホストされます。コンテナー イメージの作成に使用されるビルド システムは、リージョンに配置されています。ビルドシステムと GKE クラスタ間で効率的かつ高帯域幅のイメージ転送を実現するには、コンテナ イメージ レジストリを最適化する必要があります。この目標を達成するために、どのようなアプローチを取る必要がありますか?asia-east1us-central1
質問：リージョン内のビルドシステムとリージョン内の Google Kubernetes Engine(GKE)クラスタ間のコンテナイメージの転送を最大化し、最適化するには、コンテナ レジストリにイメージをプッシュするためにどのオプションを選択する必要がありますか?us-central1asia-east1
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:リージョン内のビルドシステムとリージョン内の Google Kubernetes Engine(GKE)クラスタ間のコンテナイメージの転送を最大化し、最適化するための正しいオプションは、オプション C: ホスト名を使用して Google Container Registry(GCR)にイメージをプッシュする です。us-central1asia-east1asia.gcr.io

説明：コンテナ イメージは、特定のリージョン内の Google Container Registry(GCR)に保存されます。異なるリージョン間のイメージ転送を最適化するには、GKE クラスタが配置されているリージョンに対応する GCR ホスト名を選択する必要があります。この場合、GKE クラスタはリージョン内にあるため、コンテナ イメージをプッシュするときにホスト名を使用する必要があります。これにより、画像が GKE クラスタと同じリージョンに保存され、レイテンシが短縮され、画像転送の帯域幅が最大化されます。asia-east1asia.gcr.io
他のオプションが正しい選択ではない理由:
gcr.io ホスト名 (オプション A):ホスト名の使用は有効なオプションですが、リージョン固有ではないため、リージョン間のイメージ転送の帯域幅使用率が低下する可能性があります。gcr.io
us.gcr.io ホスト名 (オプション B):ホスト名を使用すると、イメージ ストレージがリージョンに送られますが、リージョン内の GKE クラスタの場所と一致しません。us.gcr.ious-central1asia-east1
Compute Engine のプライベート イメージ レジストリ(オプション D):このオプションを検討することもできますが、フルマネージド ソリューションを提供する Google Container Registry(GCR)を使用する場合と比較して、複雑さと管理が増します。
結論として、リージョン内のビルドシステムとリージョン内の GKE クラスタ間のコンテナイメージ転送を最適化するには、ホスト名を使用してイメージを Google Container Registry(GCR)にプッシュする必要があります。us-central1asia-east1asia.gcr.io
<details><div>

### Q. 問題20: 未回答
シナリオ：あなたは、組織のクラウドベースのアプリケーションで広範なサービス中断を引き起こした重大なインシデントのインシデント指揮官です。役割の一環として、インシデントドキュメントを作成して、インシデントの詳細とそれを解決するために実行されたアクションを文書化する必要があります。インシデント ドキュメントが Google Site Reliability Engineering(SRE)のベスト プラクティスに従っていることを確認し、効果的なインシデント対応とインシデント事後分析を促進する必要があります。
質問：インシデント管理に関する Google SRE のベスト プラクティスのコンテキストで、インシデント ドキュメントに含めない情報は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:Google SRE のインシデント管理に関するベスト プラクティスのコンテキストでは、インシデント ドキュメントに含めるべきではない情報は、オプション D: インシデントの責任者とその役割の特定です。

説明：Google SRE は、インシデント管理に対する非の打ちどころのないアプローチを重視しており、インシデントから学び、プロセスを改善し、将来の同様のインシデントを防ぐことに重点が置かれています。インシデントの責任者を明示的に名指しすると、非難の文化が生まれ、オープンなコミュニケーションが妨げられる可能性があります。代わりに、インシデント ドキュメントは、特定の個人に責任を負わせることなく、技術的な詳細、タイムライン、アクション、およびチーム間のコラボレーションをキャプチャすることに焦点を当てる必要があります。
他のオプションが正しい選択ではない理由:
イベントの詳細なタイムライン(オプションA):イベントの詳細なタイムラインを文書化することは、一連のアクションとインシデントへの影響を理解するために不可欠です。これは、根本原因を特定し、インシデント対応プロセスを改善するのに役立ちます。
実行されたアクションのリスト (オプション B):インシデント中に取られたアクションを文書化することは、知識を共有し、一貫した対応を確保するために重要です。これは、インシデント後の分析と改善すべき領域の特定に役立ちます。
コミュニケーションとコラボレーションの概要(オプションC):チーム間のコミュニケーションとコラボレーションを文書化することで、インシデントに対処するためにさまざまなチームがどのように作業を調整したかについての分析情報が得られます。これにより、インシデント対応プラクティスの学習と改善が容易になります。
結論として、Google SRE のベスト プラクティスに従って、インシデント ドキュメントは技術的な詳細、タイムライン、アクション、コラボレーションに焦点を当て、インシデントの責任者を明示的に非難したり、名指ししたりすることは避ける必要があります。
<details><div>

### Q. 問題21: 未回答
シナリオ：Google Kubernetes Engine(GKE)にデプロイされた複雑なマイクロサービスベースのアプリケーションのパフォーマンスを最適化する責任があります。アプリケーションは、相互に通信する複数のサービスで構成されています。アプリケーションは多数のユーザーにサービスを提供するため、パフォーマンスのボトルネックを特定し、リソース使用率を最適化して、スムーズなユーザーエクスペリエンスを確保する必要があります。
質問：Google Kubernetes Engine(GKE)にデプロイされたマイクロサービスベースのアプリケーションのパフォーマンスを最適化する場合、パフォーマンス データを収集、分析してボトルネックを特定し、リソース使用率を最適化するには、どのオプションが適していますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
B

説明：
Google Cloud Profiler:Google Cloud Profiler は、アプリケーションをプロファイリングして、実行時の動作、パフォーマンス特性、リソース使用率を把握するための強力なツールです。最小限のオーバーヘッドできめ細かなパフォーマンスデータを収集します。
マイクロサービス・プロファイリング:Cloud Profiler エージェントを各マイクロサービスと一緒にデプロイすると、各サービスに固有の詳細なパフォーマンス情報を取得し、その動作とボトルネックに関する洞察を得ることができます。
集中分析:Cloud Profiler は、収集されたパフォーマンスデータを分析および視覚化するための一元化されたインターフェイスを提供します。豊富な視覚化ツール、フレームグラフ、その他の洞察を提供し、最適化すべき領域を特定するのに役立ちます。
他のオプションが最良の選択ではない理由:
A. カスタムログステートメントを実装します。カスタム ログでは、パフォーマンスに関する分析情報は限られていますが、各マイクロサービス内の特定のボトルネックを特定するために必要なきめ細かな詳細が提供されない場合があります。
C. Prometheus モニタリングを設定します。Prometheus は人気のある監視ツールですが、プロファイリングではなくメトリックの収集に重点を置いています。マイクロサービスプロファイリングの Cloud Profiler と同じレベルの詳細なインサイトは提供されない場合があります。
D. パフォーマンス テストを手動で実行します。合成ワークロードを使用した手動パフォーマンス テストでは、貴重な分析情報を提供できますが、実際の運用条件下でのマイクロサービス間の実際の動作と相互作用をキャプチャできない場合があります。
結論として、Google Cloud Profiler では各マイクロサービスのパフォーマンス特性とリソース使用率の詳細なプロファイリングと分析が可能であるため、GKE でマイクロサービス ベースのアプリケーションのパフォーマンスを最適化するには、オプション B の方が適しています。
<details><div>

### Q. 問題22: 未回答
シナリオ：Google Cloud Platform(GCP)にデプロイされた複雑なマイクロサービスベースのアプリケーションを管理しており、さまざまなマイクロサービス全体でリクエストフローを効果的に監視および追跡する必要があります。これは、問題の診断、パフォーマンスの最適化、信頼性の維持に不可欠です。
質問：Google Cloud Platform(GCP)にデプロイされたマイクロサービスベースのアプリケーションのリクエストフローを追跡するには、さまざまなマイクロサービス間の相互作用に関する洞察を得るために、どのサービスまたはツールを使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
 A(Google Stackdriver トレース)

説明：
Google Stackdriver トレース:Stackdriver Trace を使用すると、アプリケーション内のさまざまなマイクロサービスを通過するリクエストのパス全体をトレースして可視化できます。個々のサービスのレイテンシーデータをキャプチャし、それらの間の相互作用を示すことで、ボトルネックの特定やパフォーマンスの問題のトラブルシューティングに役立てることができます。
Google Cloud Profiler:Google Cloud Profiler は、アプリケーションのコード内の CPU とメモリの使用量をキャプチャするのに役立ちますが、マイクロサービス間のリクエスト フローを特に追跡するわけではありません。
Google Cloud Logging:Cloud Logging は、アプリケーションやサービスによって生成されたログの収集と分析に重点を置いています。これは、個々のマイクロサービス内で何が起こっているかを理解する上では重要ですが、要求フローの全体像を把握できない場合があります。
Google Cloud モニタリング:Cloud Monitoring は、サービスの健全性とパフォーマンスに関する分析情報を提供しますが、Stackdriver Trace のような詳細なリクエスト フロー トレース機能は提供されない場合があります。
要約すると、GCP 上のマイクロサービスベースのアプリケーションでリクエストフローを効果的に追跡するには、Google Stackdriver Trace を使用して、さまざまなマイクロサービス間の相互作用に関する分析情報を取得し、パフォーマンスの問題を診断する必要があります。
<details><div>

### Q. 問題23: 未回答
あなたは、重要なWebアプリケーションをホストするKubernetesクラスタを管理するDevOpsエンジニアです。アプリケーションのコンポーネントは、Deploymentによって制御されるPodとしてデプロイされます。最近、アプリケーションのコンテナイメージの新しいバージョンがリリースされたため、新しいイメージを使用するようにDeploymentを更新する必要があります。新しいコンテナイメージでデプロイ仕様を更新すると、Kubernetesクラスタはどのように動作しますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
D. 新しいReplicaSetは、新しいPod内で実行される新しいイメージで作成され、古いReplicaSetのPodを徐々に置き換えます。
以下はその説明です。
KubernetesでDeploymentのコンテナイメージを更新する場合、動作は次のようになります。
Kubernetes は、更新されたコンテナー イメージを使用して新しい ReplicaSet を作成します。
新しいReplicaSetは、新しいイメージで新しいPodを作成することで徐々にスケールアップします。
以前のイメージでPodを管理していた古いReplicaSetは引き続き存在し、既存のPodを管理します。
新しいPodの準備が整い、利用可能になると、Kubernetesは古いイメージでPodを徐々に終了させることで、古いReplicaSetをスケールダウンします。
このプロセスにより、新しいPodが完全に動作し、古いPodが置き換えられるまでの間、古いPodと新しいPodが共存するため、更新中にダウンタイムが発生しないことが保証されます。
オプションA、B、およびCは、Deploymentのコンテナイメージを更新する際のKubernetesの動作を正確に記述していません。最も正確な表現は、オプションDで説明されています。
<details><div>

### Q. 問題24: 未回答
あなたは、複数の開発チームによって使用されるKubernetesクラスターを管理するDevOpsエンジニアです。チームはさまざまなワークロードをクラスターにデプロイし、CPU やメモリなどのリソースを公平に分散して、チームがリソースを過剰に消費しないようにする必要があります。この目標を達成するために、各チームに特定のリソース制約を持たせながら、どのKubernetes機能またはコンポーネントを実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
d) リソースクォータ
以下はその説明です。
ResourceQuota を使用します。Kubernetes の ResourceQuota 機能を使用すると、名前空間内のワークロードのリソース消費に関する特定の制約を定義できます。これにより、さまざまな開発チームが CPU とメモリの使用量に制限を設け、1 つのチームが過剰なリソースを消費してクラスター全体のパフォーマンスに影響を与えるのを防ぐことができます。ResourceQuotas は、同じクラスターを共有する複数のチーム間で公平性と効率的なリソース使用率を維持するのに役立ちます。
オプションa) Vertical Pod Autoscalerは、コンテナのリソースリクエストと制限を調整しますが、異なるチームのリソース消費に制約を適用するように特別に設計されていません。
オプション b) ロールベースのアクセス制御 (RBAC) は、リソースへのアクセスとアクセス許可を制御しますが、リソースの消費を直接管理しません。
オプション c) ネットワーク ポリシーでは、ポッド間のネットワーク トラフィックのルールが定義されますが、リソース制約の適用には対応しません。
<details><div>

### Q. 問題25: 未回答
シナリオ：Google Cloud Platform(GCP)内のApacheサーバーでホストされているウェブアプリケーションを管理しています。最適なパフォーマンスを確保し、問題のトラブルシューティングを行うには、アプリケーションのパフォーマンス、リソースの使用状況、およびユーザー操作に関連するメトリックをキャプチャして分析する必要があります。
質問：Google Cloud Platform の Apache サーバー内で実行されているアプリケーションの指標を取得するには、どのような手順が必要ですか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある

説明：
Apache mod_status モジュール:モジュールをApacheサーバーにインストールして構成し、Webインターフェイスを介してサーバーメトリックを公開します。これにより、サーバーのパフォーマンスとアクティビティに関するリアルタイムの洞察が得られます。mod_status
Cloud Monitoring を有効にします。Google Cloud Console で、アプリケーションがホストされている Google Cloud プロジェクトの Cloud Monitoring を有効にします。Cloud Monitoring を使用すると、Apache サーバーをホストしている Compute Engine インスタンスなど、さまざまなサービスから指標を収集して可視化できます。
カスタムメトリクスの作成:Cloud Monitoring で、モジュールによって公開されるデータに基づいてカスタム指標を作成します。これには、アクティブな接続、サーバーの負荷、毎秒の要求数などのメトリックが含まれます。mod_status
オプションB:Google Cloud Profiler は、アプリケーションのパフォーマンスをプロファイリングするために設計されており、Apache 内のサーバー指標をキャプチャするためのものではありません。アプリケーションのボトルネックに関する貴重な洞察を提供できますが、サーバー固有のメトリックに直接対処するものではありません。
オプションC:Cloud Trace は、分散リクエスト トレースとレイテンシ指標をキャプチャしますが、さまざまなサービス間のリクエスト フローの分析に重点を置いています。アプリケーションのパフォーマンスを理解する上では有用ですが、Apacheサーバー内からサーバーレベルのメトリックを直接キャプチャするわけではありません。
オプションD:ログファイルを手動で抽出し、BigQuery を使用して分析するのは、手作業で時間のかかるプロセスです。サーバーのパフォーマンスに関するリアルタイムの洞察は提供されず、最適なサーバー監視に必要なすべてのメトリックをカバーできない場合があります。
要約すると、Google Cloud Platform の Apache サーバー内で実行されているアプリケーションの指標をキャプチャするには、Apache mod_status モジュールをインストールして構成し、Cloud Monitoring を有効にして、モジュールによって公開されるデータに基づいて Google Cloud でカスタム指標を作成する必要があります。この手順の組み合わせにより、サーバーのパフォーマンスとアクティビティに関するリアルタイムの洞察が得られます。mod_status
<details><div>

### Q. 問題26: 未回答
質問：あなたは、複数のクラウド環境とオンプレミスのデータセンター間でシームレスに実行する必要がある複雑なアプリケーションをデプロイする任務を負っています。展開戦略では、一貫性、セキュリティ、および運用効率を確保する必要があります。このシナリオに最も適した展開方法はどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
回答と説明:正解は次のとおりです。
オプションC:Google Cloud のハイブリッドおよびマルチクラウド アプリケーション プラットフォームである Anthos を活用することで、あらゆる環境で一貫したデプロイと運用を実現できます。

説明：Anthos は、さまざまなクラウド プロバイダやオンプレミスのデータセンターなど、さまざまな環境にアプリケーションをデプロイする際の課題に対処するように設計されています。Kubernetes と Istio テクノロジを活用して、アプリケーションの一貫したデプロイ、監視、管理を可能にする統合プラットフォームを提供します。Anthos には、運用の複雑さの軽減、セキュリティとコンプライアンスの基準の維持、ハイブリッドおよびマルチクラウド ランドスケープ全体でアプリケーションを管理するための単一のコントロール プレーンの提供などのメリットがあります。
オプションA:環境ごとに異なるツールや方法を使用してアプリケーションを個別にデプロイすると、不整合、複雑さの増大、運用上の課題が発生し、アプリケーションを効果的に管理および保守することが困難になる可能性があります。
オプションB:単一のクラウドプロバイダーのツールに依存していると、複数のクラウド環境やオンプレミスのデータセンターにアプリケーションを一貫してデプロイする必要性に対応できない可能性があります。また、このアプローチでは、柔軟性やベンダーロックインが制限される可能性もあります。
オプションD:各環境のカスタム スクリプトと構成の開発は、時間がかかり、エラーが発生しやすく、アプリケーションの拡張と進化に伴う保守が困難な場合があります。また、さまざまな環境でアプリケーションを管理するための統合プラットフォームも提供されません。
結論として、Anthos は、一貫性、セキュリティ、運用効率を確保しながら、ハイブリッド環境とマルチクラウド環境にアプリケーションをデプロイするための包括的なソリューションを提供するため、このシナリオで推奨される選択肢です。
<details><div>

### Q. 問題27: 未回答
複数のサービスをホストする Google Cloud Platform(GCP)プロジェクトの管理者である。チームは、分析とモニタリングの目的で Stackdriver ログをエクスポートする必要があります。チームメンバーに、ログのエクスポートを許可するために必要な権限を付与します。チームメンバーにはどのような権限を割り当てる必要がありますか?最も適切なオプションを選択してください。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は A

説明：チームメンバーが Stackdriver ログをエクスポートできるようにするには、そのメンバーに役割を割り当てる必要があります。このロールは、ログを Logging に書き込むために必要なアクセス許可を付与します。このロールを使用すると、チーム メンバーは分析と監視の目的でログをエクスポートできます。これは、ログをエクスポートするための最も適切な権限です。roles/logging.logWriter
オプション B: roles/logging.viewer

説明：このロールは、ユーザーにログの表示を許可しますが、ログのエクスポートに必要な権限は付与しません。このロールは、ログをエクスポートせずに表示するために読み取り専用アクセスを必要とするユーザーに適しています。roles/logging.viewer
オプション C: roles/stackdriver.resourceMetadata.writer

説明：このロールは、Stackdriver ログをエクスポートするために必要な権限を付与しません。この役割は、リソース メタデータの書き込みに重点が置かれており、ログのエクスポートについては説明しません。roles/stackdriver.resourceMetadata.writer
オプション D: roles/monitoring.viewer

説明：この役割は、モニタリング データを表示する権限をユーザーに付与しますが、Stackdriver ログをエクスポートするために必要な権限は付与されません。このロールは、ログのエクスポートではなく、監視データへの読み取り専用アクセスを必要とするユーザーに適しています。roles/monitoring.viewer
要約すると、分析とモニタリングの目的で Stackdriver ログをエクスポートする権限をチーム メンバーに付与するには、その役割を割り当てる必要があります。このロールは、ログのエクスポートに適切なアクセス許可を提供します。roles/logging.logWriter
<details><div>

### Q. 問題28: 未回答
組織は、アプリケーション開発にコンテナーベースのワークフローを採用し、自動化されたビルド パイプラインを介して運用環境の Kubernetes クラスターにアプリケーションをデプロイしています。セキュリティは最大の関心事であり、承認されテストされたコード変更のみが運用環境にプッシュされるようにする必要があります。セキュリティチームは、クラスターにデプロイされたコンテナイメージの整合性を確保することに特に関心を持っています。
これらの懸念に対処するために何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
答え： D

説明：
このシナリオでは、組織は Kubernetes クラスターにデプロイされたコンテナー イメージの整合性を懸念しています。バイナリ認証は、デプロイされたイメージが信頼できるソースからのものであり、クラスターに許可される前に特定の基準を満たしていることを確認するのに役立つ Kubernetes の機能です。選択したオプションが懸念事項にどのように対処するかは次のとおりです。
バイナリ認証の有効化: Kubernetesクラスタ内でバイナリ認証を有効にすることで、デプロイ前にイメージにポリシーを適用するセキュリティメカニズムを確立します。これにより、未承認のイメージやテストされていないイメージがクラスターで使用されるのを防ぎ、セキュリティ上の懸念に対処できます。
ビルド パイプラインをアテスターとして構成する: バイナリ承認のコンテキストでは、アテスターは、ビルド パイプラインによって生成されたコンテナー イメージの整合性を証明するロールです。ビルド パイプラインを構成証明者として構成することで、ビルド プロセスとデプロイ プロセスの間にリンクを確立します。これにより、ビルド パイプラインによって承認およびサインオフされたイメージのみをデプロイできます。
この手順の組み合わせにより、イメージの整合性に関する組織の懸念に直接対処し、信頼できるテスト済みのイメージのみが運用 Kubernetes クラスターにデプロイされるようにします。
<details><div>

### Q. 問題29: 未回答
あなたは、Google Compute Engine(GCE)インスタンスで実行される複雑なマイクロサービスベースのアプリケーションを管理するDevOpsチームの一員です。アプリケーションには、ユーザーの要求を満たすために連携する複数の相互接続されたサービスがあります。最近、リクエストフロー全体でボトルネックが発生し、遅延が発生し、ユーザーエクスペリエンスに影響を与えていることに気付きました。
勤勉なチーム メンバーとして、マイクロサービス アーキテクチャ内のどの特定のサービスが要求フローのボトルネックを引き起こしている可能性があるかを特定する任務を負います。速度低下の原因を特定するために、異なるサービス間の要求フローを追跡および分析できるソリューションを見つける必要があります。
望ましい結果を得るには、どのアプローチを取るべきですか?
1. 
2. 
3. 
4. 
<details><div>
答え： D

説明：
マイクロサービスベースのアーキテクチャでは、サービスの分散性により、要求フローのボトルネックの特定が複雑になる場合があります。Google Cloud Trace は、個々のサービスのパフォーマンスとその相互作用に関する分析情報を得ることができる強力なツールであり、このシナリオに最適なアプローチです。
リクエストフローのトレース: Google Cloud Trace は、リクエストがさまざまなマイクロサービスを通過する際のリクエストのレイテンシと実行時間に関する詳細な分析情報を提供します。サービス間で要求のパスをトレースし、遅延が発生している可能性のある場所を特定するのに役立ちます。
依存関係の可視化: Google Cloud Trace を使用すると、マイクロサービス間の依存関係と相互作用を可視化できます。これは、要求フローのボトルネックの原因となっている可能性のある特定のサービスを特定するのに役立ちます。
レイテンシ分析: Google Cloud Trace はレイテンシ分析を提供し、リクエストフローのどの部分でレイテンシが高くなっているかを強調表示します。この情報により、調査と最適化の取り組みをより効果的に行うことができます。
他のオプション (A、B、C) は、パフォーマンスの最適化に貢献する可能性がありますが、ボトルネックを特定するためにマイクロサービス間の相互作用をトレースして分析する必要性に直接対処するものではありません。詳細なログ記録 (オプション A) の実装は時間がかかり複雑になる可能性があり、ネットワーク構成の変更 (オプション B) ではサービス レベルの問題に直接対処できない可能性があり、根本原因がサービス間の相互作用にある場合は追加のインスタンスのデプロイ (オプション C) が役に立たない可能性があります。
結論として、Google Cloud Traceを活用することで、リクエストフローとマイクロサービス間の相互作用を可視化し、ボトルネックの原因となっている特定のサービスを特定し、ターゲットを絞った最適化アクションを実行できます。
<details><div>

### Q. 問題30: 未回答
組織で使用される重要な内部アプリケーションのデプロイを管理する責任があります。スケジュールされた週末のメンテナンス期間中に、アプリケーションの新しいリリースをデプロイし、新機能と改善点の導入を目指します。残念ながら、デプロイ後に、新機能の 1 つが運用環境で予期しない問題を引き起こしていることに気付きました。アプリケーションの停止が長引くと、組織の運用に影響が出ます。この状況に対処し、今後同様のインシデントが発生しないようにするために、リリース プロセスを調整することにしました。リリースプロセスを改善し、平均復旧時間(MTTR)を短縮するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： B

説明：
カナリア リリース戦略を実装するには、新機能や変更を "カナリア グループ" と呼ばれるユーザーの小さなサブセットにデプロイする必要があります。これにより、変更をユーザー ベース全体にロールアウトする前に、制御された環境での変更の影響を監視できます。このアプローチは、デプロイ プロセスの早い段階で問題、バグ、またはパフォーマンスのボトルネックを特定するのに役立ち、問題が発生した場合の平均復旧時間 (MTTR) を短縮します。
新機能を少数のユーザー グループに徐々に公開することで、ユーザーベース全体に影響を与えることなく、アプリケーションの動作を綿密に監視し、フィードバックを収集し、予期しない問題を検出できます。問題が発生した場合は、変更がすべてのユーザーにロールアウトされる前に迅速に対処できます。このプラクティスは、サイト信頼性エンジニアリング (SRE) の原則に沿っており、アプリケーションの全体的な信頼性の向上に役立ちます。
提供されている他のオプションは、MTTRを短縮し、リリースプロセスを改善するという目標に対処するのにそれほど効果的ではありません。
B. オフピーク時に定期的なメンテナンス期間をスケジュールします。オフピーク時にメンテナンスをスケジュールすると、展開がユーザーに与える影響を軽減できますが、ユーザー ベース全体に影響を与える前に問題を特定して解決するという問題に直接対処することはできません。
C. デプロイ スクリプトの複雑さを増します。デプロイ スクリプトの複雑さが増すと、問題が増え、トラブルシューティングが困難になる可能性があります。デプロイ スクリプトをシンプルに保ち、十分にテストすることが重要です。
D. チームメンバーに責任を負わせる:問題をチームメンバーのせいにすることは逆効果であり、リリースプロセスの改善には貢献しません。継続的な改善とコラボレーションに重点を置く必要があります。
カナリア リリース戦略の実装は、問題を早期に発見し、ユーザーへの影響を最小限に抑え、よりスムーズなデプロイ プロセスを確保できるプロアクティブなアプローチです。
<details><div>

### Q. 問題31: 未回答
シナリオ：
あなたは、クラウドネイティブ アプリケーションを開発する最先端のソフトウェア会社の DevOps チームを率いています。組織では、コンテナオーケストレーションにKubernetesを採用し、推奨される継続的デプロイ(CD)ツールとしてSpinnakerを採用しています。アプリケーションのコンテナ イメージは、Google Container Registry(GCR)に保存されます。チームは、GCR 内のコンテナ イメージに変更があるたびにトリガーされる Spinnaker を使用して、自動デプロイ プロセスを確立したいと考えています。
質問：
Google Container Registry(GCR)内に保存されているコンテナイメージに変更があるたびにSpinnakerを使用して自動デプロイを実現するには、どのアプローチを採用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：ある

説明：
Spinnaker でトリガー パイプラインを作成することで、GCR イメージ リポジトリの変更に対応する合理化された自動化されたプロセスを確立します。このアプローチは、既存の CD ツールである Spinnaker とシームレスに統合され、新しいイメージ バージョンが検出されるたびに展開プロセスを自動的に管理できるようになります。
Google Container Registry(GCR)内に保存されているコンテナイメージの変更に応答するトリガーパイプラインをSpinnakerで設定するには、次の手順に従います。
Spinnakerへのアクセス:Spinnaker インスタンスの Web インターフェイスにログインします。
[パイプライン] に移動します。Spinnakerの「Pipelines」セクションを見つけてナビゲートします。ここで、トリガー パイプラインを定義して構成します。
新しいパイプラインを作成します。オプションをクリックして、新しいパイプラインを作成します。パイプラインに名前を付け、その他の設定を指定するように求められます。
トリガー・ステージを追加します。
パイプライン エディターで、新しいステージを追加するオプションを見つけます。
「トリガー」ステージタイプを選択します。ここで、GCR の変更を監視するトリガーを設定します。
トリガーを設定します。
トリガーの種類として [Google Container Registry] を選択します。
サービスアカウントの認証情報やその他の認証メカニズムなどの認証情報を提供して、Spinnaker が GCR にアクセスできるようにします。
GCR リポジトリと、トリガーをアクティブ化するタイミングを定義するフィルタまたは条件を指定します。たとえば、ブランチ、タグ、またはその他の条件を指定できます。
展開ステージを定義します。
トリガーを設定したら、デプロイメント・パイプラインの後続のステージを構成します。これらのステージは、GCR リポジトリの変更によってトリガーがアクティブ化されたときに実行されます。
デプロイメントの構成:
デプロイ ステージ内で、更新されたイメージを Spinnaker が Kubernetes クラスタにデプロイする方法を定義します。これには、ターゲットの名前空間、展開戦略、ロールアウト オプション、および追加設定の指定が含まれる場合があります。
保存して検証します。トリガーと後続のデプロイ ステージを構成したら、パイプライン構成を保存します。一部のSpinnakerインスタンスは、設定が正確であることを確認するための検証チェックを提供します。
パイプラインをアクティブ化します。パイプラインがアクティブで、アプリケーションとターゲットの Kubernetes クラスターに適切にリンクされていることを確認します。
トリガーをテストします。トリガーで設定した条件に一致する新しいイメージ バージョンを GCR リポジトリにプッシュします。トリガーがアクティブになり、Spinnaker が定義された展開プロセスを自動的に開始します。
監視と改善:初期設定とテストの後、トリガー パイプラインの動作を監視します。必要に応じて調整を行い、精度、効率、信頼性を向上させます。
正確な手順は、使用しているSpinnakerのバージョンと、環境に実装した特定の統合またはカスタマイズによって異なる場合があることに注意してください。最も正確で最新のガイダンスについては、常にSpinnakerのドキュメントを参照してください。
<details><div>

### Q. 問題32: 未回答
シナリオ：
あなたは、コンテナ化されたマイクロサービスを開発およびデプロイする急成長中の技術系スタートアップのDevOpsリーダーです。コンテナー イメージはプライベート コンテナー レジストリに格納され、強力なセキュリティ プラクティスを維持することをお約束します。セキュリティ戦略の一環として、脆弱性スキャンツールを実装して、デプロイ前にコンテナイメージに既知の脆弱性がないことを確認する必要があります。
質問：
スタートアップのコンテナ化されたマイクロサービスのコンテキストでは、デプロイ前にコンテナイメージの既知の脆弱性を特定して対処するのに最も適した脆弱性スキャンツールはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
このシナリオでは、正解は次のとおりです。
A. Clair: Clair は、既知の脆弱性のコンテナー イメージのスキャンを専門とするオープンソースの脆弱性スキャナーです。コンテナ化された環境でうまく機能するように設計されており、コンテナイメージのソフトウェアコンポーネントで見つかった脆弱性に関する詳細なレポートを提供します。これにより、デプロイ前にスタートアップのコンテナ化されたマイクロサービスの既知の脆弱性を特定して対処するための最適なオプションになります。
<details><div>

### Q. 問題33: 未回答
あなたは、毎日何千人ものユーザーにサービスを提供するクラウドベースの分析プラットフォームのDevOpsリーダーです。詳細な分析を行い、アプリケーションのログから分析情報を得るために、Stackdriver ログを BigQuery にエクスポートすることにしました。これを実現するにはどのような手順を踏むべきか、また、BigQuery でこれらのログへのアクセス権を付与するには、どのロールを割り当てるべきか。
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
A. Stackdriver ログを BigQuery にエクスポートする:
Stackdriver エクスポート シンクを構成して、目的のログを BigQuery にエクスポートします。
BigQuery でログの種類、フィルタ、宛先データセットを指定します。
BigQuery でのアクセスのロール:
BigQuery のログにアクセスする必要がある個人に BigQuery データ閲覧者ロールを割り当てます。このロールは、データセットへの読み取り専用アクセスを提供します。
以下はその説明です。
Stackdriver ログを BigQuery にエクスポートする:正しいアプローチは、Stackdriver エクスポート シンクを使用して、目的のログを BigQuery に直接エクスポートすることです。これにより、不要な中間ステップが回避され、ログが分析のために BigQuery に直接取り込まれるようになります。
BigQuery でのアクセスのロール:BigQuery データ閲覧者ロールは、BigQuery のログにアクセスする必要があるユーザーに最適です。このロールは、データセットへの読み取り専用アクセス権を付与し、ユーザーがデータを変更または編集することなく、ログを表示してクエリを実行できるようにします。
オプション A は、Stackdriver ログを BigQuery に直接的かつ効率的にエクスポートする方法を概説すると同時に、ログを分析する必要があるユーザーに適切なレベルのアクセス権を割り当てるため、正しい選択です。その他のオプションには、中間ストレージや追加のコネクタなど、不必要な複雑さが伴い、割り当てられたロールは、BigQuery のログへの安全で制御されたアクセスのベスト プラクティスと一致しません。
<details><div>

### Q. 問題34: 未回答
あなたは、Google Cloud Platform でグローバルに分散された大規模なウェブ アプリケーションを管理するチームを率いています。このアプリケーションは、何百万人ものユーザーに同時にサービスを提供しており、高可用性が重要です。アプリケーション アーキテクチャでは、ネットワーク ロード バランサー (NLB) の背後で実行されている複数のリージョンとインスタンスを利用して、トラフィックを効率的に分散します。シームレスなユーザーエクスペリエンスとインスタンス障害からの迅速な復旧を実現するには、高度な自動修復戦略を実装する必要があります。このレベルの自動修復を実現するには、どのような手順を実行しますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B.カスタムヘルスチェックを利用します。
アプリケーションレベルとシステムレベルの両方のメトリクスを組み込んだカスタムヘルスチェックを作成します。
複雑なヘルスチェックの間隔とタイムアウトを設定して、一時的な問題をキャプチャします。
カスタムヘルスチェックの結果に基づいて自動修復するようにインスタンスグループを設定します。
C. プロアクティブなインスタンスのリバランスを実装します。
モニタリングツールとメトリクスツールを使用して、インスタンスの使用率を定期的に分析します。
インスタンスグループ内でインスタンスのリバランスを自動的にトリガーして、負荷を均等に分散します。
予測自動スケーリングを実装して、トラフィックの急増を予測し、インスタンスの過負荷を防ぎます。
D. カスタムAuto Scalingポリシーを利用します。
リクエストレイテンシーやエラー率などの詳細なメトリクスに基づいてカスタムAuto Scalingポリシーを定義します。
定義されたポリシーに基づいてサイズを動的に調整するようにインスタンスグループを設定します。
予測分析を利用して、トラフィックパターンに基づいてインスタンスをプロアクティブに追加または削除します。
以下はその説明です。
カスタムヘルスチェックの利用:アプリケーションレベルとシステムレベルのメトリクスを組み合わせたカスタムヘルスチェックを作成すると、インスタンスのヘルスを包括的に評価できます。複雑なヘルスチェックの間隔とタイムアウトを設定することで、一時的な問題をキャプチャし、誤検知/陰性を回避できます。カスタムヘルスチェックに基づいて自動修復を設定すると、異常なインスタンスの正確な検出と置換が保証されます。
プロアクティブなインスタンスのリバランスを実装します。インスタンスの使用率を定期的にモニタリングし、プロアクティブなリバランスを実装することで、インスタンスグループ内の負荷の均等な分散を維持できます。この戦略により、インスタンスが過負荷になるのを防ぎ、安定したアプリケーションパフォーマンスに貢献します。
カスタム Auto Scaling ポリシーを利用します。リクエストレイテンシーやエラー率などの特定のメトリクスを考慮するカスタム自動スケーリングポリシーを作成することで、スケーリングの決定がアプリケーションのパフォーマンス目標に合致するようになります。これらのポリシーに基づいてサイズを動的に調整するようにインスタンスグループを設定すると、トラフィックの変動にプロアクティブに対応できます。
オプション A (マルチリージョンインスタンスグループの実装) には、オプション B、C、および D に記載されている詳細な高度な自動修復戦略がありません。後者のオプションは、グローバルに分散された Web アプリケーションのシナリオで高可用性、迅速な復旧、および最適化されたパフォーマンスを確保するための、より具体的で高度なアプローチを提供します。
<details><div>

### Q. 問題35: 未回答
あなたは DevOps エンジニアで、Google Kubernetes Engine(GKE)クラスタにデプロイされたマイクロサービス アプリケーションを管理しています。アプリケーションはミッションクリティカルであり、個々のコンテナが応答し、正しく機能していることを確認するために継続的な監視が必要です。アプリケーションのクラッシュやハングアップによって応答しなくなったインスタンスを自動的に検出して回復するメカニズムが必要です。この要件を効果的に達成するには、どのようなアプローチを実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
B. Liveness Probe の設定:
マイクロサービス内のコンテナーごとに liveness probe を定義します。
Liveness Probeはコンテナを継続的に監視し、応答しなくなった場合は自動的に再起動します。
以下はその説明です。
Liveness Probe の設定:Liveness Probe は、コンテナーが応答し、正しく機能するように設計されています。Liveness Probe は、コンテナーに定期的に要求を送信し、その応答を分析することで、コンテナーが正常か応答しないかを判断できます。コンテナがliveness probeに失敗した場合、Kubernetesはコンテナを自動的に再起動し、その機能の復元を試みます。これにより、liveness probe は、アプリケーションのクラッシュやハングが原因で応答しなくなったインスタンスを検出して回復するという要件に対処するのに適した選択肢になります。
オプションA(ポッドの水平自動スケーリングの実装)は、CPU使用率などのメトリックに基づくスケーリングに焦点を当てていますが、応答しないコンテナの検出とリカバリには特に対応していません。
オプション C (Readiness Probe の実行) では、新しいインスタンスがロードバランサーに追加される前に動作していることが保証されますが、応答しないコンテナーの監視と回復には重点が置かれていません。
オプションD(StatefulSetの利用)は、安定した順序付けされたデプロイとスケーリングを維持するのに役立ちますが、応答しないコンテナを検出して回復するという要件には直接関係しません。
<details><div>

### Q. 問題36: 未回答
あなたは、Kubernetesでホストされている複雑なeコマースプラットフォームの管理を担当するチームを率いています。アプリケーションではトラフィックが多く、継続的な可用性が要求されます。チームは、アプリケーションのマイクロサービスへの変更を必要とする新しい機能更新プログラムをデプロイする準備をしています。中断を最小限に抑え、インフラストラクチャの複雑さを回避するには、ダウンタイムを短縮してスムーズな更新を可能にするデプロイ アプローチを採用する必要があります。このシナリオでは、どの展開戦略を優先する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
b) ローリングアップデート
以下はその説明です。
ローリングアップデート:ローリングアップデートのデプロイ戦略では、アプリケーションのコンテナの新しいバージョンは徐々にデプロイされ、古いコンテナは正常にフェーズアウトされます。このアプローチは、一定数のインスタンスが常に実行されていることを確認し、ダウンタイムを短縮することで、アプリケーションの可用性を維持するのに役立ちます。また、完全に個別の環境をプロビジョニングする必要がないため、インフラストラクチャのニーズへの影響も最小限に抑えられます。
オプション a) 再作成では、既存のインスタンスをすべて終了し、新しいインスタンスに置き換えるため、ダウンタイムが発生し、インフラストラクチャのニーズが増加する可能性があります。
オプション c) カナリア デプロイでは、ユーザーのサブセットに新しいバージョンが徐々に導入され、テストには役立ちますが、ダウンタイムやインフラストラクチャのニーズが必ずしも最小限に抑えられるとは限りません。
オプション d) ブルー/グリーン デプロイでは、2 つの個別の環境 (ブルーとグリーン) を維持し、それらの間でトラフィックを切り替える必要があります。ダウンタイムを最小限に抑えることができますが、両方の環境を維持するには追加のインフラストラクチャ リソースが必要です。
説明されているシナリオのコンテキストでは、ローリング アップデートは、中断とインフラストラクチャの複雑さを最小限に抑えてスムーズな更新を実現するための最も適切な戦略です。
<details><div>

### Q. 問題37: 未回答
組織内のさまざまなサービスのパフォーマンス メトリックを収集して分析する監視システムのアクセス許可を管理しています。監視データをワークスペースに書き込み、メトリックが分析用に適切に記録されるようにする責任を持つチームがあります。ただし、アクセスをデータの書き込みのみに制限し、コンソールからメトリクスを表示できないようにします。この特定のレベルのアクセスを実現するには、このチームにどのロールを割り当てる必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
b) メトリックライターの監視
以下はその説明です。
メトリック・ライターの監視:このロールを使用すると、ユーザーは監視データをワークスペースに書き込むことができ、分析および監視の目的でメトリック データを送信できます。ただし、このロールは、コンソールまたはインターフェイスを介して監視データを表示する権限をユーザーに付与しません。これは、書き込みは許可するが表示は許可しないというシナリオの要件に沿ったきめ細かなレベルのアクセスを提供します。
オプション a) 監視ビューアー、c) 監視管理者、d) 監視エディターは、監視データの表示、管理、または編集に関連するアクセス許可を付与しますが、これはシナリオで説明されている特定のアクセス要件と一致しません。
<details><div>

### Q. 問題38: 未回答
Google App Engine にデプロイされた複雑なマイクロサービス ベースのアプリケーションのパフォーマンスと信頼性を監督する責任があります。アプリケーションのアーキテクチャは、相互に対話するさまざまなサービスで構成されています。応答時間の追跡、異常の検出、サービスの依存関係の視覚化など、包括的な監視ソリューションを確立する必要があります。これを実現するための多段階の性質を考慮すると、Google App Engine 内で実行されているアプリケーションに対してこのような高度なモニタリング戦略を実装するために、Google Cloud サービスとツールのどの組み合わせを使用しますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は次のとおりです。
a)Google Cloud Logging でカスタムログエントリを構成して詳細な応答時間をキャプチャし、異常検出のためのアラート ポリシーを Google Cloud Monitoring で設定し、Google Cloud Trace を使用してサービスの依存関係を視覚化します。
各ステップの説明は次のとおりです。
Google Cloud Logging でカスタムログエントリを設定して、詳細な応答時間をキャプチャします。Google Cloud Logging でログエントリをカスタマイズすることで、マイクロサービスからの応答時間やその他の関連指標に関する詳細情報を含めることができます。これにより、監視するデータをきめ細かく制御できます。
異常検出のために Google Cloud Monitoring でアラート ポリシーを設定します。Google Cloud Monitoring では、カスタム指標、ログ、その他のモニタリング データに基づいてアラート ポリシーを定義できます。アラートポリシーを設定することで、応答時間やその他の主要なメトリックの異常をプロアクティブに検出し、しきい値を超えたときに通知を受け取ることができます。
Google Cloud Trace を使用して、サービスの依存関係を視覚化します。Google Cloud Trace には分散トレース機能が用意されており、リクエストがアプリケーション内のさまざまなマイクロサービスをどのように流れるかを視覚化できます。これにより、サービスの依存関係を理解し、ボトルネックを特定し、パフォーマンスを最適化できます。
オプション b は Google Cloud Error Reporting で、レスポンス時間やサービスの依存関係ではなく、主にアプリケーション エラーの追跡と分析に重点を置いています。
オプション c では、Google Cloud Logging を使用してサービスの依存関係を可視化しますが、これは Cloud Logging の本来のユースケースではありません。さらに、詳細な応答時間の追跡の必要性には対応していません。
オプション d は、応答時間の追跡に Google Cloud エラー報告を使用するなど、主に設計されていない目的でさまざまなサービスを混在させます。
オプション a は、Google App Engine にデプロイされた複雑なマイクロサービスベースのアプリケーションの高度な監視に対する包括的で適切に構造化されたアプローチを提供します。
<details><div>

### Q. 問題39: 未回答
Cloud Build と別の継続的デプロイ(CD)ツールをシームレスに統合して、DevOps ワークフローを合理化します。CD ツールは、Cloud Build がデプロイ用の新しいアーティファクトを生成するたびに、速やかに通知される必要があります。この目標を効果的に達成するには、どのアプローチをお勧めしますか?
1. 
2. 
3. 
4. 
<details><div>
答え：C

説明：
Cloud Build は Webhook ペイロードを送信します。Cloud Build は、継続的デプロイ(CD)ツールによって提供される特定のエンドポイントに Webhook ペイロードを送信するように設定できます。このペイロードには、ビルドされた新しいアーティファクトに関する情報が含まれています。その後、CD ツールのエンドポイントは、このペイロードを処理し、デプロイ プロセスを開始できます。
オプション a) リレーショナル データベースを使用すると、Webhook などのリアルタイム通信方法と比較して、不必要な複雑さと待機時間が発生する可能性があります。
メール通知をトリガーするオプション b) では、Cloud Build と CD ツールの統合に必要な即時かつ自動化された通信が提供されない場合があります。
クラウドストレージバケットに新しいファイルを生成し、新しいファイルを監視するオプションd)も、直接Webhookアプローチと比較して、追加の手順とレイテンシが発生する可能性があります。
Cloud Build と CD ツールをシームレスかつリアルタイムに統合する場合、オプション c は、効率的なコミュニケーションと、デプロイ用に新しく生成されたアーティファクトに関するタイムリーな通知のために推奨されるアプローチです。
<details><div>

### Q. 問題40: 未回答
バイナリ認証を有効にして GKE クラスタを設定し、コンテナ イメージのセキュリティを確保しました。アプリケーションのデプロイ中に、「アテスターによって拒否されました」というエラーメッセージが表示されます。このエラーは、イメージが構成証明者によって定義されたポリシーを満たしていないことを示します。この問題に効果的に対処するには、次に何をすべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：D

説明：
構成証明を作成し、バイナリ承認に送信します。バイナリ認証が有効になっている GKE クラスタで「Denied by Attestor」エラーが発生した場合は、コンテナ イメージがアテスターによって設定された定義されたポリシー要件を満たしていないことを意味します。構成証明は、イメージの整合性を確認する署名付きステートメントです。この問題を解決するには、コンテナー イメージの構成証明を作成し、適切な秘密キーで署名してから、署名された構成証明をバイナリ承認に送信する必要があります。このアクションにより、イメージがポリシーに準拠し、展開できることを確認します。
バイナリ認証を無効にするオプションa)は、セキュリティチェックをバイパスし、コアの問題に対処しないため、推奨されません。
新しいセキュリティ証明書を生成するオプション b) では、構成証明とイメージの整合性の問題に直接対処できません。
除外を作成するオプション c) は、確立されたポリシーをバイパスすることでセキュリティを侵害する可能性があります。
バイナリ認証の GKE クラスタで「Denied by Attestor」エラーを解決するコンテキストでは、イメージがポリシー要件を満たし、正常にデプロイできるようにするための正しいアクションがオプション d です。
<details><div>

### Q. 問題41: 未回答
相互接続された複数のアプリケーションをホストする複雑なKubernetes環境を監督しています。これらのアプリケーションのうちの 2 つは "Alpha" と "Beta" という名前で、シームレスなデータ共有のために相互に通信する必要があります。ただし、セキュリティを強化するために、これらのアプリケーションと外部との間の厳密な境界を維持する必要があります。外部アクセスを制限したままこの通信を有効にするには、サービスを作成することにしました。どのサービスタイプを選ぶべきですか?
1. 
2. 
3. 
4. 
<details><div>
答え： C

説明：
クラスタ IP:このサービスの種類は、クラスター内の内部 IP でアプリケーションを公開します。これにより、アプリケーションは外部トラフィックから分離されたまま、相互に通信できます。
Ingress のオプション a) は、外部トラフィックをクラスター内のサービスにルーティングするために使用されますが、このシナリオでは必要ありません。
NodePort のオプション b) は、各ノードのポートでサービスを公開し、外部アクセスを許可する可能性がありますが、これは厳密な境界を維持するという要件に反します。
LoadBalancer のオプション d) は、一連のポッドに外部トラフィックを分散しますが、これは内部通信と制限された外部アクセスの必要性と一致しません。
外部アクセスを制限しながら安全な内部通信を可能にするコンテキストでは、ClusterIP のオプション c) が適切なサービスの種類です。
<details><div>

### Q. 問題42: 未回答
A 社は、サイト信頼性エンジニアリング (SRE) のプラクティスに従って、エラー バジェットのバーン レートに基づいて効果的なアラート ポリシーを確立したいと考えています。目標は、サービスの低下が許容範囲内にとどまるようにすることです。SRE の原則に沿ったアラート ポリシーを設定するには、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： D

説明：
サイト信頼性エンジニアリング (SRE) プラクティスのコンテキストでは、エラー バジェットとサービスの低下を効果的に管理するには、アラート ポリシーに対するバランスの取れたアプローチが必要です。オプション d は、SRE の原則に沿った、ルックバック期間の短縮と長期化の両方を検討することを提案しています。
ルックバック期間の短縮:この側面は、短期的にエラーバジェットの消費に影響を与える可能性のある即時の異常や逸脱を検出するのに役立ちます。
より長いルックバック期間:より長い時間枠を考慮することで、組織は時間の経過に伴うエラー予算消費の傾向とパターンを把握し、意思決定のためのより広範なコンテキストを提供できます。
オプションaとcは、固定されたルックバック期間またはより長いルックバック期間のみに焦点を当てることを提案しており、短期的な異常を見落としたり、即時の変化に対する反応が低下したりする可能性があります。
オプション b では、短期的な異常に対して 1 つのルックバック期間が強調されますが、効果的なアラート戦略では、長期的な傾向も考慮する必要があります。
エラーバジェット管理とSREプラクティスとの整合性のコンテキストでは、サービスのパフォーマンスとエラーバジェット使用率を包括的に把握するために、ルックバック期間の短縮と延長の両方を考慮するオプションdが最も適切なアプローチです。
<details><div>

### Q. 問題43: 未回答
ある企業で、仮想プライベートクラウド(VPC)環境内で断続的な接続の問題が発生しています。これらの問題の原因を特定し、サブネット レベルでネットワーク トラフィック パターンを分析して、潜在的なボトルネックや異常を特定したいと考えています。これを実現するには、VPC内のネットワークトラフィックに関する詳細情報をキャプチャするソリューションが必要です。
次のオプションのうち、会社が目標を達成するのに最も適しているのはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え： C

説明：
このシナリオでは、仮想プライベートクラウド(VPC)内のサブネットレベルでネットワークトラフィックパターンを分析し、潜在的な接続の問題を特定することを目標としています。フローログは、この目的を達成するための最も適したオプションです。
フローログ:フローログは、サブネットレベルでネットワークトラフィックに関する詳細なメタデータを提供し、送信元と宛先のIPアドレス、送信元と宛先のポート、プロトコル、転送されたバイト数などの情報をキャプチャします。この情報は、ネットワーク トラフィック パターンの分析、ボトルネックの特定、および潜在的な接続の問題の診断に不可欠です。
データアクセスログを有効にするオプションa)は、GCPサービスへのユーザーアクセスを追跡するために使用され、サブネットレベルのネットワークトラフィック情報は提供されません。
管理ログを有効にするオプションb)は、GCPプロジェクト内の管理アクションの追跡に重点が置かれており、ネットワークトラフィックを分析するようには設計されていません。
ファイアウォールログを有効にするオプションd)は、ファイアウォールルールによって許可または拒否されたトラフィックを追跡するためのものであり、ネットワークトラフィックパターンを分析する主な目的ではありません。
サブネットレベルのネットワーク トラフィックを分析して接続の問題を特定するという会社の目的には、フロー ログを有効にするオプション c) が最も適切な選択です。
<details><div>

### Q. 問題44: 未回答
ある会社は、顧客にオンラインストリーミングサービスを提供しています。彼らは、信頼性の高いサービスを提供し、サービスレベル指標(SLI)を一貫して満たすことを望んでいます。ストリーミングサービスへのリクエストの成功と失敗に関するデータを収集します。この会社は、成功した要求の SLI を計算したいと考えています。
成功した要求のサービス レベル インジケーター (SLI) を計算するための会社にとって適切な方法は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある

説明：
このシナリオでは、オンラインストリーミングサービスの信頼性の尺度である、成功したリクエストのサービスレベル指標(SLI)を計算したいと考えています。成功した要求の SLI を計算する適切な方法は、成功した要求の数を要求の総数で割ることです。
SLI = (成功した要求の数) / (要求の総数)
このメソッドは、サービスに対して行われた要求の合計数のうち、正常に処理された要求の割合を表すパーセンテージ値を提供します。
要求の総数に対する失敗した要求の比率を計算するオプション b) では、失敗した要求の SLI が得られますが、これはこのシナリオでは望ましいメトリックではありません。
成功した要求に対する失敗した要求の比率を計算するオプション c) では、サービスの信頼性を包括的に把握することはできません。
オプションd)は、サービスの合計期間に対する成功したリクエストの比率を計算することは、SLIを計算するための標準的なアプローチではありません。
したがって、このシナリオでは、オプション a) が、成功した要求の SLI を正確に計算するための適切な方法です。
<details><div>

### Q. 問題45: 未回答
あるソフトウェア開発チームは、Google Cloud サービスを使用して、最新の継続的インテグレーション / 継続的デプロイ(CI / CD)ワークフローに移行しています。パイプラインを設定するためのベストプラクティスに従っていることを確認したいと考えています。チームは、アプリケーションの構築、テスト、デプロイのプロセスを合理化することを目指しています。
CI / CD パイプラインに Google Cloud サービスを実装するための推奨順序は何ですか?
1. 
2. 
3. 
4. 
<details><div>
答え： C

説明：
Google Cloud サービスを使用して継続的インテグレーション / 継続的デプロイ(CI/CD)パイプラインを実装するための推奨順序は次のとおりです。
クラウドソースリポジトリ:これは、ソースコードを保存する場所です。バージョン管理の使用は、変更を管理し、開発チームで共同作業を行うために不可欠です。
Cloud Build:このサービスでは、アプリケーションのビルドとテストのプロセスを自動化します。リポジトリからソースコードを取得し、アプリケーションをビルドし、テストを実行し、コンテナイメージなどの成果物を生成します。
コンテナレジストリ:コンテナ・レジストリは、コンテナ・イメージを格納および管理するための安全な方法を提供します。Cloud Buildで生成したコンテナイメージは、Container Registryに格納できます。
GKE(Google Kubernetes Engine):GKE は Google のマネージド Kubernetes サービスです。アプリケーションをビルドしてテストした後、それをKubernetesクラスターにデプロイすることは、オーケストレーションとスケーリングの一般的な方法です。
オプション a)、b)、d) には、Google Cloud サービスを使用して CI / CD パイプラインを設定する一般的なフローと一致しないサービスの順序が異なります。
したがって、オプション c)は、Cloud Source Repositories、Cloud Build、Container Registry、GKE を使用して CI / CD パイプラインを確立するためのベスト プラクティスに沿った推奨順序です。
<details><div>

### Q. 問題46: 未回答
ある企業は、インスタンスごとに一意のネットワーク ID と安定したストレージを必要とするステートフル アプリケーションのデプロイを計画しています。アプリケーションは、重要なデータを格納する分散データベースであり、データの一貫性と可用性を確保する必要があります。さらに、アプリケーション インスタンスは個別にスケーラブルであり、サービスを中断することなくローリング アップデートをサポートする必要があります。
上記の要件を満たしながら、このステートフルなアプリケーションをデプロイするのに最も適したKubernetesリソースはどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え： B

説明：
StatefulSetは、インスタンスごとに一意のネットワークIDと安定したストレージを必要とするステートフルアプリケーションをデプロイするのに最も適した選択肢です。StatefulSetは、以下の機能を提供します。
一意のネットワーク ID:StatefulSetによって作成された各インスタンスは、安定したホスト名とネットワークIDを持つため、インスタンス間で一貫した通信と検出を必要とするアプリケーションに適しています。
安定したストレージ:StatefulSetを使用すると、インスタンスごとに永続ボリュームを指定できるため、ポッドが再スケジュールまたは再作成された場合でもデータが保持されます。
データの一貫性と可用性:StatefulSetは、Podの順序付けとスケーリングを一貫した方法で管理するため、データの一貫性と可用性に依存する分散データベースなどのアプリケーションにとって重要です。
ローリングアップデート:StatefulSetはローリングアップデートをサポートしているため、アプリケーションの安定性を維持しながら、インスタンスを一度に1つずつ更新することができます。
一方、Deployments と ReplicaSet は、通常、データの一貫性や一意の ID を気にせずにインスタンスを置き換えることができるステートレス アプリケーションに使用されます。DaemonSet は、ノードごとに 1 つのポッドをデプロイするために使用されますが、独立したスケーリングと順序付けされた操作を必要とするアプリケーションには適していない場合があります。
したがって、このシナリオではオプションd)が正解です。
<details><div>

### Q. 問題47: 未回答
シナリオ：
ソフィアはXYZ Inc.で働いています。同社は、新しいアプリケーションを社内で使用するために複数のオンプレミスの場所と、外部で使用するためにGoogle Cloud Platform(GCP)にデプロイしているところです。リソース使用率を最適化するために、すべての受信トラフィックは Google Cloud ロードバランサを経由します。
要件：
アプリケーションのパフォーマンスを効果的に監視するために、Sophia はオンプレミスの VM と GCP VM の両方から包括的なレイテンシと可用性のデータを収集する必要があります。これは、さまざまな環境でのアプリケーションのパフォーマンスを分析するのに役立ちます。
要件を満たすには、次のうちどれを選択する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え： D

説明：
オンプレミスの VM と GCP VM とは別にデータを収集して分析し、個別のサービス レベル インジケーター (SLI) を作成することは、効果的な監視と分析のためのベスト プラクティスと一致します。このアプローチにより、各環境のパフォーマンスと可用性をより正確に把握できます。個別の SLI を作成することで、Sophia はさまざまなコンテキストでアプリケーションがどのように実行されるかについての洞察を得ることができ、トラブルシューティングと最適化に役立てることができます。
オプション A、B、および C には制限があります。
A. データを一緒に処理する:異なる環境からのデータを組み合わせると、ネットワークの状態やインフラストラクチャのばらつきにより、不正確な比較につながる可能性があります。
B. GCP ロードバランサーからのみデータを収集します。このアプローチでは、社内のユース ケースにとって重要なオンプレミスのパフォーマンスは考慮されません。
C. GCP VM からのみデータを収集します。GCP 環境だけに注目すると、オンプレミスで直面するパフォーマンスの課題を見落とす可能性があります。
最も効果的なアプローチは、D. 両方の環境から別々にデータを収集して分析し、異なるシナリオにおけるアプリケーションのパフォーマンスを包括的に把握するために比較できる個別のSLIを作成することです。
<details><div>

### Q. 問題48: 未回答
シナリオ：
エマはABC Inc.で働いています。彼女は、重要な本番アプリケーションの応答時間の継続的な低下を調査するタスクを割り当てられました。アプリケーションは、5 つのインスタンスで構成されるマネージド インスタンス グループにデプロイされます。
要件：
Emma は、応答時間の低下に寄与する要因を特定するために、不要なオーバーヘッドを最小限に抑えながら、問題を徹底的に調査する必要があります。
この問題を効果的に調査するために、Emma は次のうちどれを講じるべきですか?
1. 
2. 
3. 
4. 
<details><div>
答え： B

説明：
不要なオーバーヘッドを最小限に抑えながら、本番環境アプリケーションのレスポンス時間の継続的な低下を効果的に調査するために、Emma はログ エージェントをインストールし、Cloud Logging でログベースの指標を確立する必要があります。このアプローチにより、アプリケーション ログを分析し、応答時間の低下の原因となっている可能性のあるパターンやイベントを特定できます。ログベースの総計値を作成することで、応答時間に関連する特定の条件を追跡し、タイムリーな通知のアラートを設定できます。
他のオプションが最も適していない理由は次のとおりです。
A. デバッガ エージェントをインストールします。デバッガー エージェントは、コード レベルのデバッグに重点を置いており、応答時間の継続的な低下など、パフォーマンスの問題に関する必要な分析情報を提供しない場合があります。
C. 監視エージェントを設定し、ダッシュボードを作成します。監視エージェントとダッシュボードは、パフォーマンス メトリックを追跡するのに役立ちますが、応答時間の低下の原因となる特定の要因を理解するために必要な詳細な分析を提供しない場合があります。
D. トレース エージェントを実装します。トレース エージェントは、待機時間のボトルネックを特定するために使用されますが、時間の経過に伴う応答時間の継続的な低下を調査するための最適なソリューションではない可能性があります。
最も効果的な方法は、B. ロギング エージェントをインストールし、Cloud Logging でログベースの指標を確立することです。これにより、Emma はアプリケーションログを詳細に調べて、不要なオーバーヘッドを最小限に抑えながら、応答時間の低下の原因となっている問題を特定することができます。
<details><div>

### Q. 問題49: 未回答
シナリオ：
ソフィアは、さまざまなクライアントにクラウドセキュリティサービスを提供するサイバーセキュリティ企業であるTechSecure Inc.のクラウドセキュリティアナリストです。クライアントの 1 つである CyberApp は、アプリケーションのデプロイに Google Cloud Platform(GCP)を使用しています。ソフィアは、CyberApp の GCP リソースのセキュリティの監視と監査を担当しています。
要件：
Sophia は、CyberApp の GCP 環境での Cloud Storage バケットの権限の設定または変更に関連するログエントリを特定する必要があります。
質問：
CyberApp の GCP 環境での Cloud Storage バケットの権限の設定や変更に関連するログエントリを特定するために、Sophia は Google Cloud Logging でどのログタイプを探すべきですか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある

説明：
Google Cloud Logging の管理者アクティビティ ログは、権限の変更、IAM ポリシーの変更、Google Cloud リソース(Cloud Storage バケットを含む)内のその他の管理アクティビティなどの管理アクションをキャプチャします。そのため、Sophia が Cloud Storage バケットの権限の設定や変更に関連するログエントリを特定する場合は、管理アクティビティのログを探す必要があります。
このシナリオでは、Sophia が Cloud Storage バケットの権限の設定または変更に関連するログエントリを特定するのに最も適したオプションは、A) 管理アクティビティ ログです。
<details><div>

### Q. 問題50: 未回答
シナリオ：
あなたは、大規模なeコマースWebサイトの保守を担当するDevOpsチームの一員です。Webサイトでは、断続的な速度低下と時折のダウンタイムが発生しており、ユーザーエクスペリエンスと売上に悪影響を及ぼしています。チームは、これらのインシデントを迅速に検出して対応するための監視戦略を実装することを決定します。
質問：
eコマースWebサイトのユーザーエクスペリエンスに影響を与えている断続的な速度低下やダウンタイムの問題を検出してチームに警告するには、どの監視アプローチが最も適していますか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある

説明：
ブラックボックス監視は、eコマースWebサイトのユーザーエクスペリエンスに影響を与える断続的な速度低下やダウンタイムの問題を検出してチームに警告するための最適なアプローチです。このシナリオでは、チームは、速度低下やダウンタイム イベント自体など、システムの症状と動作に関心があります。ブラックボックス監視では、システムの内部の詳細にアクセスすることなく、システムの外部動作を観察します。これは、すでに発生しているインシデントを特定し、事前定義された条件に基づいてアラートまたは通知をトリガーする場合に特に役立つため、eコマースWebサイトのユーザーエクスペリエンスに影響を与える問題を迅速に検出して対応するのに適しています。
<details><div>

### Q. 問題51: 未回答
シナリオ：あなたは、Google Cloud プラットフォームでホストされる待望のオンラインゲームのリード アーキテクトです。ゲームは現在パブリックベータ版であり、一般公開の正式リリースが近づいています。堅牢なサービスレベル目標(SLO)を確立することは、プレイヤーにシームレスなゲーム体験を提供するために重要です。ただし、可用性と待機時間の適切なバランスを実現することは困難です。
質問：優れたゲーム体験を確保することの複雑さを考慮すると、オンラインゲームのサービスレベル目標(SLO)を定義するにはどのようなアプローチを取るべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え： ある

説明：
各オプションを分解し、そのうちの1つが正しい選択である理由を説明しましょう。
オプションA:1 つの SLO を 99.9% のゲーム サーバーの可用性として定義し、もう 1 つの SLO を 50 ミリ秒以内のレイテンシーを返すリクエストの 99% として定義します。
このオプションでは、サーバーの可用性と応答時間の両方に重点が置かれます。高可用性 SLO (99.9%) を設定すると、ほとんどの時間、プレイヤーがゲームサーバーにアクセスできるようになります。さらに、99% の応答時間を 50 ミリ秒未満に指定することは、ゲームのインタラクションの応答性が高く、優れたプレイヤー エクスペリエンスを提供する必要があることを示します。
オプションB:1 つの SLO を 1 週間以内のゲームサーバーの合計稼働時間として定義し、もう 1 つの SLO を 100 ミリ秒未満のすべての HTTP リクエストの平均応答時間として定義します。
このオプションでは、合計稼働時間と応答時間が重視されますが、オプション A のように高可用性は指定されません。 稼働時間を 1 週間以内に測定すると、ダウンタイムが長くなり、プレイヤー エクスペリエンスに悪影響を与える可能性があります。さらに、100 ミリ秒の応答時間は、リアルタイムのゲーム操作には十分ではない可能性があります。
オプションC:1 つの SLO を 2xx ステータスコードを返す HTTP リクエストの 99% として定義し、もう 1 つの SLO を 100 ミリ秒以内に返されるリクエストの 99% として定義します。
このオプションは、応答時間と要求の成功率に重点を置きます。HTTPリクエストの成功率は99%と良好ですが、サーバーの可用性には対応していません。100 ミリ秒の応答時間も許容されますが、シームレスなゲーム体験のために可能な限り低い待機時間が保証されない場合があります。
オプションD:1 つの SLO を Google Cloud の可用性に一致するサービスの可用性として定義し、もう 1 つの SLO をすべてのゲームサーバーとのやり取りの 100 ミリ秒のレイテンシとして定義します。
このオプションでは、Google Cloud の高可用性基準を満たすことを目指して、可用性が優先されます。ただし、特定の応答時間の目標が設定されていないため、プレイヤーのエクスペリエンスが異なる場合があります。100 ミリ秒の固定レイテンシーでは、ゲーム内アクションの複雑さの違いが考慮されない可能性があります。
正解：オプションAは正しい選択です。これは、高いサーバー可用性 SLO (99.9%) と応答性の高い応答時間 SLO (50 ミリ秒未満で 99%) を組み合わせたものです。このバランスにより、ゲームがアクセシブルな状態を維持し、低遅延のインタラクションで優れたプレイヤー体験を提供することができます。稼働時間と応答性の両方に対応しているため、両方の要素が成功に不可欠なオンラインゲームのシナリオに適しています。
他のオプションが正しくない理由:
オプション B では、1 週間以内の稼働時間が重視されるため、ダウンタイムが長くなる可能性があり、100 ミリ秒の応答時間はリアルタイム ゲームには十分な速さではない可能性があります。
オプション C では、応答時間と要求の成功に重点が置かれていますが、サーバーの可用性は無視されています。
オプション D では高可用性が優先されますが、応答時間の目標が指定されていないため、一貫性のないプレイヤー エクスペリエンスにつながる可能性があります。
オンラインゲームのコンテキストでは、優れたゲーム体験を提供するためには、高いサーバー可用性と低遅延のインタラクションのバランスをとることが重要であり、オプションAが最も適した選択肢となります。
<details><div>

### Q. 問題52: 未回答
シナリオ：
あなたは、Google Cloud Platform(GCP)でホストされているクラウドベースのアプリケーションのパフォーマンスを最適化する責任を持つDevOpsアーキテクトです。アプリケーションで待機時間の問題が発生しており、応答性を向上させるためにパフォーマンスのボトルネックを特定する必要があります。
質問：
GCP でアプリケーションのパフォーマンスを診断して最適化する場合、アプリケーションのプロファイリングを開始してパフォーマンス データを収集するには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は C です。 Google Cloud Profiler モジュールをインポートし、その start 関数を呼び出してプロファイリングを開始します。
このオプションが正しい理由は次のとおりです。
C. Google Cloud Profiler モジュールをインポートし、その start 関数を呼び出してプロファイリングを開始する: Google Cloud Profiler は、アプリケーション プロファイリング用に Google Cloud が提供するネイティブ ツールです。そのモジュールをインポートし、start 関数を呼び出すことで、アプリケーションを効率的にプロファイルし、パフォーマンス データを収集できます。これは、GCP でアプリケーションをプロファイリングする場合に推奨される方法です。
他のオプションが正しくない理由:
A. カスタム パフォーマンス モニタリング ツールをゼロから作成してデータを収集する: カスタム ツールをゼロから開発するには時間がかかり、多大な労力が必要であり、Google Cloud Profiler などの確立されたツールを使用するほど効果的ではない可能性があります。
B. サードパーティのパフォーマンス監視エージェントをインストールしてアプリケーションをプロファイリングする: サードパーティのツールを使用するのも効果的ですが、Google Cloud Profiler は GCP 専用に設計されたネイティブ ソリューションであり、シームレスな統合と使いやすさを提供します。
D. Cloud Logging でカスタム パフォーマンス ログを設定してアプリケーション パフォーマンス データを取得する: Cloud Logging はログのキャプチャに役立ちますが、主に詳細なパフォーマンス プロファイリング用に設計されていないため、この目的にはあまり適していません。
要約すると、GCP でパフォーマンス データをプロファイリングして収集する最も効率的で簡単な方法は、ネイティブの Google Cloud Profiler モジュールを使用することです。このアプローチは、プラットフォームと十分に統合されており、この特定のタスクに合わせて調整されています。
<details><div>

### Q. 問題53: 未回答
シナリオ：
あなたは DevOps エンジニアで、Google Cloud でホストされている重要なアプリケーションの保守を担当しています。最近の運用環境の更新後、一部のお客様から、応答時間が遅くなり、サービスが断続的に中断されるという報告が寄せられています。このような状況では、効果的なトラブルシューティングを行うために、迅速な対応と Google の SRE のベスト プラクティスの遵守が必要です。
質問：
効果的なトラブルシューティングのための Google の SRE(Site Reliability Engineering)のベスト プラクティスに従って、最近の製品版の更新後に応答時間が遅くなり、サービスが中断したという顧客からの報告に直面した場合、最初のステップは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は A. さらに調査するために問題チケットを作成します。
このオプションが正しい理由は次のとおりです。
A. 詳細な調査のために問題チケットを作成する: Google の SRE ガイドラインに従って、最初のアクションはインシデント チケットを作成して問題を文書化することです。これにより、問題の重大度、影響、必要なアクションの評価を含むインシデント管理プロセスが開始されます。
他のオプションが正しくない理由:
B. 問題の影響の大きさを評価する: これはインシデント管理プロセスにおける重要なステップですが、問題の影響の評価はインシデント チケットの作成後に行われます。重大度の割り当ては、この評価の一部として行われます。
C. トラブルシューティングの実施中にシステム機能を強化する: インシデント発生時にシステム機能を維持することは重要ですが、これは、最初の問題のドキュメントに従って、トラブルシューティング プロセスの一部として行う必要があります。
D. アプリケーション ログを調べる: ログの確認はトラブルシューティングに不可欠な手順ですが、通常はインシデントが文書化され、適切な重大度レベルが割り当てられた後に実施されます。
要約すると、Google の SRE のベスト プラクティスに従った最初のアクションは、さらなる調査のためにインシデント チケットを作成することです。これにより、インシデント管理プロセスが開始され、その間に問題の重大度と影響が評価され、その後のアクションが通知されます。
<details><div>

### Q. 問題54: 未回答
あなたは、さまざまな Google Cloud Platform(GCP)サービスを利用する急成長中の組織で DevOps アーキテクトとして働いています。財務チームは、クラウドの費用を効果的に追跡および分析することに関心を持っています。彼らは、支出パターンに関する詳細な洞察を得て、コストを最適化できるソリューションを求めています。
GCP での費用管理のベスト プラクティスを遵守しながら、財務チームの懸念に対処するには、次のうちどれを推奨すべきですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は C. 請求専用のプロジェクトを別に作成し、ロールベースのアクセス制御を適用し、請求プロジェクト内の BigQuery データセットに Cloud Billing データをエクスポートします。

説明：
このオプションは、GCP の費用管理に関するおすすめの方法に沿ったものです。その理由は次のとおりです。
請求用に別のプロジェクトを作成します。請求専用のプロジェクトを作成することで、請求関連のリソースを効果的に分離し、個別に管理することができます。
ロールベースのアクセス制御を適用します。請求プロジェクトへのアクセスを制限することで、財務チームなどの許可された担当者のみがコスト関連のデータや設定にアクセスできるようになります。これにより、セキュリティと制御が強化されます。
Cloud Billing のデータを請求プロジェクト内の BigQuery データセットにエクスポートします。請求データを BigQuery にエクスポートすると、詳細な分析とレポート作成が可能になり、支出パターンと費用最適化の機会に関する貴重な分析情報が得られます。
他のオプションが正しくない理由:
A. 高度な請求アラート システムを設定する: 予算アラートは費用のモニタリングに役立ちますが、このオプションでは、BigQuery が提供する詳細な費用分析と最適化の要件には対応できません。
B. 請求レポートを手動で確認する: Google Cloud Console で請求レポートを手動で確認するには時間がかかり、BigQuery が提供する高度なデータ分析機能がない場合があります。
D. カスタムのメール通知ソリューションを実装する: このオプションでは、通知に重点が置かれていますが、BigQuery の詳細な分析機能は提供されません。
要約すると、アクセスが制限された別の請求プロジェクトを作成し、請求データを BigQuery にエクスポートすることは、支出パターンに関する詳細な分析情報を取得し、費用を最適化するための推奨されるアプローチであり、このシナリオで最も効果的なソリューションになります。
<details><div>

### Q. 問題55: 未回答
あなたは、組織の Google Cloud Platform(GCP)環境でのアクセスと権限の管理を担当する DevOps エンジニアです。財務チームは、GCP の請求と費用関連の側面を監督、管理するための支援を必要としています。彼らは、他のリソースへの広範なアクセス権を持たずに請求情報にアクセスできるチーム メンバーに必要なアクセス許可を付与したいと考えています。
財務チームの要件を効果的に満たすために、このチームメンバーに割り当てるべきGCP IAM(Identity and Access Management)の役割はどれですか?
1. 
2. 
3. 
4. 
<details><div>
答え：
正解は「D. Cloud Billing Administrator」です。

説明:
プロジェクトビューア(A):このロールは、GCP プロジェクト内のリソースへの読み取り専用アクセスを提供しますが、請求アクセスは付与しません。
請求先アカウント管理者 (B):このロールを使用すると、ユーザーは課金アカウントとサブスクリプションを管理できますが、コストの監視と分析のみに重点を置いているユーザーには、必要以上に制御できる可能性があります。
請求先アカウント ユーザー (C):このロールでは、ユーザーは請求先アカウント情報を表示してアクセスできますが、詳細な請求管理に必要なレベルの制御は提供されません。
Cloud 請求管理者 (D):このロールは、請求とコスト管理のアクティビティを監督する必要がある個人向けに設計されているため、財務チームの要件に最も適した選択肢です。請求関連のデータや設定へのアクセスは許可されますが、より広範なプロジェクトへのアクセスは提供されません。
このシナリオでは、Cloud Billing 管理者のロールは、他のプロジェクト リソースへの不要なアクセスではなく、請求情報への適切なレベルのアクセス権を付与することで、財務チームのニーズに合致します。
</details><div>
