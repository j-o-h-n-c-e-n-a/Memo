# 1
## 1
### Q. 問題1: 未回答
あなたは現在、組織のGoogle CloudプロジェクトのCloud Monitoringメトリクスを表示する方法を計画しています。あなたの組織には以下の3つのフォルダと6つのプロジェクトがあります：
Developmentフォルダー
- cjp-one-dev
- cjp-two-dev
Stagingフォルダー
- cjp-one-staging
- cjp-two-staging
Productionフォルダー
- cjp-one-prod
- cjp-two-prod
Cloud Monitoringダッシュボードを構成して、1つのフォルダ内のプロジェクトのメトリクスのみを表示させたいとします。ダッシュボードが他のフォルダのプロジェクトのメトリクスを表示しないようにする必要があります。Googleが推奨するプラクティスに従います。
あなたはこの要件を満たすために、どうすればよいですか？
1. 新しいスコーププロジェクトを1つ作成します
2. フォルダごとに新しいスコーププロジェクトを作成します
3. 現在のcjp-one-dev、cjp-one-staging、cjp-one-prodプロジェクトを、各フォルダのスコーププロジェクトとして使用します
4. 現在のcjp-one-prodプロジェクトをスコーププロジェクトとして使用します
<details><div>
    答え：2
説明
この問題では、複数のGoogle CloudプロジェクトにわたるCloud Monitoringメトリクスを表示する方法を計画する必要があります。ここで重要なのは、各フォルダ内のプロジェクトのメトリクスだけを表示したいという要件です。そのため、各フォルダ単位でうまくスコープを分ける方法を考える必要があります。Googleの推奨するプラクティスを適用しつつ、適切なスコーププロジェクトの作成や使用方法を選択することが求められている状況です。
基本的な概念や原則：
Cloud Monitoring：Google Cloudのリソースとアプリケーションのパフォーマンスを監視、評価、保護するためのツールです。異常を迅速に特定し、通知を送ることが可能です。
Cloud Monitoringダッシュボード：Cloud Monitoringの情報を視覚的に表示するインターフェースです。パフォーマンスメトリクス、リソース利用率などをリアルタイムで把握することができます。
フォルダ：Google Cloudリソースの管理単位です。あるフォルダの中にあるプロジェクトとそのリソースは、そのフォルダの設定に基づいて制御されます。
Cloudプロジェクト：Google Cloud上でリソースを作成・管理するためのコンテナです。各プロジェクトは一意のIDを持ち、1つ以上のフォルダ内に配置されます。
スコーププロジェクト：特定の目的に合わせて設定・管理されるCloudプロジェクトです。特定のスコープ（例えば、特定のフォルダ内のプロジェクト）に対するリソース使用状況やメトリクスを把握するために使用されます。
Google Cloudの推奨プラクティス：Googleから公表されている、Cloudの効率的でセキュアな利用方法のガイドラインです。特定の要件を満たすための最良のソリューションを提供します。
正解についての説明：
（選択肢）
・フォルダごとに新しいスコーププロジェクトを作成します
この選択肢が正解の理由は以下の通りです。
Google Cloud Monitoringでは、ダッシュボードは一つのプロジェクトに対して設定されたものであり、他のプロジェクトのメトリックを表示することはできません。そのため、フォルダごとにスコーププロジェクトを作成することにより、フォルダ内の各プロジェクトのメトリクスを一元化することができます。そのスコーププロジェクトにダッシュボードを作成することで、特定のフォルダー内のプロジェクトのメトリクスのみを表示させることができます。これにより、他のフォルダのプロジェクトのメトリクスと混在することなく、1つのフォルダ内の各プロジェクトのメトリクスを効率よく管理・モニタリングすることが可能になります。これはGoogleが推奨するプラクティスにより適合しており、この要件を効果的に満たします。
不正解についての説明：
選択肢：新しいスコーププロジェクトを1つ作成します
この選択肢が正しくない理由は以下の通りです。
新しいスコーププロジェクトを1つだけ作成すると、全てのフォルダのメトリクスが混在して表示されてしまい、特定のフォルダのメトリクスだけを分離して表示することができません。
それに対して、フォルダごとに新しいスコーププロジェクトを作成すると、そのフォルダ内のプロジェクトのメトリクスのみを絞り込んで表示できます。
選択肢：現在のcjp-one-prodプロジェクトをスコーププロジェクトとして使用します
この選択肢が正しくない理由は以下の通りです。
cjp-one-prodプロジェクトをスコーププロジェクトとして使用した場合、他のフォルダのプロジェクトのメトリクスを表示できる範囲が制限されます。新しいスコーププロジェクトを作成することにより、各フォルダ内のプロジェクトのメトリクスのみを表示するダッシュボードを構成できます。
選択肢：現在のcjp-one-dev、cjp-one-staging、cjp-one-prodプロジェクトを、各フォルダのスコーププロジェクトとして使用します
この選択肢が正しくない理由は以下の通りです。
現在のプロジェクトを使用すると、フォルダ全体のメトリクスを正確に表示することが難しいです。そのため、新しいスコーププロジェクトを作成して、必要なメトリクスを的確に把握する方がベストプラクティスとされています。
参考リンク：
https://cloud.google.com/monitoring/dashboards
https://cloud.google.com/resource-manager/docs/creating-managing-folders
https://cloud.google.com/monitoring/api/v3/aggregation
</div></details>

### Q. 問題2: 未回答
あなたの会社は最近Google Cloudに移行しました。新しいプロジェクトと基本リソースをGoogle Cloudにプロビジョニングするために、高速で信頼性が高く、再現性のあるソリューションを設計する必要があります。
この要件を満たすために、どうすればよいですか？

1. Terraformモジュールを書いてソース管理リポジトリに保存します。terraform applyコマンドをコピーして実行し、新しいプロジェクトを作成します
2. リクエストから適切なパラメータを渡すgcloud CLIを使用してスクリプトを記述します。スクリプトをGitリポジトリに保存します
3. プロジェクトを作成するために、Google Cloudコンソールを使用します
4. Cloud Foundation ToolkitのTerraformリポジトリを使用します。適切なパラメータを指定してコードを適用し、Google Cloudプロジェクトと関連リソースを作成します
<details><div>
    答え：4
説明
この問題では、Google Cloudに移行した会社が新しいプロジェクトと基本リソースをプロビジョニングするための高速で信頼性が高く、再現性のあるソリューションを設計することが要求されています。ここで求められているのは、信頼性と再現性が高く、高速で新しいプロジェクトとリソースを作成する効率的な方法です。そのため、選択肢を見るときは、これらの要素を満たす各オプションを探すことが重要となります。この問題を解く上でのキーポイントはプロビジョニングの自動化と一貫性です。
基本的な概念や原則：
Cloud Foundation Toolkit：Google Cloudの環境を迅速にセットアップし、管理するためのモジュラーでスケーラブルなテンプレート群です。TerraformとDeployment Managerのモジュールが用意されています。
Terraform：HashiCorpが開発したオープンソースのインフラストラクチャアズコードツールです。異なるクラウドプロバイダやサービス間で標準的な操作と自動化を提供します。
Google Cloudプロジェクト：Google Cloudでリソースを作成して管理する基本的な組織単位です。
Google Cloud Console：Google Cloudの各種リソースやサービスを管理するためのWebベースのユーザインターフェースです。
gcloud CLI：Google Cloudのリソースやサービスをコマンドラインから操作するためのツールです。スクリプトでの利用にも適しています。
Git：分散型バージョン管理システムです。プロジェクトの全ての開発履歴を保管し、分散作業を効率的に行うことが可能です。
正解についての説明：
（選択肢）
・Cloud Foundation ToolkitのTerraformリポジトリを使用します。適切なパラメータを指定してコードを適用し、Google Cloudプロジェクトと関連リソースを作成します
この選択肢が正解の理由は以下の通りです。
まず、Cloud Foundation Toolkit（CFT）のTerraformリポジトリは、Google Cloudのリソースを管理するための通常の慣行に基づいたさまざまなモジュールとテンプレートを提供します。これにより、新しいプロジェクトとその基本的なリソースを効率的にプロビジョニングするための強力なツールセットが利用可能になります。
また、TerraformはInfrastructure as Code（IaC）ツールの一つであり、コードを通じてインフラストラクチャを定義およびプロビジョニングすることを可能にします。これは、設定の再現性、バージョン管理、および多くの環境に対する一貫性の確保など、多くの利点を提供します。
したがって、CFTのTerraformリポジトリを使用してパラメータを指定し、コードを適用することで、高速で信頼性が高く、再現可能なソリューションを設計できます。これは新しいプロジェクトと関連リソースの作成に適しています。
不正解についての説明：
選択肢：プロジェクトを作成するために、Google Cloudコンソールを使用します
この選択肢が正しくない理由は以下の通りです。
Google Cloudコンソールを使用してプロジェクトを作成する方法は、高速で信頼性が高く、再現性のあるソリューションとは言えません。マニュアル操作で時間がかかるため、量が多いとミスも発生しやすくなります。
一方で、Cloud Foundation ToolkitのTerraformリポジトリを使用すると、コードによる管理が可能となり、高速、信頼性、再現性全ての要素を満たすが可能となります。
選択肢：リクエストから適切なパラメータを渡すgcloud CLIを使用してスクリプトを記述します。スクリプトをGitリポジトリに保存します
この選択肢が正しくない理由は以下の通りです。
gcloud CLIを使用してスクリプトを記述する方法でも基本的なリソースのプロビジョニングは実現できますが、再現性と信頼性の面で不十分です。
それに対して、Cloud Foundation ToolkitのTerraformリポジトリは高速で信頼性が高く再現性のあるソリューションを提供し、コード化されたインフラストラクチャ管理を実現します。
選択肢：Terraformモジュールを書いてソース管理リポジトリに保存します。terraform applyコマンドをコピーして実行し、新しいプロジェクトを作成します
この選択肢が正しくない理由は以下の通りです。
Terraformモジュールを書き、Terraform applyコマンドを使用する方法も新しいプロジェクトの作成に使用できますが、これだけでは再現性や信頼性が必ずしも保証されるわけではありません。
それに対して、Cloud Foundation ToolkitのTerraformリポジトリを利用すれば、既に信頼性が確認されたTerraformテンプレートを再利用できるため、要件をより確実に満たせます。
参考リンク：
https://cloud.google.com/resource-manager/docs/creating-managing-projects
https://cloud.google.com/foundation-toolkit
https://www.terraform.io/docs/providers/google/guides/getting_started.html
</div></details>

### Q. 問題3: 未回答
あなたは、アプリケーションのビルドにCloud Buildを使用しています。コストと開発工数を最小限に抑えながら、ビルド時間を短縮したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？

1. 実行時間を最小化するために、複数の小さなビルドステップを使用します
2. Cloud Storageを使用して中間成果物をキャッシュします
3. ビルドを並列化するために複数のJenkinsエージェントを実行します
4. マシンタイプオプションを使用して、より大きなCloud Build仮想マシン（VM）を使用します
<details><div>
    答え：2
説明
この問題では、Cloud Buildを用いたアプリケーションのビルド工程において、ビルド時間を短縮することを目指し、かつコストと開発工数を最小限に抑える方策を選ぶことが求められています。選択肢を評価する際には、ビルド時間の短縮とコスト・工数の最小化のバランスを考慮する必要があります。選択肢の中には、時間短縮効果があるもののコストや開発工数が増大する可能性があるものも含まれているため、全体の最適解を見つけることがキーとなります。
基本的な概念や原則：
Cloud Build：Google CloudのCI/CDプラットフォームです。アプリケーションのビルドとテストをサーバレスで高速に行うことができます。
Cloud Storage：Google Cloudのオブジェクトストレージサービスです。低遅延と高い伸縮性を提供します。
ビルドキャッシュ：ビルドプロセス中に生成される中間成果物を再利用してビルド時間を短縮するテクニックです。
Jenkinsエージェント：Jenkinsの仕事を実行するためのサーバです。並列化によりビルド時間を短縮できますが、管理が必要であり、余分なコストが発生する可能性があります。
ビルドステップ：ビルドプロセスの各フェーズを表す一連のタスクです。小さなステップに分割することは管理を容易にしますが、各ステップのオーバーヘッドによりビルド時間が長くなる可能性があります。
Cloud Build仮想マシン（VM）：ビルドプロセスを実行するための環境です。仮想マシンのサイズを大きくすることで、ビルド時間を短縮できますが、コストが増加する可能性があります。
正解についての説明：
（選択肢）
・Cloud Storageを使用して中間成果物をキャッシュします
この選択肢が正解の理由は以下の通りです。
まず、Cloud BuildはGoogle Cloudのフルマネージド型のビルドサービスであり、ソースコードからコンテナイメージやアプリケーションを効率的にビルドするために使用されます。ビルドプロセスの間に生成される一時的なファイルや中間成果物をCloud Storageにキャッシュすることで、それらを再利用し、同じビルドステップを繰り返すことなく、ビルド時間を大幅に短縮することができます。
加えて、Cloud Storageは高可用性と耐久性に優れ、大量のデータを安全に保存できるスケーラブルなストレージサービスです。
したがって、ビルドプロセスで生じる多数の中間成果物を保存、管理し、すぐにアクセスできることが可能となります。
さらに、中間成果物のキャッシュによるビルド時間の短縮は、ビルドに関連するコスト削減にも寄与します。これは、ビルドプロセスが時間をかけて同じ操作を繰り返すことを避けることで、計算リソースの使用を最適化するからです。これにより、開発工数も最小限に抑えることが可能となります。
従って、Cloud Storageを使用して中間成果物をキャッシュすることは、ビルド時間の短縮とコスト削減の両方を実現する効率的な手法と言えます。
不正解についての説明：
選択肢：ビルドを並列化するために複数のJenkinsエージェントを実行します
この選択肢が正しくない理由は以下の通りです。
ビルドを並列化するために複数のJenkinsエージェントを走らせると、確かにビルド時間を短縮できますが、それは追加の計算リソースを必要とします。これはコストと開発工数を増加させる可能性があります。
一方で、Cloud Storageを使用して中間成果物をキャッシュする方法は、再ビルド時間を削減し、コストと開発工数を抑えることができます。
選択肢：実行時間を最小化するために、複数の小さなビルドステップを使用します
この選択肢が正しくない理由は以下の通りです。
ビルドプロセスを複数の小さなステップに分割することは、ビルド時間を必ずしも短縮しません。それどころか、各ステップ間のオーバーヘッドによりビルド時間が延長する可能性があります。
一方、Cloud Storageを使用して中間成果物をキャッシュすることは、再利用可能なコンポーネントを保存し再ビルドを避けることで、ビルド時間の短縮に効果的です。
選択肢：マシンタイプオプションを使用して、より大きなCloud Build仮想マシン（VM）を使用します
この選択肢が正しくない理由は以下の通りです。
より大きなCloud Build仮想マシンを使用する方法はビルド時間を短縮する可能性がありますが、コストと開発工数を最小限に抑える目標には合わない可能性があります。
それに対して、Cloud Storageを使用して中間成果物をキャッシュする方法は、再利用可能な成果物を保存し次回のビルドで再利用することでビルド時間を短縮し、コストと開発工数も抑えることが可能です。
参考リンク：
https://cloud.google.com/build/docs/speeding-up-builds
https://cloud.google.com/build/docs/build-config-file-schema#options
https://cloud.google.com/storage/docs/uploading-objects
</div></details>

### Q. 問題4: 未回答
あなたはGoogle CloudリソースのTerraformデプロイを実行するCI/CDパイプラインを作成しています。CI/CDツーリングはGoogle Kubernetes Engine（GKE）で実行され、各パイプラインの実行にエフェメラルなポッドを使用します。ポッド内で実行されるパイプラインが、Terraformデプロイを実行するための適切なIAM（Identity and Access Management）権限を持っていることを確認する必要があります。ID管理についてはGoogleが推奨するプラクティスに従う必要があります。
この要件を満たすために、どうすればよいですか？（2つ選択）

1. 新しいKubernetesサービスアカウントを作成し、そのサービスアカウントをポッドに割り当てます。Workload Identityを使用して、Googleサービスアカウントとして認証します
2. 新しいGoogleサービスアカウントを作成し、適切なIAM権限を割り当てます
3. Googleサービスアカウント用の新しいJSONサービスアカウントキーを作成し、そのキーをKubernetesシークレットとして保存し、そのキーをポッドに注入し、GOOGLE_APPLICATION_CREDENTIALS環境変数を設定します
4. Googleサービスアカウント用の新しいJSONサービスアカウントキーを作成し、CI/CDツールのシークレット管理ストアにキーを保存し、認証にこのキーを使用するようにTerraformを設定します
5. ポッドを実行するCompute Engine VMインスタンスに関連付けられたGoogleサービスアカウントに、適切なIAMパーミッションを割り当てます
<details><div>
    答え：1,2
説明
この問題では、CI/CDパイプラインでGoogle CloudリソースのTerraformデプロイを実行するためのIAM権限管理について質問がされています。特に、Google Kubernetes Engineで実行されるエフェメラルなポッドが適切な権限を持っていることが必要で、Googleの推奨するID管理のプラクティスに従うことが求められています。ここで、ロールを持つインスタンスとそのアクセス管理を安全に管理する方法として、Kubernetesサービスアカウントの作成とその割り当て、Workload Identityの使用、Googleサービスアカウントの作成とIAM権限の管理などが試験対策となります。選択肢を読み解く際には、GoogleのID管理のベストプラクティスと、セキュリティや運用面でのコンソールからの各種設定の違いに注目することが必要です。
基本的な概念や原則：
Terraform：Infrastructure as Codeツールで、クラウドリソースのデプロイを自動化します。Google CloudのリソースやIAMポリシーなど、多くのGoogle Cloudサービスがサポートされています。
Google Kubernetes Engine（GKE）：Kubernetesのマネージド環境を提供するGoogle Cloudサービスです。クラスター管理やオートスケーリングなどの機能があります。
エフェメラルポッド：一時的に作成され、タスク完了後にすぐに削除されるKubernetesのポッドです。CI/CDパイプラインなど短期間のタスクに適しています。
IAM権限：Google Cloudサービスのリソースに対するアクセスを制御する機能です。サービスアカウントに対して適切な権限を割り当てることで、適切なアクセス制御が可能になります。
Kubernetesサービスアカウント：Kubernetes内での身元証明とアクセス制御を行うアカウントです。特定のポッドに割り当てることができます。
Workload Identity：GKE上のサービスアカウントをGoogle Cloudのサービスアカウントにマッピングする機能です。これにより、GKEの各ポッドは適切なIAM権限を持つことになります。
Googleサービスアカウント：Google Cloudリソースに対するアクセスを認証・認可するための特殊なアカウントです。適切なIAM権限を持つことで、リソースへの安全なアクセスが可能になります。
正解についての説明：
（選択肢）
・新しいKubernetesサービスアカウントを作成し、そのサービスアカウントをポッドに割り当てます。Workload Identityを使用して、Googleサービスアカウントとして認証します
・新しいGoogleサービスアカウントを作成し、適切なIAM権限を割り当てます
この選択肢が正解の理由は以下の通りです。
まず、問題文の最初の要件である"ポッド内で実行されるパイプラインが、Terraformデプロイを実行するための適切なIAM権限を持っていること"を満たすために、新しいGoogleサービスアカウントを作成し、適切なIAM権限を割り当てることが必要です。このようにすることで、CI/CDツールが実行されるGKE環境を、実際にリソースのデプロイメントや更新を行うためのIAM権限を持つアカウントとして扱うことができます。
次に、Googleの推奨するプラクティスに従うために、新しいKubernetesサービスアカウントを作成し、そのサービスアカウントをポッドに割り当てます。
そして、Workload Identityを使用してGoogleサービスアカウントとして認証します。Workload IdentityはGKEのサービスアカウントとGoogle Cloudのサービスアカウントを関連付けるシステムで、これによってCI/CDパイプラインが実行されるポッドが適切なアイデンティティと権限を持つようになります。これにより、エフェメラルなポッドがTerraformデプロイを実行するための十分な権限を持つように、安全かつ効率的に許可とアイデンティティの管理が可能になります。
不正解についての説明：
選択肢：Googleサービスアカウント用の新しいJSONサービスアカウントキーを作成し、そのキーをKubernetesシークレットとして保存し、そのキーをポッドに注入し、GOOGLE_APPLICATION_CREDENTIALS環境変数を設定します
この選択肢が正しくない理由は以下の通りです。
JSONサービスアカウントキーの使用は、キーのライフサイクル管理や機密性保持が難しく、Googleの推奨するプラクティスではありません。
代わりに、Workload Identityを使用してサービスアカウントを認証する方法が推奨されています。
選択肢：Googleサービスアカウント用の新しいJSONサービスアカウントキーを作成し、CI/CDツールのシークレット管理ストアにキーを保存し、認証にこのキーを使用するようにTerraformを設定します
この選択肢が正しくない理由は以下の通りです。
JSONサービスアカウントキーの使用はセキュリティ上のリスクを引き起こし、Googleが推奨するプラクティスを満たしません。
これに対して、適切なIAM権限を持たせてWorkload Identityを使用することで、セキュリティが強化され、Terraformデプロイの認証も確実に行えます。
選択肢：ポッドを実行するCompute Engine VMインスタンスに関連付けられたGoogleサービスアカウントに、適切なIAMパーミッションを割り当てます
この選択肢が正しくない理由は以下の通りです。
ポッドのIAM権限はCompute Engine VMインスタンスのサービスアカウントからではなく、Kubernetesサービスアカウントから派生すべきです。これはGoogleの推奨するプラクティスに従っていません。GKE上でのID管理にはWorkload Identityを使用すべきです。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
https://cloud.google.com/iam/docs/creating-managing-service-accounts
https://www.terraform.io/docs/providers/google/guides/provider_reference.html
</div></details>

### Q. 問題5: 未回答
あなたの会社では、Google Kubernetes Engine（GKE）を使用してサービスを実行しています。開発環境のGKEダスターは、冗長ロギングを有効にしてアプリケーションを実行します。開発者はkubectl logsコマンドを使用してログを表示し、Cloud Loggingは使用しません。アプリケーションには統一されたロギング構造が定義されていません。アプリケーションのロギングに関連するコストを最小限に抑えつつ、GKEの運用ログを収集する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？

1. severity >= DEBUG resource.type = "k8s_container"の除外フィルタを、開発環境に関連するプロジェクトの_Defaultロギングシンクに追加します
2. 開発クラスターに対して、gcloud container clusters update --logging=SYSTEMコマンドを実行します
3. 開発環境に関連付けられたプロジェクトで、gcloud logging sinks update _Default --disabledコマンドを実行します
4. 開発クラスターに対して、gcloud container clusters update --logging=WORKLOADコマンドを実行します
<details><div>
    答え：1
説明
この問題では、Google Kubernetes Engine（GKE）上で動作するアプリケーションのロギングに関連するコストを抑えつつ、運用ログを適切に収集する方法が求められています。この要件を満たすためには、ロギングの範囲やレベルを適切に制御する方法を選ぶ必要があります。試験問題文からは、kubectlを用いたログ表示とともにCloud Loggingは使用せず、またアプリケーションには統一されたロギング構造がないことが明らかにされています。これらの条件を踏まえると、ログ収集の範囲や集約度を調整するオプションが適切です。そのため、適切なログフィルタを設定したり、gcloudコマンドを用いたクラスターの更新などが考えられますので、それぞれの選択肢の意味を理解しながら解答する必要があります。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google CloudのフルマネージドなKubernetesサービスです。大規模なワークロードの管理を簡素化し、ワークロードの運用・管理を自動化します。
kubectl logsコマンド：KubernetesのCLIであるkubectlを使用して、ポッド内のコンテナのログを表示するコマンドです。
Cloud Logging：Google Cloudのログ管理サービスです。アプリケーションとシステムのログを一元化して保存、分析、可視化することができます。
_Defaultロギングシンク：Google Cloud Loggingで提供されるデフォルトの投稿先です。ログエントリが最初に送信される先を指します。
冗長ロギング：一つのイベントに対して2つ以上のログが生成される状態です。これは、重複した情報を提供するため、ストレージの消費が増加し、コストが上昇する可能性があります。
severityフィルタリング：ログエントリの重要度（severity）に基づいてフィルタリングを行う機能です。重要度が低いログを除外することでロギングのコストを抑えることができます。
gcloudコマンド：Google Cloud SDKのコマンドラインツールです。これを使用して、Google Cloudのリソースとアプリケーションを管理・操作することができます。
正解についての説明：
（選択肢）
・severity >= DEBUG resource.type = "k8s_container"の除外フィルタを、開発環境に関連するプロジェクトの_Defaultロギングシンクに追加します
この選択肢が正解の理由は以下の通りです。
GKEでは、_Defaultロギングシンクが自動的に各種ログをCloud Loggingに送信します。この運用ログの収集は、システムの運用や監視に必要ですが、アプリケーションログの冗長な収集は大量のストレージとコストを必要とします。開発チームはkubectl logsコマンドを使用したアプリケーションログの表示が可能であり、Cloud Loggingを必要としていないため、ログの保存費用を最小限にすることができます。そのため、無駄なコストの増大を防ぐために、"severity >= DEBUG resource.type = "k8s_container"の除外フィルタを_Defaultロギングシンクに追加することで、DEBUGレベル以上のアプリケーションログのCloud Loggingへの保存を防ぐことが可能となるため、この設定が最も適しています。
不正解についての説明：
選択肢：開発クラスターに対して、gcloud container clusters update --logging=SYSTEMコマンドを実行します
この選択肢が正しくない理由は以下の通りです。
gcloud container clusters update --logging=SYSTEMコマンドは、ログをCloud Loggingに送信しないでローカルに保存しますが、それを行うと開発者はkubectl logsコマンドでログを参照できなくなります。
また、このコマンドを使用しても冗長なロギングの発生を防ぐことはできません。これは、要件の最小限のコストとkubectl logsコマンドの利用を満たさないため不適切です。
選択肢：開発クラスターに対して、gcloud container clusters update --logging=WORKLOADコマンドを実行します
この選択肢が正しくない理由は以下の通りです。
gcloud container clusters update --logging=WORKLOADコマンドは既存のクラスターの構成を変更し、作業ログの収集を有効にしますが、すでに冗長なロギングが有効にされていると述べられているので、これは不必要です。
加えて、この選択肢は開発者がkubectl logsコマンドを使ってログを見るためには役立たず、またアプリケーションのロギングに関連するコストを最小化することもできません。
選択肢：開発環境に関連付けられたプロジェクトで、gcloud logging sinks update _Default --disabledコマンドを実行します
この選択肢が正しくない理由は以下の通りです。
gcloud logging sinks update _Default --disabledコマンドが実行されると、すべてのログが無効化されてしまいます。その結果、GKEの運用ログが収集されなくなるため、問題の要件を満たすことができません。正解の選択肢は特定のフィルタを適用し、必要なログのみを収集するので、不必要なコストを抑えることが可能です。
参考リンク：
https://cloud.google.com/logging/docs/exclusions
https://cloud.google.com/kubernetes-engine/docs/how-to/logging
https://cloud.google.com/logging/docs/basic-concepts#log_entries
</div></details>

### Q. 問題6: 未回答
あなたの会社はGoogle Kubernetes Engine（GKE）でアプリケーションを実行しています。いくつかのアプリケーションはエフェメラルボリュームに依存しています。ワーカーノードのDiskPressureノード状態が原因で、いくつかのアプリケーションが不安定であることに気づきました。問題の原因となっているポッドを特定する必要がありますが、ワークロードとノードへの実行アクセスがありません。
この要件を満たすために、どうすればよいですか？

1. Metrics Explorerを使用して、container/ephemeral_storage/used_bytesメトリクスを確認します
2. emptyDirボリュームのあるポッドをすべて探します。df --hコマンドを使ってボリュームのディスク使用量を測定します
3. emptyDirボリュームのあるポッドをすべて探します。df --sh * コマンドを使用してボリュームのディスク使用量を測定します
4. Metrics Explorerを使用して、node/ephemeral_storage/used_bytesメトリックを確認します
<details><div>
    答え：1
説明
この問題では、Google Kubernetes Engine（GKE）においてディスク逼迫による不安定なアプリケーションの問題を診断する方法が求められています。まず問題文から、問題の原因となるポッドとそのエフェメラルボリュームの使用状況を特定する必要があることを理解する必要があります。ただし、アクセス制限により直接的な確認はできないため、間接的な手法を用いる必要があります。正解や不正解の選択肢からは、Metrics Explorerを使用して各種メトリクスを確認する方法や特定のコマンドを用いてディスク使用量を調査する方法が示されています。これらの情報を踏まえて解答を選ぶことが重要です。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するマネージドKubernetesサービスです。クラスターのセットアップ、スケーリング、アップグレードなどを自動化します。
エフェメラルボリューム：Kubernetesにおける一時的なストレージ領域です。ポッドがノードにバインドされている間だけ存在し、データは持続的ではありません。
DiskPressureノード状態：Kubernetesノードにおけるディスク使用状況を監視するパラメーターです。これが高いと、アプリケーションのパフォーマンスに影響を与えることがあります。
Metrics Explorer：Google Cloud Monitoringの機能で、Google Cloudのプロジェクトに関するメトリクスを視覚的に調査し、分析するためのツールです。
container/ephemeral_storage/used_bytesメトリクス：Kubernetesにおける各コンテナが使用しているエフェメラルストレージの量を示すメトリクスです。
emptyDirボリューム：Kubernetesのポッド内でデータを共有するためのテンポラリストレージです。ポッドがノードから削除されると、その内容も削除されます。
正解についての説明：
（選択肢）
・Metrics Explorerを使用して、container/ephemeral_storage/used_bytesメトリクスを確認します
この選択肢が正解の理由は以下の通りです。
Google Cloud MonitoringのMetrics Explorerは、Google Cloud環境上のメトリクスを視覚的に観察・分析するためのツールです。特に、次のような詳細な情報を抽出し、フィルタリングやグループ化が行えます。つまり、特定のメトリクス（ここではcontainer/ephemeral_storage/used_bytes）を確認するために使用できます。
これは、エフェメラルストレージの使用状況を示しており、各ポッドがどれだけのエフェメラルストレージを使用しているかを示すものです。この情報により、DiskPressureの問題を引き起こしている可能性のあるポッドを特定することができます。
ワークロードやノードへの実行アクセスがなくても、Metrics Explorerを使えばセンシティブな操作なしにこの情報を取得できるので、この場合の問題解決に適しています。
不正解についての説明：
選択肢：Metrics Explorerを使用して、node/ephemeral_storage/used_bytesメトリックを確認します
この選択肢が正しくない理由は以下の通りです。
問題の特定は、ノード全体のエフェメラルストレージ使用情報ではなく、各コンテナのエフェメラルストレージ使用情報に基づいて行う必要があります。不正解の選択肢は、ノード全体のエフェメラルストレージ使用量をチェックするものであるため、問題のある特定のポッドを特定するのには適していません。
選択肢：emptyDirボリュームのあるポッドをすべて探します。df --hコマンドを使ってボリュームのディスク使用量を測定します
この選択肢が正しくない理由は以下の通りです。
問題文にはワークロードとノードへの直接的な実行アクセスがないとありますが、df --hコマンドを使うにはそれらへの直接的な実行アクセスが必要です。そのため、この選択肢は要件を満たしません。対して正解のMetrics Explorerを使用すれば、ノードのアクセスなしにディスク使用量を確認することができます。
選択肢：emptyDirボリュームのあるポッドをすべて探します。df --sh * コマンドを使用してボリュームのディスク使用量を測定します
この選択肢が正しくない理由は以下の通りです。
問題文にはワークロードやノードへの実行アクセスがないと述べられており、それはdfコマンドを具体的なポッド上で実行することは不可能であることを意味します。
対照的に、Metrics Explorerを使用すれば、コマンドラインアクセスなしで容量情報を取得することが可能となるため、正解の選択が妥当です。
参考リンク：
https://cloud.google.com/monitoring/metrics-explorer
https://cloud.google.com/kubernetes-engine/docs/how-to/monitoring#monitoring_metrics_with_cloud_monitoring
</div></details>

### Q. 問題7: 未回答
あなたはクライアントのために新しいGoogle Cloudの組織を設計しています。クライアントは、Google Cloudで作成された長期間のクレデンシャルに関連するリスクを懸念しています。あなたは、運用上のオーバーヘッドを最小限に抑えながら、JSONサービスアカウントキーの使用に関連するリスクを完全に排除するソリューションを設計する必要があります。
この要件を満たすために、どうすればよいですか？

1. 組織管理者のみにroles/iam.serviceAccountKeyAdmin IAMロールを付与します
2. constraints/iam.disableServiceAccountKevCreation制約を組織に適用します
3. 定義済みロールのカスタムバージョンを使用して、すべてのiam.serviceAccountKeys.*サービスアカウントロールパーミッションを除外します
4. constraints/iam.disableServiceAccountKeyUpload制約を組織に適用します
<details><div>
    答え：2
説明
この問題では、新しいGoogle Cloudの組織を設計する中で、JSON形式のサービスアカウントキー使用をリスクと見なし、そのリスクを無くす方法を設計することが求められています。同時に、運用上のオーバーヘッドを最小限に抑える必要も要求されています。したがって、この問題を解くためには、これらの要件を満たすためのIAMポリシーや制約の適用について理解することが重要で、選択肢間で細かな違いに注目しながらその影響を評価することが求められます。
基本的な概念や原則：
Service Account：Google Cloudのサービスを利用するエンティティで、特定の権限が付与されまます。これを利用して、アプリケーションやユーザーがGoogle Cloudのリソースにアクセスします。
JSON Service Account Key：Service Accountを認証するための暗号化キーで、JSONフォーマットで提供されます。キーが漏洩するとService Accountによる不正アクセスのリスクが発生します。
組織ポリシー制約：Google Cloudのあるリソース(tree, folder, project)で適用可能な特定のポリシーです。これを設定することによって、そのリソースに対するアクションを制約します。
constraints/iam.disableServiceAccountKeyCreation：Service Accountのキー作成を禁止する制約です。これを設定することで、新規にService Accountのキーを作成することを防ぎます。
IAMロール：Google Cloudリソースへのアクセスの許可を集めたものです。仕事のロールに基づいてユーザーに権限を付与するために使用します。ロールは1つ以上のパーミッションで構成されます。
カスタムIAMロール：プロジェクトや組織で定義したIAMロールのことで、必要に応じて特定のパーミッションを組み合わせて作成します。ただし、管理に手間がかかったり落とし穴があるため注意が必要です。
正解についての説明：
（選択肢）
・constraints/iam.disableServiceAccountKevCreation制約を組織に適用します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloudのプロジェクト設計でセキュリティを確保するためには、JSONサービスアカウントキーの利用リスクを制御することが重要です。JSONサービスアカウントキーはクレデンシャルとして使用され、不当に取得されるとシステムに大きなリスクをもたらす可能性があります。
ここで提案されている"constraints/iam.disableServiceAccountKeyCreation"という制約は、組織全体に新たなJSONサービスアカウントキーの作成を禁止するものであり、これによりリスクを排除することができます。この制約を適用することで、サービスアカウントキー作成が無効化され、長期間のクレデンシャルに関連するリスクを軽減することができます。
また、この制約を適用すれば、それぞれのプロジェクトやサービスで個別に設定を変更するという運用上のオーバーヘッドを大幅に減らすことが可能となり、効率的な運用が可能となります。
不正解についての説明：
選択肢：定義済みロールのカスタムバージョンを使用して、すべてのiam.serviceAccountKeys.*サービスアカウントロールパーミッションを除外します
この選択肢が正しくない理由は以下の通りです。
定義済みロールのカスタムバージョンを使用して特定のパーミッションを除外するだけでは、JSONサービスアカウントキーに関連するリスクを完全に排除することはできません。
また、制約を使用する方が操作のオーバーヘッドを最小限に抑えられます。
選択肢：constraints/iam.disableServiceAccountKeyUpload制約を組織に適用します
この選択肢が正しくない理由は以下の通りです。
まず、制約constraints/iam.disableServiceAccountKeyUploadは、既に作成されたキーのアップロードを制限しますが、新たなJSONサービスアカウントキーの作成を防ぐものではありません。これはクライアントが懸念しているリスクを排除するためには不適切です。対照的に正解の選択肢は、新規キー作成を無効化し、問題を根本的に解決します。
選択肢：組織管理者のみにroles/iam.serviceAccountKeyAdmin IAMロールを付与します
この選択肢が正しくない理由は以下の通りです。
組織管理者のみにroles/iam.serviceAccountKeyAdmin IAMロールを付与するだけでは、JSONサービスアカウントキーの作成を完全に防ぐことができません。新たな管理者が登場した場合や権限が変わった場合等、インシデンとを引き起こす可能性がまだ残ってしまいます。
逆に、制約を組織全体に適用することで、管理者のロールにかかわらずサービスアカウントキーの作成を無効にする事が出来ます。
参考リンク：
https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints
https://cloud.google.com/identity/docs/how-to/setup#auth
https://cloud.google.com/iam/docs/understanding-service-accounts
</div></details>

### Q. 問題8: 未回答
Compute Engine上でアプリケーションサーバーをプールしています。最小限の構成で済み、開発者がトラブルシューティングのためにアプリケーションログに簡単にアクセスできる、安全なソリューションを提供する必要があります。
Google Cloud上でどのようにソリューションを実装しますか？

1. アプリケーションサーバにGoogle Cloud Operation Suiteロギングエージェントをデプロイします。開発者に、Google Cloud Operation Suiteにアクセスしてログを表示するIAM Logs Viewerロールを付与します
2. アプリケーションサーバーにgsutilコマンドラインツールをインストールします。アプリケーションログをCloud Storageバケットにアップロードするスクリプトをgsutilを使って書き、cronで5分ごとに実行するようにスケジュールします。開発者にIAM Object Viewerのアクセス権を付与し、指定したバケット内のログを閲覧できるようにします
3. アプリケーションサーバにGoogle Cloud Operation Suiteモニタリングエージェントをデプロイします。開発者に、Google Cloud Operation Suiteにアクセスしてメトリクスを表示するIAM Monitoring Viewerロールを付与します
4. アプリケーションサーバにGoogle Cloud Operation Suiteロギングエージェントをデプロイします。開発者に、Google Cloud Operation Suiteにアクセスしてログを表示するためのIAM Logs Private Logs Viewerロールを付与します
<details><div>
    答え：1
説明
この問題では、Compute Engine上でプールされているアプリケーションサーバーのログを、可能な限りシンプルなセットアップで、開発者がトラブルシューティングのために安全にアクセスできる方法が求められています。設問の形状から、Google Cloud内の適切なツールやロールを置くことで問題に対する最適な解決策を選択することが重要となります。それぞれの選択肢のデプロイやロール付与について理解すると同時に、セキュリティ維持と最小限の構成を保つことの重要性を考慮することが求められています。
基本的な概念や原則：
Compute Engine：Google Cloudの仮想マシンを提供するサービスです。あらかじめ定義された機械タイプから選択するか、カスタムマシンタイプを作成して、必要なリソースに合わせてインスタンスを設定できます。
Google Cloud Operation Suite：Google Cloudの監視、トラブルシューティング、診断、アラート作成などの機能を提供する統合型ツールスイートです。
ロギングエージェント：ログを収集し、指定されたストレージ先に転送するソフトウェアです。Google Cloud Operation Suiteの一部として、アプリケーションログを自動的にGoogle Cloudのロギングサービスに送信します。
IAM Logs Viewerロール：Google Cloud IAM（Identity and Access Management）のロールの一つで、ユーザーにログを表示する権限を与えます。
モニタリングエージェント：システムやアプリケーションの動作を監視し、異常を検出するソフトウェアです。Google Cloud Operation Suiteの一部として働きます。
gsutil：Google Cloud Storageをコマンドラインから操作するためのツールです。バケットの作成や削除、オブジェクトのアップロードやダウンロードなど、多くの操作をサポートしています。
IAM Object Viewerロール：Google Cloud IAM（Identity and Access Management）のロールの一つで、ユーザーにオブジェクトの閲覧権限を与えます。
正解についての説明：
（選択肢）
・アプリケーションサーバにGoogle Cloud Operation Suiteロギングエージェントをデプロイします。開発者に、Google Cloud Operation Suiteにアクセスしてログを表示するIAM Logs Viewerロールを付与します
この選択肢が正解の理由は以下の通りです。
Google Cloud Operation SuiteはGoogle Cloudの一部であり、アプリケーションログを一元的に管理し、追跡できるため、開発者はこのツールを使用してアプリケーションのログに簡単にアクセスできます。Google Cloud Operation Suiteにはロギングエージェントが含まれています。アプリケーションサーバにこのロギングエージェントをデプロイすれば、アプリケーションログは自動的にGoogle Cloud Operation Suiteに転送され、一元化された場所でアクセス可能となります。この選択肢がさらに優れている点は、アクセス制御をIAMロールを通じて制御することができることです。開発者にIAM Logs Viewerロールを付与することで、開発者はログを表示するためだけの最低限の権限を持つことになり、サーバーやその他のリソースへの不適切なアクセスリスクが軽減されます。以上の理由により、この選択肢は最小限の構成で開発者がログに簡単にアクセスできる、そして安全なソリューションを提供するために適しています。
不正解についての説明：
選択肢：アプリケーションサーバにGoogle Cloud Operation Suiteロギングエージェントをデプロイします。開発者に、Google Cloud Operation Suiteにアクセスしてログを表示するためのIAM Logs Private Logs Viewerロールを付与します
この選択肢が正しくない理由は以下の通りです。
Google Cloudには"IAM Logs Private Logs Viewer"ロールというものは存在しません。
したがって、この選択肢は誤りです。正解の選択肢は開発者に"IAM Logs Viewer"ロールを付与することで、これにより開発者はアプリケーションのログを閲覧することができます。
選択肢：アプリケーションサーバにGoogle Cloud Operation Suiteモニタリングエージェントをデプロイします。開発者に、Google Cloud Operation Suiteにアクセスしてメトリクスを表示するIAM Monitoring Viewerロールを付与します
この選択肢が正しくない理由は以下の通りです。
課題はアプリケーションログへのアクセスですが、モニタリングエージェントはメトリクスを収集します、ログを収集するものではありません。
また、イベントの記録及び問題解析のためには、IAM Logs Viewerロールを使ってCloud Operation Suiteのログを開発者が見ることが必要です。
選択肢：アプリケーションサーバーにgsutilコマンドラインツールをインストールします。アプリケーションログをCloud Storageバケットにアップロードするスクリプトをgsutilを使って書き、cronで5分ごとに実行するようにスケジュールします。開発者にIAM Object Viewerのアクセス権を付与し、指定したバケット内のログを閲覧できるようにします
この選択肢が正しくない理由は以下の通りです。
gsutilとcronを使用してログをCloud Storageに定期的に移動することは、冗長で効率的でなく、またトラブルシューティングのためにリアルタイムのログが必要な場合、適切でない可能性があります。
対照的に、Cloud Operation Suiteロギングエージェントはリアルタイムのログを提供し、管理が簡単であり、開発者のトラブルシューティングにより適しています。
参考リンク：
https://cloud.google.com/logging/docs/agent
https://cloud.google.com/iam/docs/understanding-roles#logging-roles
https://cloud.google.com/logging/docs/view/overview
</div></details>

### Q. 問題9: 未回答
あなたのチームは、Google Cloudの内外にデプロイする新しいアプリケーションを設計しています。システムリソースの使用率などの詳細なメトリクスを収集する必要があります。この収集システムのセットアップに必要な作業量を最小限に抑えながら、一元化されたGoogle Cloudサービスを使用したいと考えています。
この要件を満たすために、どうすればよいですか？

1. Google Cloud Operation Suite Debuggerパッケージをインポートし、タイミング情報を含むデバッグメッセージを出力するようにアプリケーションを設定します
2. 両方のロケーションにアプリケーションパフォーマンスモニタリング（APM）ツールをインストールし、分析のために中央データストレージロケーションへのエクスポートを構成します
3. Google Cloud Operation Suite Profilerパッケージをインポートし、さらなる解析のためにGoogle Cloud Operation Suiteに関数のタイミングデータをリレーするように設定します
4. タイミングライブラリを使ってコードを計測し、Google Cloud Operation Suiteによってスクレイピングされるヘルスチェックエンドポイント経由でメトリクスを公開します
<details><div>
    答え：3
説明
この問題では、Google Cloudと他のプラットフォームにデプロイする新しいアプリケーションで詳細なメトリクスを収集し、それらメトリクスの設定に必要な作業量を最小限に抑えるための戦略が求められています。システムのメトリクスの収集のためのツールやサービスを選択する際のポイントとしては、その手軽さ、一元化への対応力、そしてGoogle Cloudとの互換性が考慮すべき特徴となります。またGoogle Cloud内外両方の環境で使用できることも求められています。そこで選択肢は、適切なGoogle Cloud Operation Suiteのパッケージかツール選択に関するものとなります。
基本的な概念や原則：
Google Cloud Operation Suite：Google Cloudの監視、トラブルシューティング、アプリケーションのパフォーマンス分析を行うための統合されたツールセットです。このツールを使用すると、システムに関する詳細な情報を容易に収集できます。
Profilerパッケージ：Google Cloud Operation Suiteの一部で、実行中のアプリケーションからのパフォーマンス情報を収集します。詳細なプロファイリングデータを提供し、ボトルネックの解析などに使用できます。
Debuggerパッケージ：Google Cloud Operation Suiteの一部のデバッグツールです。アプリケーションの動作を詳細に追跡し、エラーや不具合の原因を特定します。
タイミングライブラリ：コードの実行時間を計測するためのツールやライブラリです。これを使って特定の処理のパフォーマンスを測定することが可能です。
ヘルスチェック：システムの健全性をモニタリングするためのテストです。Google Cloud Operation Suiteはヘルスチェックの結果を監視し、問題があればアラートを出します。
アプリケーションパフォーマンスモニタリング（APM）ツール：アプリケーションのパフォーマンスを継続的に監視し、問題を検出するためのツールです。実行時間、エラーレートなどのパフォーマンス指標を収集します。
正解についての説明：
（選択肢）
・Google Cloud Operation Suite Profilerパッケージをインポートし、さらなる解析のためにGoogle Cloud Operation Suiteに関数のタイミングデータをリレーするように設定します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Operation Suite（旧Google Cloud Operation Suite）は、Google Cloudの内外で動作するアプリケーションの監視、トラブルシューティング、アラートのためのパワフルな監視ツールです。ここで提案されているCloud Operations Profilerは、Operations Suiteの一部であり、実行中のアプリケーションをプロファイリングし、関数のタイミングやシステムリソースの利用状況などの詳細なメトリクスを収集し分析します。この情報をCloud Operation Suiteにリレーすることで、すべての情報を一元化し、パフォーマンス分析や問題解決を容易にします。
また、Profilerパッケージをインポートするだけで、アプリケーションにプロファイリング機能を追加できるため、セットアップに必要な作業量を最小限に抑えることができます。これらの理由から、この選択肢が正解となります。
不正解についての説明：
選択肢：Google Cloud Operation Suite Debuggerパッケージをインポートし、タイミング情報を含むデバッグメッセージを出力するようにアプリケーションを設定します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Debuggerパッケージは、主にアプリケーションの不具合を解析する仕組みで、詳細なメトリクスやリソース使用状況の収集を主な目的としていません。設問の要件は詳細なメトリクスの収集と一元化された管理を求めているため、Cloud Operation Suite Profilerパッケージの使用が適切です。
選択肢：タイミングライブラリを使ってコードを計測し、Google Cloud Operation Suiteによってスクレイピングされるヘルスチェックエンドポイント経由でメトリクスを公開します
この選択肢が正しくない理由は以下の通りです。
タイミングライブラリを使用してコードを計測すると、セットアップに必要な作業量が増えます。
一方、Google Cloud Operation Suite Profilerパッケージをインポートする方が作業量を最小限に抑えながら詳細なメトリクスを取得できます。
選択肢：両方のロケーションにアプリケーションパフォーマンスモニタリング（APM）ツールをインストールし、分析のために中央データストレージロケーションへのエクスポートを構成します
この選択肢が正しくない理由は以下の通りです。
APMツールを両方のロケーションにインストールする方法は、セットアップに必要な作業量を増大させます。
そして、中央データストレージロケーションへのエクスポートを構成しなければならない点も面倒な作業となります。
それに対し、Google Cloud Operation Suite Profilerパッケージを利用することで作業量が大幅に削減できます。
参考リンク：
https://cloud.google.com/profiler/docs/profiling-external
https://cloud.google.com/profiler/docs
https://cloud.google.com/monitoring/docs
</div></details>

### Q. 問題10: 未回答
Google Kubernetes Engine（GKE）クラスター全体にいくつかの制約テンプレートを適用する必要があります。制約には、Kubernetes APIを制限するなどのポリシーパラメータが含まれます。ポリシーパラメータがGitHubリポジトリに保存され、変更が発生したときに自動的に適用されるようにする必要があります。
この要件を満たすために、どうすればよいですか？

1. GitHubに変更があった場合、Webフックを使ってAnthos Service Meshにリクエストを送信し、変更を適用します
2. GitHubリポジトリでConfig Connectorを設定します。リポジトリに変更があった場合、Config Connectorを使用して変更を適用します
3. Anthos Config ManagementをGitHubリポジトリに設定します。リポジトリに変更があった場合、Anthos Config Managementを使って変更を適用します
4. パラメータが変更されたときにCloud BuildをトリガーするGitHubアクションを設定します。Cloud Buildで、gcloud CLIコマンドを実行して変更を適用します
<details><div>
    答え：3
説明
この問題では、Google Kubernetes Engine（GKE）クラスター全体に制約テンプレートを適用し、その設定がGitHubリポジトリで管理されている状況を想定しています。GitHubリポジトリの変更内容が自動的に反映される仕組みを構築するための適切なGoogle Cloudのサービスを選択する必要があります。GitHubリポジトリの変更発生時に自動的にポリシーを適用する方法について、これらの情報から適切な選択肢を考えることが求められています。Google Cloudの各サービスの特性や機能を理解し、それらが問題の要件や目的にどのように対応するのかを検討することが重要です。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するKubernetesクラスター管理サービスです。Kubernetesの実行環境を簡単に作成、管理できます。
制約テンプレート：ポリシーや制約を設定するためのテンプレートです。Kubernetes APIの制限など、特定のポリシーパラメータを含むことができます。
Anthos Config Management：Anthosの一部で、gitリポジトリに保存された設定をクラスター全体に適用する機能を提供します。
GitHubリポジトリ：ソークスコードや設定などを格納しているGitHubの保存領域です。コードの変更履歴を管理することができます。
Cloud Build：Google Cloudのフルマネージド型CI/CDプラットフォームです。ソースコードの変更を自動的に検出し、ビルド、テスト、デプロイを自動化することができます。
gcloud CLIコマンド：Google Cloudのコマンドラインツールです。このツールを使用して、Google Cloudのリソースを作成、管理することができます。
Config Connector：Google CloudのリソースをKubernetesのカスタムリソースとして管理するためのアドオンです。Google Cloudリソースの設定をKubernetesのマニフェスト形式で扱うことができます。
正解についての説明：
（選択肢）
・Anthos Config ManagementをGitHubリポジトリに設定します。リポジトリに変更があった場合、Anthos Config Managementを使って変更を適用します
この選択肢が正解の理由は以下の通りです。
まず、Anthos Config ManagementはGoogle Cloudが提供するサービスの一つで、一元化された制御、セキュリティ、シンプルさを提供することを目的としています。その一部として、Kubernetesのカスタムリソース定義（CRD）を使用して、クラスター間での設定の一貫性を維持します。
具体的には、Anthos Config ManagementはGitリポジトリを使用してKubernetesの設定を管理します。すなわち、ポリシーパラメータのような設定をGitHubリポジトリに保存し、それらの設定を直接キューブクラスターに適用することが可能です。
また、GitHubリポジトリの内容が変更されると、それがAnthos Config Managementによって自動的に検出され、適用されます。これにより、GitHubリポジトリとクラスターの設定との間に一貫性を持たせることができます。
以上の理由から、Anthos Config ManagementをGitHubリポジトリに設定することは、要件を満たす最適な解決策であると言えます。
不正解についての説明：
選択肢：パラメータが変更されたときにCloud BuildをトリガーするGitHubアクションを設定します。Cloud Buildで、gcloud CLIコマンドを実行して変更を適用します
この選択肢が正しくない理由は以下の通りです。
まず、Cloud Buildとgcloud CLIを用いたこの方法では、GitHubリポジトリに制約テンプレートの変更があるたびに手動でトリガーを設定し、適用対象のGKEクラスターを直接指定する必要があります。これに対してAnthos Config Managementは、一度設定することで自動的に変更を検知し適用します。そのため、制約テンプレートの自動適用という要件をより効率的に満たします。
選択肢：GitHubに変更があった場合、Webフックを使ってAnthos Service Meshにリクエストを送信し、変更を適用します
この選択肢が正しくない理由は以下の通りです。
Anthos Service Meshはサービス間の通信を管理し、制御することに特化したツールですが、制約テンプレートやポリシーパラメータの自動適用に関しては特に対応していません。
これに対して、Anthos Config Managementはポリシーの一元管理や自動適用に適したツールであり、正解の選択肢です。
選択肢：GitHubリポジトリでConfig Connectorを設定します。リポジトリに変更があった場合、Config Connectorを使用して変更を適用します
この選択肢が正しくない理由は以下の通りです。
Config ConnectorはKubernetesのリソースモデルを使用してGoogle Cloudリソースを管理しますが、Kubernetes APIを制限するなどの制約テンプレートの適用はサポートしていません。そのため、この要件を満たすAnthos Config Managementとは異なる動作を提供します。
参考リンク：
https://cloud.google.com/anthos-config-management/docs
https://cloud.google.com/kubernetes-engine/docs/concepts/policy-controller
https://github.com/kubernetes-sigs/kustomize
</div></details>

### Q. 問題11: 未回答
あなたは会社のマルチクラウド環境でアプリケーションのCI/CDパイプラインを実装しています。アプリケーションは、カスタムのCompute Engineイメージと他のクラウドプロバイダーの同等のものを使用してデプロイされます。現在の環境にイメージをビルドしてデプロイでき、将来の変更にも対応できるソリューションを実装する必要があります。どのソリューションスタックを使用すべきですか？

1. kptによるCloud Build
2. PackerによるCloud Build
3. Google Cloud DeployによるGoogle Kubernetes Engine
4. Google Cloud DeployによるCloud Build
<details><div>
    答え：2
説明
この問題では、マルチクラウド環境でのCI/CDパイプライン実装に焦点を当てています。選択肢を考える際の重要な要点は、アプリケーションがカスタムのCompute Engineイメージと他のクラウドプロバイダーの同等のものを使用してデプロイされていることです。このため、画像のビルドとデプロイに対応し、同時に将来の変更にも対応可能なソリューションを選択する必要があります。これは、複数のクラウド環境での操作と互換性をサポートするツールを特定するための鍵となります。
基本的な概念や原則：
Packer：HashiCorpが開発したオープンソースのツールで、さまざまなプラットフォームやクラウドサービスに対応したイメージを自動でビルドできます。マルチクラウド環境での利用が可能です。
Cloud Build：Google Cloudのフルマネージド型のCI/CDプラットフォームで、ソースコードからコンテナイメージやアプリケーションをビルドし、テストし、デプロイすることができます。
Compute Engineイメージ：Google Cloud上で仮想マシンを作成するときに使用するディスクイメージです。Compute Engineイメージを使ってアプリケーションをデプロイすることも可能です。
Google Cloud Deploy：Google Cloudのマネージドサービスで、安全で高速なソフトウェアデプロイを可能にします。
Google Kubernetes Engine（GKE）：Google CloudのフルマネージドKubernetesサービスで、コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を行うことができます。
kpt：Google Cloudのオープンソースツールで、Kubernetesアプリケーションのパッケージングと設定を管理します。
正解についての説明：
（選択肢）
・PackerによるCloud Build
この選択肢が正解の理由は以下の通りです。
まず、PackerはHashiCorpが提供するオープンソースのツールで、さまざまなプラットフォーム（例えば、Amazon EC2、VMware、Dockerなど）のための一貫した環境の構築を自動化する機能があります。これは、カスタムのCompute Engineイメージ、そして対象となるマルチクラウド環境の他クラウド上のイメージを作成する際に大いに役立ちます。
次に、Cloud BuildはGoogle Cloudのフルマネージド型のCI/CD（Continuous Integration & Continuous Deployment）プラットフォームで、ソースコードからコンテナイメージやアプリケーションのアーティファクトをビルドしたりテストしたりするために利用されます。Cloud BuildとPackerを組み合わせれば、イメージをビルドしデプロイするプロセスやそのアップデートに対応するCI/CDパイプラインを効率的に実装でき、マルチクラウド環境でのアプリケーションの配備や管理を効率化し、自動化することが可能となります。
不正解についての説明：
選択肢：Google Cloud DeployによるCloud Build
この選択肢が正しくない理由は以下の通りです。
Google Cloud Deployは、Google Cloud上でのデプロイメントの自動化を提供しますが、マルチクラウド環境のアプリケーションデプロイには適用できません。
一方、Packerを用いたCloud Buildは、多様なクラウド環境でのイメージのビルドとデプロイが可能で、この要件に適合します。
選択肢：Google Cloud DeployによるGoogle Kubernetes Engine
この選択肢が正しくない理由は以下の通りです。
Google Cloud DeployによるGoogle Kubernetes Engineは、デプロイメントの自動化を実現しますが、特定のCompute Engineイメージや他のクラウドプロバイダーのイメージを生成する能力がありません。
一方、PackerによるCloud Buildは、任意のイメージを自動的にビルドし、マルチクラウド環境でのデプロイを容易にするため、問題の要件を満たします。
選択肢：kptによるCloud Build
この選択肢が正しくない理由は以下の通りです。
kptはYAMLファイルのパッケージングとカスタマイゼーションを行うツールであり、Compute Engineのイメージのビルドやデプロイには対応しておりません。
一方、Packerは複数のプラットフォームで使用できるイメージを作成し、Cloud Buildと組み合わせることでCI/CDパイプラインを構築するのに適しています。
参考リンク：
https://cloud.google.com/build/docs/building/build-vm-images-using-packer
https://cloud.google.com/build
https://www.packer.io/docs/builders/googlecompute
</div></details>

### Q. 問題12: 未回答
アプリケーションコンテナイメージをビルドするために、Cloud BuildでCI/CDパイプラインを作成しています。アプリケーションコードはGitHubに保存されています。あなたの会社では、本番イメージビルドはメインブランチに対してのみ実行され、変更管理チームはメインブランチへのすべてのプッシュを承認する必要があります。イメージのビルドはできるだけ自動化したいと考えています。
この要件を満たすために、どうすればよいですか？（2つ選択）

1. リポジトリ上のメインブランチにブランチ保護ルールを設定します
2. トリガーのIncluded filesフィルターにOWNERSファイルを追加します
3. Cloud Buildジョブにトリガーを作成します。リポジトリイベントの設定を"ブランチにプッシュ"に設定します
4. トリガーの承認オプションを有効にします
5. Cloud Buildジョブにトリガーを作成します。リポジトリイベント設定を"Pull request"に設定します
<details><div>
    答え：1,3
説明
この問題では、ユーザーがCI/CDパイプラインを設定し、特定のルールに従ってイメージのビルドを制御したいという要件を明らかにしています。具体的には、本番イメージビルドをメインブランチに限定し、メインブランチへの全てのプッシュが変更管理チームによって承認される必要があるという条件が述べられています。これらの要件を満たすために適切な選択肢が2つ求められています。選択肢を評価するときには、これらの特定の要件を満たす選択肢を適切に識別することが試験のポイントとなります。
基本的な概念や原則：
Cloud Build：ソースコードからソフトウェアアーティファクトをビルドするためのGoogle Cloudのサービスです。CI/CDパイプラインの自動化に利用します。
トリガー：Cloud Buildを特定のイベント（例えば、リポジトリ上のブランチへのプッシュ）に対して自動的に反応させるための設定です。
リポジトリ：ソースコードやその他のデータを保存し、履歴管理や共有を行うためのシステムです。GitHubはその一例です。
ブランチ保護ルール：Gitリポジトリへの変更を制限するための設定です。例えば、あるブランチへのプッシュを特定のユーザーやチームだけに制限することができます。
GitHub：ソフトウェア開発プロジェクトのためのホスティングプラットフォームです。Gitを用いてソースコードのバージョン管理を行います。
正解についての説明：
（選択肢）
・Cloud Buildジョブにトリガーを作成します。リポジトリイベントの設定を"ブランチにプッシュ"に設定します
・リポジトリ上のメインブランチにブランチ保護ルールを設定します
この選択肢が正解の理由は以下の通りです。
まず、Cloud Buildジョブにトリガーを作成し、リポジトリイベントの設定を"ブランチにプッシュ"に設定することで、メインブランチに対する変更があるたびに自動的にビルドがトリガーされます。これによりイメージのビルドを効率的に自動化し、メインブランチに対してのみ本番イメージビルドを実行するという要件を満たします。
また、リポジトリ上のメインブランチにブランチ保護ルールを設定することで、変更管理チームがメインブランチへの全てのプッシュを承認する必要がなくなります。具体的には、ブランチ保護ルールを使用すれば、特定のブランチに対する直接のコミットやプッシュを制限したり、プルリクエストの承認やステータスチェックの成功などの条件を満たさなければマージができないように設定できます。これにより、不適切な変更がメインブランチに適用されることを防ぐことができます。この2つの手段を組み合わせることで、上記の要件を満たすことができます。
不正解についての説明：
選択肢：Cloud Buildジョブにトリガーを作成します。リポジトリイベント設定を"Pull request"に設定します
この選択肢が正しくない理由は以下の通りです。
"Pull request"に設定されたトリガーはプルリクエストの作成時にイメージのビルドが開始しますが、要件はメインブランチへのプッシュ時のみビルドを行うことです。そのため、リポジトリイベントの設定を"ブランチにプッシュ"に設定するのが正しいです。
選択肢：トリガーのIncluded filesフィルターにOWNERSファイルを追加します
この選択肢が正しくない理由は以下の通りです。
OWNERSファイルをIncluded filesフィルターに追加することで、特定のファイルの変更にトリガーを絞ることができます。しかし、変更管理チームがメインブランチへの全てのプッシュを承認するという要件には関連性がありません。
また、この方法ではビルドの自動化という目的にも寄与しません。正解の選択肢では、特定のブランチへのプッシュをトリガーに設定し、保護ルールで承認制を加えます。
選択肢：トリガーの承認オプションを有効にします
この選択肢が正しくない理由は以下の通りです。
Cloud Buildには"トリガーの承認オプション"を有効にする機能が存在しません。代わりにブランチ保護ルールを設定することで、メインブランチへのプッシュが変更管理チームによる承認を経てから行われるように制御することが可能です。
参考リンク：
https://cloud.google.com/build/docs/automating-builds/create-manage-triggers
https://cloud.google.com/build/docs/securing-builds/configure-access-control
https://docs.github.com/en/github/administering-a-repository/managing-branch-protection-rules
</div></details>

### Q. 問題13: 未回答
あなたの会社は、すべての組織のログを7年間保存することを要求する、高度に規制された産業に属しています。マネージドサービスを利用することで、ロギングインフラの複雑さを最小限に抑えたいと考えています。将来、設定ミスや人為的ミスによるログの取得や保存されたログの損失を回避する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？

1. Cloud Loggingを使用して、組織レベルで集約されたシンクを構成し、7年間の保持ポリシーとバケットロックですべてのログをCloud Storageにエクスポートします
2. Cloud Loggingを使用して、各プロジェクトレベルでエクスポートシンクを構成し、すべてのログをBigQueryデータセットにエクスポートします
3. Cloud Loggingを使用して、組織レベルで集約シンクを構成し、すべてのログをBigQueryデータセットにエクスポートします
4. Cloud Loggingを使用して、各プロジェクトレベルでエクスポートシンクを構成し、7年間の保持ポリシーとバケットロックですべてのログをCloud Storageにエクスポートします
<details><div>
    答え：1
説明
この問題では、高度に規制された産業に属する会社がマネージドサービスを使用して、複雑なログインフラを最小限に抑えつつ、適切にロギングを行うための最善の方法が問われています。ロギングの要件として、ログは7年間保存され、設定ミスや人為的ミスによるログの損失を防ぐ必要がある点に注目すると良いでしょう。これらを踏まえると、ログの収集や保存、エクスポートの方法、そしてダウンタイムやミスの防止策などが選択肢を選ぶ観点となります。適切な保存場所、保持ポリシー、防止策を選択することがポイントです。
基本的な概念や原則：
Cloud Logging：Google Cloudのログ管理サービスです。アプリケーションとシステムのログを一元的に収集、分析、ストレージという一連のワークフローを可能にします。
組織レベルでの集約シンク：組織全体のログを一つの場所に集約する設定を指します。これにより、ログの管理を簡素化し、全体的な可視性を向上させます。
Cloud Storage：Google Cloudのオブジェクトストレージサービスです。大量のデータを格納、分析、取得するためのサービスで、保持ポリシーとバケットロック機能を用いて長期間のログ保持が可能です。
保持ポリシー：指定した期間データを保存することを指定する設定を意味します。特定の業界規制や企業方針に準拠するために使用されます。
バケットロック：Cloud Storageの機能で、データの不変性を保証します。設定された保持期間中、バケット内のデータは変更や削除ができない様になります。
BigQuery：Google Cloudの大規模データ分析ツールです。ただし、一部の規制を満たすためにはCloud Storageを使用する必要がある場合もあります。
プロジェクトレベルでのエクスポートシンク：各プロジェクト毎のログを対象とする設定を指します。組織レベルでの設定に比べて、余計な設定や管理が増える可能性があります。
正解についての説明：
（選択肢）
・Cloud Loggingを使用して、組織レベルで集約されたシンクを構成し、7年間の保持ポリシーとバケットロックですべてのログをCloud Storageにエクスポートします
この選択肢が正解の理由は以下の通りです。
まず、Cloud LoggingはGoogle Cloudのマネージドサービスであり、アプリケーションとシステムのログを保管し分析するためのツールです。このサービスを使用することで、インフラの複雑さを大いに減らすことができます。
ここでは、組織レベルでのシンクの設定が求められています。シンクは、特定のログエントリをフィルタリングして他のGoogle Cloudサービスにエクスポートするための機能です。組織レベルでシンクを設定することによって、すべての組織のログを包括的に管理できます。
次に、ログをCloud Storageにエクスポートするというアプローチも 制約に適しています。Cloud Storageは大量のデータを安全に、長期間保存するためのサービスであるため、7年間の保持要件を満たすことができます。
最後に、バケットロックはオブジェクトの削除を防ぐ保護機能で、設定ミスや人為的なミスからデータを保護することができます。これにより、一度保存されたログが未然に消失することを防ぐことができます。
DevOps 向けに Google Cloud 組織をブートストラップする
選択肢：Cloud Loggingを使用して、組織レベルで集約シンクを構成し、すべてのログをBigQueryデータセットにエクスポートします
この選択肢が正しくない理由は以下の通りです。
 
BigQueryデータセットは、長期間のログ保存に最適ではありません。BigQueryは主に分析クエリ実行用のツールであり、それによって発生するコストは7年間のログ保持に対しては配慮すべき要素です。Cloud Storageの方が長期保管に適しており、バケットロック機能も利用できます。
 
選択肢：Cloud Loggingを使用して、各プロジェクトレベルでエクスポートシンクを構成し、すべてのログをBigQueryデータセットにエクスポートします
この選択肢が正しくない理由は以下の通りです。
 
BigQueryデータセットへのエクスポートではなく、Cloud Storageへのエクスポートを選択する理由は、Cloud Storageがバケットロックという機能を提供していて、これにより設定ミスや人為的ミスによるログの損失を防ぐことが可能だからです。
 
また、各プロジェクトレベルでシンクを設定するよりも、組織全体で一度にロギング設定を行う方が効率的です。
 
選択肢：Cloud Loggingを使用して、各プロジェクトレベルでエクスポートシンクを構成し、7年間の保持ポリシーとバケットロックですべてのログをCloud Storageにエクスポートします
この選択肢が正しくない理由は以下の通りです。
 
プロジェクトレベルでのエクスポートシンク構成では、一部のプロジェクトで設定ミスが発生しやすく、全組織のログを完全にカバーするのが困難です。
 
また、プロジェクト数が多い場合、管理が煩雑になる可能性があります。反対に組織レベルでシンクを構成すると、一元管理が可能で、全組織のログが適切に保存されることを担保できます。
 
参考リンク：
https://cloud.google.com/logging/docs/storage
 
https://cloud.google.com/storage/docs/using-bucket-lock
 
https://cloud.google.com/logging/docs/aggregated-exports
 
</div></details>

### Q. 問題14: 未回答
Google Cloudでコンテナ化されたアプリケーションのCI/CDパイプラインを構築する必要があります。開発チームは、トランクベースの開発のために中央のGitリポジトリを使用しています。あなたは、品質を向上させるために、アプリケーションの新しいバージョンに対してパイプラインですべてのテストを実行したいと考えています。
この要件を満たすために、どうすればよいですか？

1. 
    1.Cloud Buildをトリガーしてアプリケーションコンテナをビルドし、コンテナでユニットテストを実行します
    2.単体テストが成功したら、アプリケーションコンテナをテスト環境にデプロイし、統合テストを実行します
    3.統合テストが成功したら、パイプラインはアプリケーションコンテナを本番環境にデプロイします。その後、受け入れテストを実行します
2. 
    1.コードを中央リポジトリにプッシュする前にユニットテストを実行するよう、開発者に要求するGitフックをインストールします
    2.Cloud Buildをトリガーしてアプリケーションコンテナをビルドします。アプリケーションコンテナをテスト環境にデプロイし、統合テストを実行します
    3.統合テストが成功したら、アプリケーションコンテナを本番環境にデプロイし、受け入れテストを実行します
3. 
    1.Gitフックをインストールして、開発者が中央リポジトリにコードをプッシュする前にユニットテストを実行するようにします。すべてのテストが成功したら、コンテナをビルドします
    2.Cloud Buildをトリガーして、アプリケーションコンテナをテスト環境にデプロイし、統合テストと受け入れテストを実行します
    3.すべてのテストに成功したら、コードを本番環境としてタグ付けします。Cloud Buildをトリガーして、アプリケーションコンテナをビルドし、本番環境にデプロイします
4. 
    1.コードがプッシュされると、Cloud Buildをトリガーしてユニットテストを実行します。すべてのユニットテストが成功したら、アプリケーションコンテナをビルドして中央レジストリにプッシュします
    2.Cloud Buildをトリガーして、コンテナをテスト環境にデプロイし、統合テストと受け入れテストを実行します
    3.すべてのテストが成功した場合、パイプラインはアプリケーションを本番環境にデプロイし、スモークテストを実行します
<details><div>
    答え：4
説明
この問題では、トランクベースの開発を行っている開発チームのために品質向上を目指したCI/CDパイプラインを構築する方法を選びます。具体的には、アプリケーションの新しいバージョンに対して、パイプライン内で全てのテストを実行したいとの要望があるので、アプリケーションの変更を持ち込む際にテストを組み込むことで品質を確保するシナリオを選ばなければなりません。この要求を満たすためには、Google CloudのCI/CDツールに対する理解と、効率的なテストとデプロイのフローについての知識が必要です。
基本的な概念や原則：
Cloud Build：Google Cloudの完全マネージド型のビルドサービスです。ソースコードからコンテナイメージやアプリケーションのアーティファクトをビルドし、テストすることができます。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）を組み合わせた開発プロセスです。コードの変更を自動的にビルド、テスト、デプロイすることで、高速かつ安定したソフトウェアリリースを支援します。
ユニットテスト：コードの一部（ユニット）が期待通りに動作するかを確認するテストです。全体の機能を保証するための基礎となります。
統合テスト：複数のユニットを組み合わせて、それらが正しく協調して働くかを確認するテストです。単体で動作したユニットが組み合わさったときにも動作保証をします。
スモークテスト：アプリケーションの主要機能が全体的に動作するか確認する基本的なテストです。デプロイの成功を素早く確認します。
正解についての説明：
（選択肢）
・1.コードがプッシュされると、Cloud Buildをトリガーしてユニットテストを実行します。すべてのユニットテストが成功したら、アプリケーションコンテナをビルドして中央レジストリにプッシュします
2.Cloud Buildをトリガーして、コンテナをテスト環境にデプロイし、統合テストと受け入れテストを実行します
3.すべてのテストが成功した場合、パイプラインはアプリケーションを本番環境にデプロイし、スモークテストを実行します
この選択肢が正解の理由は以下の通りです。
まず、プッシュされたコードに対してCloud Buildを使用してユニットテストを実行することで、新しいバージョンのアプリケーションの基本的な動作確認ができます。ユニットテストの目的は、早い段階で問題を見つけ出し修正することで、アプリケーションの品質を確保することです。
次に、ユニットテストが成功した後にコンテナをビルドし、中央レジストリにプッシュすることで、そのコンテナのバージョン管理が可能になります。テスト環境にデプロイし、統合テストと受け入れテストを実行することで、アプリケーション全体の動作確認ができ、予期しないバグや問題を発見することができます。
最後に、すべてのテストが成功した場合にのみ、アプリケーションを本番環境にデプロイし、スモークテストを実行します。これにより、リリース直後の深刻な問題をすぐに検出することができます。これらの手順はCI/CDパイプラインのステージごとにテストを実行することで、品質を向上させるためのビルドからデプロイまでの操作を自動化します。
不正解についての説明：
選択肢：1.コードを中央リポジトリにプッシュする前にユニットテストを実行するよう、開発者に要求するGitフックをインストールします
2.Cloud Buildをトリガーしてアプリケーションコンテナをビルドします。アプリケーションコンテナをテスト環境にデプロイし、統合テストを実行します
3.統合テストが成功したら、アプリケーションコンテナを本番環境にデプロイし、受け入れテストを実行します
この選択肢が正しくない理由は以下の通りです。
不正解選択肢は、コードがリポジトリにプッシュされる前にユニットテストを行うため、開発者がユニットテストをスキップしたり失敗してもプッシュしたりする可能性があります。しかし、正解選択肢では、コードがプッシュされると自動的にユニットテストが行われ、その結果次の段階に進めるかが決定されます。これは自動化と一貫性を確保するためのより良いアプローチです。
選択肢：1.Gitフックをインストールして、開発者が中央リポジトリにコードをプッシュする前にユニットテストを実行するようにします。すべてのテストが成功したら、コンテナをビルドします
2.Cloud Buildをトリガーして、アプリケーションコンテナをテスト環境にデプロイし、統合テストと受け入れテストを実行します
3.すべてのテストに成功したら、コードを本番環境としてタグ付けします。Cloud Buildをトリガーして、アプリケーションコンテナをビルドし、本番環境にデプロイします
この選択肢が正しくない理由は以下の通りです。
ユニットテストを開発者がコードをプッシュする前に行うというアプローチは、テストの速度と信頼性を損なう恐れがあります。
また、コンテナはすべてのテストが成功した後にのみビルドされるべきで、すべてのテストが成功した段階で本番環境としてタグ付けするという手順は効率的ではありません。正しいのは、まずコードがプッシュされたときにCloud Buildをトリガーしてすべてのテストを行い、その結果次第でコンテナのビルドとデプロイを行う手順です。
選択肢：1.Cloud Buildをトリガーしてアプリケーションコンテナをビルドし、コンテナでユニットテストを実行します
2.単体テストが成功したら、アプリケーションコンテナをテスト環境にデプロイし、統合テストを実行します
3.統合テストが成功したら、パイプラインはアプリケーションコンテナを本番環境にデプロイします。その後、受け入れテストを実行します
この選択肢が正しくない理由は以下の通りです。
まず、ユニットテストはビルド前に実施するべきで、ビルド後に行うと再ビルドの必要性や不必要なリソース使用を引き起こします。
また、スモークテストが含まれていない点も問題です。本番環境へデプロイ後に一定の機能を確認するスモークテストは重要で、これがないとデプロイ直後のアプリケーションの安定性確認が行えません。
参考リンク：
https://cloud.google.com/build/docs/automating-builds/build-repos-from-github
https://cloud.google.com/build/docs/deploying-builds/deploy-cloud-run
https://docs.docker.com/ci-cd/github-actions/
</div></details>

### Q. 問題15: 未回答
間もなく公開されるサービスのCloud Monitoring SLOを作成する必要があります。サービスに対するリクエストが、毎月あたり少なくとも90% の割合で300ms未満で処理されることを検証したいと考えています。使用するメトリックと評価方法を特定する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？

1. リクエストベースの評価方法として、可用性の指標を選択します
2. ウィンドウベースの評価方法でレイテンシメトリックを選択します
3. ウィンドウベースの評価方法で、可用性の指標を選択します
4. リクエストベースの評価方法として、待ち時間の指標を選択します
<details><div>
    答え：4
説明
この問題では、サービスのパフォーマンスを評価するための適切なメトリックと評価方法を選択する必要があります。ここでのキーポイントは、リクエストが毎月あたり少なくとも90%の割合で300ms未満で処理されることを検証したいという目標です。したがって、この目標に合致するようなメトリックと評価方法を特定する必要があります。
基本的な概念や原則：
Cloud Monitoring：Google Cloudの監視、ログ記録、診断のためのフルスタックソリューションです。アプリケーションのパフォーマンス、可用性を監視し、問題発生時にアラートを作成します。
SLO（Service Level Objectives）：サービスレベル目標です。サービスの目標パフォーマンスを定義したもので、具体的な数値目標を含むことが一般的です。
リクエストベースの評価：個々のリクエストの成功または失敗を判定するために使用される評価方法です。
待ち時間メトリック：リクエストがどれだけ早く処理されたかを測定する指標です。一般的には、低い値がより良いパフォーマンスを意味します。
ウィンドウベースの評価：特定の時間窓内の成功率または失敗率を判定するために使用される評価方法です。
可用性メトリック：システムやサービスが機能しているかどうかを示す指標です。一般的には、高い値がより良いパフォーマンスを意味します。
正解についての説明：
（選択肢）
・リクエストベースの評価方法として、待ち時間の指標を選択します
この選択肢が正解の理由は以下の通りです。
まず、Cloud Monitoringを使用してサービスレベル目標（SLO）を作成する場合、評価方法として特定のメトリックを選択します。SLOが要求された特性（この場合はリクエストの応答速度）に対する期待値を設定するためのガイドライン提供します。
待ち時間メトリックは、サービスの応答速度を監視するのに適しています。これは明確に"リクエストが300ミリ秒以内で処理されるべき"というSLOを測定するための指標となります。リクエストベースの評価方法を選択することで、毎月の90%達成率を定期的に確認することが容易になります。そのため、この選択肢は要件を正確に満たすことができると言えます。
不正解についての説明：
選択肢：ウィンドウベースの評価方法でレイテンシメトリックを選択します
この選択肢が正しくない理由は以下の通りです。
ウィンドウベースの評価方法は、一定期間内に特定の状態にあった時間の割合を測定するためのものです。今回の目的はリクエストが一定時間内に処理される割合を計測することなので、リクエストベースの評価方法が適しています。
選択肢：リクエストベースの評価方法として、可用性の指標を選択します
この選択肢が正しくない理由は以下の通りです。
サービスがリクエストを300ms未満で処理することを確認するために、待ち時間の指標が最も適当です。可用性の指標では、リクエストがどれだけ早く処理されるかを検証することはできず、サービスが利用可能であるかどうかを評価できます。これは今回の要件とは一致しません。
選択肢：ウィンドウベースの評価方法で、可用性の指標を選択します
この選択肢が正しくない理由は以下の通りです。
ウィンドウベースの評価方法と可用性の指標は、公開されたサービスが稼働している時間の割合を測定するのに適していますが、リクエストが特定の時間（300ms未満）で処理される割合を検証するには不適切です。
一方、リクエストベースの評価方法と待ち時間の指標は、各リクエストの処理時間を直接的に測定し、それらが設定値以内に収まっている割合を検証するのに適しています。
参考リンク：
https://cloud.google.com/monitoring/service-level-objectives#performance_slo
https://cloud.google.com/monitoring/api/metrics
https://www.oreilly.com/library/view/google-cloud-for/9781492057676/
</div></details>

### Q. 問題16: 未回答
あなたの組織は、サイト信頼性エンジニアリング（SRE）の文化と原則を導入したいと考えています。最近、あなたがサポートするサービスに限定的な障害が発生しました。他のチームのマネージャーは、彼らが改善策を講じることができるように、何が起こったのかについて正式な説明を提供するようあなたに求めています。
この要件を満たすために、どうすればよいですか？

1. 根本原因、解決策、学んだ教訓、優先順位をつけたアクションアイテムのリストを含むポストモーテムを作成します。これを、マネージャーとだけ共有します
2. 根本原因、解決策、学んだ教訓、責任者のリスト、各人のアクションアイテムのリストを含むポストモーテムを作成します。これをエンジニアリング組織の文書ポータルで共有します
3. 根本原因、解決策、学んだ教訓、責任者のリスト、各人のアクションアイテムのリストを含むポストモーテムを作成します。これを、マネージャーとだけ共有します
4. 根本原因、解決策、学んだ教訓、優先順位をつけたアクションアイテムのリストを含むポストモーテムを作成します。これをエンジニアリング組織の文書ポータルで共有します
<details><div>
    答え：4
説明
この問題では、サイト信頼性エンジニアリング（SRE）の文化を組織に浸透させるためのアプローチについて問われています。特に、障害発生後の対応として、どのように情報を共有し、各チームが改善策を講じることができるようにするかに焦点が置かれています。ここで重要なのは、ポストモーテムの内容と、それをどの範囲で共有するかです。情報を適切に共有することで、組織全体が共有しやすいSREの文化を醸成することができます。したがって、情報の詳細性と共有範囲に注意しながら選択肢を読み解くことが必要です。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：ソフトウェアエンジニアリングの原則をインフラストラクチャと運用の問題に適用することで、システムの信頼性、可用性、性能を向上させる方法です。
ポストモーテム：障害やインシデントが発生した後に行われる分析で、根本原因、解決策、学んだ教訓、優先順位付けされた行動アイテムを明らかにします。これにより、同様の問題が再発するのを防ぐことが目的です。
根本原因分析：問題の真の原因を特定し、それが再発しないようにするためのプロセスです。これにより、問題が再発するのを防ぐための改善策を立案することができます。
学んだ教訓：プロジェクトやタスクの完了後、またはインシデントの発生後に得られる知識や経験です。これにより、将来のプロジェクトやインシデント対応の効率化や改善が可能となります。
優先順位付けされた行動アイテム：特定の問題を解決するために実行する必要があるタスクのリストです。これは優先順位付けされ、最も重要なものから実行されます。
透明性：SREの核心的な原則の一つで、インシデントの詳細を関係者全員と共有することです。これにより、全員が同じ理解を持ち、効果的に対応することが可能となります。
正解についての説明：
（選択肢）
・根本原因、解決策、学んだ教訓、優先順位をつけたアクションアイテムのリストを含むポストモーテムを作成します。これをエンジニアリング組織の文書ポータルで共有します
この選択肢が正解の理由は以下の通りです。
SREの文化は、障害性や問題の反射的な改善、学習に重心が置かれています。問題や障害が発生したときには、問題の深層的な解析を通じて根本原因を把握し、解決策を練り、学んだ教訓を共有、そして問題を再発させないための具体的なアクションプランを立てることが求められます。このプロセスは、多くの場合、"ポストモーテム"と称される文書にまとめられます。その結果をエンジニアリング組織全体で共有することで、同じ問題が再発するのを防いだり、他のチームが類似の問題に直面したときに役立てたりすることができます。
したがって、一連の問題解析とその結果をまとめたポストモーテムを作成し、それをエンジニアリング組織全体で共有することが、SRE文化導入に沿った適切な対応と言えます。
不正解についての説明：
選択肢：根本原因、解決策、学んだ教訓、優先順位をつけたアクションアイテムのリストを含むポストモーテムを作成します。これを、マネージャーとだけ共有します
この選択肢が正しくない理由は以下の通りです。
サイト信頼性エンジニアリング（SRE）の原則には透明性が含まれており、限定的な障害に対するポストモーテムの共有はマネージャーだけでなくエンジニアリング組織全体に対して行うべきです。このような全体的な共有により、組織全体での学習と改善が促進されます。
選択肢：根本原因、解決策、学んだ教訓、責任者のリスト、各人のアクションアイテムのリストを含むポストモーテムを作成します。これを、マネージャーとだけ共有します
この選択肢が正しくない理由は以下の通りです。
SREの原則は共有と透明性を重視します。
したがって、ポストモーテムはマネージャーだけではなく、エンジニアリング組織全体で共有するべきです。
また、アクションアイテムは優先順位付けされ、個々の人に対する責任付けよりも、問題解決のアプローチに重点が置かれるべきです。
選択肢：根本原因、解決策、学んだ教訓、責任者のリスト、各人のアクションアイテムのリストを含むポストモーテムを作成します。これをエンジニアリング組織の文書ポータルで共有します
この選択肢が正しくない理由は以下の通りです。
ポストモーテムの目的は改善策のための共有と学習であり、特定の人を責任者として抽出することを目的としていません。個々の人のアクションアイテムも個人を非難するものになりかねません。そのため、これはSREの文化と一致していません。正解の選択肢と比べますと、この選択肢は文化的な側面に配慮していません。
参考リンク：
https://cloud.google.com/incident-response/docs/postmortem-culture
https://cloud.google.com/sre
https://landing.google.com/sre/sre-book/chapters/postmortem-culture/
</div></details>

### Q. 問題17: 未回答
新しいサービスを本番環境にデプロイする必要があります。サービスはマネージドインスタンスグループ（MIG）を使用して自動的にスケールする必要があり、複数のリージョンにデプロイする必要があります。このサービスは各インスタンスに多数のリソースを必要とするため、キャパシティを計画する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？

1. サービスを1つのリージョンに配備し、グローバルロードバランサーを使ってこのリージョンにトラフィックをルーティングします
2. MIGのコンフィギュレーションでn1-highcpu-96マシンタイプを使用します
3. リソース要求が各リージョンの利用可能なクォータ制限内であることを検証します
4. Google Cloud Operation Suite Traceの結果を監視し、必要なリソースの量を決定します
<details><div>
    答え：3
説明
この問題では、新しいサービスを本番環境にデプロイする際のリソース管理とスケーリング戦略について考える必要があります。サービスはマネージドインスタンスグループ（MIG）を使って自動的にスケールし、複数のリージョンにデプロイされるべきで、各インスタンスには大量のリソースが必要とされています。ここで重要なのは、リソース要求が各リージョンの利用可能なクォータ制限内であるかどうかを検証しなければならないという点です。つまり、製品のデプロイメントとスケーリングに必要なリソースの需要と供給を正確に理解し、それが各リージョンのクォータ内に収まるように計画することが求められています。
基本的な概念や原則：
マネージドインスタンスグループ（MIG）：同一の設定を持つ仮想マシン（VM）の集合を管理するサービスです。自動スケーリングやロードバランシング、VMのヘルスチェック等が可能です。
クォータ：Google Cloud上で利用できるリソースの最大数です。リージョンごとに異なる可能性があり、注意深く管理する必要があります。
Google Cloud Operation Suite Trace：アプリケーションのパフォーマンスモニタリングを行うサービスです。サービス間の呼び出し依存関係を可視化し、パフォーマンス問題を特定しやすくします。
グローバルロードバランサー：リージョン間でトラフィックの分散を行うサービスです。マルチリージョン展開のアプリケーションで用いられ、トラフィックをユーザーに最も近いリージョンに自動的にルーティングします。
正解についての説明：
（選択肢）
・リソース要求が各リージョンの利用可能なクォータ制限内であることを検証します
この選択肢が正解の理由は以下の通りです。
全てのGoogle Cloudプロジェクトにはリージョン毎に利用可能な各種リソースに対するクォータ制限が設定されており、それを超える使用は許可されません。これは、ユーザーが無意識にリソースを過剰に消費することを防ぎ、さらにGoogle Cloudのリソースが適切に配分されることを確認するための重要な管理ツールです。
したがって、新しいサービスを複数のリージョンにデプロイし、マネージドインスタンスグループ（MIG）を使用して自動的にスケールさせる場合、そのサービスが各インスタンスに多数のリソースを必要とするため、初めに利用可能なクォータ制限内であることを検証することが必要となります。これにより、本番環境でのサービスデプロイメントがスムーズに行え、予期せぬ中断が避けられます。
不正解についての説明：
選択肢：MIGのコンフィギュレーションでn1-highcpu-96マシンタイプを使用します
この選択肢が正しくない理由は以下の通りです。
n1-highcpu-96マシンタイプを使用するだけでは、サービスが各インスタンスに多数のリソースを必要とするためのキャパシティを計画するという要件は満たせません。反対に、リソース要求が各リージョンの利用可能なクォータ制限内であることを検証する方が、キャパシティ計画を行う適切なアプローチとなります。
選択肢：Google Cloud Operation Suite Traceの結果を監視し、必要なリソースの量を決定します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Traceはアプリケーションのパフォーマンス問題を診断するツールであり、キャパシティ計画のための具体的なリソース要求量を提供するものではありません。
一方、各リージョンのクォータ制限内にリソース要求があることを確認するのは、サービスを複数のリージョンにデプロイし、適切にスケールするために必要です。
選択肢：サービスを1つのリージョンに配備し、グローバルロードバランサーを使ってこのリージョンにトラフィックをルーティングします
この選択肢が正しくない理由は以下の通りです。
問題文はサービスが複数のリージョンにデプロイされるべきであることを明示していますが、この選択肢はサービスを1つのリージョンに配置することを提案しています。これは問題の要件を満たしていません。
それに対して、正解はMIGを使用して複数のリージョンにデプロイするためのリソース計画を行うことを提案しており、要件に適合します。
参考リンク：
https://cloud.google.com/compute/docs/instance-groups/managing-groups
https://cloud.google.com/compute/quotas
https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources#globalresources
</div></details>

### Q. 問題18: 未回答
あなたはインフラストラクチャを定義するTerraformテンプレートの作成と修正を担当します。2人の新しいエンジニアが同じコードで作業することになるため、お互いのコードを上書きしないようなプロセスを定義し、ツールを採用する必要があります。また、すべてのアップデートを最新バージョンに取り込むことも必要です。
この要件を満たすために、どうすればよいですか？

1. コードをテキストファイルとしてGoogle Driveに保存し、ファイルを整理するフォルダ構造を定義します。毎日の終わりに、すべての変更がフォルダ構造内のファイルに取り込まれたことを確認します。フォルダ構造の名前を、バージョンをインクリメントする定義済みの命名規則で変更します
2. コードをGitベースのバージョン管理システムに保存します。コードを統合する前に、ピアコードレビューや、整合性と機能性を保証する単体テストを含むプロセスを確立します。リポジトリに完全に統合されたコードが最新のマスターバージョンになるプロセスを確立します
3. コードをテキストファイルとしてGoogle Driveに保存し、ファイルを整理するフォルダ構造を定義します。毎日の終わりに、すべての変更がフォルダ構造内のファイルに取り込まれたことを確認し、定義済みの命名規則で新しい.zipアーカイブを作成します。.zipアーカイブをバージョン管理されたCloud Storageバケットにアップロードし、最新バージョンとして受け入れます
4. コードをGitベースのバージョン管理システムに保存します。毎日の終わりに、開発者が自分の変更をマージできるプロセスを確立します。コードをパッケージ化し、最新のマスターバージョンとしてバージョン管理されたCloud Storageバスケットにアップロードします
<details><div>
    答え：2
説明
この問題では、何人ものエンジニアが同じコードに対して編集を行う状況下で、コードの競合を防ぎながら全てのアップデートを最新バージョンに統合するためのプロセスとツールの選択が求められています。バージョン管理システムであるGitが適切か、あるいはテキストファイルを保存するツールで対応するか、そのプロセスをどのように設定すべきかを探る問題です。重要な観点としては、コードの更新を適切に反映し競合を防ぐメカニズムと、全てのアップデートを最新バージョンに統合する効率的なプロセスが必要です。
基本的な概念や原則：
Terraform：インフラストラクチャーをコードで管理するためのオープンソースのツールです。一貫した方法でサービスやリソースを作成、更新、削除することができます。
Git：ソースコードのバージョン管理を行うためのツールです。複数人での開発を支援し、コードの上書きを防ぐためのブランチ機能や、バージョンの記録と復元が可能です。
ピアコードレビュー：他の開発者がコードを確認し、バグの特定やコード品質の向上を支援するプロセスです。同じチームのメンバーが互いのコードをレビューして、問題点を見つけ出します。
単体テスト：コードの正確性と品質を確認するためのテストです。特定の機能やメソッドが期待通りに動作することを確認します。
バージョン管理システム：ソースコードの変更履歴を管理するためのシステムです。複数人での開発作業を円滑に進めるために利用されます。
Google Drive：ファイルの保存と共有を行うためのCloud Storageサービスです。コードの保存場所としては適していません。
Cloud Storageバケット：Google Cloud Storageの基本的なコンテナです。データを格納するために使用され、バージョン管理の機能も提供します。
正解についての説明：
（選択肢）
・コードをGitベースのバージョン管理システムに保存します。コードを統合する前に、ピアコードレビューや、整合性と機能性を保証する単体テストを含むプロセスを確立します。リポジトリに完全に統合されたコードが最新のマスターバージョンになるプロセスを確立します
この選択肢が正解の理由は以下の通りです。
まず、Gitベースのバージョン管理システムの導入により、複数のエンジニアが同時にコードを編集しても、お互いの作業を上書きせず、また各自が作成や変更したコードのバージョンを追跡することが可能になります。このシステムがあれば、作業が重複したり、意図しない変更が加えられることを防ぐことができます。
また、ピアコードレビューのプロセスを設けることで、コードが統合される前に他のエンジニアによる検討とフィードバックが得られます。これにより、一貫性のあるアーキテクチャを維持し、品質を向上させることができます。
さらに、単体テストの導入により、変更がシステムの整合性や機能性に影響を与えないことを確認します。これは、エンジニアがコードを統合する前に、バグやエラーがないかどうかを検証する一環として不可欠です。
最後に、リポジトリの最新マスターバージョンとして完全に統合されたコードを採用するプロセスを確立することで、全てのアップデートを最新バージョンに取り込むという要件も満たせます。
不正解についての説明：
選択肢：コードをGitベースのバージョン管理システムに保存します。毎日の終わりに、開発者が自分の変更をマージできるプロセスを確立します。コードをパッケージ化し、最新のマスターバージョンとしてバージョン管理されたCloud Storageバスケットにアップロードします
この選択肢が正しくない理由は以下の通りです。
ただマージもしくはアップロードをするだけでは、コードの整合性及び機能性を保証するプロセスが存在しないため、互いのコードを上書きしないような状況を作ることが難しいです。
また、単に最新マスターバージョンをCloud Storageにアップロードするだけでは、バージョン管理がしっかりと行えていない可能性もあります。
選択肢：コードをテキストファイルとしてGoogle Driveに保存し、ファイルを整理するフォルダ構造を定義します。毎日の終わりに、すべての変更がフォルダ構造内のファイルに取り込まれたことを確認します。フォルダ構造の名前を、バージョンをインクリメントする定義済みの命名規則で変更します
この選択肢が正しくない理由は以下の通りです。
Google Driveにテキストファイルを保存するのは、拡張性が低く、バージョン管理やエンジニア間のコード上書き防止が困難です。ピアレビューやテスト機能も提供されていないため、コードの整合性と機能性の保証ができません。
一方、Gitのバージョン管理システムはこれらの問題を解決し、効率的な作業を可能にします。
選択肢：コードをテキストファイルとしてGoogle Driveに保存し、ファイルを整理するフォルダ構造を定義します。毎日の終わりに、すべての変更がフォルダ構造内のファイルに取り込まれたことを確認し、定義済みの命名規則で新しい.zipアーカイブを作成します。.zipアーカイブをバージョン管理されたCloud Storageバケットにアップロードし、最新バージョンとして受け入れます
この選択肢が正しくない理由は以下の通りです。
この方法では、複数のエンジニアが同時にコードを編集する場合、相互にコードを上書きする可能性があります。
また、Google Driveや.zipアーカイブはコードのバージョン管理には不適切で効率的ではありません。Gitなどのバージョン管理システムを使用することで、各エンジニアの変更を追跡し、必要に応じてマージできます。
参考リンク：
https://cloud.google.com/solutions/devops/devops-tech-version-control
https://cloud.google.com/architecture/devops/devops-process-promoting-and-deploying
https://www.terraform.io/docs/cli-index.html
</div></details>

### Q. 問題19: 未回答
あなたは、Google Kubernetes Engine（GKE）上で動作する本番アプリケーションの問題を調査しています。あなたは、コードの正確な変更は特定されていないものの、問題の原因が最近更新されたコンテナイメージであると判断しました。デプロイは現在、最新のタグを指しています。意図したとおりに機能するコンテナのバージョンを実行するようにクラスターを更新する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？

1. 以前のGitタグから新しいコンテナをビルドし、新しいコンテナへのデプロイでローリングアップデートを行います
2. デプロイメントを変更し、以前作業していたコンテナのsha256ダイジェストを指すようにします
3. 以前に作業していたコンテナを指すstableという新しいタグを作成し、新しいタグを指すようにデプロイメントを変更します
4. 最新のタグを以前のコンテナイメージに適用し、デプロイメントのローリングアップデートを行います
<details><div>
    答え：2
説明
この問題では、GKEにデプロイされているアプリケーションの問題を解決するための適切な手順について尋ねています。問題は更新されたコンテナイメージにあると判断されており、機能していたバージョンに戻す必要があることが分かります。この問題を解く上で、特に重要なのは、GKEやコンテナのデプロイに関連する基本的な知識、具体的にはバージョニングやロールバックのプロセスを理解することです。選択肢をふるいにかける時には、各選択肢が提供する解決策が最善の方法であるか、または問題の解決につながるかどうかを注意深く調べる必要があります。
基本的な概念や原則：
sha256ダイジェスト：特定のイメージ準拠のユニークで変わらない識別子です。コンテナイメージのバージョンを特定・管理するために使用されます。
コンテナイメージタグ：イメージのバージョンを表す可変の識別子です。"最新"のようなタグは変更可能で、新しいイメージを古いタグに関連付け直すことが可能です。
ローリングアップデート：GKEのデプロイメントにおいて、新しいバージョンのアプリケーションを段階的にデプロイし、古いバージョンを取り下げる更新戦略です。ダウンタイムを最小限に抑えることができます。
デプロイメント：Kubernetesによって管理されるアプリケーションの生存期間を通じてレプリカの数が維持されるリソースです。Rolling Updateなど、アップデート戦略を指定することも可能です。
Google Kubernetes Engine：Google CloudのマネージドKubernetesサービスで、容易なコンテナ化アプリケーションのデプロイメント、スケーリング、管理を可能にします。
正解についての説明：
（選択肢）
・デプロイメントを変更し、以前作業していたコンテナのsha256ダイジェストを指すようにします
この選択肢が正解の理由は以下の通りです。
問題の原因が最近更新されたコンテナイメージにあると判断している状況で、以前動作していた状態に戻すためには、動作確認済みのコンテナイメージを指定してデプロイする必要があります。"Latest"タグを使用すると、イメージが更新されるたびにその最新のイメージがデプロイされます。これが原因で問題が生じているのなら、特定のバージョンのイメージを明示的に指定することで問題を解決できます。
sha256ダイジェストはコンテナイメージの一意で不変な識別子であり、特定のコンテナイメージバージョンを一意に指すものです。
したがって、デプロイメントを変更して以前動作していたコンテナのsha256ダイジェストを指定することで、クラスターを更新して特定のイメージを実行させることが可能になります。
不正解についての説明：
選択肢：以前に作業していたコンテナを指すstableという新しいタグを作成し、新しいタグを指すようにデプロイメントを変更します
この選択肢が正しくない理由は以下の通りです。
新しいタグを作成してデプロイメントを変更する方式は、以前のバージョンへのロールバックも可能ですが、タグはソースの変更を特定できず、それが更改されると問題が再発する可能性があります。
一方、sha256ダイジェストを指定すると、特定のイメージビルドを固定的に参照し、再現性と安定性が確保されます。
選択肢：以前のGitタグから新しいコンテナをビルドし、新しいコンテナへのデプロイでローリングアップデートを行います
この選択肢が正しくない理由は以下の通りです。
以前のGitタグから新しいコンテナをビルドすると、場合によっては完全に一致する保証がないため、問題が解消されるとは限りません。
一方、sha256ダイジェストを指定することで特定のコンテナイメージバージョンを厳密に指定でき、意図したとおりの動作を保証できます。
選択肢：最新のタグを以前のコンテナイメージに適用し、デプロイメントのローリングアップデートを行います
この選択肢が正しくない理由は以下の通りです。
最新のタグを以前のイメージに適用するとタグの一貫性が失われます。
また、ローリングアップデートは新しいポッドの作成と古いポッドの削除を行いますが、同じ最新タグが使われていれば新旧の区別がつかず正しく動作しません。
一方、SHA256ダイジェストはイメージの内容自体を一意に特定するので、これを指定することで直接問題のないバージョンを選択できます。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment
https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps
https://kubernetes.io/docs/concepts/configuration/overview/
</div></details>

### Q. 問題20: 未回答
あなたは、キャッシュメモリに製品情報を保存するアプリケーションをサポートしています。キャッシュミスが発生するたびに、Cloud Loggingにエントリが記録されます。あなたは、キャッシュミスが発生する頻度を時系列で可視化したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？

1. Google Cloud Operation Suite Profilerを設定し、ログに基づいてキャッシュミスが発生したタイミングを特定し、視覚化します
2. Cloud LoggingをGoogle Data Studioのソースとしてリンクします。キャッシュミスのログにフィルタをかける
3. BigQueryをGoogle Cloud Operation Suiteログのシンクとして設定します。キャッシュミスログをフィルタリングし、別のテーブルに書き込むスケジュールクエリを作成します
4. Cloud Loggingでログベースのメトリックを作成し、Google Cloud Operation Suite Monitoringでそのメトリックのダッシュボードを作成します
<details><div>
    答え：4
説明
この問題では、Cloud Loggingに記録されたキャッシュミスの頻度を時系列で可視化する方法が求められています。ここでは、リアルタイムでの監視と分析、メトリクスの視覚化、Cloud Loggingと他のGoogle Cloudサービスとの連携などが重要な要素となります。主にはログからメトリクスを生成し、それを視覚化するサービスを選択すれば解決策となります。Google Cloud Operation Suite MonitoringやGoogle Data Studio、Profiler、BigQueryなどが選択肢となるため、特定の要件に最適なサービスを選ぶことが適切な解答に繋がります。
基本的な概念や原則：
Cloud Logging：Google Cloudのロギングサービスです。システムやアプリケーションのログデータを管理し、リアルタイムで分析可能なツールを提供します。
ログベースのメトリック：Cloud Loggingに記録されたログエントリに基づいて生成されるカスタムメトリックです。これを使用すると、ログエントリの特定のデータを継続的に監視し、分析することが可能となります。
Google Cloud Operation Suite（旧オペレーションスイート）：Google Cloud上でのシステムのモニタリング、トラブルシューティング、アプリケーションのパフォーマンス向上などを支援するツール群です。
Google Cloud Operation Suite Monitoring：時系列データを表示、分析、アラート設定を行えるMonitoringサービスを提供します。これを使用すると特定のメトリックの振る舞いを時間軸で視覚化することが可能です。
Google Data Studio：Googleのビジュアル分析ツールです。データの吸上げをしダッシュボードの作成、共有が行えますが、直接的にCloud Loggingのデータを時系列で視覚化することはできません。
Google Cloud Operation Suite Profiler：アプリケーションのパフォーマンスボトルネックを視覚化するツールですがログの可視化には向いていません。
BigQuery：Google Cloudのビッグデータ分析ツールです。大量のデータに対してSQLクエリを実行できますが、リアルタイムの視覚化やログの直接的なハンドリングには適していません。
正解についての説明：
（選択肢）
・Cloud Loggingでログベースのメトリックを作成し、Google Cloud Operation Suite Monitoringでそのメトリックのダッシュボードを作成します
この選択肢が正解の理由は以下の通りです。
まず、Cloud LoggingはGoogle Cloudのログ管理サービスで、アプリケーションのさまざまなアクティビティのログを記録します。キャッシュミスの時系列データを取得するためには、このログ情報をメトリクスとして抽出する必要があります。そのためにログベースのメトリックを作成します。このメトリックは、ログデータの特定のパターンを数値化し、それを時間とともに追跡する機能を提供します。つまり、定義したパターンがログに登場するたびにそのカウントがインクリメントされ、これによりキャッシュミスの頻度を追跡することができます。
その後、Google Cloud Operation Suite Monitoringを使用してこれらのメトリクスを時系列で視覚化します。MonitoringはCloud上の資源とアプリケーションのパフォーマンスを時間経過に沿って監視したり可視化したりするためのサービスです。ここで作成されたダッシュボードを利用すれば、キャッシュミスの発生頻度を時系列で視覚的に理解することが可能となります。
不正解についての説明：
選択肢：Cloud LoggingをGoogle Data Studioのソースとしてリンクします。キャッシュミスのログにフィルタをかける
この選択肢が正しくない理由は以下の通りです。
Cloud Loggingは直接Google Data Studioにリンクすることができません。キャッシュミスの頻度を時系列で可視化するためには、Cloud Loggingでログベースのメトリックを作成し、それをGoogle Cloud Operation Suite Monitoringで視覚化するのが適切です。
選択肢：Google Cloud Operation Suite Profilerを設定し、ログに基づいてキャッシュミスが発生したタイミングを特定し、視覚化します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Profilerはアプリケーションの性能問題を診断するためのツールであり、ログデータから時系列での可視化を作成するのには適しません。それに対してCloud LoggingとMonitoringはログデータを基にメトリックを作成、視覚化するのに最適なツールです。
選択肢：BigQueryをGoogle Cloud Operation Suiteログのシンクとして設定します。キャッシュミスログをフィルタリングし、別のテーブルに書き込むスケジュールクエリを作成します
この選択肢が正しくない理由は以下の通りです。
BigQueryを使用してログのフィルタリングを行うと確かに情報を取得できますが、これでは時系列での可視化を直接行うことはできません。問題の要件は時間経過と共にキャッシュミスの発生頻度を可視化することであり、これはGoogle Cloud Operation Suite Monitoringが提供する機能で可能です。
参考リンク：
https://cloud.google.com/logging/docs/logs-based-metrics
https://cloud.google.com/monitoring/api/metrics#Google Cloud-logging
https://cloud.google.com/logging/docs/view/overview
</div></details>

### Q. 問題21: 未回答
あなたはグローバルな組織で働いており、Compute Engine上でモノリシックアプリケーションを実行しています。最も少ないステップ数でCPU使用率を最適化するアプリケーションのマシンタイプを選択する必要があります。過去のシステムメトリクスを使用して、使用するアプリケーションのマシンタイプを特定したいと考えています。あなたは、Googleが推奨するプラクティスに従いたいと考えています。
この要件を満たすために、どうすればよいですか？

1. gcloud CLIを使用して、VMのフリート内にOps Agentをインストールします
2. すべてのVMにOps Agentを自動的にインストールするエージェントポリシーを作成します
3. VMのCloud Monitoringダッシュボードを確認し、CPU使用率が最も低いマシンタイプを選択します
4. レコメンダーAPIを使用し、提案された推奨事項を適用します
<details><div>
    答え：4
説明
この問題では、Compute Engineで実行されるモノリシックアプリケーションのCPU使用率を最適化するためのマシンタイプの選択に関して、Googleが推奨する行動を特定することが求められています。ここで注意すべきポイントは、"最も少ないステップ数で"の要求と"過去のシステムメトリクスを使用"するという要求、そしてこれらをふまえた上で、提示された選択肢の中から最適な手段を選ぶことです。
基本的な概念や原則：
レコメンダーAPI：Google Cloud上のリソースに対して最適化の推奨事項を提供するAPIです。利用することで、費用、パフォーマンス、セキュリティについて改善の可能性を見つけることができます。
Compute Engine：Google Cloudの仮想マシンを提供するサービスです。柔軟な仮想マシンの設定と自動スケーリングが可能です。
モノリシックアプリケーション：一つの単一のユニットとして構築、テスト、デプロイされるアプリケーションのこと。全ての機能が統合され、個々の機能が個別にスケーリングされることはないです。
Ops Agent：Google Cloudのリソース監視とログの収集を助けるAgentです。ただし、ワークロードの最適化自体はOps Agentではなくレコメンダーや他のツールを使うことが推奨されます。
Cloud Monitoring：Google Cloudのリソースやアプリケーションのパフォーマンスを監視し、警告を設定するサービスです。しかし、機械タイプの選択はCloud Monitoringではなく、レコメンダーAPIを使用することが推奨されます。
正解についての説明：
（選択肢）
・レコメンダーAPIを使用し、提案された推奨事項を適用します
この選択肢が正解の理由は以下の通りです。
まず、Google CloudのレコメンダーAPIは、過去のシステムメトリクスを元に最適なリソースの使用方法を提案する機能を持っています。つまり、Compute Engine上で実行しているモノリシックアプリケーションのCPU使用率を最適化するためのマシンタイプを提案してくれます。レコメンダーAPIを使用すれば、自身で各種メトリクスを分析し、マシンタイプを選ぶよりもはるかに少ないステップ数でCPU使用率を最適化することが可能です。
また、レコメンダーAPIはGoogleが提供する公式の機能なので、Googleの推奨するプラクティスに従っていると言えます。
したがって、この選択肢が要件を満たす最適解となります。
不正解についての説明：
選択肢：すべてのVMにOps Agentを自動的にインストールするエージェントポリシーを作成します
この選択肢が正しくない理由は以下の通りです。
Ops Agentを自動的にインストールするエージェントポリシーは統計データやメトリクスを取得するためには役立ちますが、それだけではCPU使用率を最適化するためのマシンタイプを特定することはできません。
それに対して、正解のレコメンダーAPIは過去のメトリクスを基にした最適なマシンタイプの推奨を提供します。
選択肢：gcloud CLIを使用して、VMのフリート内にOps Agentをインストールします
この選択肢が正しくない理由は以下の通りです。
Ops Agentのインストールはシステムメトリクスの収集に役立ちますが、これ自体がマシンタイプを最適化または推奨するものではありません。
それに対して、レコメンダーAPIは過去のシステムメトリクスに基づいて最適なマシンタイプを推奨します。そのため、最も少ないステップ数でCPU使用率を最適化するにはレコメンダーAPIの利用が適しています。
選択肢：VMのCloud Monitoringダッシュボードを確認し、CPU使用率が最も低いマシンタイプを選択します
この選択肢が正しくない理由は以下の通りです。
Cloud Monitoringダッシュボードを使用して最もCPU使用率の低いマシンタイプを選択する方法は最適化の一部ですが、最も少ないステップ数や全体的かつ詳細な推奨事項には対応できません。
それに対して、レコメンダーAPIは全体的な使用パターンとトレンドに基づき具体的な推奨事項を提案し、より効率的な最適化を実現します。
参考リンク：
https://cloud.google.com/compute/docs/machine-types
https://cloud.google.com/recommender/docs/using-vm-right-sizing
https://cloud.google.com/monitoring/api/v3/aggregation
</div></details>

### Q. 問題22: 未回答
あなたの組織は、200,000円の投資で、アプリケーションの可用性目標を99.9%から99.99%に引き上げたいと考えています。このアプリケーションの現在の収益は100,000,000円です。可用性の向上が、1年間の使用に対する投資に値するかどうかを判断する必要があります。
この要件を満たすために、どうすればよいですか？

1. 稼働率向上の価値を100,000円と計算し、稼働率向上は投資に値すると判断します
2. 稼働率向上の価値を100,000円と計算し、稼働率向上は投資に見合わないと判断します
3. 稼働率向上の価値を900,000円と計算し、稼働率向上は投資に値すると判断します
4. 稼働率向上の価値を90,000円と計算し、稼働率向上は投資に見合わないと判断します
<details><div>
    答え：4
説明
この問題では、アプリケーションの可用性を向上させるための投資が適切かどうかを判断するため、まず可用性の現状と目標、そしてその向上に要する投資額を把握する必要があります。次に、それらの数値をもとに、アプリケーションの収入と可用性向上の価値を計算し、その価値が投資に見合うかどうかを評価する必要があります。選択肢の中には、同じ結論を導くものの計算方法が異なるものや、異なる結論を導くものが含まれているため、正確な計算と理解が求められます。
基本的な概念や原則：
可用性：システムが正常に動作し、ユーザーに利用可能である割合を表す指標です。通常、パーセンテージで表示され、大きい方が良いです。
SLA（Service Level Agreement）：プロバイダと顧客間のサービスレベルを定義した合意文書です。システムの可用性目標など、サービスの品質を保証するための基準が記載されます。
ROI（Return on Investment）：投資対効果を測定するための指標です。投資によって得られる収益（又は費用削減）を投資額で割ります。ROIが1以上なら、投資は元が取れると判断されます。
ダウンタイム：システムが停止してサービスを提供できない時間のことです。可用性が高いほどダウンタイムは少なくなります。
機会損失：何かを行う機会を逃したことによる損失のことです。本問題では、高い可用性を維持しないことによる収益の損失がこれに該当します。
正解についての説明：
（選択肢）
・稼働率向上の価値を90,000円と計算し、稼働率向上は投資に見合わないと判断します
この選択肢が正解の理由は以下の通りです。
まず、現在の可用性が99.9%から99.99%に引き上げることを考えると、これは年間のダウンタイムが0.1%から0.01%への改善を意味します。つまり、ダウンタイムが年間で0.09％改善されます。
したがって、ダウンタイムの改善による収益増加は、現在の収益である100,000,000円において、0.09%の増加、つまり90,000円と見積もられます。
しかしながら、改善には200,000円の投資が必要であるため、この投資による純利益は90,000円（改善による収益）から200,000円（投資）を引いた合計-110,000円となります。
以上の計算から、稼働率向上は投資に見合わないと判断できます。
不正解についての説明：
選択肢：稼働率向上の価値を100,000円と計算し、稼働率向上は投資に見合わないと判断します
この選択肢が正しくない理由は以下の通りです。
稼働率向上の価値を100,000円と計算するという方法は、アプリケーションの可用性目標を適切に評価できていません。正解の選択肢では稼働率向上の価値を90,000円と正確に計算していることから、投資が必要な量よりも価値が低いと正しく判断されています。
選択肢：稼働率向上の価値を100,000円と計算し、稼働率向上は投資に値すると判断します
この選択肢が正しくない理由は以下の通りです。
この選択肢は、稼働率向上の価値を100,000円と計算していますが、これは間違いです。正解の選択肢は稼働率向上の価値を90,000円と計算しており、これは200,000円の投資よりも低いため、投資に見合わないと判断されています。
したがって、不正解の選択肢は投資判断の数値が誤っているため、不適切です。
選択肢：稼働率向上の価値を900,000円と計算し、稼働率向上は投資に値すると判断します
この選択肢が正しくない理由は以下の通りです。
可用性の向上がもたらす収益増加は、現在の収益から可用性の向上分を計算することで導き出されます。これにより200,000円の投資に見合う収益増加が見込めるかを判断します。しかし、この選択肢での計算結果900,000円は、正しく計算された価値90,000円と比べて10倍も高く、実際の可用性向上分を大幅に過大評価しているため、不正解です。
参考リンク：
https://cloud.google.com/architecture/framework/reliability/availability
https://cloud.google.com/solutions/devops/devops-tech-high-availability
https://en.wikipedia.org/wiki/High_availability
</div></details>

### Q. 問題23: 未回答
あなたの組織にポストモーテムを導入する必要があります。ポストモーテムプロセスが好評であることを確認したいと考えています。
この要件を満たすために、どうすればよいですか？（2つ選択）

1. 新入社員には、練習を通じてチームにポストモーテムを行うよう奨励します
2. シニアリーダーに、ポストモーテムを認め、参加するよう奨励します
3. 効果的なポストモーテムを書くことが報われ、称賛されるようにします
4. すべてのポストモーテムの実施に責任を持つ指定チームを作ります
5. 過去のポストモーテムをレビューする場を組織に提供します
<details><div>
    答え：2,3
説明
この問題では、ポストモーテム（プロジェクトまたはイベント後の逆説的な分析手続き）の導入とその成果物の評価について考える必要があります。選択肢の中で考えられるアクションは、主に組織の一員に対する行動を促し、その効果を評価するものです。評価の正しさを考えるためには、ポストモーテムが成功するための重要な要素の理解が必要です。選択肢の中から、ポストモーテムの適用を推進し、その成果を評価・称える方法を選ぶことが求められます。
基本的な概念や原則：
ポストモーテム：システム障害や重大なエラーが発生した後に行われる評価のことです。原因の特定、再発防止策の検討、改善のためのプロセスを理解し、学べます。
シニアリーダーの参加：運営のトップからの参加とサポートがあることで、組織全体の関与を促し、ポストモーテムが重視される雰囲気を作ることができます。
報われるシステム：ポストモーテムが正確で、有意義な情報を提供し、問題の解決に貢献するような行為を評価し、報いることで、全体の質を高めることができます。
正解についての説明：
（選択肢）
・シニアリーダーに、ポストモーテムを認め、参加するよう奨励します
・効果的なポストモーテムを書くことが報われ、称賛されるようにします
この選択肢が正解の理由は以下の通りです。
まず、シニアリーダーにポストモーテムを認め、参加するよう奨励することは重要です。リーダーシップがその価値を理解し、積極的に参加することで、他のメンバーもポストモーテムを重要なプロセスと認識し、受け入れます。リーダーシップからの推進とサポートなしには、新しいプロセスを組織全体に定着させることは非常に困難です。
次に、効果的なポストモーテムを書くことが報われ、称賛されるようにすることも重要です。ポストモーテムを成功させるためには、そのプロセスが価値ありと見なされ、感謝されるべきであり、これによりすべての関連スタッフが率先して参加します。これらの措置により、ポストモーテムプロセスが好評であることを確認できます。
不正解についての説明：
選択肢：新入社員には、練習を通じてチームにポストモーテムを行うよう奨励します
この選択肢が正しくない理由は以下の通りです。
新入社員だけにポストモーテムを行うよう奨励すると、組織全体の改善プロセスが偏ってしまいます。シニアリーダーにポストモーテムを認め、参加するよう奨励することで全体的な文化の変化を促し、効果的なポストモーテムが称賛されるようにすると全員がより良い結果を目指します。
選択肢：すべてのポストモーテムの実施に責任を持つ指定チームを作ります
この選択肢が正しくない理由は以下の通りです。
ポストモーテムは事象発生者が自身でアクションを振り返り、改善点を見つける手法であり、全てのポストモーテムを一部のチームが実施してしまうと、直接関与した者の学習の場が失われ、組織全体の透明性や改善などポストモーテムの目的に反します。
選択肢：過去のポストモーテムをレビューする場を組織に提供します
この選択肢が正しくない理由は以下の通りです。
過去のポストモーテムをレビューする場を提供することは、確かにポストモーテムの理解を深めるのに役立ちます。しかし、これだけではポストモーテムプロセスが好評であることを確認する手段にはならないからです。
一方、シニアリーダーにこのプロセスへの参加を奨励することや、効果的なポストモーテムを称賛することは、プロセスが尊重され活用されていることを確認する具体的な方法となります。
参考リンク：
https://cloud.google.com/sre/docs/sre-book/managing-incidents
https://cloud.google.com/blog/topics/inside-google-cloud/google-clouds-incident-management-process
https://landing.google.com/sre/sre-book/chapters/managing-incidents/
</div></details>

### Q. 問題24: 未回答
本番環境でJavaアプリケーションを分析しています。すべてのアプリケーションには、デフォルトでCloud ProfilerとCloud Traceがインストールされ構成されています。どのアプリケーションにパフォーマンスチューニングが必要かを判断したいと考えています。
この要件を満たすために、どうすればよいですか？（2つ選択）

1. アプリケーションのウォールクロック時間とCPU時間のレイテンシを調べます。もしレイテンシ時間が徐々にエラーバジェットを消費し、ウォールクロック時間とCPU時間の差が最小であれば、アプリケーションを最適化のためにマークします
2. アプリケーションのウォールクロック時間とCPU時間を調べます。差が大きい場合は、CPUリソースの割り当てを増やします
3. アプリケーションのウォールクロック時間とCPU時間を調べます。この差が大きい場合は、ローカルディスクストレージの割り当てを増やします
4. アプリケーションのウォールクロック時間とCPU時間を調べます。差が大きい場合は、メモリリソースの割り当てを増やします
5. アプリケーションのヒープ使用量を調べます。使用率が低い場合は、アプリケーションを最適化の対象とします
<details><div>
    答え：1,5
説明
この問題では、Javaアプリケーションのパフォーマンスを最適化するための必要性を判断する方法が求められています。Cloud ProfilerとCloud Traceを用いてパフォーマンス監視をしており、このツールから得られる情報をもとに、どのアプリケーションにパフォーマンスチューニングが必要かを判断することが問われています。具体的には、ウォールクロック時間とCPU時間のレイテンシ、ヒープの使用率といった指標を見て、パフォーマンスチューニングの必要性を判断します。また、選択肢の中にはパフォーマンスの問題を診断し、解決するためにリソースを増やすような項目が含まれていますが、質問文の最適化という要件からすると、これらは適切な選択肢とは言えません。
基本的な概念や原則：
Cloud Profiler：Google Cloudのパフォーマンス分析ツールです。CPU使用率やメモリ使用率などのパフォーマンスデータを収集し、アプリケーションの最適化を補助します。
Cloud Trace：Google Cloudのアプリケーションのパフォーマンス監視ツールです。リクエストのレイテンシを視覚化し、パフォーマンスの問題を特定します。
ウォールクロック時間：計算またはタスクにかかった実際の時間です。CPU時間と比較することにより、パフォーマンスのボトルネックを特定できます。
CPU時間：プロセスがCPUを占有した実時間です。ウォールクロック時間と比較してプロセス効率性を評価します。
ヒープ使用量：Javaアプリケーションのランタイムヒープの使用状況を示します。ヒープの使用率が低い場合、アプリケーションが効率的にメモリを使用していない可能性があります。
CPUリソースの割り当て：CPUリソースをアプリケーションに割り当てることで、パフォーマンスを向上させます。ただし、過剰なCPU割り当ては非効率的で、コスト増につながります。
メモリリソースの割り当て：メモリリソースをアプリケーションに割り当てることで、パフォーマンスを向上させます。ただし、過剰なメモリ割り当ては非効率的で、コスト増につながります。
正解についての説明：
（選択肢）
・アプリケーションのウォールクロック時間とCPU時間のレイテンシを調べます。もしレイテンシ時間が徐々にエラーバジェットを消費し、ウォールクロック時間とCPU時間の差が最小であれば、アプリケーションを最適化のためにマークします
・アプリケーションのヒープ使用量を調べます。使用率が低い場合は、アプリケーションを最適化の対象とします
この選択肢が正解の理由は以下の通りです。
まず、ウォールクロック時間とCPU時間のレイテンシを調べることで、パフォーマンスボトルネックを特定することが可能です。ウォールクロック時間はアプリケーションがタスクを完了するために必要な実時間を示し、CPU時間はそのタスクにCPUが費やした時間を示します。これらの差が小さい＝CPUがタスクにほぼ全力で取り組んでいる、と解釈でき、その場合、そのアプリケーションは最適化の対象となります。これはCloud Traceを使用して調査します。
また、アプリケーションのヒープ使用量を調査することも重要です。ヒープとは、動的にメモリを割り当てるための領域で、使用率が低いということは、割り当てられたメモリが十分に活用されていない可能性を示します。つまり、メモリ管理の最適化が必要であることを示し、それがパフォーマンスのボトルネックとなっている可能性があります。ヒープ使用量はCloud Profilerを使用して調査します。これらの分析を通じて、パフォーマンスチューニングが必要なアプリケーションを特定したり、その最適化を進めたりすることができます。
不正解についての説明：
選択肢：アプリケーションのウォールクロック時間とCPU時間を調べます。差が大きい場合は、CPUリソースの割り当てを増やします
この選択肢が正しくない理由は以下の通りです。
ウォールクロック時間とCPU時間の差が大きい場合、それはCPUよりも他の要素（I/O、ネットワーク、待機時間など）がネックになっていることを示す可能性があります。そのため、単にCPUリソースの割り当てを増やすだけでは問題解決につながらない可能性が高いです。
選択肢：アプリケーションのウォールクロック時間とCPU時間を調べます。差が大きい場合は、メモリリソースの割り当てを増やします
この選択肢が正しくない理由は以下の通りです。
ウォールクロック時間とCPU時間の差が大きい場合、それはCPUが待機している時間が多いことを示していますが、これはメモリリソースの不足ではなく、IO待ちなど他の要因が主な原因となります。
したがって、メモリリソースの割り当てを増やすというアプローチは適切ではありません。
選択肢：アプリケーションのウォールクロック時間とCPU時間を調べます。この差が大きい場合は、ローカルディスクストレージの割り当てを増やします
この選択肢が正しくない理由は以下の通りです。
ウォールクロック時間とCPU時間の差が大きいことは、アプリケーションがディスクI/Oで待機している可能性を示しています。しかし、その解決策としてローカルディスクストレージの割り当てを増やすとは限りません。実際にはI/O操作の最適化や他のリソースの調整が必要かもしれません。
参考リンク：
https://cloud.google.com/profiler/docs
https://cloud.google.com/trace/docs
https://cloud.google.com/architecture/examining-application-performance-using-cloud-profiler-and-cloud-trace
</div></details>

### Q. 問題25: 未回答
アプリケーションの成果物は、CI/CDパイプラインを介してビルドされ、デプロイされます。CI/CDパイプラインがアプリケーションシークレットに安全にアクセスできるようにしたいと考えています。また、セキュリティ侵害に備えて、より簡単にシークレットをローテーションしたいと考えています。
この要件を満たすために、どうすればよいですか？

1. シークレットを暗号化し、ソースコードリポジトリに保管します。復号鍵を別のリポジトリに保管し、パイプラインにアクセス権を与えます
2. ビルド時に開発者に秘密の入力を促します。開発者に対し、休息時にシークレットを保存しないよう指示します
3. Cloud KMSからのキーで暗号化されたCloud Storageにシークレットを保存します。CI/CDパイプラインがIAM経由でCloud KMSにアクセスできるようにします
4. Git上の別の設定ファイルにシークレットを保存します。選ばれた開発者に設定ファイルへのアクセスを提供します
<details><div>
    答え：3
説明
この問題では、アプリケーションのシークレットを安全に管理し、ローテーションを容易に行う方法を求められています。解答を選ぶ際には、シークレットのセキュリティ保護の要件（暗号化など）およびローテーション（更新や交換）の容易さを重視しなければなりません。不適切な選択肢は、シークレットの安全性が不十分であったり、ローテーションが困難であったりする場合があります。選択肢を評価する際には、これらの側面を念頭に置いてください。
基本的な概念や原則：
Cloud KMS：Google Cloudの暗号鍵管理サービスです。データの暗号化と復号化、またキーのローテーションと管理を行うことができます。
Cloud Storage：Google Cloudのオブジェクトストレージサービスです。データを安全に保存し、どこからでもアクセスできます。
IAM：Google CloudのIAM（Identity and Access Management）は、特定のユーザーが特定のリソースにどのようなアクセスを許可するかを制御します。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）の実践を組み合わせた工程です。コードの変更を自動的にビルド、テスト、デプロイすることで、高速かつ安定したソフトウェアリリースを支援します。
アプリケーションシークレット：アプリケーションが正常に機能するために必要な機密情報です。これにはAPIキー、データベースパスワードなどが含まれます。
暗号化：情報を秘密に保つために、元の情報を読み取れない形に変換することです。暗号化された情報は、適切な鍵を持つ人だけが復号化できます。
キーローテーション：定期的に暗号化キーを変更することで、セキュリティを強化する方法です。これにより、既存のキーが漏洩した場合でも情報が保護されます。
正解についての説明：
（選択肢）
・Cloud KMSからのキーで暗号化されたCloud Storageにシークレットを保存します。CI/CDパイプラインがIAM経由でCloud KMSにアクセスできるようにします
この選択肢が正解の理由は以下の通りです。
まず、Google Cloudでは、情報を保護するために暗号化が強力に推奨されています。Cloud KMS（Key Management Service）を使用することにより、シークレットに対するアクセスを管理し、それらを安全に暗号化することが可能です。Cloud KMSから取得したキーでシークレットを暗号化してCloud Storageに保存することで、データの安全性が高まります。
さらに、IAM（Identity and Access Management）を通じてCI/CDパイプラインからCloud KMSへのアクセスを認証および許可することは、セキュリティ上重要なロールを果たします。パイプラインがCloud KMSにアクセスできるようにすることで、自動化された方式でシークレットの管理とローテーションが可能になります。これにより、シークレットの使用やローテーションが適切に行われ、セキュリティの堅牢性が向上します。それぞれのパイプラインのアクセス権をIAMポリシーを通じて管理することで、セキュリティが強化され、CI/CDパイプラインの信頼性も向上します。
不正解についての説明：
選択肢：ビルド時に開発者に秘密の入力を促します。開発者に対し、休息時にシークレットを保存しないよう指示します
この選択肢が正しくない理由は以下の通りです。
ビルド時に開発者に秘密の入力を促す方法は、開発者が画面上に秘密情報を記入する操作が必要となり、セキュリティリスクが高まるほか、ローテーションの際の負荷が大きくなります。
一方、Cloud KMSとCloud Storageを利用する方法では、認証情報を一元管理してシームレスに更新できるため、要件に対してより適切です。
選択肢：Git上の別の設定ファイルにシークレットを保存します。選ばれた開発者に設定ファイルへのアクセスを提供します
この選択肢が正しくない理由は以下の通りです。
Git上の設定ファイルにシークレットを保存すると、暗号化されずに公開されてしまい、セキュリティ侵害のリスクが高まります。
また、シークレットのローテーションが煩雑になる可能性があります。
一方、Cloud KMSとCloud Storageを使用すると、セキュアにシークレットを保管し、必要な権限があるパイプラインだけがアクセスできるなど、セキュリティと操作性を向上させることができます。
選択肢：シークレットを暗号化し、ソースコードリポジトリに保管します。復号鍵を別のリポジトリに保管し、パイプラインにアクセス権を与えます
この選択肢が正しくない理由は以下の通りです。
シークレットをソースコードリポジトリに保管する方法はセキュリティリスクが高く、シークレットをローテーションする際にも手間がかかります。
一方、Cloud KMSとCloud Storageを利用すると、IAMを通したアクセス制御と鍵のローテーションが容易になり、要件を満たすことが可能です。
参考リンク：
https://cloud.google.com/storage/docs/encryption
https://cloud.google.com/kms/docs
https://cloud.google.com/solutions/secrets-management
</div></details>

### Q. 問題26: 未回答
Cloud Runアプリケーションは、構造化されていないログをテキスト文字列としてCloud Loggingに書き込みます。非構造化ログをJSONベースの構造化ログに変換したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？

1. Cloud Runコンテナイメージにログエージェントをインストールし、ログエージェントを使用してログをCloud Loggingに転送します
2. Cloud Loggingソフトウェア開発キット（SDK）を使用するようにアプリケーションを修正し、jsonPayloadフィールドを持つログエントリを送信します
3. Fluent Bitサイドカーコンテナをインストールし、JSONパーサーを使います
4. ログのテキストペイロードをJSONペイロードに変換するようにログエージェントを設定します
<details><div>
    答え：2
説明
この問題では、Cloud Runアプリケーションが現在テキスト文字列としてCloud Loggingに書き込んでいる非構造化ログを、JSONベースの構造化ログにどう変換するかを問われています。アプリケーションが現在使用しているログ出力方法と、目指すログ形式の違いを理解することが重要です。選択肢を検討する際には、適切なツールや手法を通じてテキスト文字列をJSON形式に変換する解決策を選ぶ必要があります。
基本的な概念や原則：
Cloud Logging：Google Cloudのログ管理サービスです。アプリケーションとシステムからのログデータを集め、分析、エクスポートすることができます。
非構造化ログ：特定のデータ構造を持たないログデータのことです。分析や管理が難しい場合があります。
Cloud Logging SDK：Cloud Loggingにログエントリを送信するためのソフトウェア開発キットです。jsonPayloadフィールドを使用して、非構造化ログを構造化ログに変換することが可能です。
構造化ログ：特定の構造（例えばJSON）を持つログデータのことです。データ分析や管理が容易になります。
jsonPayloadフィールド：Cloud Loggingで使用されるフィールドで、ログエントリの本文をJSON形式で格納します。構造化ログの作成に使用されます。
正解についての説明：
（選択肢）
・Cloud Loggingソフトウェア開発キット（SDK）を使用するようにアプリケーションを修正し、jsonPayloadフィールドを持つログエントリを送信します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Loggingソフトウェア開発キット（SDK）は、アプリケーションログの作成と管理をGoogle Cloudで直接実行できるようにするためのツールキットです。その主な機能は通常のテキストベースのログだけでなく、構造化されたJSONベースのログを含んでいます。
したがって、Cloud Logging SDKを使用すると、アプリケーションから直接構造化ログをCloud Loggingに送信できます。
具体的には、SDKを使用してアプリケーションを修正し、jsonPayloadフィールドを持つログエントリを送信すれば、サーバーレス環境のCloud Runアプリケーションから生成された非構造化ログをJSONベースの構造化ログに変換することができます。これにより、ログデータの分析と管理が大幅に容易になります。この選択肢は正確で効率的な解決策を提供します。
不正解についての説明：
選択肢：Fluent Bitサイドカーコンテナをインストールし、JSONパーサーを使います
この選択肢が正しくない理由は以下の通りです。
Fluent BitサイドカーコンテナとJSONパーサーを使用する方法は、すでに存在するログデータの形式を変換する為によく使用されますが、それは既存のログを解析してJSON形式に変換するものであり、アプリケーションが直接構造化ログを出力する方法とは違います。正解の選択肢のCloud Logging SDKを使うことで、アプリケーション自身がJSONベースの構造化ログを出力、Cloud Loggingへと送信することが可能になります。
選択肢：Cloud Runコンテナイメージにログエージェントをインストールし、ログエージェントを使用してログをCloud Loggingに転送します
この選択肢が正しくない理由は以下の通りです。
Cloud Runの環境ではログエージェントをインストールすることができません。Cloud Runのログ取得は自動的に行われ、特別なログエージェントをインストールする必要はありません。
一方、Cloud Logging SDKを使用すればアプリケーションレベルでログの形式を制御し、jsonPayloadフィールドを持つログエントリを送信することが可能です。
選択肢：ログのテキストペイロードをJSONペイロードに変換するようにログエージェントを設定します
この選択肢が正しくない理由は以下の通りです。
Google Cloudのログエージェントはログの収集と転送を担いますが、ログの形式をテキストからJSONに変換する機能はありません。
一方、Cloud Logging SDKを使用すれば、アプリケーションが直接JSON形式のログを生成できるため、この問題の要件を満たします。
参考リンク：
https://cloud.google.com/logging/docs/structured-logging
https://cloud.google.com/logging/docs/reference/libraries
https://cloud.google.com/run/docs/logging
</div></details>

### Q. 問題27: 未回答
アプリケーションサービスはGoogle Kubernetes Engine（GKE）で実行されます。cloudjp-imagesプロジェクトで一元管理されているGoogle Container Registry（GCR）イメージレジストリからのイメージのみをクラスターにデプロイできるようにし、開発時間を最小限に抑えたいと考えています。
この要件を満たすために、どうすればよいですか？

1. イメージをgcr.io/cloudjp-imagesにのみプッシュするCloud Build用のカスタムビルダーを作成します
2. デプロイメントパイプラインにロジックを追加し、すべてのマニフェストにgcr.io/cloudjp-imagesからのイメージのみが含まれていることをチェックします
3. ホワイトリスト名パターンgcr.io/cloudjp-images/を含むバイナリ認証ポリシーを使用します
4. gcr.io/cloudjp-imagesの各イメージにタグを追加し、画像のデプロイ時にこのタグが存在することを確認します
<details><div>
    答え：3
説明
この問題では、Google Kubernetes Engine上でのアプリケーションサービスのデプロイ時に、特定のGoogle Container Registryからのイメージのみを使用することを求めています。また、開発時間を最小限に抑える必要があるため、繊細で時間をかける作業や追加のステップの導入は避けなければなりません。さらに言えば、GCRの一元管理されたプロジェクトに絞るという要件からは、すべてのトラフィックを特定のレジストリに制限するためのポリシー設定が関与することが推測されます。具体的なアクションとしては、バイナリ認証ポリシーの使用やカスタムビルダーの作成などが考えられます。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google CloudのマネージドKubernetesサービスです。Kubernetesのパワフルな機能をフル活用しながら、サーバーのメンテナンスやアップグレードなどの運用負荷をGoogleに委ねることができます。
Google Container Registry（GCR）：Dockerイメージをプライベートに保存・共有できるレジストリサービスです。高い可用性と安全性を備えており、Dockerクライアントからのプルプッシュやバージョン管理などが可能です。
バイナリ認証：Google Cloudのサービスで、クラスターにデプロイされるイメージの信頼性を確保します。特定の条件を満たすイメージのみをデプロイするように制限することができます。
ホワイトリスト：指定されたエンティティ（ここではGCRイメージ）だけがリソースへのアクセスを許可されるリストです。安全性の向上や不必要なアクセスの防止に役立ちます。
Cloud Build：Google Cloudの継続的インテグレーション/デリバリーサービスで、コードのビルド、テスト、デプロイを自動化します。まとまったコードの変更に対して反復的なプロセスを適用することで、開発効率を高めます。
デプロイメントパイプライン：アプリケーションのリリースを自動化するプロセスです。ビルド、テスト、デプロイなどのステップを一つのパイプラインにまとめて管理することで、新しいバージョンのリリースをスムーズに行えます。
正解についての説明：
（選択肢）
・ホワイトリスト名パターンgcr.io/cloudjp-images/を含むバイナリ認証ポリシーを使用します
この選択肢が正解の理由は以下の通りです。
まず、Binary Authorizationは、Google Cloud上でのデプロイ時にコンテナイメージの信頼性を確認するためのサービスで、デプロイするイメージのソースを制限するためのポリシーを適用できる機能があります。これにより、特定のイメージレジストリからのデプロイのみを許容するといった制御が可能になります。この問題の要件では、特定のGCR（Google Container Registry）レジストリからのイメージのみをデプロイできるようにするという制約があったため、このBinary Authorizationの用途が該当します。
したがって、ホワイトリスト名パターンgcr.io/cloudjp-images/を含むバイナリ認証ポリシーを使用することで、この制約を満たすことができます。
また、この方法は開発プロセスの重大な変更なしにこれらの制約を実装できるため、開発時間の増加も最小限に抑えられます。
不正解についての説明：
選択肢：イメージをgcr.io/cloudjp-imagesにのみプッシュするCloud Build用のカスタムビルダーを作成します
この選択肢が正しくない理由は以下の通りです。
カスタムビルダーを作成すると、開発時間を最小限に抑えられない可能性があります。
また、この方法ではクラスターにデプロイされるイメージのソースを制限することはできません。
一方、バイナリ認証ポリシーを使用すると、指定したソースからのイメージのみをクラスターにデプロイすることが可能です。
選択肢：デプロイメントパイプラインにロジックを追加し、すべてのマニフェストにgcr.io/cloudjp-imagesからのイメージのみが含まれていることをチェックします
この選択肢が正しくない理由は以下の通りです。
デプロイメントパイプラインにロジックを追加すると、開発時間が増加する可能性があります。
また、マニフェストのチェックは人為的なエラーが発生する可能性があるため、バイナリ認証ポリシーを使用して自動的に管理する方が効率的です。
選択肢：gcr.io/cloudjp-imagesの各イメージにタグを追加し、画像のデプロイ時にこのタグが存在することを確認します
この選択肢が正しくない理由は以下の通りです。
イメージにタグを追加し、デプロイ時にその存在を確認することは、手動でのチェック作業が増え、要件の"開発時間を最小限に抑える"ことが難しくなります。
一方、バイナリ認証ポリシーを使用すると、イメージの場所に基づいた自動化されたホワイトリスト作成が可能で、開発の効率化に寄与します。
参考リンク：
https://cloud.google.com/binary-authorization/docs
https://cloud.google.com/container-registry/docs/overview
https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry
</div></details>

### Q. 問題28: 未回答
サイト信頼性エンジニアとして、あなたは本番環境でGoogle Kubernetes Engine（GKE）上で動作するGoで書かれたアプリケーションをサポートしています。アプリケーションの新バージョンをリリースした後、アプリケーションが約15分間実行され、その後再起動することに気づきました。アプリケーションにCloud Profilerを追加することにしましたが、アプリケーションが再起動するまでヒープ使用量が常に増加していることに気づきました。
どうしたらよいですか？

1. アプリケーション展開のCPU制限を増やします
2. 高メモリのコンピュートノードをクラスターに追加します
3. アプリケーションデプロイメントのメモリ制限を増やします
4. アプリケーションにCloud Traceを追加し、再デプロイします
<details><div>
    答え：3
説明
この問題では、本番環境で動作するアプリケーションが新バージョンのリリース後に一定時間内で再起動し、ヒープ使用量が常に増加しているという状況を把握する必要があります。これはメモリリークと思われるため、問題解決に向けては、メモリ使用量やメモリリークに焦点を当てる必要があります。選択肢を見る時には、メモリに関する操作を重視供給、またはメモリリークの原因を探るアクションが必要です。
基本的な概念や原則：
Cloud Profiler：Google Cloudのパフォーマンス分析ツールです。アプリケーションのCPU使用量やメモリ使用量などの情報を視覚化し、パフォーマンス問題を診断しやすくします。
ヒープメモリ：プログラムがランタイムに動的に割り当てることができるメモリ領域です。アプリケーションのヒープ使用量が増加し続けると、メモリリークを示す可能性があります。
メモリ制限：Kubernetesで使用できるメモリの最大量を設定するパラメーターです。アプリケーションの需要に合わせて調整可能です。
Cloud Trace：Google Cloudの分散トレーシングシステムです。アプリケーションのパフォーマンス問題の特定と診断に役立ちます。
Google Kubernetes Engine：Google Cloudの登録管理サービスです。アプリケーションがKubernetesクラスター上でスムーズに動作することを保証します。
正解についての説明：
（選択肢）
・アプリケーションデプロイメントのメモリ制限を増やします
この選択肢が正解の理由は以下の通りです。
アプリケーションが15分間実行された後で突然再起動する原因としては、リソース不足が考えられます。特に、Cloud Profilerによりヒープ使用量が増加し続けていることが確認できているので、これが原因と考えられます。GKEのポッドにはメモリリソースの制限が設定されており、これを超えてしまうとKubernetesがポッドを終了し、再起動を試みます。
したがって、アプリケーションの再起動を防ぐためには、アプリケーションデプロイメントのメモリ制限を増やすことが有効です。
ただし、メモリリークなどの問題が存在する場合は、それを修正する必要があります。
不正解についての説明：
選択肢：アプリケーション展開のCPU制限を増やします
この選択肢が正しくない理由は以下の通りです。
Cloud Profilerが示している問題はヒープ使用量の増加すなわちメモリリークで、これはCPU制限の増加では解決できません。そのため、CPU制限を増やすという選択は目の前の問題解決に直接寄与しません。
それに対して、メモリ制限を増やすことは一時的な対応として適切であり、長期的にはメモリリークの原因を調査し修正する必要があります。
選択肢：高メモリのコンピュートノードをクラスターに追加します
この選択肢が正しくない理由は以下の通りです。
高メモリのコンピュートノードをクラスターに追加すると、それは単にアプリケーションが必要とする全体のメモリ容量を増やすことになりますが、それは一時的な対応に過ぎず、本質的なメモリリークの問題は解決しません。
一方、デプロイメントのメモリ制限を増やすことにより、アプリケーションごとのメモリ使用量が制限され、アプリケーションによるメモリの過剰使用を防ぐことができます。
選択肢：アプリケーションにCloud Traceを追加し、再デプロイします
この選択肢が正しくない理由は以下の通りです。
Cloud Traceはアプリケーションのパフォーマンスの問題を診断するためのものであり、メモリの問題、特にヒープの使用量が増え続ける問題を解決するものではありません。
一方、メモリ制限を増やすと、問題が再起動まで引き延ばせる可能性があります。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling
https://cloud.google.com/profiler/docs/profiling-go
https://golang.org/doc/faq#garbage_collection
</div></details>

### Q. 問題29: 未回答
Google Cloud上でホストされているアプリケーションを監視するためにGoogle Cloud Operation Suiteを使用しています。最近、新しいアプリケーションをデプロイしましたが、そのログがGoogle Cloud Operation Suiteのダッシュボードに表示されません。
この問題をトラブルシューティングする必要があります。
この要件を満たすために、どうすればよいですか？

1. ファイアウォールでポート25が開かれ、Google Cloud Operation Suiteへのメッセージが許可されていることを確認します
2. ホスト仮想マシンにGoogle Cloud Operation Suiteエージェントがインストールされていることを確認します
3. アカウントにGoogle Cloud Operation Suiteダッシュボードを使用するための適切な権限があることを確認します
4. アプリケーションが必要なクライアントライブラリを使用しており、サービスアカウントキーが適切な権限を持っていることを確認します
<details><div>
    答え：4
説明
この問題では、Google Cloud Operation Suiteを用いたサービス監視において、新しいアプリケーションのログがダッシュボードに表示されないトラブルシューティングを要求しています。この時、当該アプリケーションのクライアントライブラリの使用とサービスアカウントキーの権限に焦点を当てる必要があります。一般にロギングの問題は、適切なライブラリの使用、適切な権限の確保、またはそれらが適切に設定されていないことが原因で発生します。これらの要素をチェックすることが問題解決の主要な手段となります。
基本的な概念や原則：
Google Cloud Operation Suite：モニタリング、トラブルシューティング、アプリケーションのパフォーマンス改善を支援するGoogle Cloudのツール群です。
クライアントライブラリ：Google Cloudの各サービスを利用するためのプログラミングインターフェースです。適切なライブラリを使用することで、所定の機能やサービスを操作できます。
サービスアカウントキー：Google Cloudのサービスアカウントを認証するためのキーです。適切な権限が付与されていなければ、目的の操作を行うことはできません。
エージェント：特定の処理やタスクを実行するためのソフトウェアプログラムです。Google Cloud Operation Suiteエージェントは、特定のデータを取得し、解析するために使用します。
権限：特定のリソースへのアクセスや行動を許可する認証設定です。適切な権限が付与されていなければ、所定の操作を行うことはできません。
ファイアウォール：ネットワークを不正アクセスから守るためのシステムです。特定のポートが開いていても、適切な設定がなければ、特定のサービスとの通信は阻害されます。
正解についての説明：
（選択肢）
・アプリケーションが必要なクライアントライブラリを使用しており、サービスアカウントキーが適切な権限を持っていることを確認します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Operation Suiteにアプリケーションのログが表示されない一般的な問題の一つは、アプリケーションがログを適切に送信するためのクライアントライブラリを使用していないことです。各Google Cloudサービスには特定のクライアントライブラリが存在し、これを使用してサービスとのインターフェースを提供します。
したがって、アプリケーションが必要なライブラリを使用していることを確認することは非常に重要です。
また、Google Cloud Operation Suiteでデータを表示するためには、サービスアカウントが適切な権限を持っていることが必要です。それがなければ、アプリケーションはログを送信したとしても、それを表示する権限がないため、ダッシュボード上には表示されないでしょう。
したがって、サービスアカウントキーが適切な権限を持っていることも確認する必要があります。
不正解についての説明：
選択肢：ホスト仮想マシンにGoogle Cloud Operation Suiteエージェントがインストールされていることを確認します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suiteエージェントのインストールは、主に仮想マシンのシステムメトリクス収集を目的としており、特定のアプリケーションのログ表示問題を解決するものではありません。正しくログを送信するためには、アプリケーションが適切なクライアントライブラリを使用し、適切な権限を持つサービスアカウントキーがあることが重要です。
選択肢：アカウントにGoogle Cloud Operation Suiteダッシュボードを使用するための適切な権限があることを確認します
この選択肢が正しくない理由は以下の通りです。
ダッシュボード利用権限がアカウントにあっても、それはアプリケーションがGoogle Cloud Operation Suiteにログを送信するための問題解決に直接関連していません。そのため、この解決策は新しいアプリケーションのログが表示されない問題を解決することはできません。
選択肢：ファイアウォールでポート25が開かれ、Google Cloud Operation Suiteへのメッセージが許可されていることを確認します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suiteのログが表示されない原因は通常、ファイアウォールの設定やポート25が開いているかではなく、アプリケーションがクライアントライブラリを適切に使用していない、またはサービスアカウントキーが必要な権限を持っていないことによるものです。
したがって、この選択肢は問題解決には適切ではありません。
参考リンク：
https://cloud.google.com/logging/docs/setup
https://cloud.google.com/logging/docs/reference/libraries
https://cloud.google.com/iam/docs/understanding-roles#logging-roles
</div></details>

### Q. 問題30: 未回答
あなたは、マイクロサービスアーキテクチャでトラフィックの多いウェブアプリケーションをサポートしています。アプリケーションのホームページには、現在の天気、株価、ニュースヘッドラインなどのコンテンツを含む複数のウィジェットが表示されます。メインのサービングスレッドは、ウィジェットごとに専用のマイクロサービスを呼び出し、ユーザーにホームページを表示します。マイクロサービスは時々失敗します。そのような場合、サービングスレッドはホームページのコンテンツが欠落した状態で提供します。このデグレードがあまりに頻繁に発生すると、アプリケーションのユーザーは不満を感じますが、コンテンツがまったく提供されないよりは、何らかのコンテンツが提供される方がまだ良いと考えます。ユーザーエクスペリエンスが過度に低下しないように、サービスレベル目標（SLO）を設定します。
これを測定するために、どのようなサービスレベル指標（SLI）を使用すべきですか？

1. レイテンシSLI：マイクロサービス呼び出しの総数に対する、100ミリ秒未満で完了するマイクロサービス呼び出しの比率
2. 品質SLI：全回答に対するデグレードしていないレスポンスの比率
3. フレッシュネスSLI：過去10分以内に更新されたウィジェットの割合
4. 可用性SLI：マイクロサービスの総数に対する健全なマイクロサービスの比率
<details><div>
    答え：2
説明
この問題では、あなたがウェブアプリケーションをサポートしているマネージャーという立場から、マイクロサービスが時々失敗してウェブページのコンテンツが欠落する問題をどのように測定するかを解決しようとしています。この問題の重要な部分は、ユーザーが完全な欠落よりも一部のコンテンツ提供を好むという事実です。したがって、これを捉えるためには、完全なレスポンスではなく部分的なレスポンスも考慮して測定する必要があります。それを念頭に置いて、適切なサービスレベル指標（SLI）を選択することがポイントです。
基本的な概念や原則：
マイクロサービスアーキテクチャ：独立したサービス群で構築されるアプリケーションのアーキテクチャスタイルです。各マイクロサービスは個々にデプロイ、スケールすることが可能で、他のサービスとの通信はAPIを通じて行います。
サービスレベル目標（SLO）：サービスの品質や性能に関する具体的な目標を定めたものです。一定期間で達成すべきサービス運用の目標を指します。
サービスレベル指標（SLI）：SLOを測定するための定量的な指標です。SLIのデータはログやモニタリングシステムから集められます。
品質SLI：サービスの全体的な品質を測定するための指標です。例えば、全回答に対するデグレードしていないレスポンスの比率などが考えられます。
可用性SLI：サービスが正常に稼働している状態を測定するための指標です。マイクロサービスの総数に対する正常に動作しているサービスの割合で示されることが多いです。
フレッシュネスSLI：情報が最新のものであることを測定するための指標です。過去一定時間以内に更新されたデータの割合で示されます。
レイテンシSLI：サービスがユーザーの要求にどれくらい早く応答するかを測定するための指標です。特定の時間内で完了する処理の割合で示されます。
正解についての説明：
（選択肢）
・品質SLI：全回答に対するデグレードしていないレスポンスの比率
この選択肢が正解の理由は以下の通りです。
まず、サービスレベル指標（SLI）は、システムの品質、性能、可用性などを評価するための具体的な指標です。今回の問題文からは、マイクロサービスが一部失敗したときにユーザーが一部のコンテンツしか提供されない状態（デグレード状態）を避けることが重要と読み取れます。そのため、"品質SLI：全回答に対するデグレードしていないレスポンスの比率"というSLIを採用することで、ユーザー体験が低下しないようなサービス運用を目指すことが可能となります。
また、品質SLIはウェブサービスのアプリケーション性能を測定するのに非常に有用で、特にマイクロサービスアーキテクチャにおいては、個々のマイクロサービスの品質を正確に追跡することが重要であるため、この選択肢が最も効果的です。
不正解についての説明：
選択肢：可用性SLI：マイクロサービスの総数に対する健全なマイクロサービスの比率
この選択肢が正しくない理由は以下の通りです。
健全なマイクロサービスの比率はマイクロサービスの可用性を示すが、各マイクロサービスが何らかのコンテンツを提供できているか、つまり"ユーザーエクスペリエンスが過度に低下しない"ことを確実に保証する指標ではありません。
それに対して、デグレードしていないレスポンスの比率は直接ユーザーエクスペリエンスを反映する指標です。
選択肢：フレッシュネスSLI：過去10分以内に更新されたウィジェットの割合
この選択肢が正しくない理由は以下の通りです。
問題設定ではウィジェットの内容が古いことについては言及しておらず、デグレードした状態で提供されていないかどうかが重視されています。フレッシュネスSLIはウィジェットの更新頻度を測定しますが、ユーザーエクスペリエンスの低下を防ぐには、デグレードしていないレスポンスの比率を測定する品質SLIの方が適切です。
選択肢：レイテンシSLI：マイクロサービス呼び出しの総数に対する、100ミリ秒未満で完了するマイクロサービス呼び出しの比率
この選択肢が正しくない理由は以下の通りです。
レイテンシSLIはマイクロサービスの応答速度を測定しますが、問題の要件はデグレードの発生頻度の把握であり、データの欠落（品質）に関連するものです。レイテンシSLIは品質の問題を捉えることはできません。
参考リンク：
https://cloud.google.com/monitoring/api/metrics_Google Cloud#Google Cloud-lb
https://cloud.google.com/architecture/framework/reliability/slos
https://sre.google/workbook/implementing-slos/
</div></details>

### Q. 問題31: 未回答
あなたはCloud Loggingにログを書き込むアプリケーションを管理しています。あなたは一部のチームメンバーにログをエクスポートする機能を与える必要があります。
この要件を満たすために、どうすればよいですか？

1. logging.sinks.listとlogging.sink.getパーミッションを持つカスタムIAMロールを作成し、付与します
2. チームメンバーにCloud IAMのlogging.configWriterのIAMロールを付与します
3. これらのメンバーにのみログのエクスポートを許可するように、Access Context Managerを構成します
4. Cloud IAMで組織ポリシーを作成し、これらのメンバーにのみログエクスポートの作成を許可します
<details><div>
    答え：2
説明
この問題では、Cloud Loggingからログをエクスポートする能力を特定のチームメンバーに付与する方法を探しています。それを行うためには、特定のIAMロールやポリシーの付与、リソースに対するパーミッションの設定といった操作が考えられます。選択肢を検討するにあたっては、それぞれのメソッドの特性とCloud Loggingのエクスポート操作に必要な権限について理解を深めることが必要です。
基本的な概念や原則：
Cloud IAM（Identity and Access Management）：Google Cloudのリソースへのアクセスを制御するためのツールです。特定のユーザーに特定のリソースへの許可を与えたり、その許可を取り消したりすることが可能です。
IAMロール：IAMポリシーに付与できる許可の集合です。例えば、logging.configWriterロールは、Cloud Loggingの設定を作成および管理する権限を持つユーザーに付与されます。
ログのエクスポート：Cloud Loggingから外部のストレージやツールにログ数据を移動させる行為です。エクスポートは、ログの長期保存、コンプライアンス監査、または外部のログ分析ツールでの処理が必要な場合に使用されます。
Access Context Manager：Google Cloudのサービスで、組織全体のアクセスレベルを設定し管理することが可能です。しかし、特定の個々の操作（例えばログのエクスポート）を制御するためには、より細かい制御を提供するIAMが適しています。
組織ポリシー：Google Cloudの全てのリソースに適用されるルールで、特定の設定や制限を提供します。しかし、個々のユーザーの特定の操作に対する権限を制御するためには、よりパーソナライズされた権限を提供するIAMが適しています。
正解についての説明：
（選択肢）
・チームメンバーにCloud IAMのlogging.configWriterのIAMロールを付与します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud IAM（Identity and Access Management）は、Google Cloudの資源へのアクセスを制御する重要なツールであり、特定のロールを特定のユーザーまたはグループに付与することで精緻なアクセス制御が可能です。その中のlogging.configWriterロールは、Cloud Loggingの設定を操作するための許可を与えます。具体的には、Cloud Loggingのログビューやログベースのメトリクスを作成・更新し、ログエクスポート先に新しいシンクを作成する権限を持っています。
したがって、このロールをチームメンバーに付与することで、ログのエクスポートを可能にするという要件を満たすことができます。
不正解についての説明：
選択肢：これらのメンバーにのみログのエクスポートを許可するように、Access Context Managerを構成します
この選択肢が正しくない理由は以下の通りです。
Access Context Managerは、Google Cloudリソースへのアクセスを制御するために使用されますが、特定の操作（ログのエクスポートなど）へのアクセスを許可するロールを果たすものではありません。
これに対して、IAMのlogging.configWriterのロールはログのエクスポートを許可する権限を持っています。
選択肢：logging.sinks.listとlogging.sink.getパーミッションを持つカスタムIAMロールを作成し、付与します
この選択肢が正しくない理由は以下の通りです。
logging.sinks.listとlogging.sink.getパーミッションを持つカスタムIAMロールを作成し付与すると、ロールを付与されたユーザーはロギングシンクのリストを取得や特定のシンクデータの取得をすることができますが、エクスポートの設定を行う（書き込み）権限はありません。
それに対して、logging.configWriterのIAMロールはログエクスポートの設定に必要な全ての権限を持つため、正解の選択肢となります。
選択肢：Cloud IAMで組織ポリシーを作成し、これらのメンバーにのみログエクスポートの作成を許可します
この選択肢が正しくない理由は以下の通りです。
組織ポリシーは組織全体のリソースに対するアクセス制御を設定するもので、特定のメンバーにログエクスポートの操作権限を付与する趣旨とは異なります。
それに対して、IAMロールのlogging.configWriterを付与することでログのエクスポート許可を具体的なメンバーに与えることが可能です。
参考リンク：
https://cloud.google.com/logging/docs/access-control
https://cloud.google.com/iam/docs/understanding-roles#logging-roles
https://cloud.google.com/logging/docs/export/configure_export_v2
</div></details>

### Q. 問題32: 未回答
あなたの会社は、サイト信頼性エンジニアリングの手法に従っています。あなたは、顧客向けアプリケーションに影響を及ぼす大規模で進行中のインシデントのコミュニケーション担当者です。障害解決までの予定時間はまだありません。あなたは、障害に関する最新情報を求める社内の関係者からのメールや、何が起きているかを知りたがっている顧客からのメールを受け取っています。あなたは、障害の影響を受けるすべての人に効率的に最新情報を提供したいと考えています。
この要件を満たすために、どうすればよいですか？

1. すべてのステークホルダーに定期的な最新情報をタイムリーに提供します。すべてのコミュニケーションにおいて"次の更新"の時間を約束します
2. 社内関係者の電子メールへの対応を、インシデント対応チームの別のメンバーに委ねます。顧客に直接回答を提供することに専念します
3. 少なくとも30分ごとに社内のステークホルダーに対応することに重点を置きます。"次の更新"の時間を約束します
4. すべての社内関係者の電子メールをインシデントコマンダーに提供し、彼らが社内コミュニケーションを管理できるようにします。顧客への直接の回答に重点を置きます
<details><div>
    答え：1
説明
この問題では、大規模なインシデント発生時に、内部関係者と顧客へのコミュニケーションをどのように最適化するかが問われています。影響を受けるすべてのステークホルダーへ効率的に最新情報を提供する必要があり、その手法が重要です。また、次の更新のタイミングを明示することもキーとなります。明確なコミュニケーション戦略を持つことで、現在の状況と進行状況をステークホルダーと顧客に適切に伝達することができます。したがって、コミュニケーション戦略の最適化と情報更新の頻度が解答の選択に影響を与えます。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：システムやサービスの信頼性を高めるための手法で、ソフトウェアエンジニアリングの原則をインフラストラクチャと運用問題に適用します。
インシデント：サービスやシステムに予期しない問題が発生した場合の事象です。インシデントは通常、システムのダウンタイムやパフォーマンスの低下を引き起こし、ユーザー体験に影響を及ぼします。
インシデントのコミュニケーション：障害や問題が発生した場合の情報共有のプロセスです。定期的な更新を提供し、次の更新の時間を常に共有することが重要です。
ステークホルダー：プロジェクトやサービスの成果に関心を持つ人や組織です。これには、顧客、従業員、投資家、サプライヤー等が含まれます。
社内のステークホルダー：企業内部の関係者で、例えば経営陣や他の部門のメンバーなどが含まれます。彼らに対するコミュニケーションは重要ですが、顧客からの問い合わせにも対応することが重要です。
インシデント対応チーム：インシデント発生時に対応する専門のチームです。彼らのロールは、障害の原因を特定し対策を講じることで、サービスを早急に復旧させることです。
インシデントコマンダー：インシデント対応チームのリーダーで、問題の解決と合理的な判断を行う責任があります。
正解についての説明：
（選択肢）
・すべてのステークホルダーに定期的な最新情報をタイムリーに提供します。すべてのコミュニケーションにおいて"次の更新"の時間を約束します
この選択肢が正解の理由は以下の通りです。
まず、大規模なシステムの障害では、関係者全員が最新の情報を把握することが重要です。特に、期待された解決時間を提供することができない場合、定期的に情報を提供することがそれ以上の混乱を防ぐ上で不可欠となります。こうすることで、関係者が何が起こっているのか、どのように対処されているのかについて把握しやすくなり、信頼性が保たれます。
また、"次の更新"の時間を約束することは、関係者が情報を得られる時間を明確にする上で重要です。これにより、関係者は定期的な更新を確実に得られ、不要な問い合わせや混乱が軽減されます。
したがって、"すべてのステークホルダーに定期的な最新情報をタイムリーに提供し、すべてのコミュニケーションにおいて"次の更新"の時間を約束する"という選択肢は、大規模で進行中のインシデントに対する効率的でタイムリーなコミュニケーションスキーマとなります。
不正解についての説明：
選択肢：少なくとも30分ごとに社内のステークホルダーに対応することに重点を置きます。"次の更新"の時間を約束します
この選択肢が正しくない理由は以下の通りです。
障害に影響を受けているのは社内のステークホルダーだけではなく、何が起きているかを知りたいと思っている顧客もいます。
したがって、適切な通信は、社内のステークホルダーだけでなく、影響を受ける全ての人々に対してタイムリーに行わなければならないため、この選択肢は不適切です。
選択肢：社内関係者の電子メールへの対応を、インシデント対応チームの別のメンバーに委ねます。顧客に直接回答を提供することに専念します
この選択肢が正しくない理由は以下の通りです。
指定された選択肢では、社内関係者と顧客を別々に扱うことになり、情報が一貫していない可能性があります。正解は同じ情報を全員にタイムリーに提供することです。分散した対応は情報のニーズを満たす際の効率性と一貫性を損なう可能性があります。
選択肢：すべての社内関係者の電子メールをインシデントコマンダーに提供し、彼らが社内コミュニケーションを管理できるようにします。顧客への直接の回答に重点を置きます
この選択肢が正しくない理由は以下の通りです。
インシデントコマンダーに全ての社内関係者からのメールを管理させるというのは、彼らが本来の障害解決の任務から離れてしまうので効率的ではありません。
また、顧客への回答に重点を置くよりも、全てのステークホルダーに対して定期的に最新情報を提供する方が公平で透明性も担保できます。
参考リンク：
https://cloud.google.com/incident-response/docs
https://cloud.google.com/iam/docs/granting-changing-revoking-access
https://sre.google/sre-book/managing-incidents/
</div></details>

### Q. 問題33: 未回答
あなたは、同じGoogle Cloudプロジェクトで、Compute Engine上で動作する複数の本番システムを管理しています。それぞれのシステムには、専用のCompute Engineインスタンスがあります。各システムの実行にかかるコストを知りたいです。
この要件を満たすために、どうすればよいですか？

1. すべてのインスタンスに、実行するシステムに固有のラベルを割り当てます。BigQueryの課金エクスポートとラベルごとのクエリコストを設定します
2. すべてのインスタンスを、実行するシステムに固有のメタデータで強化します。BigQueryにエクスポートするCloud Loggingを設定し、メタデータに基づいてコストをクエリします
3. 各仮想マシン（VM）に実行するシステムの名前を付けます。Cloud Storageのバケットへの使用状況レポートのエクスポートを設定します。そのバケットをBigQueryのソースとして設定し、VM名に基づいてコストをクエリします
4. Google Cloud Consoleでは、Cost Breakdownセクションを使ってシステムごとのコストを可視化します
<details><div>
    答え：1
説明
この問題では、Google Cloudプロジェクトで実行される複数の本番システムがそれぞれCompute Engineインスタンスに存在し、それぞれのシステムの実行にかかるコストを知る方法を考えることが求められています。課題は、個々のシステムのコストを特定できるように（すなわちそれを追跡、履歴化、分析できるように）適切なリソース識別とコスト分析手段をどのように配置するかです。したがって、この問題を解くには、Google Cloudの課金とコスト管理、特にリソースへのラベルの適用、およびGoogle Cloudのデータ分析ツール（特にBigQuery）を理解することが必要です。
基本的な概念や原則：
Compute Engine：Google CloudのIaaS（Infrastructure as a Service）で、仮想マシンを提供します。
ラベル：Google Cloudのリソースをカテゴリに分ける機能です。ラベルを利用することで、リソースの管理や費用の追跡を容易にします。
BigQueryの課金エクスポート：Google Cloudの課金データをBigQueryにエクスポートする機能です。詳細なコスト分析や、長期間のトレンド分析が可能になります。
Google Cloud Console：Google Cloudのリソースを管理するためのウェブベースのインターフェースです。しかし、Cost Breakdownセクションでは各システムの実行コストを確認することはできません。
Cloud Logging：Google Cloudのログまり管理システムです。仮想マシンのメタデータに基づくコストのクエリは、Cloud Loggingではサポートされていません。
Cloud Storage：Google Cloudのオブジェクトストレージサービスです。しかし、VM名に関するコスト情報は使用状況レポートには含まれません。
正解についての説明：
（選択肢）
・すべてのインスタンスに、実行するシステムに固有のラベルを割り当てます。BigQueryの課金エクスポートとラベルごとのクエリコストを設定します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloudではラベルを使用して、リソース（この場合はCompute Engineインスタンス）を分類し、それぞれのリソースにタグをつけることが可能です。これにより特定のラベルが付けられたリソース（インスタンス）の利用状況やコストを分析することが容易になります。
また、BigQueryを使用して課金情報をエクスポートすることでコスト情報を詳細に分析できます。BigQueryは高速でフレキシブルなデータ分析ツールなので、ラベルに基づくクエリを設定し、各システムの実行にかかるコストを把握するのに適しています。
したがって、各システムに専用のインスタンスが存在し、そのそれぞれの運用コストを把握したい場合、各インスタンスにシステム固有のラベルを付け、BigQueryの課金エクスポートとラベルごとのクエリコストを設定することで、必要な情報を取得できます。
不正解についての説明：
選択肢：Google Cloud Consoleでは、Cost Breakdownセクションを使ってシステムごとのコストを可視化します
この選択肢が正しくない理由は以下の通りです。
Google Cloud ConsoleのCost Breakdownセクションは、全体的なプロジェクトのコストブレークダウンを提供しますが、具体的なインスタンスレベルのコスト詳細は提供しないため、各システムの実行にかかるコストを特定するためには、ラベルと課金エクスポートを使用する方法が必要となります。
選択肢：すべてのインスタンスを、実行するシステムに固有のメタデータで強化します。BigQueryにエクスポートするCloud Loggingを設定し、メタデータに基づいてコストをクエリします
この選択肢が正しくない理由は以下の通りです。
まず、メタデータは課金に関連する情報を管理するためのものではなく、インスタンスに関する具体的な情報を提供します。
また、Cloud Loggingは主にシステムやアプリケーションのイベントをロギングするものであり、コスト管理のためには不適切です。反対に、ラベルと課金エクスポートを使用すると正確にコストを追跡できます。
選択肢：各仮想マシン（VM）に実行するシステムの名前を付けます。Cloud Storageのバケットへの使用状況レポートのエクスポートを設定します。そのバケットをBigQueryのソースとして設定し、VM名に基づいてコストをクエリします
この選択肢が正しくない理由は以下の通りです。
Google Cloudのコストを追跡するための主流の方法は、リソースへのラベルの割り当てと、それに基づいて課金エクスポートで取得することであり、VMの名前に基づいてコストを追跡することは可能ではありません。
また、使用状況レポートをCloud Storageにエクスポートしても、各VMの詳細なコストデータを提供するものではなく、そのためコスト追跡の要求を満たしません。
参考リンク：
https://cloud.google.com/billing/docs/how-to/labels
https://cloud.google.com/billing/docs/how-to/export-data-bigquery
https://cloud.google.com/bigquery/docs
</div></details>

### Q. 問題34: 未回答
あなたの会社のCTOは、社内で使用するために、すべてのインシデントに関するポストモーテムポリシーを導入するようあなたに依頼しました。あなたは、そのポリシーがあなたの会社で成功するように、良いポストモーテムとは何かを定義したいと思います。
この要件を満たすために、どうすればよいですか？（2つ選択）

1. すべてのポストモーテムに、インシデントの原因、インシデントを引き起こした責任者またはチームの特定、インシデントの今後の発生を防止する方法を含めるようにします
2. すべてのポストモーテムに、何が原因でインシデンとが発生したのか、インシデンとがどのように悪化した可能性があるのか、また、将来、インシデンとが発生するのを防ぐにはどうすればよいのかを含めるようにします
3. すべてのポストモーテムに、すべてのインシデント参加者が含まれるようにし、ポストモーテムをできるだけ広く共有します
4. すべてのポストモーテムに、インシデントの重大性、インシデントの今後の発生を防止する方法、および内部システムコンポーネントの名前を挙げずにインシデントの原因を含めるようにします
5. すべてのポストモーテムに、顧客情報を名指しすることなく、インシデントの解決方法とインシデントの原因を含めるようにします
<details><div>
    答え：2,3
説明
この問題では、それぞれの選択肢が提供するポストモーテムのベストプラクティスと効用、それに関連する社内のimpacted stakeholders（影響を受けるステークホルダー）の参照や彼らへの情報共有の有無、その情報の具体性と精度を試されています。ポストモーテムを成功させるためには、全体的な透明性と責任の免責が重要であり、それはインシデントの理解と改善に役立つ全体的な学習と知識を共有することを目指すべきです。情報を十分に提供しつつ、個別のチームまたは個人への非難を避ける選択肢を見つけるのが重要です。
基本的な概念や原則：
ポストモーテム分析：インシデンとやインシデント後に行われるレビューのことで、何がうまくいかなかったのか、何が原因であったのか、どのように改善可能だったのかを理解し、将来のインシデントを防止するためのプラクティスです。
レビューの公開：ポストモーテムレビューは、関与者全員が参加できるように共有するべきです。これにより、全員が問題の解決に参加し、学びを共有することができます。
インシデントの原因分析：インシデンとやインシデントが発生した原因を適切に識別し、分析することが重要です。この情報は、同様の問題が再発するのを防ぐために使用されます。
フェイルセーフの設計：システムが故障した場合に問題がさらに悪化する可能性を考慮し、それを防止するための方法を特定することが重要です。
適切な情報の共有：ポストモーテム分析には適切な情報が含まれていることが重要で、個々の人やチームを責めたり、顧客情報を不適切に公開したりするべきではありません。重要なのは問題の解決と将来の問題の予防です。
正解についての説明：
（選択肢）
・すべてのポストモーテムに、何が原因でインシデンとが発生したのか、インシデンとがどのように悪化した可能性があるのか、また、将来、インシデンとが発生するのを防ぐにはどうすればよいのかを含めるようにします
・すべてのポストモーテムに、すべてのインシデント参加者が含まれるようにし、ポストモーテムをできるだけ広く共有します
この選択肢が正解の理由は以下の通りです。
まず、インシデンとの原因、インシデンとが悪化した可能性のある要因、そしてそれを防ぐための予防策を明確にすることで、ポストモーテムが具体的な学習をもたらす価値ある過程となります。このような情報は、それぞれのインシデントから学び、同じ問題が再発するのを防ぎ、システムの改善に直接つながる情報であり、ポストモーテムの主要なロールを果たします。
次に、全てのインシデント参加者の意見を反映し、ポストモーテムを広く共有することもまた重要です。このアプローチにより、ポストモーテムは専門家だけでなく関連するすべてのステークホルダーにとっての学習の機会となります。
また、インシデンとが関連するすべての部分を正確に把握し、全体像を理解することができ、一部の人だけが情報を持つという状況を防ぎます。これらがポストモーテムポリシーが社内で成功するための重要な要素です。
不正解についての説明：
選択肢：すべてのポストモーテムに、インシデントの原因、インシデントを引き起こした責任者またはチームの特定、インシデントの今後の発生を防止する方法を含めるようにします
この選択肢が正しくない理由は以下の通りです。
ポストモーテム文化の目的は、非難せずにインシデンとから学ぶためです。
したがって、インシデントを引き起こした責任者またはチームを特定することは、ポストモーテムの目的に反します。適切なポストモーテムは、原因と予防策を特定することに焦点を当てつつ、非難から遠ざけます。
選択肢：すべてのポストモーテムに、インシデントの重大性、インシデントの今後の発生を防止する方法、および内部システムコンポーネントの名前を挙げずにインシデントの原因を含めるようにします
この選択肢が正しくない理由は以下の通りです。
インシデント原因の分析と改善策の提案において、内部システムコンポーネントの名前を伏せてしまうと、真の原因を特定しにくく、解決の道筋を立てるのが難しくなります。効率的な問題解決のため、具体的な情報が必要となるためです。
選択肢：すべてのポストモーテムに、顧客情報を名指しすることなく、インシデントの解決方法とインシデントの原因を含めるようにします
この選択肢が正しくない理由は以下の通りです。
ポストモーテムではインシデントの解決方法と原因だけでなく、インシデンとがどのように悪化した可能性があり、将来のインシデンとを防ぐための改善策も含める必要があります。つまり、ただ原因と解決策を記すだけでは、重要な部分が欠けてしまいます。
参考リンク：
https://cloud.google.com/incident-response
https://cloud.google.com/architecture/creating-a-culture-of-observability
https://landing.google.com/sre/sre-book/chapters/postmortem-culture/
</div></details>

### Q. 問題35: 未回答
本番サービスの一部は、eu-west-1リージョンのGoogle Kubernetes Engine（GKE）で稼働しています。ビルドシステムはus-west-1リージョンで稼働しています。ビルドシステムからコンテナイメージをスケーラブルなレジストリにプッシュして、イメージをクラスターに転送する帯域幅を最大化したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？

1. eu-west-1リージョンのCompute Engineインスタンスで実行されているプライベートイメージレジストリにイメージをプッシュします
2. gcr.ioホスト名を使用してGoogle Container Registry（GCR）にイメージをプッシュします
3. us.gcr.ioホスト名を使用して、Google Container Registry（GCR）にイメージをプッシュします
4. eu.gcr.ioホスト名を使用してGoogle Container Registry（GCR）にイメージをプッシュします
<details><div>
    答え：4
説明
この問題では、Google Kubernetes Engineとビルドシステムが異なるリージョンで稼働しており、その中で最も効率的にコンテナイメージを転送する方法を問われています。ここでは、Google Container Registry（GCR）の特性と利用方法を深く理解し、リージョナルGCRエンドポイントを適切に使用することが求められています。そのために、具体的なリージョンにまで細かく指定してアクセスする方法がかなり重要であり、リージョンの準拠性やイメージリポジトリを適切に管理するための理解が必要です。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudのマネージドコンテナオーケストレーションサービスです。コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を自動化します。
Google Container Registry（GCR）：Google Cloudのプライベートコンテナイメージレジストリサービスです。Dockerイメージを安全にホストし、共有します。
ホスト名によるレジストリの位置指定：GCRでは、ホスト名を変えることでレジストリの地理的な位置を指定できます。例えば、eu.gcr.ioはヨーロッパのレジストリ、us.gcr.ioはアメリカのレジストリを指します。
リージョナルリソース：Google Cloudでは、リソースを指定の地理的なリージョンに配置することができます。これはサービスの公用性、パフォーマンス、コンフライアンス要件に影響します。
ネットワーク帯域幅：データ転送速度を最大化するためには、物理的に近いレジストリを選択することが有効です。理想的には、コンテナイメージの実行場所と同じリージョンのレジストリが最適です。
プライベートイメージレジストリ：独自のコンテナイメージをホストするためのプライベートレジストリです。セキュリティを強化したい場合や特定の要件を満たすために使用しますが、管理が必要です。
正解についての説明：
（選択肢）
・eu.gcr.ioホスト名を使用してGoogle Container Registry（GCR）にイメージをプッシュします
この選択肢が正解の理由は以下の通りです。
Google Container Registry（GCR）は、欧州のeu.gcr.ioというホスト名でアクセス可能な、プライベートなコンテナイメージのストレージサービスです。このホスト名を使用することで、イメージはヨーロッパリージョン、つまり本番サービスが稼働しているeu-west-1と同じリージョンに保存されます。この構成により、クラスターへのイメージ転送時にGKEが高速でアクセスでき、帯域幅が最大化されます。
また、GCRはスケーラブルなレジストリであり、ビルドシステムからプッシュされるイメージの保存や転送に問題なく対処できます。
したがって、これが最適な選択肢になります。
不正解についての説明：
選択肢：gcr.ioホスト名を使用してGoogle Container Registry（GCR）にイメージをプッシュします
この選択肢が正しくない理由は以下の通りです。
"gcr.io"は北米に位置するデータセンターにデータを送ります。ビルドシステムはus-west-1にありますが、イメージを最終的に使用する本番サービスはeu-west-1に存在するため、ヨーロッパの"eu.gcr.io"を利用してイメージをプッシュした方が転送時間が短縮されます。
選択肢：us.gcr.ioホスト名を使用して、Google Container Registry（GCR）にイメージをプッシュします
この選択肢が正しくない理由は以下の通りです。
ビルドシステムがus-west-1リージョンで稼働しているとはいえ、本番サービスはeu-west-1リージョンで稼働しているため、us.gcr.ioを使用すると転送する帯域幅が最大化されません。そのため、eu.gcr.ioを使用することで、転送の帯域幅を最大化することができます。
選択肢：eu-west-1リージョンのCompute Engineインスタンスで実行されているプライベートイメージレジストリにイメージをプッシュします
この選択肢が正しくない理由は以下の通りです。
Compute Engineインスタンスで運用するプライベートイメージレジストリでは、帯域幅の最大化やスカラビリティの確保が困難です。逆にGCRはイメージ配布に最適化されており、帯域幅の最大化を実現できます。
参考リンク：
https://cloud.google.com/container-registry/docs/pushing-and-pulling
https://cloud.google.com/container-registry/docs/overview
https://kubernetes.io/docs/concepts/containers/images/#image-registries
</div></details>

### Q. 問題36: 未回答
あなたの会社は、サイト信頼性エンジニアリングの実践に従っています。あなたは、顧客に影響を与える新しいインシデントのインシデントコマンダーです。効果的なインシデント対応を支援するために、すぐに2つのインシデント管理のロールを割り当てる必要があります。どのようなロールを割り当てるべきですか？（2つ選択）

1. コミュニケーションリード
2. エンジニアリングリード
3. 社外顧客コミュニケーションリーダー
4. オペレーションリード
5. カスタマーインパクト評価者
<details><div>
    答え：1,4
説明
この問題では、サイト信頼性エンジニアリング（SRE）の実践が重視されており、新しいインシデントが発生した際に、どのようなロールを効果的に割り当てるべきかに焦点を当てています。ロールの割り当てがインシデント管理の成熟度、効率性、効果性にどのように影響するかを理解することが求められます。選択肢はSREの主要なロールを代表しており、これらのうちどの2つが具体的な状況に最も適しているかを判断する必要があります。
基本的な概念や原則：
インシデントコマンダー：インシデント対応の全体的な戦略を立て、他のロールと協力して問題の解決を図る責任者のロールです。
オペレーションリード：具体的なインシデントの解決策や作業手順を指示し、技術的な観点からインシデントを解決するロールです。
コミュニケーションリード：インシデントに関するコミュニケーションの責任を持ち、他のチームや顧客への情報の伝達を行うロールです。
サイト信頼性エンジニアリング：システムの信頼性、スケーラビリティ、効率性を向上させるためのエンジニアリング手法です。ソフトウェアエンジニアとシステムエンジニアのスキルセットを組み合わせており、開発と運用のギャップを埋めるロールを果たします。
正解についての説明：
（選択肢）
・オペレーションリード
・コミュニケーションリード
この選択肢が正解の理由は以下の通りです。
オペレーションリードとコミュニケーションリードは、インシデント管理における2つの重要なロールです。オペレーションリードは技術的側面を担当し、インシデントの解決に必要な技術的判断を下します。このロールは具体的な問題解決に焦点を当て、そのための戦略やタスクを導き出すロールを果たします。これにより、インシデントコマンダーは全体的な戦略に焦点を当てることができ、複数の問題の優先順位付けや多面的な問題解決に取り組むことが可能になります。
一方、コミュニケーションリードはインシデントのコミュニケーションを担当します。顧客、ステークホルダー、チーム間など、ステークホルダー全体との交流が必要な場合に重要なロールを果たします。インシデントが発生した際には、迅速かつ適切な情報の提供は必須であり、その情報を関係者に対して明確に伝える重要性は非常に大きいです。このロールが存在することで、インシデントコマンダーは混乱を避け、インシデントへの対応に注力でき、全体的な対策の効率化を図りつつ、信頼性を確保することが可能となります。
不正解についての説明：
選択肢：エンジニアリングリード
この選択肢が正しくない理由は以下の通りです。
サイト信頼性エンジニアリングの実践において、エンジニアリングリードというロールは存在しません。オペレーションリードはインシデント対応の実行、コミュニケーションリードは対外的な情報伝達を担当する重要なロールです。
選択肢：カスタマーインパクト評価者
この選択肢が正しくない理由は以下の通りです。
カスタマーインパクト評価者は具体的なインシデント管理のロールではありません。
一方、オペレーションリードはインシデント対応の詳細な技術的な実行を担当し、コミュニケーションリードは情報の伝達や外部とのコミュニケーションを担当するというロールが明確なロールです。
選択肢：社外顧客コミュニケーションリーダー
この選択肢が正しくない理由は以下の通りです。
インシデント管理において、社外顧客コミュニケーションリーダーという特定のロールは存在しません。正解選択肢のコミュニケーションリード以外の具体的なロールを設ける必要はありません。コミュニケーションリードは、社内外のコミュニケーション全般を担当します。
参考リンク：
https://cloud.google.com/incident-response/docs/incident-response-process#roles
https://cloud.google.com/blog/products/management-tools/incident-management-solution-in-cloud-operations
https://landing.google.com/sre/books/
</div></details>

### Q. 問題37: 未回答
あなたは、Webアプリケーションの新機能がユーザに好評かどうかを確認するための実験を行っています。その機能をカナリアリリースとしてデプロイした直後、ユーザに送信される500エラーの数が急増し、監視レポートが遅延の増加を示しました。あなたは、ユーザーへの悪影響を迅速に最小化したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？

1. インシデントの事後処理のためのデータを記録します
2. レイテンシ、トラフィック、エラー、サチュレーションの監視を開始します
3. 実験的なカナリアのリリースを撤回します
4. 500エラーの発生源と遅延増加の根本原因を追跡します
<details><div>
    答え：3
説明
この問題では、システムへの変更後のエラー増加への対応を求められています。重要な観点は、"ユーザーへの悪影響を迅速に最小化したい"という意図です。選択肢を評価する際は、問題の症状に対処するものだけでなく、ユーザーペインの最小化に貢献する解決策を選ぶ必要があります。対症療法よりも早急な対応が求められていることを念頭に置くことが重要です。
基本的な概念や原則：
カナリアリリース：新しいバージョンのアプリケーションを限定的なユーザー群に対して先にリリースする方法です。全体にリリースする前に新機能の動作確認やユーザの反応を見ることができます。
500エラー：サーバーサイドのエラーを示すHTTPステータスコードです。アプリケーションのランタイムエラーやサーバーの設定ミスなどが原因で発生します。
リリースの撤回：不具合が発見された際や予期しない影響が出た時に、リリースしたバージョンを停止して元のバージョンに戻す操作です。ユーザーへの影響を最小限に抑えるために用いられます。
レイテンシ、トラフィック、エラー、サチュレーション（四つの黄金指標）：システムの健全性を評価するための指標です。これらの情報は監視システムで収集・分析され、アプリケーションのパフォーマンスと信頼性を維持するために使われます。
インシデントの事後処理：システム障害やエラーが発生した後に行われるプロセスで、問題の原因を特定し、再発を防止するための措置を立案します。
正解についての説明：
（選択肢）
・実験的なカナリアのリリースを撤回します
この選択肢が正解の理由は以下の通りです。
カナリアリリースは、新機能をすべてのユーザーに一度にリリースするのではなく、一部のユーザーに先行してリリースし、機能が適切に機能しユーザーにとって有益であることを確認する方法です。
この問題では、新機能のリリース後に500エラーが急増し、レスポンスタイムも遅くなっているため、新機能に問題があることが明らかです。最優先事項はユーザーへの悪影響を最小限に抑えることで、最も迅速な方法は実験的なカナリアリリースを撤回し、システムを新機能リリース前の状態に戻すことです。これにより、それ以上のユーザーが悪影響を受けるのを防ぎ、問題の特定と修正に時間をかけることができます。
不正解についての説明：
選択肢：レイテンシ、トラフィック、エラー、サチュレーションの監視を開始します
この選択肢が正しくない理由は以下の通りです。
新機能リリース後にエラー急増と遅延増加が発生しており、ユーザへの影響を迅速に最小化する必要がある状況で、レイテンシ、トラフィック、エラー、サチュレーションの監視を開始することは、時間的な緊急性を考慮すると妥当な対応とは言えません。直ちに問題の解決が必要であるため、カナリアリリースの撤回が適切です。
選択肢：インシデントの事後処理のためのデータを記録します
この選択肢が正しくない理由は以下の通りです。
インシデントの事後処理のためのデータを記録することはエラー解決に貢献しますが、ユーザへの影響を即座に最小化することはできません。実際の問題の解決はその後のステップで行われます。
一方、カナリアリリースを撤回すれば直ちに問題の影響を最小限に抑えることが可能です。
選択肢：500エラーの発生源と遅延増加の根本原因を追跡します
この選択肢が正しくない理由は以下の通りです。
500エラーの発生源と遅延増加の根本原因を追跡するのは時間がかかります。要件は"ユーザーへの悪影響を迅速に最小化"したいというものなので、即座に実験的なカナリアのリリースを撤回することが最も効率的な対応となります。
参考リンク：
https://cloud.google.com/architecture/best-practices-for-operating-containers
https://cloud.google.com/run/docs/rolling-out-updates
https://cloud.google.com/monitoring/api/metrics_Google Cloud#Google Cloud-loadbalancing
</div></details>

### Q. 問題38: 未回答
あなたはロードバランサーを使わずにHTTPエンドポイントを公開するアプリケーションを管理しています。HTTPレスポンスの待ち時間はユーザーエクスペリエンスにとって重要です。すべてのユーザーが経験しているHTTPレイテンシを把握したいと考えています。あなたはGoogle Cloud Operation Suite Monitoringを使用します。
この要件を満たすために、どうすればよいですか？

1. アプリケーションで、metricKindをMETRIC_KIND_UNSPECIFIEDに設定し、valueTypeをINT64に設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、Stacked Areaグラフを使用してメトリックを視覚化します
2. アプリケーションで、metricKindをCUMULATIVEに設定し、valueTypeをDOUBLEに設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、折れ線グラフを使用してメトリックを視覚化します
3. アプリケーションで、metricKindをGAUGEに設定し、valueTypeをDISTRIBUTIONに設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、Heatmapグラフを使用してメトリックを視覚化します
4. アプリケーションで、metricKindをDELTAに設定し、valueTypeをDOUBLEに設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、Stacked Bar graphを使用してメトリックを視覚化します
<details><div>
    答え：3
説明
この問題では、Google Cloud Operation Suite Monitoringを活用して、ユーザーエクスペリエンスに影響を与えるHTTPレイテンシを把握する方法について問われています。問題文から、ユーザー全体のHTTPレイテンシの分布を把握することが目的であり、メトリックの種類と値、さらには視覚化手段の選択が重要な要素であることが読み取れます。したがって、適切なメトリックスとその種類を選んで設定し、そしてこのデータをどのように視覚化するかがこの問題の重要なポイントです。
基本的な概念や原則：
Google Cloud Operation Suite Monitoring：Google Cloud上のアプリケーションやインフラストラクチャのパフォーマンスをモニタリングするサービスです。リアルタイムでの監視、優れたアラート機能、洞察に対する対応を可能にします。
metricKind：モニタリングデータの種類を指定します。ユースケースに応じてGAUGE、DELTA、CUMULATIVEから選択します。
valueType：モニタリングデータの型を指定します。数値型(double, int64等)や分布型（Distribution）、文字列型（string）などがあります。
メトリックの作成：Google Cloud Operation Suite Monitoringでは、質量（例えばHTTPレイテンシ）を表すメトリックを作成します。メトリックの値は時間とともに変化し、特定の時間帯におけるパフォーマンスを可視化するのに役立ちます。
メトリックの視覚化：Google Cloud Operation SuiteのMetrics Explorerを用いてメトリックを視覚化します。多様なグラフィカル表現が利用可能で、例えばHeatmapグラフはデータの分布を色彩を用いて表現します。
HTTPレイテンシ：ユーザがリクエストを送信してから、それがサーバに到達するまでの時間を指します。高い待ち時間はユーザーエクスペリエンスに悪影響を与えます。
ロードバランサー：トラフィックをリソースに均等に分散して負荷を管理するシステムです。回答例ではロードバランサーを使用せずにHTTPエンドポイントを公開するパターンに触れています。
正解についての説明：
（選択肢）
・アプリケーションで、metricKindをGAUGEに設定し、valueTypeをDISTRIBUTIONに設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、Heatmapグラフを使用してメトリックを視覚化します
この選択肢が正解の理由は以下の通りです。
まず、HTTPレスポンスの待ち時間を計測するためには、メトリックによるモニタリングが必要です。Google Cloud Operation Suite Monitoringでは、カスタムメトリックを作成し、その値を視覚化することが可能です。
また、"metricKind"の設定はメトリックがどのように解釈されるかを管理します。GAUGEを設定すると、一定の時間間隔で計測されたその時点の値が記録されるため、レイテンシのようなリアルタイムな待ち時間を捉えることが可能となります。
次に、"valueType"のDISTRIBUTIONは、値の分布を表現するために使用されます。これにより、HTTPレスポンスの待ち時間の分布を取得できます。
加えて、Metrics ExplorerのHeatmapグラフを使用することで、全てのユーザーが経験しているHTTPレイテンシの分布を視覚的に確認することができます。このため、この選択肢が求められている解答であると言えます。
不正解についての説明：
選択肢：アプリケーションで、metricKindをDELTAに設定し、valueTypeをDOUBLEに設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、Stacked Bar graphを使用してメトリックを視覚化します
この選択肢が正しくない理由は以下の通りです。
metricKindをDELTAに設定した場合、それは値の変化量を表すので、HTTPレイテンシのような一定期間の値分布を視覚化するには不適切です。
また、valueTypeをDOUBLEに設定したメトリックは単一の数値を表すので分布情報を持てません。
それに対して、正解では、GAUGEで瞬間的な値を取り、DISTRIBUTIONで分布情報を持つことで要件を満たします。
選択肢：アプリケーションで、metricKindをCUMULATIVEに設定し、valueTypeをDOUBLEに設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、折れ線グラフを使用してメトリックを視覚化します
この選択肢が正しくない理由は以下の通りです。
metricKindをCUMULATIVEにし、valueTypeをDOUBLEに設定すると、時間経過に伴う累積データを得ることができますが、それだけでは全ユーザーのHTTPレイテンシの分布を正確に把握することができません。比較的に、GAUGEとDISTRIBUTION設定を用いると、ユーザー全体のHTTPレイテンシの分布が視覚的に理解しやすくなります。
選択肢：アプリケーションで、metricKindをMETRIC_KIND_UNSPECIFIEDに設定し、valueTypeをINT64に設定したメトリックを作成します。Google Cloud Operation SuiteのMetrics Explorerで、Stacked Areaグラフを使用してメトリックを視覚化します
この選択肢が正しくない理由は以下の通りです。
METRIC_KIND_UNSPECIFIEDやINT64では、値の分布やパターンを捉えることができません。これに比べ、正解の選択肢ではGAUGEとDISTRIBUTIONが使われ、全ユーザーの経験するHTTPレイテンシの詳細な分布を把握できます。Stacked Areaグラフでは分布の視覚化に限界があります。
参考リンク：
https://cloud.google.com/monitoring/custom-metrics/creating-metrics
https://cloud.google.com/monitoring/charts/metrics-explorer
https://www.oreilly.com/library/view/google-cloud-certified/9781492092234/
</div></details>

### Q. 問題39: 未回答
Cloud Runを使用してサーバーレスアプリケーションを構築し、そのアプリケーションを本番環境にデプロイしました。あなたはコスト最適化のためにアプリケーションのリソース使用率を特定したいと考えています。
この要件を満たすために、どうすればよいですか？

1. Cloud Opsを使用してログベースのメトリクスを作成し、アプリケーションのリソース使用率を監視します
2. 分散トレース機能付きのCloud Traceを使用して、アプリケーションのリソース使用率を監視します
3. Ops AgentとCloud Profilerを使用して、アプリケーションのCPUとメモリの使用率を監視します
4. Cloud Monitoringを使用して、アプリケーションのコンテナCPUとメモリ使用率を監視します
<details><div>
    答え：4
説明
この問題では、サーバーレスアプリケーションのリソース使用率を特定することが目的で、そのアプリケーションはCloud Runにデプロイされているという事実が重要です。そのため問題解決のためには、Cloud Run用の適切な監視ツールを選択することが重要です。選択肢の中からCloud Runのアプリケーションに適切な監視ツールを選ぶことが求められます。ここでは、リソース使用率を監視するために適切なツールの選択が問われているのでそれに注目することが求められています。
基本的な概念や原則：
Cloud Run：コンテナ化されたアプリケーションをフルマネージドで実行するサービスです。サーバーレス環境での実行を可能にします。
Cloud Monitoring：Google Cloudのアプリケーションやインフラストラクチャのパフォーマンスを追跡し、視覚化するための監視サービスです。リソース使用率などの重要なメトリクスを提供します。
Cloud Trace：アプリケーションのパフォーマンスボトルネックを特定するための分散トレーシングシステムです。しかし、リソース使用率の監視には使用できません。
Ops Agent：Google Cloudのロギング、メトリクス、トレーシングエージェントです。しかし、メモリやCPU使用率に特化した監視能力はなく、Cloud Monitoringと組み合わせて使用されることが一般的です。
Cloud Profiler：Google Cloudのアプリケーションパフォーマンス管理ツールです。アプリケーションの実行時間を視覚化し、パフォーマンス問題を特定します。しかし、リソース使用率の監視には使用できません。
Cloud Ops：Google Cloudのオペレーションスイートです。ログ管理、エラーレポーティング、トレーシング、モニタリングなどの機能を提供します。しかし、ログベースのメトリクス作成はリソース使用率の監視だけでなく、他の多くの目的にも使用されます。
正解についての説明：
（選択肢）
・Cloud Monitoringを使用して、アプリケーションのコンテナCPUとメモリ使用率を監視します
この選択肢が正解の理由は以下の通りです。
まず、Cloud MonitoringはGoogle Cloudの監視、警告、診断ツールであり、アプリケーションのパフォーマンス、稼働状況、リソース使用率を追跡および可視化する機能を提供します。これはコスト最適化のためにあなたが必要としている機能であり、リソース使用率の特定に役立ちます。
また、Cloud MonitoringではCloud Runの各アプリケーションが使用しているCPUとメモリの詳細データを取得することができます。これにより、過負荷や過剰なリソース配分を特定し、最適化することが可能です。
したがって、Cloud Runのアプリケーションのコスト最適化を追求する場合、Cloud Monitoringは有効なツールと言えます。特に、サーバーレスアプリケーションのリソース使用状況を監視し、パフォーマンスを最適化する上で欠かせないサービスです。
不正解についての説明：
選択肢：分散トレース機能付きのCloud Traceを使用して、アプリケーションのリソース使用率を監視します
この選択肢が正しくない理由は以下の通りです。
Cloud Traceの分散トレース機能はアプリケーションのパフォーマンス問題を診断するのに役立ちますが、リソースのCPUやメモリ利用率を監視するためのものではありません。
それに対して、Cloud Monitoringはリソース使用率を視覚的に監視し解析する機能を提供しています。
選択肢：Ops AgentとCloud Profilerを使用して、アプリケーションのCPUとメモリの使用率を監視します
この選択肢が正しくない理由は以下の通りです。
Ops AgentとCloud Profilerはインフラやアプリケーションのパフォーマンスの詳細監視やプロファイリングには役立つが、サーバーレス環境でのリソース使用率の監視は直接的には行えません。
一方、Cloud MonitoringはコンテナのCPUやメモリ使用率を直接監視可能なので、この問題の要件を満たします。
選択肢：Cloud Opsを使用してログベースのメトリクスを作成し、アプリケーションのリソース使用率を監視します
この選択肢が正しくない理由は以下の通りです。
Cloud Opsは、Cloud Operationsの略であり、その一部にCloud Monitoringが含まれます。しかし、リソースの使用率を監視するためには、Cloud Monitoringの機能が直接必要となるため、正解の選択肢の方が具体的に正しい操作を指しています。ログベースのメトリクスではリソース使用率の監視が十分に行われません。
参考リンク：
https://cloud.google.com/monitoring
https://cloud.google.com/run/docs/monitoring/metrics
https://cloud.google.com/trace/docs
</div></details>

### Q. 問題40: 未回答
あなたの組織には、オンプレミスで動作するコンテナ化されたWebアプリケーションがあります。Google Cloudへの移行計画の一環として、以下の受け入れ基準を満たすデプロイ戦略とプラットフォームを選択する必要があります：
1.プラットフォームは、AndroidデバイスからのトラフィックをAndroid固有のマイクロサービスに誘導できなければなりません。
2.プラットフォームは、任意のパーセンテージベースのトラフィック分割が可能である必要があります。
3.デプロイメント戦略により、あらゆるマイクロサービスの複数のバージョンを継続的にテストできなければなりません。
あなたはこの要件を満たすために、どうすればよいですか？

1. アプリケーションのカナリアリリースをApp Engineにデプロイします。トラフィックの分割を使用して、IPアドレスに基づいてユーザートラフィックのサブセットを新しいバージョンに誘導します
2. アプリケーションのカナリアリリースをCompute Engineにデプロイします。Compute EngineでAnthos Service Meshを使用し、仮想サービスを設定することで、ユーザートラフィックの10%をカナリアリリースに誘導します
3. アプリケーションのカナリアリリースをCloud Runにデプロイします。トラフィック分割を使用して、リビジョンタグに基づいて、ユーザートラフィックの10%をカナリアリリースに誘導します
4. Anthosサービスメッシュを使ってGoogle Kubernetes Engineにカナリアリリースをデプロイします。トラフィックの分割を使用して、仮想サービスに設定されたユーザーエージェントヘッダーに基づき、ユーザートラフィックの10%を新しいバージョンに誘導します
<details><div>
    答え：4
説明
この問題では、コンテナ化されたWebアプリケーションをGoogle Cloudに適切に移行し、特定の要件を満たす最適なデプロイ戦略とプラットフォームを選ぶ方法を問います。目標とする要件には、Androidデバイスのトラフィックを特定のマイクロサービスに誘導する能力、任意のパーセンテージベースでのトラフィック分割が可能であること、複数のマイクロサービスのバージョンを同時にテストできるデプロイ戦略の必要性が含まれています。Google Cloudのサービスの理解と、それがどのように上記の要件を満たすのかを評価することがキーとなります。選択肢を吟味する際には、それぞれが提供する機能と限界を考慮しながら選択肢の詳細を注意深く確認することが重要です。
基本的な概念や原則：
Anthos Service Mesh：Google Cloudのサービスで、マイクロサービス間の通信を統制し、セキュリティ、ロードバランシング、モニタリングおよび診断の機能を提供します。
Google Kubernetes Engine（GKE）：マネージドなKubernetesサービスで、エンタープライズレベルのコンテナ化されたアプリケーションを構築、デプロイ、スケーリングするためのプラットフォームです。
カナリアリリース：新バージョンのアプリケーションを一部のユーザーだけに配布するデプロイ戦略で、新機能の影響をリスクを最小限に留めながら確認することが可能です。
トラフィック分割：異なるバージョンのアプリケーションやサービス間でトラフィックを分割します。パーセンテージベースで調整可能で、段階的なロールアウトやABテストに使われます。
Cloud Run：フルマネージド型のサーバーレスプラットフォームで、コンテナ化されたアプリケーションをデプロイし、自動的にスケールアップダウンします。
App Engine：開発者がアプリケーションをビルド、デプロイ、スケールできるフルマネージド型のPaaSです。自動的なスケーリング、ロードバランシング、マネージドランタイムなどの機能を持ちます。
Compute Engine：Google CloudのIaaSプロダクトで、仮想マシンを作成、管理するためのサービスです。
正解についての説明：
（選択肢）
・Anthosサービスメッシュを使ってGoogle Kubernetes Engineにカナリアリリースをデプロイします。トラフィックの分割を使用して、仮想サービスに設定されたユーザーエージェントヘッダーに基づき、ユーザートラフィックの10%を新しいバージョンに誘導します
この選択肢が正解の理由は以下の通りです。
まず、Anthos Service Meshは、マイクロサービス間での通信の管理と制御を提供するサービスメッシュプラットフォームであり、Google Kubernetes Engine（GKE）と一緒に使用されます。この組み合わせは、ANDROIDデバイスからのトラフィックをAndroid固有のマイクロサービスに誘導する機能を持つエンヴォイベースのプロキシを提供します。これは、WebアプリケーションでUser-Agent HTTPヘッダーを使用してトラフィックルーティングを行う機能を意味します。
また、Anthos Service Meshは任意のパーセンテージのトラフィック分割もサポートしています。これにより、新旧バージョン間でトラフィックを効率的に分割し、新機能の影響を評価することが可能となります。
さらに、カナリアリリースとは新バージョンのサービスを限定的に展開し、問題がないことを確認しながら全体に展開していくデプロイ戦略のことを指し、これによって複数のバージョンを継続的にテストする要件にも対応します。この機能により、問題がある場合にはすぐにロールバックするなど、運用の柔軟性と安全性が高まります。
不正解についての説明：
選択肢：アプリケーションのカナリアリリースをCloud Runにデプロイします。トラフィック分割を使用して、リビジョンタグに基づいて、ユーザートラフィックの10%をカナリアリリースに誘導します
この選択肢が正しくない理由は以下の通りです。
Cloud Runはトラフィック分割機能を提供しますが、ユーザーエージェントヘッダーに基づいたトラフィックの誘導はサポートしておらず、特定のマイクロサービスにAndroidからのトラフィックを誘導するという要件を満たせません。これに対してAnthosサービスメッシュならこの機能をサポートしています。
選択肢：アプリケーションのカナリアリリースをApp Engineにデプロイします。トラフィックの分割を使用して、IPアドレスに基づいてユーザートラフィックのサブセットを新しいバージョンに誘導します
この選択肢が正しくない理由は以下の通りです。
App EngineのカナリアリリースはIPアドレスに基づいてユーザートラフィックを分割することが可能ですが、Androidデバイスからのトラフィックを特定のマイクロサービスに誘導することはできません。この要件を満たすためには、ユーザーエージェントヘッダーに基づくトラフィック誘導が可能なAnthosサービスメッシュのようなソリューションが必要です。
選択肢：アプリケーションのカナリアリリースをCompute Engineにデプロイします。Compute EngineでAnthos Service Meshを使用し、仮想サービスを設定することで、ユーザートラフィックの10%をカナリアリリースに誘導します
この選択肢が正しくない理由は以下の通りです。
Compute EngineはAnthos Service Meshのサポート対象外であるため、この設定を利用することはできません。
したがって、Compute Engineを使用したシナリオは提供要件を満たさない適切な解決策ではなく、Google Kubernetes Engineを使用するほうが適しています。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/traffic-management
https://cloud.google.com/anthos-service-mesh/docs
https://cloud.google.com/kubernetes-engine/docs/how-to/deploying-apps
</div></details>

### Q. 問題41: 未回答
あなたの開発チームは、自社サービスのAPIの新バージョンを作成しました。あなたは、サードパーティの開発者やサードパーティがインストールしたアプリケーションのエンドユーザーへの混乱を最小限に抑えながら、新バージョンのAPIをデプロイする必要があります。
この要件を満たすために、どうすればよいですか？

1. 旧バージョンのAPIの廃止をアナウンスします。新バージョンのAPIを導入します。旧バージョンのAPIを使用しているユーザーに連絡します。旧バージョンのAPIを廃止します。旧バージョンのAPIを停止します。旧バージョンのAPIを使用しているユーザーにベストエフォートサポートを提供します
2. 新バージョンのAPIを導入します。旧APIの残存ユーザーに連絡します。旧バージョンのAPIの非推奨をアナウンスします。旧バージョンのAPIを廃止します。旧バージョンのAPIを停止します。旧バージョンのAPI利用者にベストエフォートサポートを提供します
3. 新しいバージョンのAPIを導入します。旧バージョンのAPIの非推奨をアナウンスします。旧バージョンのAPIを廃止します。旧APIの残存ユーザーに連絡します。旧APIの利用者にベストエフォートサポートを提供します。旧バージョンのAPIを停止します
4. 旧バージョンのAPIの廃止をアナウンスします。旧バージョンのAPIを使用しているユーザーに連絡します。新バージョンのAPIを導入します。旧バージョンのAPIを廃止します。旧バージョンのAPI利用者にベストエフォートサポートを提供します。旧バージョンのAPIを停止します
<details><div>
    答え：3
説明
この問題では、新しいAPIバージョンをデプロイするにあたってどのように進めるべきかについて問われています。サードパーティの開発者やエンドユーザーを混乱させないためにどのようなステップを踏むべきかが重要です。旧バージョンの非推奨や直接停止、新しいバージョンの導入時や旧バージョンの停止時にどのような連絡を行う必要があるかなど、変更を追跡できるような適切なプロセスが求められています。それぞれの選択肢の内容や順序を慎重に比較検討しつつ、最も適切なAPIのバージョンアップ戦略を選びます。
基本的な概念や原則：
APIバージョニング：APIのバージョン管理は、APIの変更を他のサービスやアプリケーションが利用しやすいように調整するプロセスです。新しいバージョンのAPIを導入する際には、旧バージョンのAPIの廃止前に通知を行います。
非推奨アナウンス：非推奨とするAPIバージョンの情報を公告することで、利用者にバージョンアップの必要性を伝えます。これはエンドユーザーやサードパーティ開発者に対する混乱を最小限に抑えるために重要です。
APIの廃止：APIのバージョンが非推奨となった後、一定期間を経てAPIを完全に廃止します。これにより、利用者は新しいAPIバージョンへの移行を計画することができます。
ユーザーへの通知：非推奨となるAPIの利用者に対して通知を行い、新しいバージョンへの移行を促します。これにより、利用者が突然のAPI変更による影響を受けることを防ぎます。
ベストエフォートサポート：非推奨となったAPIバージョンの利用者に対して、限られたリソース内でのサポートを提供します。これにより、利用者が新バージョンへの移行をスムーズに行うことを支援します。
APIの停止：最終的には非推奨となったAPIバージョンを完全に停止します。これは新しいバージョンのAPIの導入および旧バージョンの廃止プロセスの最終ステップです。
正解についての説明：
（選択肢）
・新しいバージョンのAPIを導入します。旧バージョンのAPIの非推奨をアナウンスします。旧バージョンのAPIを廃止します。旧APIの残存ユーザーに連絡します。旧APIの利用者にベストエフォートサポートを提供します。旧バージョンのAPIを停止します
この選択肢が正解の理由は以下の通りです。
サードパーティの開発者やエンドユーザーへの混乱を最小限に抑えるためには、変更に対する透明性と段階的な移行が非常に重要となります。まず、新しいバージョンのAPIを導入することで、開発者達は新バージョンのAPIに対して十分な理解と準備を進めることができます。
そして、旧バージョンのAPIの非推奨をアナウンスすることで、開発者達は新バージョンへの移行を開始し、新旧APIの違いを把握する時間を与えることが可能となります。
さらに、旧バージョンのAPI廃止前に、残存ユーザーに通知することで、最終的な移行を促すことが可能となります。ベストエフォートサポートを提供することで、旧APIの利用者へも必要なサポートを提供し、無駄な混乱を避けることができます。そして最終的に旧バージョンのAPIを停止し、全てのユーザーを新バージョンのAPIへと移行します。
不正解についての説明：
選択肢：旧バージョンのAPIの廃止をアナウンスします。新バージョンのAPIを導入します。旧バージョンのAPIを使用しているユーザーに連絡します。旧バージョンのAPIを廃止します。旧バージョンのAPIを停止します。旧バージョンのAPIを使用しているユーザーにベストエフォートサポートを提供します
この選択肢が正しくない理由は以下の通りです。
新バージョンのAPIを導入する前に旧バージョンのAPIの廃止をアナウンスすると、ユーザーに混乱を与える可能性があります。まず新バージョンをリリースし、その後で旧バージョンの非推奨をアナウンスすることで、ユーザーは新旧バージョンの違いを理解しやすくなります。
選択肢：旧バージョンのAPIの廃止をアナウンスします。旧バージョンのAPIを使用しているユーザーに連絡します。新バージョンのAPIを導入します。旧バージョンのAPIを廃止します。旧バージョンのAPI利用者にベストエフォートサポートを提供します。旧バージョンのAPIを停止します
この選択肢が正しくない理由は以下の通りです。
新バージョンのAPIを導入する前に旧バージョンのAPIの廃止をアナウンスすると、サードパーティの開発者やエンドユーザーに混乱を引き起こします。新バージョンの導入とその利用方法を明確に伝えてから、旧バージョンの非推奨をアナウンスするほうがスムーズな移行が可能です。
選択肢：新バージョンのAPIを導入します。旧APIの残存ユーザーに連絡します。旧バージョンのAPIの非推奨をアナウンスします。旧バージョンのAPIを廃止します。旧バージョンのAPIを停止します。旧バージョンのAPI利用者にベストエフォートサポートを提供します
この選択肢が正しくない理由は以下の通りです。
旧APIの非推奨をアナウンスするのが残存ユーザーへの連絡よりも後となるため、サードパーティ開発者やエンドユーザーは予期せぬ影響を受ける可能性があります。処理の手順が正解の選択肢と異なり、混乱を最小限に抑えるという要件を満たしていません。
参考リンク：
https://cloud.google.com/endpoints/docs/openapi/versioning-an-api
https://cloud.google.com/apigee/docs/api-platform/publish/deprecations
https://cloud.google.com/architecture/api-design/versioning-apis
</div></details>

### Q. 問題42: 未回答
Google Kubernetes Engine（GKE）で動作するアプリケーションがあります。アプリケーションは、デプロイメントとサービスを使用してGKEにデプロイされた複数のマイクロサービスで構成されています。マイクロサービスの1つに、ポッドが5時間以上実行された後に403エラーを返す問題が発生しています。開発チームは解決策に取り組んでいますが、この問題は1ヶ月間解決しません。マイクロサービスが修正されるまで運用を継続する必要があります。Googleが推奨するプラクティスに従い、最も少ないステップ数で実行したいと考えています。
この要件を満たすために、どうすればよいですか？

1. ポッドが403エラーを返すたびに通知するアラートを設定します
2. Podを監視し、5時間以上稼働しているポッドを終了します
3. 5時間以上稼働しているポッドを終了させるcronジョブを作成します
4. マイクロサービスのデプロイメントにHTTP livenessプローブを追加します
<details><div>
    答え：4
説明
この問題では、GKEを利用したマイクロサービス環境で運用上の問題が発生しているときの対処法を求められています。5時間以上実行された後に発生する403エラーに対して、開発チームが修正策を探している間も運用を続けなければならないというのが要件です。それぞれの選択肢がこの問題をどの程度解決するかを了解した上で、最も効率的でGoogleが推奨するプラクティスに適合する策を選択する必要があります。
基本的な概念や原則：
HTTP livenessプローブ：Kubernetesの一部で、稼働中のポッドが正常に動作しているかを確認する手段です。定期的にエンドポイントへのHTTPリクエストを送ることで動作を確認します。レスポンスが異常な場合、ポッドを再起動します。
マイクロサービス：アプリケーションを機能単位に分割した独立したサービスのことです。各サービスを独立して開発、デプロイ、スケーリングすることができます。
Kubernetes Deployment：アプリケーションを管理するためのKubernetesオブジェクトです。デプロイメントを更新するとポッドのローリング更新が行われます。
Kubernetes Service：Kubernetesにおける永続的なサービスエンドポイントのことです。サービスはポッドを選択し、ネットワークトラフィックをルーティングします。
cronジョブ：定期的なタスクをスケジュールするためのUNIXのユーティリティです。Kubernetesでは、時間ベースのスケジュールでジョブを実行するためのオブジェクトを提供しています。
正解についての説明：
（選択肢）
・マイクロサービスのデプロイメントにHTTP livenessプローブを追加します
この選択肢が正解の理由は以下の通りです。
HTTP livenessプローブは、Kubernetesによって定期的にマイクロサービス（Pod）に対してリクエストを行い、その応答をもとにポッドの生存状況を判断するために使用されます。403エラーを返すという問題が発生しているのであれば、HTTP livenessプローブはそれを検出し、問題のあるポッドを自動的に再起動します。これにより、問題が発生したポッドがサービスから取り除かれ、新しいポッドが問題のポッドの代わりに生成・配置されます。
したがって、アプリケーションの持続的な可用性を確認することができます。
また、この冗長性の高さと自動修復の機能は、開発チームが問題を解決するまでの一時的な対処として効果的です。HTTP livenessプローブの設定は、マイクロサービスのデプロイメント設定に追加するだけで実行できるため、これが最も少ないステップ数で実行できる解決策と言えます。
不正解についての説明：
選択肢：5時間以上稼働しているポッドを終了させるcronジョブを作成します
この選択肢が正しくない理由は以下の通りです。
cronジョブを作成して5時間以上稼働しているポッドを終了させるのは、リソース管理が複雑化し、管理が難しくなる上にスケールする際の効率も良くありません。
それに対して、HTTP livenessプローブを使用することで、異常を検知した場合にKubernetes自身が自動的にポッドを再起動し問題を修正します。これにより、最も少ないステップ数で運用を継続できます。
選択肢：Podを監視し、5時間以上稼働しているポッドを終了します
この選択肢が正しくない理由は以下の通りです。
ポッドを監視して5時間以上稼働しているポッドを手動で終了する方法は、操作が煩雑であり、Googleが推奨する自動的なヘルスチェックに従っていません。
それに対し、HTTP livenessプローブを追加することで、自動的にヘルスチェックを行い、異常があった場合に自動でポッドの再起動が行われます。よって、より少ないステップ数で問題を解決できます。
選択肢：ポッドが403エラーを返すたびに通知するアラートを設定します
この選択肢が正しくない理由は以下の通りです。
ポッドが403エラーを返すたびに通知するアラートを設定するという手法は、問題の発生を監視・通知するだけであり、問題の根本的な解決にはつながりません。
一方で、HTTP livenessプローブを追加するという手法は、ポッドがエラーを返す場合に自動的に再起動し、運用を円滑に進めることが可能となります。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/monitoring
https://cloud.google.com/kubernetes-engine/docs/concepts/pod-lifecycle#liveness_and_readiness_probes
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
</div></details>

### Q. 問題43: 未回答
アプリケーションイメージはCloud Buildを使ってビルドされ、Google Container Registry（GCR）にプッシュされます。ソース管理でタグ付けされたリリースバージョンに基づいて、デプロイ用にアプリケーションの特定のバージョンを指定できるようにしたいと考えています。
イメージをプッシュするときに何をすべきですか？

1. ソースコントロールタグをイメージ名のパラメータとして指定します
2. アプリケーションイメージにリリースバージョンタグを含めるには、Cloud Buildを使用します
3. GCRダイジェストバージョニングを使用して、ソースコントロールのタグとイメージを一致させます
4. ソースコントロールタグでイメージダイジェストを参照します
<details><div>
    答え：2
説明
この問題では、アプリケーションイメージのビルドとリリースバージョンの管理方法について扱っています。アプリケーションイメージがCloud Buildでビルドされ、Google Container Registryにプッシュされるという会社の実態を理解することが重要です。問題文から明らかになっている目標は、特定のアプリケーションのバージョンを指定してデプロイできるようにすることです。
基本的な概念や原則：
Cloud Build：Google Cloud上でのコードのビルド、テスト、デプロイなどを行うフルマネージドタイプのサービスです。Dockerコンテナのイメージビルドにも利用できます。
Google Container Registry（GCR）：Dockerイメージを安全にホストし、共有するためのプライベートコンテナイメージレジストリです。Cloud Buildと組み合わせて使うことで、CI/CDパイプラインを効率的に運用できます。
イメージタグ：Dockerイメージに付けるラベルです。通常、ソフトウェアのバージョンを示すために使われます。Cloud Buildでは、タグを使ってプッシュしたイメージを簡単に識別したり管理したりできます。
ダイジェスト：Dockerイメージの一意の識別子です。特定のイメージのインスタンスを一意に指定するために使用されます。しかし、識別にソースコントロールタグを使うことは推奨されません。
正解についての説明：
（選択肢）
・アプリケーションイメージにリリースバージョンタグを含めるには、Cloud Buildを使用します
この選択肢が正解の理由は以下の通りです。
Cloud Buildはアプリケーションコードのビルド、テスト、デプロイに役立つGoogle Cloudのサービスです。このサービスによって生成されたイメージは、Artifact Registryに保存され、ユーザーは任意のタグを付けることができます。それにより特定のバージョンのアプリケーションイメージを簡単に参照できます。
そして、タグ付けシステムは一貫性と組織の安定性を提供し、後で特定のバージョンのイメージにアクセスする際に簡単に照会を行えます。
したがって、リリースのバージョン管理が求められる場合、Cloud Buildを使用してアプリケーションイメージにリリースバージョンのタグを含めるのは良い選択と言えます。
不正解についての説明：
選択肢：ソースコントロールタグでイメージダイジェストを参照します
この選択肢が正しくない理由は以下の通りです。
ソースコントロールタグでイメージダイジェストを参照する方法は、特定のイメージバージョンを指定する事はできますが、それがリリースバージョンに基づいているとは限らず、タグ付けの自動化は行えません。
一方、Cloud Buildを使用してアプリケーションイメージにリリースバージョンタグを含めるなら、ビルドとデプロイのプロセスを自動化し、特定のリリースバージョンに基づいてイメージを指定することが可能になります。
選択肢：ソースコントロールタグをイメージ名のパラメータとして指定します
この選択肢が正しくない理由は以下の通りです。
ソースコントロールタグをイメージ名のパラメータとして指定しても、特定のバージョンのイメージをデプロイする際にそのバージョンを明示的に指定することは困難です。対比的に、Cloud Buildを使用してリリースバージョンタグを含めた場合、バージョンを指定してデプロイすることが容易になります。
選択肢：GCRダイジェストバージョニングを使用して、ソースコントロールのタグとイメージを一致させます
この選択肢が正しくない理由は以下の通りです。
ダイジェストバージョニングは、イメージのコンテンツに基づいて一意の識別子を生成するものであり、ソース管理のタグと一致させることはできません。
それに対して、Cloud Buildを使用してアプリケーションイメージにタグを付けることで、タグでバージョン管理するための関連付けを可能にします。
参考リンク：
https://cloud.google.com/build/docs/configuring-builds/store-images-artifacts
https://cloud.google.com/container-registry/docs/managing
https://docs.docker.com/engine/reference/commandline/tag/
</div></details>

### Q. 問題44: 未回答
あなたのチームは、データのバッチに対して計算負荷の高い処理を行うサービスを構築しています。データはマシンのCPUの速度と数に基づいて高速に処理されます。これらのデータバッチのサイズはさまざまで、複数のサードパーティソースから随時届く可能性があります。サードパーティがデータを安全にアップロードできるようにする必要があります。コストを最小限に抑えつつ、可能な限り迅速にデータを処理したいと考えています。
この要件を満たすために、どうすればよいですか？

1. サードパーティがデータのバッチをアップロードできるようにCloud Storageバケットを提供し、バケットへの適切なIAM（Identity and Access Management）アクセスを提供します。標準的なGoogle Kubernetes Engine（GKE）クラスターを使用し、2つのサービスを維持します。1つはデータのバッチを処理するサービス、もう1つは新しいデータのバッチがないかCloud Storageを監視するサービスです。処理するデータのバッチがないときは、処理サービスを停止します
2. 第三者がデータのバッチをアップロードできるように、Compute Engineインスタンス上に安全なファイル転送プロトコル（SFTP）サーバーを提供し、サーバーに適切な認証情報を提供します。google.storage.object.finalize Cloud Storageトリガーを使用してCloud Functionsを実装します。この関数がCompute Engineのオートスケーリングマネージドインスタンスグループをスケールアップできるようにコードを記述します。処理が完了するとインスタンスを終了するデータ処理ソフトウェアがあらかじめロードされたイメージを使用します
3. サードパーティがデータのバッチをアップロードできるようにCloud Storageバケットを提供し、バケットへの適切なIAM（Identity and Access Management）アクセスを提供します。Cloud Monitoringを使用して、バケット内の新しいデータバッチを検出し、データを処理するCloud Functionsをトリガーします。処理の実行時間を最小限に抑えるために、可能な限り最大のCPUを使用するようにCloud Functionsを設定します
4. サードパーティがデータのバッチをアップロードできるようにCloud Storageバケットを提供し、バケットへの適切なIAM（Identity and Access Management）アクセスを提供します。google.storage.object.finalize Cloud Storageトリガーを持つCloud Functionを作成します。処理が完了したらインスタンスを終了させるデータ処理ソフトウェアがあらかじめロードされたイメージを使用します
<details><div>
    答え：4
説明
この問題では、高負荷のバッチ処理を効率的かつコスト効果的に行うための最良の方法を選びます。異なるサードパーティソースからデータが任意のタイミングで送られてくるという条件に対応できる柔軟さ、データのアップロードのための安全な手段、そしてコスト効率の良さを考慮して適切な選択肢を選ぶことが求められています。Google Cloudの各サービスの特性とそれらを使ったコスト効率の高いデータ処理の方法について理解していなければなりません。
基本的な概念や原則：
Cloud Storage：Google Cloudのデータストレージサービスで、データのバッチを安全に保存し、アクセスし、共有することが可能です。
IAM（Identity and Access Management）：Google Cloudの認証と認可を管理するツールです。特定のユーザーやサービスに対してリソースへのアクセス権限を細かく制御することができます。
Cloud Functions：イベント駆動型のサーバーレス環境で、時間やリソースの管理をせずにコードを実行することができます。特定のイベントに対して自動的にスケーリングします。
Cloud Storageトリガー：Cloud Storageで特定のイベントが発生したときにCloud Functionsを自動的に呼び出す機能です。特定のバケットにデータが追加・変更された時点でトリガーされることが一般的です。
Compute Engine：仮想マシンを実行するGoogle Cloudのインフラストラクチャサービスです。必要な仕様と量を選び、デプロイ及びスケーリングが可能です。
オートスケーリング：負荷に基づいて自動的にリソースの数を増減させる機能です。Compute EngineではVMのインスタンス数を動的に調節します。
Google Kubernetes Engine（GKE）：Google CloudでKubernetes環境を簡単にセットアップ、運用できるサービスです。稼働中のアプリケーションのスケーリングと配信を管理します。
正解についての説明：
（選択肢）
・サードパーティがデータのバッチをアップロードできるようにCloud Storageバケットを提供し、バケットへの適切なIAM（Identity and Access Management）アクセスを提供します。google.storage.object.finalize Cloud Storageトリガーを持つCloud Functionを作成します。処理が完了したらインスタンスを終了させるデータ処理ソフトウェアがあらかじめロードされたイメージを使用します
この選択肢が正解の理由は以下の通りです。
まず、Cloud Storageバケットをサードパーティに提供することで、データやバッチのアップロードが容易になるだけでなく、IAMを使用して適切なアクセス制限を設けることができます。これによりセキュリティが強化されます。
次に、google.storage.object.finalize Cloud Storageトリガーを持つCloud Functionを作成すると、オブジェクトがバケットに最終的にアップロードされたタイミングで自動的に関数が実行されます。これにより、新たなデータが追加されるたびに即座にデータの処理が開始されることになります。
最後に、処理が完了したらインスタンスを終了させるソフトウェアを使用すれば、必要な時間だけリソースを使うようになり、コストを最小限に抑えることができます。これらの理由から、この選択肢が正解となります。
不正解についての説明：
選択肢：第三者がデータのバッチをアップロードできるように、Compute Engineインスタンス上に安全なファイル転送プロトコル（SFTP）サーバーを提供し、サーバーに適切な認証情報を提供します。google.storage.object.finalize Cloud Storageトリガーを使用してCloud Functionsを実装します。この関数がCompute Engineのオートスケーリングマネージドインスタンスグループをスケールアップできるようにコードを記述します。処理が完了するとインスタンスを終了するデータ処理ソフトウェアがあらかじめロードされたイメージを使用します
この選択肢が正しくない理由は以下の通りです。
SFTPサーバーの設定は管理が複雑であり、必要な認証単位の追加や削除を行うことに多くのオーバーヘッドが発生します。
これに対し、Cloud StorageバケットはIAMによりアクセス制御が可能で、より直接的で簡単な方法で第三者に安全なアップロードを可能にします。
また、コスト面でもCloud Storageの方が低コストで効率的です。
選択肢：サードパーティがデータのバッチをアップロードできるようにCloud Storageバケットを提供し、バケットへの適切なIAM（Identity and Access Management）アクセスを提供します。標準的なGoogle Kubernetes Engine（GKE）クラスターを使用し、2つのサービスを維持します。1つはデータのバッチを処理するサービス、もう1つは新しいデータのバッチがないかCloud Storageを監視するサービスです。処理するデータのバッチがないときは、処理サービスを停止します
この選択肢が正しくない理由は以下の通りです。
サービスを常に維持するための管理コストと、新たなバッチデータをチェックするために別のサービスを維持する手間が発生します。
また、処理サービスが停止している間もGKEクラスター自体は稼働し続けるためコスト効率が悪いです。正解の選択肢ではCloud Functionsがデータ処理のトリガーとして稼働し、必要に応じてインスタンスを起動・終了するため、コスト効率的です。
選択肢：サードパーティがデータのバッチをアップロードできるようにCloud Storageバケットを提供し、バケットへの適切なIAM（Identity and Access Management）アクセスを提供します。Cloud Monitoringを使用して、バケット内の新しいデータバッチを検出し、データを処理するCloud Functionsをトリガーします。処理の実行時間を最小限に抑えるために、可能な限り最大のCPUを使用するようにCloud Functionsを設定します
この選択肢が正しくない理由は以下の通りです。
まず、Cloud Monitoringはデータバッチの到着を直接検出するためのツールではありません。正解の選択肢では、google.storage.object.finalizeイベントをトリガーとして用いるCloud Functionが適切に使用されています。
また、Cloud FunctionsはCPUの選択肢に制限があるため、高度なCPU設定を求められる処理には不向きです。
参考リンク：
https://cloud.google.com/storage/docs/access-control/iam
https://cloud.google.com/functions/docs/calling/storage
https://cloud.google.com/compute/docs/autoscaler
</div></details>

### Q. 問題45: 未回答
あなたの組織は、Google CloudプロジェクトのCloud Operationsでダッシュボードを生成するために使用されるシステムログを収集したいと考えています。システムログを収集するために、現在および将来のすべてのCompute Engineインスタンスを設定する必要があります。
この要件を満たすために、どうすればよいですか？

1. スタートアップスクリプトを使用して、Compute EngineイメージにOps Agentをインストールします
2. gcloud CLIを使用してエージェントポリシーを作成します
3. gcloud CLIを使用して、Cloud Asset Inventoryにリストされている各VMにOps Agentをインストールします
4. Cloud Operations VMsダッシュボードのAgent StatusがNot detectedのVMをすべて選択します。次に、Install agentsを選択します
<details><div>
    答え：2
説明
この問題では、Google CloudプロジェクトのCloud Operationsでログを収集するための設定について問われています。特に現在および将来のすべてのCompute Engineインスタンスのシステムログを収集する方法について考える必要があります。選択肢を見る際には、現在だけでなく将来のインスタンスもカバーできる、効率的な自動化の方法を探し、その上でCloud Operationsのログ収集に適した策を選ぶべきです。この問題は、Google Cloudのログ管理に関する理解と、継続的かつ自動化されたログ収集の設定方法への理解を試すものとなります。
基本的な概念や原則：
gcloud CLI：Google Cloudコマンドラインインターフェースで、Google Cloudサービスを操作するためのツールです。各種設定やリソース管理などがコマンドラインから行えます。
エージェントポリシー：特定の操作を実行するためにシステムに導入されるソフトウェアルールのセットです。このポリシーを通じてログ収集などの行動を制御します。
Ops Agent：Google Cloudのオペレーションスイートのためのエージェントです。メトリクスの収集とログの送信を行います。
Compute Engineインスタンス：Google Cloud上で仮想マシンを実行するためのインスタンスです。特定の作業のために設定やリソースを割り当てることができます。
Cloud Operations：Google Cloudの監視、トラブルシューティング、および診断に役立つツールセットです。これにより、プロジェクトのパフォーマンスを評価し、障害の原因を特定できます。
正解についての説明：
（選択肢）
・gcloud CLIを使用してエージェントポリシーを作成します
この選択肢が正解の理由は以下の通りです。
まず、gcloud CLIを使ってエージェントポリシーを作成することは、システムログを収集するための最適な手段です。これは、Ops AgentやLogging AgentのようなエージェントはCompute Engineインスタンスでシステムメトリックやログを収集するためのツールだからです。gcloud CLIを使ってエージェントポリシーを作成することで、エージェントが必要とされるすべての現在または将来のインスタンスに自動的にインストールされます。
さらに、エージェントは収集した情報をCloud Operationsに送信し、ここで情報が統合されて分析・閲覧できます。
そして、これらの情報を基にダッシュボードを生成することが可能となります。
したがって、特定の要件を満たすためには、gcloud CLIを使用してエージェントポリシーを作成することが適切な選択肢となります。
不正解についての説明：
選択肢：gcloud CLIを使用して、Cloud Asset Inventoryにリストされている各VMにOps Agentをインストールします
この選択肢が正しくない理由は以下の通りです。
Ops AgentをCloud Asset Inventoryにリストされている各VMに個別にインストールする方法は、将来に作成されるVMに対してログ収集の設定が適用されないため、今後の全インスタンスに対応するという要件に適合しません。
代わりに、エージェントポリシーを作成することで現在及び将来作成される全VMに対する一元的なシステムログ収集を設定することができます。
選択肢：Cloud Operations VMsダッシュボードのAgent StatusがNot detectedのVMをすべて選択します。次に、Install agentsを選択します
この選択肢が正しくない理由は以下の通りです。
Cloud Operations VMsダッシュボードのAgent StatusがNot detectedのVMを選択し、Install agentsを選択する方法は、現在のCompute Engineインスタンスに対してのみエージェントをインストールします。しかし、要件は現在だけでなく将来のすべてのCompute Engineインスタンスにも適用することを求めています。
それに対して、gcloud CLIを使用してエージェントポリシーを作成すると、新たに生成されるCompute Engineインスタンスにも自動的に適用されます。
選択肢：スタートアップスクリプトを使用して、Compute EngineイメージにOps Agentをインストールします
この選択肢が正しくない理由は以下の通りです。
スタートアップスクリプトを用いてOps Agentをインストールする方法は、手動でVMを生成する度にスクリプトを実行する必要があり、将来的に追加されるすべてのCompute Engineインスタンスに対応しきれません。
一方、gcloud CLIを用いてエージェントポリシーを作成すれば、新規に作られるCompute Engineインスタンスにも自動的に適用されます。
参考リンク：
https://cloud.google.com/logging/docs/agent/ops-agent
https://cloud.google.com/compute/docs/instances/create-start-instance
https://cloud.google.com/sdk/gcloud/reference/compute/instances/add-metadata
</div></details>

### Q. 問題46: 未回答
Google Kubernetes Engine（GKE）にアプリケーションをビルドしてデプロイするために、複数ステップのCloud Buildパイプラインを使用します。ビルド情報をWebhookにHTTP POSTすることで、サードパーティの監視プラットフォームと統合したいと考えています。また、開発工数を最小限に抑えたいと考えています。
この要件を満たすために、どうすればよいですか？

1. Cloud Loggingを使用して、Cloud Buildログからログベースのメトリックを作成します。Webhook通知タイプでアラートを作成します
2. Cloud Buildのパイプラインの最後に、ビルド情報をWebhookにHTTP POSTする新しいステップを追加します
3. 各Cloud Buildステップに、ビルド情報をWebhookにHTTP POSTするロジックを追加します
4. Cloud Build cloud-builds PubSubトピックにCloud Pub/Subプッシュサブスクリプションを作成し、ビルド情報をWebhookにHTTP POSTします
<details><div>
    答え：4
説明
この問題では、Cloud Buildパイプラインを使用してアプリケーションを作成しデプロイするシナリオが提示されており、ビルド情報をサードパーティの監視プラットフォームに送信する必要があります。また、開発工数を最小限に抑えなければなりません。これは自動化と効率性が重要な要素であり、手動でのステップ追加や独自のロジック作成を避けることを示唆しています。解答選択肢を選ぶ際には、Google Cloudの既存サービスを活用し、よりシンプルで効率的な実装を重視することが求められます。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudで提供されている、Kubernetesをマネージド形式で使うことができるサービスです。Kubernetesは、コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を行うためのオープンソースのプラットフォームです。
Cloud Build：Google Cloudの継続的インテグレーションとデリバリーサービスで、ソースコードをビルド、テスト、デプロイします。
Webhook：HTTP POSTコールバックで、あるアクションが起きたときに通知を受ける方法です。
Cloud Pub/Sub：Google Cloudのリアルタイムメッセージングサービスで、独立したアプリケーション間でメッセージを送受信できます。
Cloud Logging：Google Cloudのログ管理サービスで、Google Cloudのリソースからのログを収集、分析、保存、エクスポートします。
パイプライン：連続するプロセスの一連のステップを指し、ここではソースコードのビルドからデプロイまでの一連の作業を指します。パイプラインにより、一貫性と効率性が確保されます。
正解についての説明：
（選択肢）
・Cloud Build cloud-builds PubSubトピックにCloud Pub/Subプッシュサブスクリプションを作成し、ビルド情報をWebhookにHTTP POSTします
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Pub/SubはメッセージングとイベントドリブンシステムのためのGoogle Cloudのスケーラブルなサービスで、異なるサーバーアプリケーション間でメッセージを送受信します。Cloud Buildのビルド情報はcloud-buildsというPub/Subトピックに自動的に送信されます。そのため、このトピックにプッシュサブスクリプションを作成することで、ビルドが終了するたびに自動的に情報を取得できます。
次に、Pub/Subのプッシュサブスクリプションは、新しいメッセージがトピックに追加されるたびに、登録したエンドポイント（この場合Webhook）にHTTP POSTリクエストを送信します。これにより、手間をかけずにビルド情報をサードパーティの監視プラットフォームと統合できます。
したがって、Cloud Build cloud-builds PubSubトピックにPub/Subプッシュサブスクリプションを作成することで、開発工数を最小限に抑えつつ、問題の要件を満たすことができます。
不正解についての説明：
選択肢：各Cloud Buildステップに、ビルド情報をWebhookにHTTP POSTするロジックを追加します
この選択肢が正しくない理由は以下の通りです。
各Cloud Buildステップにロジックを追加すると、開発工数が増えてしまいます。
一方、Cloud Build cloud-builds PubSubトピックにCloud Pub/Subプッシュサブスクリプションを作成する方が、開発工数を最小限に抑えることが可能です。
また、これによりビルド情報をWebhookに一元的にHTTP POSTすることが可能となります。
選択肢：Cloud Buildのパイプラインの最後に、ビルド情報をWebhookにHTTP POSTする新しいステップを追加します
この選択肢が正しくない理由は以下の通りです。
新たにビルド情報をWebhookにHTTP POSTするステップを追加すると、開発工数が増加します。これは、工数を最小限に抑えるという要求に反しています。
一方で、Pub/Subサブスクリプションを作成すれば、独自のステップを追加せずにビルド情報をWebhookにPOSTすることが可能です。
選択肢：Cloud Loggingを使用して、Cloud Buildログからログベースのメトリックを作成します。Webhook通知タイプでアラートを作成します
この選択肢が正しくない理由は以下の通りです。
Cloud Loggingでログベースのメトリクスを作成し、Webhook通知タイプでアラートを作成すると、必要な開発工数が増えます。正解の選択肢であるCloud Pub/Subプッシュサブスクリプションを使用する方が直接的な統合が可能で、開発工数を最小限に抑えることができます。
参考リンク：
https://cloud.google.com/build/docs/configuring-builds/subscribe-build-notifications
https://cloud.google.com/pubsub/docs/overview
https://cloud.google.com/monitoring/support/notification-options#webhooks
</div></details>

### Q. 問題47: 不正解
あなたはPythonで書かれ、App Engineフレキシブル環境でホストされているトレーディングアプリケーションをサポートしています。Google Cloud Operation Suite Error Reportingに送信されるエラー情報をカスタマイズしたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？

1. Python用のGoogle Cloud Operation Suite Error Reportingライブラリをインストールし、App Engineフレキシブル環境でコードを実行します
2. Python用のGoogle Cloud Operation Suite Error Reportingライブラリをインストールし、Google Kubernetes Engine上でコードを実行します
3. Google Cloud Operation Suite Error Reporting APIを使用して、アプリケーションからReportedErrorEventにエラーを書き込み、Cloud Loggingに適切な書式のエラーメッセージを含むログエントリを生成します
4. Python用のGoogle Cloud Operation Suite Error Reportingライブラリをインストールし、Compute Engine VM上でコードを実行します
<details><div>
    答え：1
説明
この問題では、Pythonで作成されたトレーディングアプリケーションがApp Engineフレキシブル環境でホストされ、そのエラー情報のカスタマイズ方法が求められています。Google Cloud Operation Suite Error Reportingに関連するAPIやライブラリ、またその属する環境（Compute Engine VMやGoogle Kubernetes Engine等）について理解している必要があります。また、エラー情報のカスタマイズとはどういう動作を指すのか、Cloud Loggingにおけるログエントリとは何かといった要素への理解も必要です。これらを踏まえた上で、問題の正解選択肢を読み解くべきです。
基本的な概念や原則：
App Engineフレキシブル環境：App Engineの一部で、コンテナベースのランタイムを使用してアプリケーションを実行します。Pythonなど様々な言語がサポートされており、自由なカスタマイズが可能です。
Google Cloud Operation Suite Error Reporting：Google Cloudのサービスで、アプリケーションが発生するエラーを自動的に検出、分析、通知してくれます。
ReportedErrorEvent：Cloud Operation Suite Error Reporting APIで使用するエンティティでエラー情報を表現します。
Cloud Logging：Google Cloudのサービスで、アプリケーションやGoogle Cloudのサービスからのログを一元管理できます。ログのエクスポートやリアルタイム分析も可能です。
Google Cloud Operation Suite Error Reportingライブラリ：Google Cloudが提供するライブラリで、Pythonなどの言語でCloud Operation Suite Error Reportingの機能を利用するために使用します。
Compute Engine：Google Cloud Serviceの一つで、仮想マシンを提供します。比較的自由度が高いサービスですが、全ての環境設定をユーザーが行わなければならない場合があります。
Google Kubernetes Engine：Google Cloud Serviceの一つで、Kubernetesの管理環境を提供します。Dockerコンテナを管理するためのプラットフォームであり、アプリケーションをデプロイ、保守、スケーリングするために使用します。
正解についての説明：
（選択肢）
・Google Cloud Operation Suite Error Reporting APIを使用して、アプリケーションからReportedErrorEventにエラーを書き込み、Cloud Loggingに適切な書式のエラーメッセージを含むログエントリを生成します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Operation Suite Error Reporting APIを使用することで、エラー情報のカスタマイズが可能になります。これにより、エラー情報を手動で操作し、アプリケーション固有のエラー内容を追加したり、エラーメッセージのフォーマットを変更したりすることができます。
そして、ReportedErrorEventはAPIが提供するデータ構造で、エラー情報をGoogle Cloud Operation Suite Error Reportingに送信するために使用します。
そして、Cloud Loggingに生成されるログエントリは、Google Cloud Operation Suite Error Reporting APIを使用して送信されたエラー情報を格納します。よって、この選択肢はApp Engineフレキシブル環境でホストされるPythonアプリケーションからのエラー情報のカスタマイズと送信を実現します。
不正解についての説明：
選択肢：Python用のGoogle Cloud Operation Suite Error Reportingライブラリをインストールし、Compute Engine VM上でコードを実行します
この選択肢が正しくない理由は以下の通りです。
要件はApp Engineフレキシブル環境でもエラー情報をカスタマイズしたいとする一方で、不正解の選択肢はCompute Engine VM上でエラーライブラリを実行するという点で乖離しています。これにより、必要でないリソースを使用し、期待されている結果が得られなかったという問題が発生する可能性があります。
選択肢：Python用のGoogle Cloud Operation Suite Error Reportingライブラリをインストールし、Google Kubernetes Engine上でコードを実行します
この選択肢が正しくない理由は以下の通りです。
要件はApp Engineフレキシブル環境でのエラー情報のカスタマイズであり、Google Kubernetes Engine上での実行は関係していません。
また、ライブラリをインストールしてもカスタマイズは可能ではなく、APIを通じてエラーメッセージを生成・送信することが必要です。
選択肢：Python用のGoogle Cloud Operation Suite Error Reportingライブラリをインストールし、App Engineフレキシブル環境でコードを実行します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Error Reportingライブラリをインストールするだけでは、エラー情報のカスタマイズは実現できません。エラーメッセージの内容を制御するために、APIを用いてReportedErrorEventにエラーを書き込む必要があります。
参考リンク：
https://cloud.google.com/error-reporting/docs
https://cloud.google.com/error-reporting/docs/formatting-error-messages
https://cloud.google.com/logging/docs/
</div></details>

### Q. 問題48: 未回答
あなたのチームは、Google Kubernetes Engine（GKE）にデプロイするための新しいアプリケーションを設計しています。様々なアプリケーションレベルのメトリクスを一元的に収集・集約するために、モニタリングをセットアップする必要があります。あなたは、モニタリングのセットアップに必要な作業量を最小限に抑えながら、Google Cloudのサービスを使用したいと考えています。
この要件を満たすために、どうすればよいですか？

1. Cloud Pub/Subクライアントライブラリをインストールし、アプリケーションからさまざまなメトリクスをさまざまなトピックにプッシュし、Google Cloud Operation Suiteで集約されたメトリクスをチェックします
2. アプリケーションから様々なメトリクスをGoogle Cloud Operation Suite Monitoring APIに直接パブリッシュし、Google Cloud Operation Suiteでこれらのカスタムメトリクスをチェックします
3. アプリケーションにOpenTelemetryクライアントライブラリをインストールし、Google Cloud Operation Suiteをメトリクスのエクスポート先として設定し、Google Cloud Operation Suiteでアプリケーションのメトリクスをチェックします
4. すべてのメトリクスをアプリケーション固有のログメッセージの形式で出力し、これらのメッセージをコンテナからGoogle Cloud Operation Suiteロギングコレクターに渡し、Google Cloud Operation Suiteでメトリクスをチェックします
<details><div>
    答え：3
説明
この問題では、Google Kubernetes Engine（GKE）にデプロイするための新しいアプリケーションのメトリクスを収集、集約するための最適なモニタリング方法を選定することが求められています。問題文からは、作業量を最小限に抑えつつ、Google Cloudのサービスを使用したいという要件が明らかにされています。メトリクスの収集と集約のための最適な方法とGoogle Cloudの適切なサービスを選び、それらを組み合わせることが重要です。OpenTelemetry、Google Cloud Operation Suite、Cloud Pub/Sub、その他のロギング手段など選択肢が出てくるので、これらの特性を理解して適切に選定することが求められます。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：コンテナ化されたアプリケーションのデプロイと管理を行うためのGoogle Cloudのフルマネージドサービスです。Kubernetes環境の設定やスケーリングなどを自動化します。
OpenTelemetry：分散トレーシングとメトリクスの収集を一元管理するためのオープンソースのフレームワークです。アプリケーションのパフォーマンス監視（APM）に使用されます。
Google Cloud Operation Suite：Google Cloudのモニタリング、ロギング、トレーシング、エラーレポートといったオペレーションのための統合ツールスイートです。
Google Cloud Operation Suite Monitoring API：Google Cloud Monitoringの機能をプログラムから制御するためのAPIです。メトリックの収集やダッシュボードの作成などが行えます。
Cloud Pub/Sub：Google Cloudのリアルタイムメッセージングサービスです。大量のデータを高速で処理し、アプリケーションの間でデータを配信します。
Google Cloud Operation Suiteロギングコレクター：Google Cloud Operation Suiteが提供する、ログデータの収集・処理・分析を行うためのエージェントです。
正解についての説明：
（選択肢）
・アプリケーションにOpenTelemetryクライアントライブラリをインストールし、Google Cloud Operation Suiteをメトリクスのエクスポート先として設定し、Google Cloud Operation Suiteでアプリケーションのメトリクスをチェックします
この選択肢が正解の理由は以下の通りです。
まず、OpenTelemetryはオープンソースの分散トレーシングおよびメトリクス収集フレームワークであり、様々なアプリケーションレベルのメトリクスを容易に収集することができます。アプリケーションにクライアントライブラリをインストールすることで、ミドルウェア、フレームワーク、言語ランタイムといった様々な部分からのデータを一元的に収集することが可能となります。
次に、Google Cloud Operation SuiteはGoogle Cloudの監視、トラブルシューティング、アプリケーションのパフォーマンス管理を行うための統合サービスです。OpenTelemetryから収集されたメトリクスをGoogle Cloud Operation Suiteにエクスポートすることで、アプリケーションのメトリクスを一元的に閲覧し、分析することができます。これにより、モニタリングのセットアップに必要な作業量を最小限に抑えつつ、効率的にアプリケーションのパフォーマンスを監視することが可能となります。
不正解についての説明：
選択肢：アプリケーションから様々なメトリクスをGoogle Cloud Operation Suite Monitoring APIに直接パブリッシュし、Google Cloud Operation Suiteでこれらのカスタムメトリクスをチェックします
この選択肢が正しくない理由は以下の通りです。
メトリクスを直接Google Cloud Operation Suite Monitoring APIにパブリッシュすると、セットアップに必要な作業量が増えます。これは、アプリケーションに直接APIを組み込む開発コストが必要になるからです。
一方、OpenTelemetryを使用すると、一般的なメトリクスが自動的に収集され、作業量を最小限に抑えられます。
選択肢：Cloud Pub/Subクライアントライブラリをインストールし、アプリケーションからさまざまなメトリクスをさまざまなトピックにプッシュし、Google Cloud Operation Suiteで集約されたメトリクスをチェックします
この選択肢が正しくない理由は以下の通りです。
Cloud Pub/Subクライアントライブラリを使うとメトリクスのプッシュに手間がかかります。
一方、OpenTelemetryクライアントライブラリを使えば、アプリケーションレベルのメトリクスを自動的にGoogle Cloud Operation Suiteへエクスポートでき、作業量が最小限に抑えられます。
選択肢：すべてのメトリクスをアプリケーション固有のログメッセージの形式で出力し、これらのメッセージをコンテナからGoogle Cloud Operation Suiteロギングコレクターに渡し、Google Cloud Operation Suiteでメトリクスをチェックします
この選択肢が正しくない理由は以下の通りです。
不正解な選択肢は、アプリケーション固有のログメッセージ形式で出力し、Cloud Loggingを使ってメトリクスを収集しようとしています。しかしメトリクス収集のためにログメッセージをパースする作業は効率的ではなく、手間がかかるため作業量を最小限にするという要件に反します。
一方、正解選択肢はOpenTelemetryクライアントライブラリを利用し、Cloud Monitoringに直接メトリクスを送ることで一元的な収集・集約が可能になります。
参考リンク：
https://cloud.google.com/stackdriver/docs/solutions/gke
https://cloud.google.com/monitoring/docs
https://opentelemetry.io/docs/collector/getting-started/
</div></details>

### Q. 問題49: 未回答
Google Cloudでのアプリケーションのパフォーマンスが、前回のリリース以降低下しています。下流の依存関係が、リクエストの完了に時間がかかる原因になっている可能性があります。原因を特定するために、アプリケーションで問題を調査する必要があります。
この要件を満たすために、どうすればよいですか？

1. アプリケーションでCloud Profilerを設定します
2. アプリケーションでGoogle Cloud Managed Service for Prometheusを設定します
3. アプリケーションでCloud Traceを設定します
4. アプリケーションでエラーレポートを設定します
<details><div>
    答え：3
説明
この問題では、Google Cloud上でパフォーマンス低下が発生しているアプリケーションのトラブルシューティングに適したツールを選ぶことが求められています。特に、"下流の依存関係がリクエストの完了に時間がかかる原因になっている可能性がある"という情報を重視し、それに対応可能なツールを選ぶことが鍵となります。各選択肢が提供する機能とその問題に対する適用度を熟知していることが必要です。
基本的な概念や原則：
Cloud Trace：Google Cloudの分散トレーシングシステムです。アプリケーションがリクエストを処理する時間を詳細に追跡し、表示することができます。これによりパフォーマンスのボトルネックや遅延の原因を特定することができます。
エラーレポート：Google Cloudのエラーレポートツールです。アプリケーションで発生したエラーを追跡し、バグの特定や修正を支援します。しかし、パフォーマンスの問題や遅延の原因を追跡するためのツールではありません。
Google Cloud Managed Service for Prometheus：Google Cloudで提供されているオープンソースの監視と警告ツールのマネージドサービスです。システムやアプリケーションのメトリックスを収集し、分析しますが、パフォーマンスの問題や遅延の原因を追跡するためのツールではありません。
Cloud Profiler：Google Cloudのパフォーマンスプロファイル生成ツールです。CPU使用率やメモリ使用率など、アプリケーションの実行時のパフォーマンスメトリクスを収集します。パフォーマンス改善のための情報を提供しますが、特定のリクエストのパフォーマンス問題の追跡には使用できません。
正解についての説明：
（選択肢）
・アプリケーションでCloud Traceを設定します
この選択肢が正解の理由は以下の通りです。
Cloud TraceはGoogle Cloudのサービスで、アプリケーションのパフォーマンスを調査し解析するためのツールです。アプリケーションが低下し、その原因を特定する必要がある場合、Cloud Traceを使用すると、アプリケーションの実行に関する詳細な情報を取得できます。特に、アプリケーションのリクエストのレイテンシの観点からパフォーマンスを分析できます。また下流の依存関係が問題になっている可能性があるという状況では、Cloud Traceのトレースデータを利用することで、依存関係のあるサービス間での時間の遅延を特定するのに役立ちます。
したがって、この情報を元に問題の解決やパフォーマンスの最適化を図ることが可能になります。
不正解についての説明：
選択肢：アプリケーションでエラーレポートを設定します
この選択肢が正しくない理由は以下の通りです。
エラーレポートはアプリケーションがエラーを投げた際にその情報を追跡・分析するためのツールであり、パフォーマンス低下の原因を追跡するのには適していません。
一方、Cloud Traceはリクエストがシステムを通過する際の遅延を視覚化し、パフォーマンスの問題の原因を特定します。
選択肢：アプリケーションでGoogle Cloud Managed Service for Prometheusを設定します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Managed Service for Prometheusは、時系列データの監視とアラートを行うサービスであり、リクエストの遅延原因具体的に特定するのではなく、システムパフォーマンス全般を監視します。
それに対して、Cloud Traceはアプリケーションのリクエストを分析し、それぞれのリクエストがどこで遅延しているかを特定することが可能です。
選択肢：アプリケーションでCloud Profilerを設定します
この選択肢が正しくない理由は以下の通りです。
Cloud Profilerはアプリケーションのパフォーマンスボトルネックを特定するためのサービスですが、それはコードレベルでの問題を追跡するのに使われます。しかし、今回のケースでは下流の依存関係が問題であるため、リクエストとレスポンスを追跡し調査するCloud Traceが適切です。
参考リンク：
https://cloud.google.com/trace/docs
https://cloud.google.com/error-reporting/docs
https://cloud.google.com/profiler/docs
</div></details>

### Q. 問題50: 未回答
ワークスペースプロジェクト内のダッシュボードで、CPU使用率のGoogle Cloud Operation Suiteチャートを作成しました。このチャートは、サイト信頼性エンジニアリング（SRE）チームでのみ共有します。あなたは、最小権限の原則に確実に従いたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？

1. ワークスペースのプロジェクトIDをSREチームと共有します。SREチームにワークスペースプロジェクトのダッシュボードビューアーIAMロールを割り当てます
2. ワークスペースのプロジェクトIDをSREチームと共有します。SREチームに、ワークスペースプロジェクトの監視ビューアIAMロールを割り当てます
3. "URLによるチャートの共有"をクリックし、SREチームにURLを提供します。ワークスペースプロジェクトのダッシュボードビューアーIAMロールをSREチームに割り当てます
4. "URLによるチャートの共有"をクリックし、SREチームにURLを提供します。ワークスペースプロジェクトで、SREチームにMonitoring Viewer IAMロールを割り当てます
<details><div>
    答え：4
説明
この問題では、Google CloudのIAMの権限管理と最小権限の原則に関する理解が必要です。特定のユーザー（この場合はSREチーム）に対し、特定のリソース（この場合はワークスペース内のCPU使用率チャート）に対する最小限の権限を付与する方法を問いています。ここでは、ユーザーが受け取るべき具体的な権限、その権限がどのリソースに対して効果を発揮するのか、そしてその権限が付与される方法を、適切に理解して選択する必要があります。具体的な権限やその付与方法について正確に理解していることが求められる問題です。
基本的な概念や原則：
Google Cloud Operation Suite：Google Cloudの監視、トラブルシューティング、および診断のためのツールスイートです。これには、Logging, Monitoring, Trace, Profiler, Debuggなどの機能が含まれています。
CPU使用率：コンピュータのCPUがどれだけ利用されているかを示す指標です。ワークロードが高い場合や、ボトルネックが発生している場合などに使用します。
ダッシュボード：ユーザーが重要な情報を一目で確認できるように設計されたユーザーインターフェースです。Google Cloudでは、ユーザーが自分のプロジェクトやリソースの状態を監視するためにダッシュボードを作成することができます。
URLを共有します：特定の情報を共有する一般的な方法です。共有したいページのURLを他のユーザーと共有することで、そのユーザーも同じページを開くことができます。
サイト信頼性エンジニアリング（SRE）：システムやサービスの信頼性を維持しつつ、新しいリリースをスムーズに展開する方法論です。
最小権限の原則：セキュリティのベストプラクティスの1つで、各ユーザーまたはプログラムがそのタスクを達成するのに必要な最小限のアクセス権限のみを持つべきであると言うものです。
Monitoring Viewer IAMロール：Google CloudのIAMロールの一つで、リソースのモニタリングデータに対する読み取りアクセス権限を提供します。ダッシュボードを共有する際には、このロールが必要な場合があります。
正解についての説明：
（選択肢）
・"URLによるチャートの共有"をクリックし、SREチームにURLを提供します。ワークスペースプロジェクトで、SREチームにMonitoring Viewer IAMロールを割り当てます
この選択肢が正解の理由は以下の通りです。
まず、ワークスペースプロジェクト内のGoogle Cloud Operation Suiteダッシュボードのチャートを共有する方法にはいくつかありますが、最小権限の原則に基づくと、不要な権限を付与せずに必要な情報だけを共有できる"URLによるチャートの共有"が適しています。この方法であれば、SREチームはURLを通じてダッシュボードを参照できますが、他のプロジェクト情報にはアクセスできません。
しかし、URLによる共有だけでは、URLを取得したSREチームがダッシュボードを閲覧できません。そのため、SREチームに対してワークスペースプロジェクトでMonitoring Viewer IAMロールを割り当てます。これにより、SREチームはダッシュボードを閲覧する権限を得ますが、それ以上の権限（例えば編集権）は付与されないため、最小権限の原則に沿っています。
このような理由から、この選択肢が正解となります。
不正解についての説明：
選択肢：ワークスペースのプロジェクトIDをSREチームと共有します。SREチームに、ワークスペースプロジェクトの監視ビューアIAMロールを割り当てます
この選択肢が正しくない理由は以下の通りです。
ワークスペースのプロジェクトIDを共有しても、それだけではSREチームはダッシュボードのチャートを参照できません。特定のダッシュボードチャートにアクセスさせるには、"URLによるチャートの共有"を選択し、具体的なURLを提供する必要があります。
選択肢：ワークスペースのプロジェクトIDをSREチームと共有します。SREチームにワークスペースプロジェクトのダッシュボードビューアーIAMロールを割り当てます
この選択肢が正しくない理由は以下の通りです。
まず、ワークスペースプロジェクトのダッシュボードビューアIAMロールは実在しません。正しいロールは、Monitoring Viewer IAMです。これによりSREチームは、必要な権限のみが割り当てられ、最小権限の原則が維持されます。
選択肢："URLによるチャートの共有"をクリックし、SREチームにURLを提供します。ワークスペースプロジェクトのダッシュボードビューアーIAMロールをSREチームに割り当てます
この選択肢が正しくない理由は以下の通りです。
ワークスペースプロジェクトのダッシュボードビューアーIAMロールというロールは存在しません。ゆえに該当のロールをSREチームに割り当てることは不可能です。対して正解の選択肢のMonitoring Viewer IAMロールは存在しており、彼らにCPU使用率のチャートを表示するための必要十分な権限を付与します。
参考リンク：
https://cloud.google.com/monitoring/charts/dashboards-sharing
https://cloud.google.com/monitoring/access-control
https://cloud.google.com/iam/docs/understanding-roles#monitoring-roles
</div></details>

## 2
### Q. 問題1: 未回答
App Engine上で動作するアプリケーションをサポートしています。アプリケーションはグローバルに使用され、さまざまな種類のデバイスからアクセスされます。あなたは、接続数を計測したいと考えています。現在、App EngineのGoogle Cloud Operation Suite Monitoringを使用しています。
要件を満たすために、どのメトリックを使用する必要がありますか。
1. flex/instance/connections/current
2. flex/connections/current
3. tcp_ssl_proxy/new_connections
4. tcp_ssl_proxy/open_connections
<details><div>
    答え：2
説明
この問題では、App Engineの状態を監視するためにどのメトリックを使用すべきかを選ばなければなりません。問題文を読む際、App Engineが動作中のアプリケーションであり、目的が"接続数を計測すること"であるという事実に焦点を当てる必要があります。正解選択肢を探す際には、特定の環境（つまりApp Engine）に適合し、必要な情報（つまり接続数）を提供するメトリックを見つけ出すことが必要です。
基本的な概念や原則：
App Engine：Google Cloudのフルマネージドサービスで、開発者がアプリケーションをビルド、デプロイ、スケールすることを簡素化します。
Google Cloud Operation Suite Monitoring：Google Cloud Serviceのメトリクス、ロギング、トレーシングを一元的に可視化、モニタリングするためのサービスです。
メトリック：システムやアプリケーションのパフォーマンスを定量的に評価するための数値やデータのことです。
flex/connections/current：現在の接続数を表すメトリックです。これにより、アプリケーションの同時接続数を追跡することが可能です。
正解についての説明：
（選択肢）
・flex/connections/current
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Operation Suite Monitoringは、アプリケーションの運用において重要な指標を収集して表示できるツールです。その中でも、App Engine flexible環境の"flex/connections/current"は、アプリケーションが現在処理している接続数を示すメトリックであり、アプリケーションの通信状況と利用状況の把握に役立ちます。グローバルに使用され、さまざまな種類のデバイスからアクセスされるアプリケーションの接続数を計測したい場合、このメトリックが適切です。
それぞれの設定によっては、App Engine standard環境とApp Engine flexible環境のメトリックが異なるため、問題のアプリケーションが稼働している環境に適したメトリックを選択する必要があります。今回の場合はApp Engine flexible環境向けのメトリックが求められているため、'flex/connections/current'が適切です。
不正解についての説明：
選択肢：tcp_ssl_proxy/new_connections
この選択肢が正しくない理由は以下の通りです。
tcp_ssl_proxy/new_connectionsは、Cloud Load Balancing SSLプロキシの正常に確立された新規接続数を示すメトリックであり、App Engineのアプリケーションの接続数を表すものではありません。
一方、flex/connections/currentはApp Engineのアプリケーションに対する現在の接続数を示すメトリックであり、要求の条件に合致します。
選択肢：tcp_ssl_proxy/open_connections
この選択肢が正しくない理由は以下の通りです。
tcp_ssl_proxy/open_connectionsはSSLプロキシの接続数を計測するメトリックであり、App Engineの接続数を計測する目的には適していません。
それに対して、flex/connections/currentはApp Engine flexible environmentの現在の接続数を計測します。
従って、この選択肢は不適切です。
選択肢：flex/instance/connections/current
この選択肢が正しくない理由は以下の通りです。
flex/instance/connections/currentメトリックは、特定のインスタンスにおける接続数を表します。
それに対して、問題では全体の接続数を求めているため、インスタンス別ではなく全接続数を表すflex/connections/currentメトリックを使うのが正解となります。
参考リンク：
https://cloud.google.com/monitoring/api/metrics_Google Cloud#Google Cloud-appengine
https://cloud.google.com/appengine/docs/flexible/java/analytics
https://cloud.google.com/monitoring/api/metrics_agent#agent-network
</div></details>

### Q. 問題2: 未回答
あなたは、オンプレミスとGoogle Cloud上にデプロイされた大規模なGoogle Kubernetes Engine（GKE）クラスター上で実行されるeコマースアプリケーションをサポートしています。アプリケーションは、コンテナで実行されるマイクロサービスで構成されています。あなたは、CPUとメモリを最も使用しているコンテナを特定したいと考えています。
この要件を満たすために、どうすればよいですか？
1. Prometheusを使ってコンテナごとにログを収集・集計し、その結果をGrafanaで分析します
2. Cloud Loggingを使用して、アプリケーションのログをBigQueryにエクスポートし、コンテナごとにログを集約して、CPUとメモリの消費量を分析します
3. Google Cloud Operation Suite Kubernetes Engine Monitoringを使用します
4. Google Cloud Operation Suite Monitoring APIを使ってカスタムメトリクスを作成し、グループを使ってコンテナを整理します
<details><div>
    答え：3
説明
この問題では、Google Kubernetes Engine（GKE）上で動作するeコマースアプリケーションのリソース使用状況を正確に追跡し、特にCPUとメモリ使用量が高いコンテナを特定することが求められています。問題文から、アプリケーションがマイクロサービスとして構成されており、各サービスがコンテナで独立して動作していることが分かります。解決策として選択するべきツールは、属性やリソース使用状況に基づいてコンテナの監視を容易にし、その情報を視覚的に提供するものです。そのため、選択肢の中から、一部のツールがこの目的に適していないか充分でない要素を理解するために、各ツールの機能と限界を比較検討する必要があります。
基本的な概念や原則：
Google Cloud Operation Suite Kubernetes Engine Monitoring：Google Cloudの監視ツールです。Kubernetes Engineのクラスターや、ワークロード、Pod、コンテナ、およびサービスのパフォーマンスを監視するものです。CPUやメモリの使用量に関する詳細な情報を提供します。
PrometheusとGrafana：オープンソースのモニタリングと視覚化ツールです。Prometheusはメトリクス情報の収集とストレージを行い、Grafanaはそれらのメトリクスを視覚化します。両者ともに、グラフ化やアラート通知などの豊富な機能を提供していますが、設定や管理が必要です。
Google Cloud Operation Suite Monitoring API：Google CloudのAPIです。このAPIを使用することで、カスタムメトリクスを作成したり、メトリクスデータを取得・書き込みしたりすることが可能です。特定のユースケースに対してカスタマイズした監視が可能となりますが、設計と実装が必要です。
Cloud LoggingとBigQuery：Google Cloudのサービスです。Cloud Loggingはログの収集・分析・エクスポートを行い、BigQueryは大規模なデータのクエリ実行や分析を行うためのサービスです。アプリケーションのログをBigQueryにエクスポートし、各コンテナのログを集約して分析することで、CPUやメモリの消費量を調査することが可能ですが、適切なクエリの設計と実行が必要です。
正解についての説明：
（選択肢）
・Google Cloud Operation Suite Kubernetes Engine Monitoringを使用します
この選択肢が正解の理由は以下の通りです。
Google Cloud Operation Suite Kubernetes Engine Monitoringは、クラスター内の全体的なパフォーマンスや各ノードおよびコンテナのリソース使用状況を追跡し、可視化する機能を提供しています。この機能は、リソースを最も消費しているコンテナを特定するための重要なツールです。
この機能はCPU、メモリ、ストレージ、ネットワークなどのリソースの使用率を監視します。これにより、リソースの過剰消費やリソース不足が発生しているコンテナを特定することが可能となります。
また、Google Cloud Operationsによってデータは自動的に集約され、分析やアラートの設定、パフォーマンスの障害を特定するための洞察の提供など、より深い洞察を得ることが可能です。
したがって、Google Cloud Operation Suite Kubernetes Engine Monitoringを使用することで、CPUとメモリ使用率が最も高いコンテナを特定する要件を満たすことができます。
不正解についての説明：
選択肢：Prometheusを使ってコンテナごとにログを収集・集計し、その結果をGrafanaで分析します
この選択肢が正しくない理由は以下の通りです。
PrometheusとGrafanaを使用する方法もCPUとメモリの使用状況を確認することは可能ですが、導入と設定が必要であり、手間がかかります。対してGoogle Cloud Operation Suite Kubernetes Engine Monitoringは、多大な設定なしにコンテナのリソース使用状況を監視でき、より簡単に要件を満たすことができます。
選択肢：Google Cloud Operation Suite Monitoring APIを使ってカスタムメトリクスを作成し、グループを使ってコンテナを整理します
この選択肢が正しくない理由は以下の通りです。
カスタムメトリクスを作成し、グループでコンテナを整理するというアプローチは、具体的なコンテナのCPUとメモリの使用状況を追跡するために必要なすべての情報を提供しない可能性があります。
また、この方法はGoogle Cloud Operation Suite Kubernetes Engine Monitoringを使用するよりも手間がかかり、より複雑な設定が必要になるため、効率的ではありません。
選択肢：Cloud Loggingを使用して、アプリケーションのログをBigQueryにエクスポートし、コンテナごとにログを集約して、CPUとメモリの消費量を分析します
この選択肢が正しくない理由は以下の通りです。
Cloud LoggingとBigQueryでは、実際のメモリやCPUの使用状況を直接解析することはできません。それらはログデータを保管、分析するためのツールで、メモリやCPUの使用量を直接的に監視、追跡する能力はありません。
それに対し、Cloud Operation Suite Kubernetes Engine Monitoringは実際のリソースの使用状況を監視し特定することが可能です。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/monitoring
https://cloud.google.com/stackdriver/docs
https://prometheus.io/docs/introduction/overview/
</div></details>

### Q. 問題3: 未回答
Global HTTP/S Cloud Load Balancer（CLB）の背後で、Google Kubernetes Engine（GKE）上で動作するマルチリージョンのウェブサービスをサポートしています。レガシーシステムの要件から、ユーザーリクエストはまずサードパーティのコンテンツデリバリーネットワーク（CDN）を経由し、その後CLBにトラフィックをルーティングします。CLBレベルではすでに可用性サービスレベルインジケータ（SLI）を実装しています。しかし、ロードバランサーの設定ミス、CDNの障害、またはその他のグローバルなネットワーキングの災害の可能性に備えて、カバレッジを高めたいと考えています。
この新しいSLIはどこで測定すべきですか？（2つ選択）
1. アプリケーションサーバーからエクスポートされたメトリクス
2. アプリケーションサーバーのGKE健全性チェック
3. アプリケーションサーバーのログ
4. シミュレートされたユーザーリクエストを定期的に送る合成クライアント
5. クライアントに直接コード化されたインストルメンテーション
<details><div>
    答え：4,5
説明
この問題では、可用性を高めるために、既存のロードバランサーの設定に加えて追加のサービスレベルインジケータ（SLI）を設定する適切な方法を考える必要があります。特に、ロードバランサーの設定ミスやCDNの障害、その他のグローバルなネットワーキングの問題に対して備えることを求められています。既存のCLBレベルのSLIとは異なる角度からの監視が必要で、ユーザーの視点からのインスタンスや合成クライアントからの指標を考えることが必要です。したがって、ユーザーやクライアント側から見たサービスの可用性や性能を正確に評価できる指標を選ぶことが肝要です。
基本的な概念や原則：
Cloud Load Balancer：Google Cloudのロードバランサーサービスで、大規模で広範囲なネットワークロードを管理することができます。高い冗長性とスケーラビリティを提供することでダウンタイムを最小限に抑えます。
Google Kubernetes Engine（GKE）：Google CloudのマネージドKubernetesサービスで、コンテナ化されたアプリケーションのデプロイ、スケーリング、運用を容易にします。
コンテンツデリバリーネットワーク（CDN）：ユーザーにカスタムコンテンツやアプリケーションを効率的に配信するためのネットワークシステムです。DNS、ルーティング、サーバーの配置などを最適化して配信速度や信頼性を向上させます。
サービスレベル指標（SLI）：サービスの品質レベルを定量的に表現したものです。システムの性能、可用性、信頼性などを客観的に評価します。
クライアント側のインストルメンテーション：システムの動作やパフォーマンスを測定するために、クライアントアプリケーションに直接組み込まれるソフトウェアコンポーネントです。
合成クライアント：システムの動作をシミュレートするために用いられるソフトウェアツールで、リアルユーザーリクエストを模擬的に送信します。これにより、システムの反応や振る舞いをテストすることができます。
正解についての説明：
（選択肢）
・クライアントに直接コード化されたインストルメンテーション
・シミュレートされたユーザーリクエストを定期的に送る合成クライアント
この選択肢が正解の理由は以下の通りです。
まず、クライアントに直接コード化されたインストルメンテーションは、ユーザーの視点から体験を計測する非常に効果的な方法です。これにより、ユーザーリクエストがサードパーティのCDNを経由し、GKEへの路線をたどる全体の過程を観察できます。これは、ロードバランサーの設定ミスやCDNの障害など、システムのどの部分が失敗する可能性があるかを特定するのに役立ちます。
次に、シミュレートされたユーザーリクエストを定期的に送る合成クライアントは、サービスがグローバルに機能しているかどうかをチェックするのに適しています。これらのリクエストは、ユーザートラフィックの様々な観点を模したものであり、運用上の問題や障害を早期に検出するための追加的な保証を提供します。
これら両方の手法を併用することで、全体のシステムパフォーマンスの広範な視点を得ることができ、より高いカバレッジを達成することができます。
不正解についての説明：
選択肢：アプリケーションサーバーのログ
この選択肢が正しくない理由は以下の通りです。
アプリケーションサーバーのログは、ネットワーキングの問題やロードバランサーの設定ミス、CDNの障害を直接検知する能力がありません。問題がアプリケーションサーバーまで到達しない場合、ログは何も報告しません。
それに対して、クライアントに直接コード化されたインストルメンテーションや合成クライアントは、全体の経路を通るリクエストを監視できます。
選択肢：アプリケーションサーバーからエクスポートされたメトリクス
この選択肢が正しくない理由は以下の通りです。
アプリケーションサーバーからエクスポートされたメトリクスはネットワーキングの災害やCDNの障害を考慮に入れません。
一方、正解の選択肢であるクライアントのインストルメンテーションまたはシミュレートされたユーザーリクエストは、エンドユーザーの視点からのサービスの状態を測定します。
選択肢：アプリケーションサーバーのGKE健全性チェック
この選択肢が正しくない理由は以下の通りです。
アプリケーションサーバーのGKE健全性チェックは有用な情報を提供しますが、クライアントからCDNを経由した全体の経路をチェックする能力がありません。
正解の選択肢は、エンドツーエンドのパスをカバーしており、それに対して不正解の選択肢はその範囲が限定的です。
参考リンク：
https://cloud.google.com/load-balancing/docs/https
https://cloud.google.com/architecture/framework/reliability/slis-slos-error-budgets
https://developers.google.com/web/tools/chrome-user-experience-report
</div></details>

### Q. 問題4: 未回答
あなたは組織のためにCloud Run上でマイクロサービスを構築し、デプロイしています。このサービスは社内の多くのアプリケーションで使用されています。あなたは新しいリリースをデプロイしており、新しいバージョンをステージング環境と本番環境で広範囲にテストする必要があります。ユーザーと開発者への影響を最小限に抑える必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. 新バージョンのサービスをステージング環境にデプロイします。トラフィックを分割し、50%のトラフィックを最新バージョンに通します。最新バージョンをテストします。テストに合格したら、すべてのトラフィックを最新バージョンに送ります。本番環境でもこれを繰り返します
2. サービスの新バージョンを、トラフィックを提供せずに新リリースタグを付けてステージング環境にデプロイします。ニューリリースバージョンをテストします。テストに合格したら、このタグ付きバージョンを徐々にロールアウトします。本番環境でもこれを繰り返します
3. 新バージョンのサービスをステージング環境にデプロイします。トラフィックを分割し、1%のトラフィックを最新バージョンに通します。最新バージョンをテストします。テストに合格したら、最新バージョンをステージング環境と本番環境に徐々にロールアウトします
4. ステージング環境として使用するために、グリーンタグで新しい環境をデプロイします。新バージョンのサービスをグリーン環境にデプロイし、新バージョンをテストします。テストに合格したら、すべてのトラフィックをグリーン環境に送り、既存のステージング環境を削除します。本番環
<details><div>
    答え：3
説明
この問題では、Cloud Run上で新しいバージョンのマイクロサービスをデプロイし、ステージング環境と本番環境で広範囲にテストすることが求められています。また、ユーザーと開発者への影響を最小限に抑えるという条件もあります。このようなケースでは、新しいサービスのバージョンを試験的に利用者に提供することを可能にする建設的なロールアウト戦略の選択が重要となるため、各選択肢が提供しているロールアウト戦略をよく理解することが求められます。それぞれの戦略がどの程度まで影響範囲を制限しつつ、効率的なテストとデプロイを実現できるかを深く考えることが必要です。
基本的な概念や原則：
Cloud Run：Google Cloudのサービスで、コンテナ化されたアプリケーションをサーバーレスで実行できます。管理が不要でスケーラビリティが高いのが特長です。
マイクロサービス：一つ一つが独立した機能を持つ小さなサービス群から構成されるアプリケーションの設計パターンです。各サービスは独立してデプロイやスケールが可能で、一部の更新でも全体のダウンタイムを防げます。
ステージング環境：本番環境とほぼ同じ条件の下でシステムの最終確認を行う環境です。本番環境への影響を避けながら、新しいバージョンのテストが可能です。
トラフィックの分割：Cloud Runの機能で、新旧バージョン間でトラフィックを割り振ることが可能です。これにより、新バージョンへの影響を徐々に広げることができ、ローリングアップデートが可能です。
タグ：Cloud Runの複数バージョンを識別するためのラベルです。リクエストのルーティングに利用することができます。
グリーン/ブルー（青）デプロイ：ゴーシュでもよくとられるデプロイ手法です。新旧の環境（グリーンとブルー）を切り替えることで、ダウンタイムなく新バージョンを適用します。ただし、環境自体を用意する必要があります。
正解についての説明：
（選択肢）
・新バージョンのサービスをステージング環境にデプロイします。トラフィックを分割し、1%のトラフィックを最新バージョンに通します。最新バージョンをテストします。テストに合格したら、最新バージョンをステージング環境と本番環境に徐々にロールアウトします
この選択肢が正解の理由は以下の通りです。
初めに、新しいバージョンをステージング環境でデプロイすることは、新機能や修正の影響を確認するための標準的な手法です。ステージング環境では本番環境と同一の設定でテストを行うことができ、予期しない問題を未然に防ぐことが可能です。
次に、新しいバージョンへのトラフィックを徐々に増やすという戦略はカナリアリリースと呼ばれ、リスクを最小限に抑えつつ新バージョンのロールアウトを行うための効果的な手法です。初めに1％のトラフィックを新バージョンに流すことで、新しいバージョンのパフォーマンスと稼働状況を確認し、大きな問題がないことを確認します。
これらの結果が良好であれば、新バージョンへとトラフィックを徐々にシフトすることで新バージョンを本番環境にロールアウトします。このステップも同様に、ユーザーや開発者への影響を最小限に抑えながら、新バージョンを全体に広げることを可能にします。以上の手順がこの選択肢を最適な答えとしています。
不正解についての説明：
選択肢：新バージョンのサービスをステージング環境にデプロイします。トラフィックを分割し、50%のトラフィックを最新バージョンに通します。最新バージョンをテストします。テストに合格したら、すべてのトラフィックを最新バージョンに送ります。本番環境でもこれを繰り返します
この選択肢が正しくない理由は以下の通りです。
新しいバージョンをテストする際に、一度に50%ものトラフィックを最新バージョンに通すのはリスクが大きい。新バージョンに問題があった場合、多くのユーザーと開発者に影響を及ぼす可能性があります。そのため、最初に少量のトラフィックを最新バージョンに送り、問題がないことを確認した上で徐々にトラフィックを増やす方法が適切です。
選択肢：サービスの新バージョンを、トラフィックを提供せずに新リリースタグを付けてステージング環境にデプロイします。ニューリリースバージョンをテストします。テストに合格したら、このタグ付きバージョンを徐々にロールアウトします。本番環境でもこれを繰り返します
この選択肢が正しくない理由は以下の通りです。
新バージョンをトラフィックを提供せずにデプロイすると、実際のユーザートラフィックの影響をテストできず、問題が起こった時にそれを検出する機会を失います。
一方、正解選択肢では1%のトラフィックから始めて影響を見て徐々に増やすことで、テストの負担を軽減しつつリスクを管理します。
選択肢：ステージング環境として使用するために、グリーンタグで新しい環境をデプロイします。新バージョンのサービスをグリーン環境にデプロイし、新バージョンをテストします。テストに合格したら、すべてのトラフィックをグリーン環境に送り、既存のステージング環境を削除します。本番環境でもこれを繰り返します
この選択肢が正しくない理由は以下の通りです。
最初にすべてのトラフィックを新環境に送るグリーン/ブルーのデプロイメントは、テスト段階で未発見の問題が発生した場合、全ユーザーに影響を及ぼします。正解の選択肢では、トラフィックの分割により新バージョンへの移行を徐々に行いながら異常を検知しやすく、ユーザーへの影響を最小限に抑えます。
参考リンク：
https://cloud.google.com/run/docs/rolling-out-updates
https://cloud.google.com/run/docs/configuring/traffic-migration
https://cloud.google.com/blog/products/serverless/how-to-deploy-a-cloud-run-service-with-minimal-downtime
</div></details>

### Q. 問題5: 未回答
ビジネスクリティカルなワークロードを、固定されたCompute Engineインスタンスセットで数ヶ月間実行する必要があります。ワークロードは、正確なリソース量が割り当てられ、安定しています。パフォーマンスに影響を与えることなく、このワークロードのコストを下げたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. インスタンスをマネージドインスタンスグループに移行します
2. 確約利用割引を購入します
3. インスタンスをプリエンプティブル仮想マシンに変換します
4. ワークロードの実行に使用するインスタンス用に、アンマネージドインスタンスグループを作成します
<details><div>
    答え：2
説明
この問題では、固定されたCompute Engineインスタンスで長期間実行されるビジネスクリティカルなワークロードの運用コストを削減する方法を選択することが主目的です。重要なことは、ワークロードの正確なリソース量が割り当てられているという情報と、パフォーマンスに影響を与えない方法でコストを下げるという要点を理解することです。適切な選択肢を選ぶ時には、リソースの需要が長期間固定され、安定しているという事実に基づき、それに最適なコスト最適化戦略を選ぶことが求められていることを覚えておいてください。
基本的な概念や原則：
確約利用割引：Google Cloudの料金割引制度で、1年または3年の契約期間で指定したCompute Engineインスタンスを予約することで、高額な割引を受けることができます。
マネージドインスタンスグループ：Google Cloud上で複数の同一設定のインスタンスを自動的に作成、管理する仕組みです。スケールアウトやアップデートの自動化が可能ですが、ワークロードが一定である場合にはコスト削減の効果はありません。
プリエンプティブル仮想マシン：Compute Engineで提供される廉価な一時的仮想マシンです。最大で24時間しか使用できず、他のリソースへの影響で任意のタイミングでシャットダウンされる可能性があるため、ビジネスクリティカルなワークロードには適していません。
アンマネージドインスタンスグループ：同一設定のインスタンスの集合体を、自動的には管理しない形で用意するグループです。インスタンスの数や設定を自身で管理する必要があり、コスト削減要素はありません。
正解についての説明：
（選択肢）
・確約利用割引を購入します
この選択肢が正解の理由は以下の通りです。
まず、確約利用割引は、Compute Engineのリソースを一定期間絶えず使用することが確定しているユーザーに対して、割引を提供する手段です。具体的には、1年または3年間の使用を事前に確約することで、その期間中の使用料金が大幅に割引される仕組みです。確約利用割引を利用することで、最大で従量課金と比較して70%近くの割引が適用されます。この選択肢は特に、ワークロードが安定しており数ヶ月間実行が予定されているケース、つまり当該シナリオのようなケースに適しています。ワークロードが一定であることから、確定したリソース範囲内での運用が可能であるため、事前に予約することによる割引効果を最大限に活かすことができます。
したがって、パフォーマンスに影響を与えずにコストを下げるための方法として、確約利用割引の購入は最適な選択となります。
不正解についての説明：
選択肢：インスタンスをマネージドインスタンスグループに移行します
この選択肢が正しくない理由は以下の通りです。
マネージドインスタンスグループは主に負荷分散や自動スケーリングを実現するものであり、コスト削減そのものには直接寄与しません。
それに対し、確約利用割引では、長期間安定して特定のリソースを利用する予定であれば、割引価格で使用することが可能となり、コストを下げることができます。
選択肢：インスタンスをプリエンプティブル仮想マシンに変換します
この選択肢が正しくない理由は以下の通りです。
プリエンプティブル仮想マシンは、リソースが必要になった場合にいつでも終了される可能性があり、ビジネスクリティカルなワークロードには不適切です。確約利用割引を購入すると、長期間利用が見込まれるインスタンスに対して大幅な割引を受けられます。
選択肢：ワークロードの実行に使用するインスタンス用に、アンマネージドインスタンスグループを作成します
この選択肢が正しくない理由は以下の通りです。
アンマネージドインスタンスグループは、一連のインスタンスを一緒に管理するものであり、コスト削減に貢献する機能は提供していません。対して確約利用割引は、事前に一定期間の使用を確約することで大幅な割引を受けられるため、コスト削減に効果的です。
参考リンク：
https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts
https://cloud.google.com/compute/docs/instance-groups/
https://cloud.google.com/compute/docs/instances/preemptible
</div></details>

### Q. 問題6: 未回答
カスタムのDebianイメージを使って仮想マシン（VM）でアプリケーションを実行しています。イメージにはCloud Loggingエージェントがインストールされています。VMにはクラウドプラットフォームのスコープがあります。アプリケーションはsyslog経由で情報をロギングしています。Google Cloud ConsoleでCloud Loggingを使ってログを可視化したいと考えています。ログビューアの "すべてのログ" ドロップダウンリストにsyslogが表示されていないことに気づきました。
この問題を解決するために、最初にすべきことは何ですか？
1. VMにSSH接続し、VM上で以下のコマンドを実行します
2. VMサービスアカウントのアクセススコープにmonitoring.writeスコープが含まれていることを確認します
3. ログビューアでエージェントのテストログエントリを探します
4. Google Cloud Operation Suiteエージェントの最新バージョンをインストールします
<details><div>
    答え：1
説明
この問題では、Google Cloud上の仮想マシン（VM）から送られたログがGoogle Cloud ConsoleのCloud Loggingに表示されていない問題に対する最初の対応を求めています。問題文から、カスタムのDebianイメージを使ったVMではCloud Loggingエージェントが既にインストールされていて、そのVMがCloud Platformのスコープを持っていてログがsyslog経由でロギングされていることが分かります。ここで重要な点は、まずVM内部でのロギング設定や状況を確認することです。どのようにシステムの中で動作しているのか、現在の設定はどうなっているのかを理解することが重要で、それを行うために、SSH接続を使ってVMにアクセスすることが最初のステップとなります。
基本的な概念や原則：
Debianイメージ：Linuxディストリビューションの一つで、オープンソースのオペレーティングシステムです。Google Cloudで提供される仮想マシン（VM）のイメージとして利用できます。
Cloud Logging：Google Cloudのロギング監視サービスです。アプリケーションやシステムからのログデータを一元的に管理、分析、可視化できます。
Syslog：システムのログメッセージを送信するための標準プロトコルです。アプリケーション、システム、ネットワークデバイスからのメッセージをログサーバに転送します。
Cloud Console：Google Cloudの統合管理ツールです。各種サービスの管理や監視、データの解析などが一元的に行えます。
SSH接続：セキュアなリモートログインとコマンドの実行を可能にするプロトコルです。暗号化により、通信内容が第三者に盗聴されるリスクを軽減します。
Google Cloud Operation Suite：Google Cloudの監視、トラブルシューティング、および診断のソリューションスイートです。Cloud Logging、Cloud Monitoringなどのサービスが含まれます。
サービスアカウントのアクセススコープ：Google Cloudリソースへのアクセスレベルを制御するものです。各サービスごとに異なるスコープを設定できます。たとえば、monitoring.writeスコープはGoogle Cloud Monitoringへの書き込みアクセスを許可します。
正解についての説明：
（選択肢）
・VMにSSH接続し、VM上で以下のコマンドを実行します
この選択肢が正解の理由は以下の通りです。
まず、VMにSSH接続することにより、直接その環境を確認することが可能となります。これは問題を解決するための最初の重要なステップです。
次に、問題の状況がCloud Loggingエージェントに関連していると想定されるため、VM上でコマンドを実行することは適切です。VM上で直接コマンドを実行することにより、その場で問題の状況を詳細に把握し、それに基づいて解決策を探ることができます。
具体的なコマンドは問題から省略されていますが、このコンテキストでは、Cloud Loggingエージェントのステータスを確認したり、エラーが発生していないかログをチェックしたりするコマンドを実行することが一般的です。エージェントの状態やログの中身を確認することで、何が問題で、どのように解決すべきかについての手がかりを得ることができます。そのため、この選択肢が問題を解決するための最初のステップとなります。
不正解についての説明：
選択肢：ログビューアでエージェントのテストログエントリを探します
この選択肢が正しくない理由は以下の通りです。
問題はsyslogのログが表示されていないことなので、エージェントのテストログエントリを探すことは解決策として不適切です。正しくは、VM上でロギングエージェントが適切に機能しているかを確認する必要があります。
選択肢：Google Cloud Operation Suiteエージェントの最新バージョンをインストールします
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suiteエージェントの最新バージョンをインストールするだけでは、"すべてのログ" リストにsyslogが表示されない問題は解決しません。まずはSSHでVMに接続し、問題の特定を試みることが最初のステップとなります。
選択肢：VMサービスアカウントのアクセススコープにmonitoring.writeスコープが含まれていることを確認します
この選択肢が正しくない理由は以下の通りです。
試問のシナリオでは、ログがCloud Loggingに表示されない問題が発生していますが、これはVMのサービスアカウントのスコープに関連する問題ではありません。シナリオでは、VMは既にCloud Platformのスコープを持つと述べられています。
したがって、monitoring.writeスコープを追加するという手段は問題を解決しません。本問の問題は、ログが適切にCloud Loggingに送信されていないので、VMにSSH接続して直接診断することが最初に必要な行動です。
参考リンク：
https://cloud.google.com/logging/docs/agent/logging
https://cloud.google.com/logging/docs/view/logs-viewer-interface
https://cloud.google.com/logging/docs/agent/installation
</div></details>

### Q. 問題7: 未回答
あなたは、Compute Engine上でホストされているWebアプリケーションをサポートしています。このアプリケーションは、何千人ものユーザーに予約サービスを提供しています。新機能のリリース直後、監視ダッシュボードに、すべてのユーザーがログイン時に待ち時間を経験していることが表示されました。そのため、このインシデントがサービスのユーザーに与える影響を軽減したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. ログインサービスを実行している仮想マシンをアップサイズします
2. 最近のリリースをロールバックします
3. オペレーションスイートのモニターを見直します
4. 新しいリリースをデプロイして、問題が修正されているかどうかを確認します
<details><div>
    答え：2
説明
この問題では、Webアプリケーションの新機能リリース直後に生じた問題を解決する最適な手段を選択する必要があります。すべてのユーザーがログイン時に待ち時間を経験しているという情報から、この問題の原因が新機能のリリースに関連していることが示唆されます。したがって、解決策として考えうる選択肢を見るときには、ユーザー体験の悪化を最小限に抑える対策として、可能な最も直接的かつ効率的な手段を探すべきです。
基本的な概念や原則：
リリースロールバック：新たにデプロイした更新が問題を引き起こす場合、以前の動作するバージョンに戻す行為です。これにより、問題の解決が行われるまでの間、ユーザーの影響を軽減することができます。
監視ダッシュボード：システムやアプリケーションのパフォーマンスや状態をリアルタイムで視覚的に表示するツールです。問題や異常の早期発見に役立ちます。
Compute Engine：Google Cloudの仮想マシンを提供するサービスです。高いパフォーマンスと拡張性を持ち、大量のユーザートラフィックを処理する能力があります。
仮想マシンのアップサイズ：仮想マシンのリソース（CPU、メモリ、ディスク容量等）を増やす行為です。ただし、単純にリソースを増やすだけでは、アプリケーションの具体的な問題は解決しない可能性があります。
オペレーションスイート：Google Cloudの監視、トラブルシューティング、アラート設定などを行うための統合ツールセットです。問題発生の原因解析や改善のための情報提供を行います。
正解についての説明：
（選択肢）
・最近のリリースをロールバックします
この選択肢が正解の理由は以下の通りです。
まず、新機能のリリース直後にユーザー全体が待機時間を経験するという問題が発生したため、この問題はおそらく新リリースに関連していると予想できます。
したがって、最近のリリースをロールバックすれば、アプリケーションを以前正常に機能していた状態に戻すことができるはずです。
この対応によって、すぐに問題の影響を軽減し、ユーザーへのサービス提供を再開できます。
その後、開発チームは新しいリリースが引き起こした問題を特定し、修正するための時間を得ることができます。
これは特に影響範囲が広範で、すべてのユーザーが影響を受けている場合、優先的に対処すべき手法です。
したがって、この選択肢は、サービスのユーザーに対する影響を即時に軽減するための最善の手段となります。
不正解についての説明：
選択肢：オペレーションスイートのモニターを見直します
この選択肢が正しくない理由は以下の通りです。
オペレーションスイートのモニターを見直すだけでは、即座にユーザーのログイン待ち時間が改善するわけではありません。問題の軽減には時間がかかる可能性があります。
一方、最近のリリースをロールバックすることで、直ちに問題を解決し、ユーザーへの影響を最小限に抑えることが可能です。
選択肢：ログインサービスを実行している仮想マシンをアップサイズします
この選択肢が正しくない理由は以下の通りです。
問題が新機能のリリース直後に発生していることから考えると、サーバーのリソース不足よりも新機能のバグが原因と思われます。従って仮想マシンをアップサイズしても問題の元となるバグを修正することはできません。そのため、直前の動作が確認された状態に戻すために最近のリリースをロールバックする方が適切です。
選択肢：新しいリリースをデプロイして、問題が修正されているかどうかを確認します
この選択肢が正しくない理由は以下の通りです。
新しいリリースをデプロイする前に問題が修正されているかを確認する必要があります。
ただし、影響をすぐに軽減する方法としては、最新のリリースをロールバックするのが最適です。これによりサービスは即座に前の正常な状態に戻ります。
参考リンク：
https://cloud.google.com/compute/docs/instances/rolling-back-a-managed-instance-group
https://cloud.google.com/monitoring/docs
https://cloud.google.com/iam/docs/granting-changing-revoking-access
</div></details>

### Q. 問題8: 未回答
Compute Engine上でアプリケーションを実行し、Google Cloud Operation Suiteを通してログを収集しています。あなたは、個人を特定できる情報（PII）が特定のログエントリフィールドに漏洩していることを発見しました。あなたは、これらのフィールドが新しいログエントリに書き込まれるのをできるだけ早く防ぎたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. アプリケーション開発者がアプリケーションにパッチを当てるのを待ち、ログエントリがPIIを暴露しなくなったことを確認します
2. filter-record-transformer Fluentdフィルタープラグインを使って、インフライトのログエントリからフィールドを削除します
3. fluent-plugin-record-reformer Fluentd出力プラグインを使用して、インフライトのログエントリからフィールドを削除します
4. ステージのログエントリをCloud Storageに保存し、Cloud Functionsをトリガーしてフィールドを削除し、Cloud Logging APIを介してエントリをGoogle Cloud Operation Suiteに書き込みます
<details><div>
    答え：2
説明
この問題では、Compute Engine上で実行されているアプリケーションが生成するログから、個人を特定できる情報（PII）を速やかに削除するための最適な方法を求めています。重要なのは、既に漏洩している情報ではなく新規に生成されるログエントリに対処することで、速やかな対応が求められています。そのためには、ログエントリが生成され、ロギングシステムに送信される過程で、PIIを含むフィールドを削除する方法を探す必要があります。選択肢の中から、Fluentdフィルタープラグインを使う方法、アプリケーションにパッチを当てる方法、Cloud Functionsなどを使って後処理をする方法などが提供されており、これらの中から最も効率的で迅速な解決策を選ぶことが求められています。
基本的な概念や原則：
Google Cloud Operation Suite: Google Cloudの監視、トラブルシューティング、アプリケーションパフォーマンス管理のためのツールセットです。これにはCloud Loggingも含まれています。
Fluentd: オープンソースデータコレクターで、プラグインにより柔軟なログ収集と転送が可能です。
filter-record-transformerプラグイン: ログエントリにおける特定のフィールドを削除したり、変換したりするFluentdのフィルタープラグインです。これにより、PIIなどの敏感な情報がログに残るのを防ぐことができます。
fluent-plugin-record-reformerプラグイン：このフィルタープラグインはFluentdで使用され、ログデータの構造化を簡素化します。しかし、フィールドの削除には使えません。
Cloud Storage: 高耐久性と高可用性を提供するGoogle Cloudのオブジェクトストレージシステムですが、リアルタイムのログフィールド削除には向いていません。
Cloud Functions: サーバーレスなコンピューティングプラットフォームで、特定のイベントに応じて自動的にトリガーされます。しかし、リアルタイムのログフィールド削除には向いていません。
個人を特定できる情報（PII）: 個々の人々を識別するのに十分な情報が含まれている可能性があります。これには、フルネーム、住所、メールアドレスなどが含まれ、厳格に規制されています。
正解についての説明：
（選択肢）
・filter-record-transformer Fluentdフィルタープラグインを使って、インフライトのログエントリからフィールドを削除します
この選択肢が正解の理由は以下の通りです。
FluentdはCompute Engine上で実行されるアプリケーションのログを収集するために使用されるオープンソースのデータコレクターであり、Google Cloud Operation SuiteではFluentdを利用してログの収集が行われます。filter-record-transformerはFluentdのプラグインであり、ログデータを一時的に保持し、その内容をフィルタリングし変換する機能を提供しています。このプラグインを使うことで、インフライトのログエントリ、つまりまだデータ出力先に送られていないログエントリから特定のフィールドを削除できます。これによって、個人を特定できる情報が新しいログエントリに書き込まれるのを防ぐことができます。
したがって、このソリューションはフィールドの取り扱いを細かくコントロールする必要性を満たしながらも、コンプライアンスを維持するためには最適な選択となります。
不正解についての説明：
選択肢：fluent-plugin-record-reformer Fluentd出力プラグインを使用して、インフライトのログエントリからフィールドを削除します
この選択肢が正しくない理由は以下の通りです。
fluent-plugin-record-reformerは出力プラグインで、ログエントリが処理された後にしか操作できません。
それに対して、filter-record-transformerはフィルタープラグインであり、ログエントリが収集され、エンコードされる前にフィールドを削除できます。よって、PII情報の漏洩を防ぐためには、filter-record-transformerの方が適切です。
選択肢：アプリケーション開発者がアプリケーションにパッチを当てるのを待ち、ログエントリがPIIを暴露しなくなったことを確認します
この選択肢が正しくない理由は以下の通りです。
アプリケーション開発者がパッチを適用するのを待つアプローチは効率的でなく、即時性が求められる問題に対処するには時間がかかりすぎます。
一方、Fluentdフィルタープラグインを使用すれば、インフライトのログからすぐにPIIフィールドを削除することが可能です。
選択肢：ステージのログエントリをCloud Storageに保存し、Cloud Functionsをトリガーしてフィールドを削除し、Cloud Logging APIを介してエントリをGoogle Cloud Operation Suiteに書き込みます
この選択肢が正しくない理由は以下の通りです。
この方法は非効率的で時間がかかり、潜在的なデータブレーチの可能性を減少させません。
また、正しい解答であるFluentdフィルタープラグインを使用すると、データがログエントリに書き込まれる前にフィールドを削除し、迅速に問題を解決できます。
参考リンク：
https://cloud.google.com/logging/docs/agent/configuration#process-payload
https://cloud.google.com/logging/docs/agent/logging/configuration#special-fields
https://docs.fluentd.org/filter/record_transformer
</div></details>

### Q. 問題9: 未回答
あなたは、異なるVPCにあるGoogle Kubernetes Engine（GKE）クラスター間の接続を構成しています。構成中に、クラスターAのノードがクラスターBのノードにアクセスできないことに気づきました。あなたは、ワークロードアクセスの問題がネットワーク構成に起因しているのではないかと考えています。この問題をトラブルシューティングする必要がありますが、ワークロードとノードへの実行アクセスがありません。この状況の中で、ネットワーク接続が壊れているレイヤーを特定したいと考えています。
この要件を満たすために、どうすればよいですか？
1. デバッグコンテナを使って、クラスターAからクラスターBへ、クラスターBからクラスターAへtracerouteコマンドを実行します
2. クラスターのノードにtoolboxコンテナをインストールします。クラスターBへのルートが適切に設定されていることを確認します
3. 両方のVPCでVPCフローログを有効にし、パケットドロップを監視します
4. Network Connectivity Centerを使用して、クラスターAからクラスターBへの接続テストを実行します
<details><div>
    答え：4
説明
この問題では、異なるVPCにあるクラスター間の接続問題を特定するための一連の手段が問われています。クラスターAとクラスターBが互いに接続できない、という問題が示されており、その問題がネットワーク構成に起因している可能性があると示唆されています。しかし、重要なことに、ワークロードとノードへの直接的な実行アクセスはないと指定されています。これは、解答選択肢を考える際の重要な制約となります。したがって、解答選択はイントラネット内の構成を調べ、その状態を把握することが可能なツールやサービスに焦点を当てるべきです。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するマネージドKubernetesサービスです。コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を自動化します。
VPC（Virtual Private Cloud）：Google Cloudの仮想的なプライベートネットワーク環境です。自分だけのGoogle Cloud内に閉じたネットワークを作成することができます。
Network Connectivity Center：Google Cloudのネットワーク接続を一元管理するためのサービスです。ネットワーク接続の可視化や診断機能を提供します。
toolboxコンテナ：各種デバッグツールを含むコンテナです。トラブルシューティング時にノードにデプロイすることで、ネットワーク状況を解析することが可能です。
tracerouteコマンド：パケットがネットワークを通過する経路を追跡するコマンドです。これにより、ネットワークの問題発生箇所を特定することが可能です。
VPCフローログ：VPCネットワークのIPトラフィック情報をキャプチャしてログを提供する機能です。リアルタイムなネットワークモニタリングや問題の診断、ネットワークセキュリティに対する洞察を提供します。
正解についての説明：
（選択肢）
・Network Connectivity Centerを使用して、クラスターAからクラスターBへの接続テストを実行します
この選択肢が正解の理由は以下の通りです。
Google CloudのNetwork Connectivity Centerは、異なるネットワークデプロイメント間の接続性に関する可視性と管理能力を提供します。標準のネットワーク接続アクセス方法であるpingやtraceroute問い合わせなども可能ですが、これらの方法は管理者がワークロードやノードへの実行アクセスを持つ必要があります。
一方、Network Connectivity Centerは接続性トラブルシューティングを行うためのエンドツーエンドの分析機能を提供しています。これは本問の要件である"ワークロードとノードへの実行アクセスがなく、ネットワーク接続が壊れているレイヤーを特定したい"状況に非常に適しています。ユーザーが接続テストを実行すれば、その結果を基にネットワーク接続が壊れているレイヤーを特定することが可能です。よって、Network Connectivity Centerを使用して接続テストを実行するという選択肢は、本設問の要件を適切に満たすものと言えます。
不正解についての説明：
選択肢：クラスターのノードにtoolboxコンテナをインストールします。クラスターBへのルートが適切に設定されていることを確認します
この選択肢が正しくない理由は以下の通りです。
問題文ではワークロードとノードへの実行アクセスがないことが明記されています。しかし、toolboxコンテナのインストールやルートの確認は、ノードにアクセスする必要があります。このため、この選択肢は要件を満たせません。対比すると、Network Connectivity Centerを使用して接続テストを行う正解の選択肢は、ノードへのアクセスが必要ないため要件に適合します。
選択肢：デバッグコンテナを使って、クラスターAからクラスターBへ、クラスターBからクラスターAへtracerouteコマンドを実行します
この選択肢が正しくない理由は以下の通りです。
問題の説明によると、ワークロードとノードへの実行アクセスがないため、デバッグコンテナを使ってtracerouteコマンドを実行することは不可能です。
逆に、Network Connectivity Centerを使用すれば、透明にネットワーク接続をテストでき、問題の特定に役立ちます。
選択肢：両方のVPCでVPCフローログを有効にし、パケットドロップを監視します
この選択肢が正しくない理由は以下の通りです。
VPCフローログを有効にしても、特定の接続パスに問題があるかどうかをテストすることはできません。そのため、これを用いてもネットワーク接続の障害箇所を特定することが難しいです。
一方、Network Connectivity Centerを使用すれば、特定の接続パスのテストが可能で、問題のあるネットワークレイヤーを特定することができます。
参考リンク：
https://cloud.google.com/network-connectivity/docs/network-connectivity-center/how-to/testing-connectivity
https://cloud.google.com/network-connectivity/docs/network-connectivity-center/concepts/overview
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl
</div></details>

### Q. 問題10: 未回答
本番環境とテスト環境を持つCompute Engine上でリアルタイムゲームアプリケーションを実行しています。それぞれの環境には、独自の仮想プライベートクラウド（VPC）ネットワークがあります。アプリケーションのフロントエンドサーバとバックエンドサーバは、環境のVPC内の異なるサブネットに配置されています。あなたは、本番環境のフロントエンドサーバで断続的に通信している悪意のあるプロセスが存在すると疑っています。あなたは、分析のためにネットワークトラフィックを確実にキャプチャしたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. テスト用と本番用のVPCネットワークのフロントエンドとバックエンドのサブネットで、VPCフローログをボリュームスケール1.0で有効にします。本番環境の前に、テスト環境の変更を適用します
2. 本番VPCネットワークのフロントエンドとバックエンドのサブネットのみで、サンプルボリュームのスケールを1.0として、VPCフローログを有効にします
3. テスト用と本番用のVPCネットワークのフロントエンドとバックエンドのサブネットで、VPCフローログをボリュームスケール0.5で有効にします。本番環境の前に、テスト環境の変更を適用します
4. 本番VPCネットワークのフロントエンドとバックエンドのサブネットのみで、サンプルボリュームのスケールを0.5として、VPCフローログを有効にします
<details><div>
    答え：2
説明
この問題では、悪意のあるネットワークトラフィックを確認するための最適なアプローチが求められています。問題文から、本番環境とテスト環境がそれぞれ異なるVPCネットワークを持ち、フロントエンドとバックエンドサーバはさらにそれぞれ別のサブネットに配置されていることが分かります。また、本番環境のフロントエンドサーバにおいて悪意のあるプロセスが存在する疑いがあるため、その確認のためにネットワークトラフィックをすべてキャプチャしたいという要件があります。これらの情報を元に、適切なネットワーク管理の手法と設定を選択することが課題となります。
基本的な概念や原則：
仮想プライベートクラウド（VPC）：Google Cloudのネットワーキングの中核をなすサービスです。クラウド内の仮想ネットワークとして機能し、インスタンスや他のリソースのネットワーク接続を管理します。
VPCネットワークフローログ：Google Cloud VPCのネットワークトラフィックをキャプチャし、記録する機能です。フローログは、トラフィックの分析や、ネットワークセキュリティやパフォーマンス監視のためのデバッグ情報を提供します。
サンプルボリュームのスケール：フローログの記録頻度を示し、特定の時間間隔における流れの数を制御します。1.0はすべてのフローを記録、0.5は半分のフローを記録することを意味します。
フロントエンドとバックエンドサーバ：一般的に、フロントエンドサーバはクライアントからのリクエストを処理し、バックエンドサーバはデータベースや他のリソースとの通信を処理します。それぞれが特定のサブネットに存在すると考えられます。
テスト環境と本番環境：ソフトウェア開発のために通常用意される2つの環境です。テスト環境は新機能や変更を試すため、本番環境は最終的にユーザーに提供するソフトウェアバージョンを運用するために使用します。通常、これらの環境は別々のVPCを持つことが多いです。
正解についての説明：
（選択肢）
・本番VPCネットワークのフロントエンドとバックエンドのサブネットのみで、サンプルボリュームのスケールを1.0として、VPCフローログを有効にします
この選択肢が正解の理由は以下の通りです。
まず、VPCフローログはGoogle Cloudが提供する、仮想プライベートクラウド（VPC）内のネットワークトラフィックのログを提供するツールです。特定のIPアドレスから送信されたパケット数や、特定のパケットの送受信元、損失したパケットの数などの詳細な情報を提供します。これにより、不審な通信や悪意のあるプロセスの検出に役立つデータが得られます。
また、サンプルボリュームを1.0に設定することで、フローログにすべてのパケットデータを含むように指定します。これにより、詳細なトラフィック分析を進める上で欠かせない情報を確実にキャプチャすることができます。
さらに、フローログをフロントエンドとバックエンドのサブネットだけに適用することで、分析の対象を限定し、結果をパーシャルに取得することができます。
不正解についての説明：
選択肢：本番VPCネットワークのフロントエンドとバックエンドのサブネットのみで、サンプルボリュームのスケールを0.5として、VPCフローログを有効にします
この選択肢が正しくない理由は以下の通りです。
サンプルボリュームのスケールを0.5と設定することは、すべてのネットワークトラフィックをキャプチャすることが出来ません。必要なネットワークトラフィックを全てキャプチャするためには、サンプルボリュームのスケールは1.0に設定する必要があります。
選択肢：テスト用と本番用のVPCネットワークのフロントエンドとバックエンドのサブネットで、VPCフローログをボリュームスケール0.5で有効にします。本番環境の前に、テスト環境の変更を適用します
この選択肢が正しくない理由は以下の通りです。
問題は本番環境のフロントエンドサーバの不審な通信をキャプチャすることで、テスト環境ではなく、本番環境に直接VPCフローログを適用する方が適切です。
また、サンプルボリュームスケールは1.0に設定するべきで、全てのパケットが記録され、悪意のある通信が逃れないようにすべきです。
選択肢：テスト用と本番用のVPCネットワークのフロントエンドとバックエンドのサブネットで、VPCフローログをボリュームスケール1.0で有効にします。本番環境の前に、テスト環境の変更を適用します
この選択肢が正しくない理由は以下の通りです。
本番環境での悪意のある通信を捉えるためには、本番環境のVPCフローログを有効にすべきであり、テスト環境ではないです。ここでは、悪意のある通信を検出すべき対象が本番環境なので、テスト環境でフローログを有効にしても不適切です。
参考リンク：
https://cloud.google.com/vpc/docs/using-flow-logs
https://cloud.google.com/vpc/docs/flow-logs#configuring
https://cloud.google.com/compute/docs/instances/logging-enable-stackdriver#enablelogging
</div></details>

### Q. 問題11: 未回答
あなたは組織のDevOpsプロジェクトをリードしています。DevOpsチームは、サービスインフラを管理し、インシデントに常駐運用で対応する責任があります。ソフトウェア開発チームは、コードの記述、提出、レビューを担当しています。どちらのチームもSLOを公表していません。あなたは、DevOpsチームとソフトウェア開発チームの間で、サービスの新しい共同所有モデルを設計したいと考えています。
新しいジョイントオーナーシップモデルでは、どの責任を各チームに割り当てるべきですか？
1. DevOpsチームの責任
- サービスインフラストラクチャを管理します
- インシデンとに備えて待機します
- コードレビューを実行しますソフトウェア開発チームの責任
- DevOpsチームによるレビューを受けるコードを送信します
- DevOpsチームが満たさなければならないSLOを公開します
2. DevOpsチームの責任
- サービスインフラストラクチャを管理します
- コードレビューを実行します
責任の共有
- 交代でインシデントに対応します
- サービスのSLOを採用して公開しますソフトウェア開発チームの責任
- レビュー対象のコードを送信する
3. DevOpsチームの責任
- サービスインフラストラクチャを管理します
- コードレビューを実行しますソフトウェア開発チームの責任
- DevOpsチームによるレビューを受けるコードを送信します
- インシデンとに備えて待機します
- DevOpsチームが満たさなければならない  SLOを公開します
4. DevOpsチームの責任
- サービスインフラストラクチャを管理します
責任の共有
- コードレビューを実行します
- 交代でインシデントに対応します
- サービスのSLOを採用して公開しますソフトウェア開発チームの責任
- レビュー対象のコードを送信する
<details><div>
    答え：4
説明
この問題では、DevOpsチームとソフトウェア開発チームが共同でサービスを所有し運用する際のロール分担について考える必要があります。まず、複数チームの協働には、各チームのロールや責任の明確化が欠かせません。それらを理解することで、どの作業をどのチームが担当するべきかを考えることが出来ます。具体的には、DevOpsの分野で重要視されるプラクティスを考え、それがDevOpsチームのロールか、それともソフトウェア開発チームのロールかなどを見極める必要があります。その結果を踏まえて、各チームの責任範囲とその間の連携ロールを設定します。また、考慮すべきは、サービスレベル目標（SLO）の公表やインシデント対応などの重要なタスクに対する責任の共有も重要であることです。
基本的な概念や原則：
DevOps：開発（Dev）と運用（Ops）を統合するアプローチで、製品のリリースサイクルを短縮し、信頼性とセキュリティを確保します。チーム間のコラボレーションを促進し、問題の早期発見と修正を目指します。
サービスインフラの管理：運用チーム（DevOpsチーム）のロールとして、サーバー、ネットワーク、データベースなどを監視し、問題が発生した場合に対応します。
インシデント対応：サービスの障害や問題が発生した場合に対応し、原因の特定と解決を行います。この責任は、開発チームと運用チームの間で共有することが効果的です。
SLO（Service Level Objective）：特定のサービスについて達成すべき品質レベルを定めた目標です。これは両チームが共同で公表し、その達成のために協力するべきです。
コードレビュー：開発チームが書いたコードを運用チームがレビューし、問題がないか確認します。これは開発ライフサイクルの一部として共有するべき責任です。
レビュープロセスの一部としてのコード提出：ソフトウェア開発チームが書いてテストしたコードをレビュープロセスに提出します。これは開発プロセスの一部です。
ジョイントオーナーシップ：開発と運用のチームが共同でサービスや製品の責任を持つことです。これには、各チームがそれぞれのロールを理解し、協力し、連携することが含まれます。
正解についての説明：
（選択肢）
・DevOpsチームの責任
- サービスインフラストラクチャを管理します
責任の共有
- コードレビューを実行します
- 交代でインシデントに対応します
- サービスのSLOを採用して公開しますソフトウェア開発チームの責任
- レビュー対象のコードを送信する
この選択肢が正解の理由は以下の通りです。
まず、DevOpsの責任はサービスインフラストラクチャの管理です。このロールは通常DevOpsの専門分野であり、インフラ管理における深い知識や経験が必要なため、DevOpsチームが負うべきです。
ソフトウェア開発チームの責任は、レビューされるコードの提出です。彼らの専門性とロール範囲はコードの作成にあるため、開発チームがコードを提出することは理にかなっています。
そして、コードレビューの実行、インシデントへの対応、SLOの採用と公開の責任は共有すべきです。コードレビューは両チームの共同作業であるべきで、開発者の視点と運用の視点の両方からコードの品質を評価するために重要です。同様に、サービスの可用性を維持するためのインシデント対応や、サービスレベル目標（SLO）の達成は、両チーム共通の責任として考えられます。これは、サービスの成功は開発と運用の両側からの労力と協力によって達成されます。
不正解についての説明：
選択肢：DevOpsチームの責任
- サービスインフラストラクチャを管理します
- コードレビューを実行しますソフトウェア開発チームの責任
- DevOpsチームによるレビューを受けるコードを送信します
- インシデンとに備えて待機します
- DevOpsチームが満たさなければならない  SLOを公開します
この選択肢が正しくない理由は以下の通りです。
新しい共同所有モデルでは、インシデント対応とSLOの公開は双方のチームで共有されるべきです。
さらに、開発チームがコードレビューを行うという制定はありません。これらの責任が片方のチームだけに偏っているため、この選択肢は不適切です。
選択肢：DevOpsチームの責任
- サービスインフラストラクチャを管理します
- インシデンとに備えて待機します
- コードレビューを実行しますソフトウェア開発チームの責任
- DevOpsチームによるレビューを受けるコードを送信します
- DevOpsチームが満たさなければならないSLOを公開します
この選択肢が正しくない理由は以下の通りです。
DevOpsの考え方は、開発と運用の活動を組織全体で連携させ、両者を一緒に考えることを推奨しています。不正解の選択肢ではDevOpsチームが待機し、ソフトウェア開発チームがSLOを単独で公開するという、この考え方に反するロール分担が提案されています。正解の選択肢では共同でコードレビューを実行し、交代でインシデントに対応し、共にSLOを採用して公開することで、両チームの協力と連携を重視しています。
選択肢：DevOpsチームの責任
- サービスインフラストラクチャを管理します
- コードレビューを実行します
責任の共有
- 交代でインシデントに対応します
- サービスのSLOを採用して公開しますソフトウェア開発チームの責任
- レビュー対象のコードを送信する
この選択肢が正しくない理由は以下の通りです。
ここではコードレビューの責任がDevOpsチームに割り当てられていますが、実際にはコードレビューはソフトウェア開発チームとDevOpsチームで共有すべき責任であり、それぞれがロールを持つべきです。DevOpsチームが単独でコードレビューを行うと、開発プロセスが分断され、効率性や品質が損なわれる可能性があります。
参考リンク：
https://cloud.google.com/solutions/devops/devops-tech-responsibility-sharing
https://cloud.google.com/architecture/devops/devops-measurement-culture
https://sre.google/sre-book/service-level-objectives/
</div></details>

### Q. 問題12: 未回答
アプリケーションのビルドとデプロイにCloud Buildを使用しています。データベースの認証情報やその他のアプリケーションの秘密を安全にビルドパイプラインに組み込みたいと考えています。また、開発工数を最小限に抑えたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. Cloud Storageバケットを作成し、ビルトインの暗号化を使用します。バケットにシークレットを保存し、Cloud Buildにバケットへのアクセスを許可します
2. クライアントサイド暗号化を使ってシークレットを暗号化し、Cloud Storageバケットに保存します。バケットに復号化キーを保存し、Cloud Buildにバケットへのアクセスを許可します
3. Cloud Key Management Service（Cloud KMS）を使用してシークレットを暗号化し、Cloud Buildの展開構成に含めます。Cloud BuildにKeyRingへのアクセス権を付与します
4. シークレットを暗号化し、アプリケーションリポジトリに保存します。復号化キーを別のリポジトリに保存し、Cloud Buildにリポジトリへのアクセス権を付与します
<details><div>
    答え：3
説明
この問題では、データベースの認証情報やアプリケーションの秘密を安全にビルドパイプラインに組み込む手段を求められています。その際、開発工数を最小限に抑える方法を選択する必要もあります。この要件を満たすためには、Google Cloudの中から適切なサービスを選び、それをどのように使用するべきか理解することが求められます。選択肢の中には、それぞれ異なるGoogle Cloudサービスや暗号化手法が含まれており、それぞれのサービスと手法が提供する機能や特性を理解し、要件に最もマッチしたものを選択することが求められます。
基本的な概念や原則：
Cloud Build：Google CloudのフルマネージドなCI/CDプラットフォームで、アプリケーションのビルドとデプロイを自動化します。
Cloud Key Management Service（Cloud KMS）：暗号化キーの作成、使用、管理、監査、破棄のライフサイクルを管理するGoogle Cloudのサービスです。
認証情報管理：重要な認証情報を安全に管理し、アプリケーションで使用します。誤った使用や悪用を防ぐためには適切なアクセス制御と保存場所が必要です。
Cloud Storageバケット：Google Cloud Storageはデータを保存するためのバケット（コンテナ）を提供します。しかし、この場合、認証情報を安全に管理するには適していません。
ビルドインの暗号化：データを保存する前に自動的に暗号化され、読み取る際に自動的に復号化されるCloud Storageの機能です。ただし、暗号化キーの管理はGoogleが行うため、ユーザーのコントロールは限定的です。
リポジトリでのシークレット管理：これはセキュリティリスクが高く、認証情報が公開される可能性があります。この方法は一般的には推奨されません。
クライアントサイド暗号化：ユーザーが自分のエンドポイントでデータを暗号化し、その後にCloud Storageに保存する暗号化方法です。これは管理負荷が高く、誤操作によるデータ漏洩のリスクがあります。
正解についての説明：
（選択肢）
・Cloud Key Management Service（Cloud KMS）を使用してシークレットを暗号化し、Cloud Buildの展開構成に含めます。Cloud BuildにKeyRingへのアクセス権を付与します
この選択肢が正解の理由は以下の通りです。
まず、Cloud Key Management ServiceはGoogle Cloudの機能であり、オブジェクトを暗号化し管理するサービスを提供しています。Mこの機能を使用することにより、データベースの認証情報やアプリケーションの秘密等といった機密情報を安全に管理することができます。
また、Cloud Buildを用いることで、Google Cloud内で完結したプロセスを形成できるため、開発工数を最小限に抑えられます。その上、Cloud KMSに対するアクセス権をCloud Buildに与えることで、ビルドパイプライン内でシークレット（秘密）を直接利用できます。
これらの特性により、Cloud KMSを使って秘密情報を暗号化し、Cloud BuildにKeyRingへのアクセス権を付与する選択肢は、安全性の確保と効率の向上を両立できる最適な選択となります。
不正解についての説明：
選択肢：Cloud Storageバケットを作成し、ビルトインの暗号化を使用します。バケットにシークレットを保存し、Cloud Buildにバケットへのアクセスを許可します
この選択肢が正しくない理由は以下の通りです。
Cloud Storageのビルトインの暗号化は、安全な秘密管理のための推奨手法ではありません。シークレット管理には、Cloud Key Management Service（Cloud KMS）のような専用の暗号化サービスが適しています。これにより、秘密のプロビジョニングとアクセス制御が容易になり、開発工数を最小限に抑えることができます。
選択肢：シークレットを暗号化し、アプリケーションリポジトリに保存します。復号化キーを別のリポジトリに保存し、Cloud Buildにリポジトリへのアクセス権を付与します
この選択肢が正しくない理由は以下の通りです。
シークレットや復号化キーをリポジトリに保存すると、不適切なアクセスが可能になり、セキュリティリスクを生じます。
一方、Cloud KMSは、セキュアにキーを管理し、アクセスを制御することができるため、よりセキュリティが強化され、開発工数も最小限に抑えられます。
選択肢：クライアントサイド暗号化を使ってシークレットを暗号化し、Cloud Storageバケットに保存します。バケットに復号化キーを保存し、Cloud Buildにバケットへのアクセスを許可します
この選択肢が正しくない理由は以下の通りです。
シークレットをCloud Storageに保存するという手法は、安全性に問題があります。復号化キーも同じバケットに保存すると、そのバケットへの不正アクセスがあった場合に、シークレットもキーも同時に漏洩する危険性が高まります。Cloud KMSを使用すると、より安全にキー管理が可能です。
参考リンク：
https://cloud.google.com/cloud-build/docs/securing-builds/use-encrypted-secrets-credentials
https://cloud.google.com/kms/docs/encrypt-decrypt
https://cloud.google.com/solutions/secrets-management
</div></details>

### Q. 問題13: 未回答
現在、仮想マシン（VM）の利用ログをGoogle Cloud Operation Suiteに保存しています。リアルタイムで更新され、四半期ごとに集計された情報を含む、共有しやすいインタラクティブなVM利用ダッシュボードを提供する必要があります。Google Cloudのソリューションを使用したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. 1.Google Cloud Operation SuiteからBigQueryにVM利用ログをエクスポートします
2.Data Studioでダッシュボードを作成します
3.ダッシュボードを関係者と共有します
2. 1.VM利用ログをGoogle Cloud Operation SuiteからCloud Pub/Subにエクスポートします
2.Cloud Pub/Subから、ログをセキュリティ情報およびイベント管理（SIEM）システムに送信します
3.SIEMシステムでダッシュボードを構築し、関係者と共有します
3. 1.VM利用ログをGoogle Cloud Operation SuiteからCloud Storageバケットにエクスポートします
2.Cloud Storage APIを有効にして、プログラムでログを引き出します
3.カスタムデータ可視化アプリケーションを構築します
4.取り出したログをカスタムダッシュボードに表示します
4. 1.Google Cloud Operation SuiteからBigQueryにVM利用ログをエクスポートします
2.BigQueryからログをCSVファイルにエクスポートします
3.CSVファイルをGoogle Sheetsにインポートします
4.Google Sheetsでダッシュボードを作成し、関係者と共有します
<details><div>
    答え：1
説明
この問題では、Google Cloud Operation Suiteから取得したVM利用ログをもとに、リアルタイムで更新されるインタラクティブなダッシュボードを作成し、それを共有する方法を選択することが求められています。その際に注意すべきは、四半期ごとに集計された情報を含めること、そしてGoogle Cloudのソリューションを用いるという制約です。これらの要件に基づき、Google Cloudの各サービスが提供する機能を理解し、その中で最適な組み合わせを見つけ出すことが重要です。また、ダッシュボードが共有しやすい形式であることも忘れてはなりません。
基本的な概念や原則：
Google Cloud Operation Suite：Google Cloudの運用・監視ツールのスイートです。ログ管理、監視、トレースなど、アプリケーションやインフラの運用をサポートします。
BigQuery：Google Cloudのフルマネージドなビッグデータ分析ツールです。大規模な読み込み、分析、エクスポートを高速に行うことができます。
Data Studio：Googleのビジュアルデータ分析ツールで、利用者が独自のレポートおよびダッシュボードを作成できます。
Cloud Pub/Sub：リアルタイムのメッセージングサービスです。高度にスケーラブルなイベントやメッセージの配信が可能です。
SIEMシステム：セキュリティ情報およびイベント管理システムのことです。セキュリティに関するログや情報を集約して分析、監視します。
CSVエクスポート：データをCSV形式でエクスポートする機能です。一般的にはスプレッドシートやデータベースに対応していますが、大規模データの管理には向いていません。
Cloud Storage：Google Cloudのオブジェクトストレージサービスです。データのバックアップやアーカイブに利用されますが、直接的なデータ分析には向いていません。
正解についての説明：
（選択肢）
・1.Google Cloud Operation SuiteからBigQueryにVM利用ログをエクスポートします
2.Data Studioでダッシュボードを作成します
3.ダッシュボードを関係者と共有します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Operation SuiteからBigQueryにVM利用ログをエクスポートします。Google Cloud Operation SuiteはGoogle Cloudの監視、ロギング、診断ツールを含む統合された監視ソリューションであり、ログデータをBigQueryにエクスポートすることで、大規模かつ複雑な分析を実行することが可能になります。このエクスポート操作は、Google Cloudの直感的なインターフェースとAPIを通じて簡単に設定できます。
次に、Data Studioでダッシュボードを作成します。Google Data Studioはビジュアルデータ分析ツールであり、BigQueryとシームレスに連携します。つまり、BigQueryにエクスポートされたVM利用ログは、Data Studioで直接アクセスできるので、独自のインタラクティブなダッシュボードをリアルタイムで作成し、四半期ごとの集計結果などを表示することが可能です。
最後に、ダッシュボードを関係者と共有します。Google Data Studioのダッシュボードはウェブベースであり、URLを共有することで特定のユーザーやグループにコンテンツを簡単に公開できます。これにより、四半期ごとのVM使用状況の透明性を提供し、他のステークホルダーと共有することが可能になり、本筋の要件を満たす解決策となります。
不正解についての説明：
選択肢：1.VM利用ログをGoogle Cloud Operation SuiteからCloud Pub/Subにエクスポートします
2.Cloud Pub/Subから、ログをセキュリティ情報およびイベント管理（SIEM）システムに送信します
3.SIEMシステムでダッシュボードを構築し、関係者と共有します
この選択肢が正しくない理由は以下の通りです。
まず、リアルタイムでの更新と四半期集計が必要とされているため、Cloud Pub/Subのリアルタイム性は適切だと思われますが、四半期ごとの集計には無理があります。
また、ユーザーがGoogle Cloudのソリューションを使いたいと明示しているため、SIEMシステムを使用することはその要求に矛盾します。
選択肢：1.Google Cloud Operation SuiteからBigQueryにVM利用ログをエクスポートします
2.BigQueryからログをCSVファイルにエクスポートします
3.CSVファイルをGoogle Sheetsにインポートします
4.Google Sheetsでダッシュボードを作成し、関係者と共有します
この選択肢が正しくない理由は以下の通りです。
Google Sheetsでダッシュボードを作成すると、リアルタイムでの更新が困難になります。
また、CSVファイルを介したエクスポートインポートの過程は手間がかかり、大規模な集計が必要な場合にはBigQueryとData Studioの採用が望ましいです。
選択肢：1.VM利用ログをGoogle Cloud Operation SuiteからCloud Storageバケットにエクスポートします
2.Cloud Storage APIを有効にして、プログラムでログを引き出します
3.カスタムデータ可視化アプリケーションを構築します
4.取り出したログをカスタムダッシュボードに表示します
この選択肢が正しくない理由は以下の通りです。
まず、Cloud Storageを介して別個にデータ可視化アプリを構築する過程は、時間とコストがかかり、効率性が低いです。
一方、正解の選択肢では既存のBigQueryとData Studioを使用してデータの分析と可視化を行うため、手間や時間が大幅に節約できます。
参考リンク：
https://cloud.google.com/bigquery/docs/exporting-data
https://cloud.google.com/logging/docs/export
https://datastudio.google.com/
</div></details>

### Q. 問題14: 未回答
あなたは大容量のエンタープライズアプリケーションの信頼性を担当しています。多くのユーザから、アプリケーションの機能の重要なサブセットであるデータ集約的なレポート機能が、HTTP 500エラーで常に失敗しているという報告がありました。アプリケーションのダッシュボードを調査したところ、失敗とレポート生成に使用される内部キューのサイズを表す指標との間に強い相関関係があることに気づきました。あなたは、I/O待ち時間が長くなっているレポートバックエンドに障害が発生していることを突き止めました。バックエンドの永続ディスク（PD）のサイズを変更することで、問題を迅速に解決します。レポート生成機能の可用性サービスレベルインジケータ（SLI）を作成する必要があります。
あなたは、このSLIどのように定義しますか？
1. アプリケーションのレポート生成キューのサイズと既知の正常な閾値との比較
2. レポート作成リクエストのうち、成功したレスポンスにつながる割合
3. 既知の良好な閾値と比較した容量全体のレポーティングバックエンドPD
4. すべてのレポート生成バックエンドに集約されたI/O待ち時間
<details><div>
    答え：2
説明
この問題では、複数のユーザからのエラー報告を元に、エンタープライズアプリケーションの可用性を保証するためのサービスレベルインジケータ（SLI）の定義方法を求められています。問題を解くためには、多くのユーザが遭遇している問題（HTTP 500エラー）とその原因（I/O待ち時間）を理解し、これらを改善するためのSLIを選択することが重要です。また、エラーレポート生成が常に失敗しているという事実から、成功したレポートの割合に注目しなければなりません。これを念頭に置いて、適切なSLIを定義する選択肢を探すべきです。
基本的な概念や原則：
サービスレベルインジケータ（SLI）：システムのパフォーマンスや可用性を定量的に評価する指標です。特定の時間範囲で成功したリクエストの割合などが一般的なSLIです。
HTTP 500エラー：サーバー側で問題が発生したことを示すHTTPステータスコードです。これが頻発すると、システムの信頼性に問題があることを示します。
I/O待ち時間：データを読み書きする際に生じる遅延のことです。長いI/O待ち時間は、ディスクの容量不足や性能不足などが原因である可能性があります。
永続ディスク（PD）：Google Cloud Compute Engineで提供されるブロックストレージです。アプリケーションが必要とする容量を柔軟に確保・拡張することが可能です。
内部キュー：システム内部で非同期的にタスクを処理するためのデータ構造です。キューの長さが長くなってしまうと、システムパフォーマンスに影響を及ぼす可能性があります。
正解についての説明：
（選択肢）
・レポート作成リクエストのうち、成功したレスポンスにつながる割合
この選択肢が正解の理由は以下の通りです。
まず、SLIとはService Level Indicatorの略で、サービスの特定のアスペクトのパフォーマンスを測定するものです。このケースでは、アプリケーションの主要な部分であるレポート生成機能のパフォーマンスをモニタリングする必要があります。そのため、"レポート作成リクエストのうち、成功したレスポンスにつながる割合"という指標を設定することで、この部分のパフォーマンスを客観的に測定することが可能になります。
また、問題の背景情報からユーザーが直面している問題はHTTP 500エラーによりレポートの作成が可能でないというものでした。
したがって、成功したレポート作成リクエストの割合をSLIとして設定することで、その問題を解決し、サービスのパフォーマンスと可用性を向上させることができます。
さらに、SLIが具体的に"成功したレスポンスの割合"を測定するという形にすることで、これが改善すれば直接的にユーザー体験が改善する、という明確な目標が設定されることになります。これは、パフォーマンスを定量的に改善し、問題解決に焦点を絞る手助けとなります。
不正解についての説明：
選択肢：すべてのレポート生成バックエンドに集約されたI/O待ち時間
この選択肢が正しくない理由は以下の通りです。
サービスレベルインジケータ（SLI）はサービスの品質を定義するためのものであり、ユーザの観点から計測すべきものです。レポート生成バックエンドのI/O待ち時間は内部のメトリックであり、直接的にユーザ体験やサービス品質を反映しているわけではありません。
したがって、成功したレポート作成リクエストの割合を計測する方が、ユーザ体験をより正確に反映します。
選択肢：アプリケーションのレポート生成キューのサイズと既知の正常な閾値との比較
この選択肢が正しくない理由は以下の通りです。
レポート生成キューのサイズと正常な閾値との比較は、障害の一部の兆候を反映するかもしれませんが、それが全ての場合に該当するわけではなく、またユーザーの視点からの可用性（成功したレスポンスの割合）を直接反映するものでもないため、SLIとしては相応しくありません。
選択肢：既知の良好な閾値と比較した容量全体のレポーティングバックエンドPD
この選択肢が正しくない理由は以下の通りです。
SLIはユーザの視点からサービスの可用性を定義するためのものです。しかし、この選択肢はバックエンドのPDの容量を見るもので、これはユーザにとって直接的な影響を持つ指標ではありません。ユーザが実際に経験するサービスの可用性、つまりレポート作成リクエストの成功率を見る方がより適切なSLIとなります。
参考リンク：
https://cloud.google.com/blog/products/Google Cloud/sre-fundamentals-slis-slas-and-slos
https://cloud.google.com/architecture/framework/reliability/define-measure-service-level-indicators
https://cloud.google.com/monitoring/api/v3/sli-design
</div></details>

### Q. 問題15: 未回答
あなたはTerraformを使ってCI/CDパイプライン内でインフラストラクチャをコードとして管理しています。Google Cloudプロジェクトにインフラスタック全体の複数のコピーが存在し、既存のインフラに変更が加えられるたびに新しいコピーが作成されていることに気づきました。インフラストラクチャスタックのインスタンスが一度に1つしか存在しないようにして、クラウド使用量を最適化する必要があります。あなたは、Googleが推奨するプラクティスに従う必要があります。
この要件を満たすために、どうすればよいですか？
1. 最新のコンフィギュレーションを適用する前に、パイプラインを更新し、既存のインフラを削除します
2. 不要になった古いインフラスタックを削除する新しいパイプラインを作成します
3. パイプラインがterraform.tfstateファイルをソース管理から保存・取得していることを確認します
4. パイプラインがTerraform gcsバックエンドでCloud Storageからterraform.tfstateファイルを保存・取得していることを確認します
<details><div>
    答え：4
説明
この問題では、Terraformを使用したCI/CDパイプラインにおける、インフラストラクチャスタックの管理方法が問われています。重要なのはインフラストラクチャスタックの同時存在するコピー数が1つであることと、Googleのベストプラクティスに従うことで、これらに注目して問題を解読する必要があります。また、tfstateファイルの保存方法やインフラスタックの削除タイミングについての選択肢も出てきます。これらは全て、インフラストラクチャコードの管理と最適化に関する理解とGoogleプラクティスへの理解を必要とします。最適な選択肢を選択するために、TerraformとGoogle Cloudの慣習について理解することが必要となります。
基本的な概念や原則：
Terraform：インフラストラクチャーをコードとして管理するためのオープンソースツールです。Google Cloudなどのクラウドプロバイダに対応しています。
terraform.tfstateファイル：Terraformが管理するインフラストラクチャの現在の状態を記録するファイルです。このファイルを通じて、Terraformは変更を適切に適用することができます。
GCSバックエンド：Terraformの状態を管理するためのGoogle Cloud Storage（GCS）ベースのバックエンドです。複数のTerraformワークスペースに対応しており、リモート状態の保存と管理を可能にします。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）を組み合わせた開発プロセスです。コードの変更を自動的にビルド、テスト、デプロイすることで、高速かつ安定したソフトウェアリリースを支援します。
ソースコード管理：ソフトウェア開発での変更を追跡し、バージョンを管理する方法です。不適切な利用は、状態管理の問題を引き起こす可能性があります。
インフラストラクチャスタック：特定のアプリケーションまたはサービスをHostするために一緒に構成されたインフラストラクチャリソースの集合です。
Cloud Storage：適用幅の広いオブジェクトストレージソリューションで、データの保存と取得に利用されます。エンタープライズレベルのセキュリティとパフォーマンスを備え、世界中からのデータアクセスを迅速に行えます。
正解についての説明：
（選択肢）
・パイプラインがTerraform gcsバックエンドでCloud Storageからterraform.tfstateファイルを保存・取得していることを確認します
この選択肢が正解の理由は以下の通りです。
まず、Terraformはインフラストラクチャをコードとして管理するためのオープンソースのツールであり、そのステート（terraform.tfstate）とは、Terraformが管理しているリソースの現在の情報を保持しているファイルのことを指します。このステートファイルをCloud Storageに保存することで、CI/CDパイプラインを通じてインフラストラクチャの変更が行われるたびにステートが一貫性を保つことが可能となります。これにより、重複したインフラのコピーが作成されることを防ぎます。
また、Terraform gcsバックエンドは、Terraformのステートを管理して、Cloud Storageのバケットに保存するためのモジュールであり、これを使用することで、Terraformのステート管理がより堅牢かつ信頼性のあるものとなります。
さらに、Terraform gcsバックエンドによるステート管理はGoogleが推奨するプラクティスであり、より効率的なクラウドリソースの利用が可能となります。
以上のように、パイプラインがTerraform gcsバックエンドでCloud Storageからterraform.tfstateファイルを保存・取得していることを確認すれば、この要件は満たせます。
不正解についての説明：
選択肢：不要になった古いインフラスタックを削除する新しいパイプラインを作成します
この選択肢が正しくない理由は以下の通りです。
古いインフラスタックを削除する新しいパイプラインを作成するという解決策はコストの最適化にはつながりますが、これは問題の根本解決にはなりません。
また、一度に1つのインフラストラクチャスタックしか存在しない状態を保つには、Terraformの状態管理であるterraform.tfstateファイルをCloud Storageに保存・取得することが最適となります。
選択肢：パイプラインがterraform.tfstateファイルをソース管理から保存・取得していることを確認します
この選択肢が正しくない理由は以下の通りです。
terraform.tfstateファイルをソース管理から保存・取得すると、状態の整合性が保たれず、互いに異なるCI/CD実行が競合する可能性があります。
正解の選択肢のように、GCSバックエンドを使用すると、Terraformの状態管理が正確に行われ、予期せぬインフラストラクチャのコピーを防ぐことができます。
選択肢：最新のコンフィギュレーションを適用する前に、パイプラインを更新し、既存のインフラを削除します
この選択肢が正しくない理由は以下の通りです。
既存のインフラを削除すると、新しいコンフィギュレーションを適用する際に全てのリソースを再度作成しなければならず、非効率的です。
また、必要なリソースが欠如し、サービスダウンタイムを引き起こす可能性があります。正解はTerraformの状態を追跡することで、既存のリソースを再利用し、効率的に適用を行うことです。
参考リンク：
https://cloud.google.com/storage/docs/using-object-versioning
https://cloud.google.com/community/tutorials/managing-infrastructure-as-code
https://www.terraform.io/docs/language/settings/backends/gcs.html
</div></details>

### Q. 問題16: 未回答
あなたは、仮想マシン（VM）のコストを削減する必要があります。さまざまなオプションを検討した結果、プリエンプティブVMインスタンスを活用することにしました。
どのアプリケーションがプリエンプティブVMに適していますか？
1. 十分なクォーラムを持つ、分散した最終的に一貫性のあるNoSQLデータベースクラスター
2. 組織の一般向けウェブサイト
3. ビデオを取得してストレージバケットに保存する、GPU高速化ビデオレンダリングプラットフォーム
4. スケーラブルなインメモリキャッシングシステム
<details><div>
    答え：3
説明
この問題では、プリエンプティブVMの特性を理解し、それがどのようなアプリケーションに適しているかを判断する能力が問われています。プリエンプティブVMは、利用時間を保証できないため、任意のタイミングで中断されても問題のないタスクに適しています。一方で、常にストレージやインメモリキャッシング、ウェブサイトの稼働状態を保つ必要があるシステムや、一貫性を確保する必要があるデータベースクラスターには不適合です。
基本的な概念や原則：
プリエンプティブVMインスタンス：Google Cloudの仮想マシンの一種で、通常よりも大幅にコスト削減可能ですが、最大で24時間しか保証されない、またはシステムによりリソースが必要な場合にはいつでも終了される可能性があるため、割り込みを許容できるワークロードに対して適しています。
GPU高速化ビデオレンダリングプラットフォーム：高い計算能力を必要とするが、終了時間が不定または割り込みを許容可能なタスクに対して適しています。ジョブが中断されても再開または再処理が可能です。
インメモリキャッシングシステム：データがメモリに直接保存され、高速なデータアクセスが必要なアプリケーションに使用されるシステムです。データの保存状態が重要であり、プリエンプティブVMインスタンスの割り込み許容性とは相性が悪いです。
一般向けウェブサイト：24時間365日稼働していることが求められるため、プリエンプティブVMインスタンスの割り込み許容性とは相性が悪いです。
NoSQLデータベースクラスター：高い可用性が求められるシステムで、データの一貫性が重要です。プリエンプティブVMインスタンスの割り込み許容性とは相性が悪いです。
正解についての説明：
（選択肢）
・ビデオを取得してストレージバケットに保存する、GPU高速化ビデオレンダリングプラットフォーム
この選択肢が正解の理由は以下の通りです。
プリエンプティブVMは、Google CloudのCPUの使用率が低いときに提供される一時的なVMインスタンスです。24時間で最長でも割り当てが解除される可能性があります。
したがって、一時的なブレークが許容でき、長時間稼働する必要がないタスクに対してコスト効率的です。
GPU高速化ビデオレンダリングプラットフォームは、ビデオのレンダリングにとても時間がかかる作業であり、それぞれのタスクが一部分を担当するように分割できます。これは通常はバッチ処理と呼ばれ、個々のタスクが互いに依存することなく完了する作業に適しています。
したがって、何らかの理由で中断すると再開するためには初めからやり直す必要があるようなタスクには不適合でありますが、このようなレンダリングタスクのように、個々の結果がストレージに保存されている場合はプリエンプティブVMを利用することでコストを抑えることができます。
不正解についての説明：
選択肢：スケーラブルなインメモリキャッシングシステム
この選択肢が正しくない理由は以下の通りです。
インメモリキャッシングシステムは一貫したパフォーマンスと信頼性を必要とします。しかし、プリエンプティブVMはコスト削減のため、Googleの需要によっていつでも停止される可能性があるため、一貫性や信頼性を確保するのが難しいです。
選択肢：組織の一般向けウェブサイト
この選択肢が正しくない理由は以下の通りです。
組織の一般向けウェブサイトは24時間稼働し、常時利用可能である必要があります。しかし、プリエンプティブVMは最大24時間で停止する可能性があるため、常時利用が必要なウェブサイトには適していません。
それに対して、ビデオレンダリングのような断続可能な作業にはプリエンプティブVMが適しています。
選択肢：十分なクォーラムを持つ、分散した最終的に一貫性のあるNoSQLデータベースクラスター
この選択肢が正しくない理由は以下の通りです。
プリエンプティブVMは最大で24時間しか稼働せず、突如停止することがあります。分散した最終的一貫性のあるNoSQLデータベースクラスターでは、クラスターに参加しているノードのダウンタイムが最小限であることが求められ、プリエンプティブVMが突然停止する可能性はこの要件に適しません。他方、ビデオレンダリングのような短期間で終了するジョブは、プリエンプティブVMが突如停止するリスクを許容できます。
参考リンク：
https://cloud.google.com/compute/docs/instances/preemptible
https://cloud.google.com/compute/docs/machine-types#gpu
https://cloud.google.com/storage/docs/json_api
</div></details>

### Q. 問題17: 未回答
あなたの会社は高度に規制された領域で事業を展開しています。セキュリティチームは、信頼できるコンテナイメージのみをGoogle Kubernetes Engine（GKE）にデプロイできることを要求しています。管理オーバーヘッドを最小限に抑えながら、セキュリティチームの要件を満たすソリューションを実装する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. Cloud Buildサービスのアカウントにroles/artifactregistry.writerロールを付与します。Artifactレジストリの書き込み権限を持っている従業員がいないことを確認します
2. GKEクラスターでバイナリ認証を構成して、デプロイ時のセキュリティポリシーを適用します
3. Cloud Runを使用してカスタムバリデータを作成し、デプロイします。新しい画像がアップロードされたときにバリデーションを実行するEventarcトリガーを有効にします
4. デプロイ時のセキュリティポリシーを実施するために、GKEクラスターで実行するKritisを設定します
<details><div>
    答え：2
説明
この問題では、規制の厳しい業界で事業を展開する会社の立場から、Google Kubernetes Engine（GKE）を安全に運用するためのソリューションをどのように設計するかが問われています。セキュリティチームから寄せられた要件を満たすためには、信頼できるコンテナイメージのみをGKEにデプロイできるような設定が必須であり、これにより不適切なデプロイを防止します。また、問題からは管理オーバーヘッドを最小限に抑える要求も明らかにされています。これらの要点を踏まえて、適切なソリューションを選択します。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Dockerコンテナとコンテナ化されたアプリケーションのデプロイ、管理、スケーリングを行うGoogle Cloudのサービスです。
バイナリ認証：Google Cloudのサービスで、GKEクラスターへのデプロイを制御し、信頼できるソースからビルドされたイメージのみがデプロイされるようにします。
Cloud Build：Google CloudのCI/CDプラットフォームで、ソースコードからコンテナイメージをビルドし、テストを行い、Google Cloud上など様々な環境にデプロイすることができます。
Artifact Registry：DockerとOCIのコンテナイメージや言語パッケージなど、アーティファクトのパッケージ管理およびデプロイメントを行うGoogle Cloudのサービスです。
Cloud Run：コンテナ化されたアプリケーションをフルマネージドで実行するサービスです。自動スケーリングと組み合わせることで、リクエストに応じてリソースを最適化します。
Eventarc：Google Cloudのイベントルーティングサービスで、クラウドサービス間または自社サービス間でのイベントの配送を管理します。
Kritis：Google Cloudで提供されている、Kubernetesのアプリケーションへのパイプラインデプロイを制御するためのソリューションです。これにより、セキュリティやコンプライアンスポリシーに基づいてデプロイメントを承認または拒否することが可能です。
正解についての説明：
（選択肢）
・GKEクラスターでバイナリ認証を構成して、デプロイ時のセキュリティポリシーを適用します
この選択肢が正解の理由は以下の通りです。
セキュリティチームが要求しているのは信頼できるコンテナイメージのみをデプロイできること、つまりコンテナイメージの整合性や信頼性の保証です。
それに対して、バイナリ認証はGoogle Cloudのサービスで、デプロイされるイメージが信頼できるものであることを保証するための技術です。信頼できる基準（例えば特定のビルドプロセスを経たものなど）を満たすイメージには署名が付けられ、この署名がチェックされます。これにより、安全でないと判断されたイメージをデプロイするのを防ぐことができます。
したがって、バイナリ認証を用いることで、セキュリティチームの要件を満たしつつ管理オーバーヘッドを最小限に抑えることが可能となります。よって、GKEクラスターでバイナリ認証を構成して、デプロイ時のセキュリティポリシーを適用するのが適切な解答です。
不正解についての説明：
選択肢：Cloud Buildサービスのアカウントにroles/artifactregistry.writerロールを付与します。Artifactレジストリの書き込み権限を持っている従業員がいないことを確認します
この選択肢が正しくない理由は以下の通りです。
roles/artifactregistry.writerロールを付与することと、特定の従業員に書き込み権限を与えないことは、セキュリティ対策の一部となりますが、これだけでは信頼できるコンテナイメージのみがGKEにデプロイされることを保証する足りません。
それに対して、バイナリ認証を構成することで、デプロイ時に特定のセキュリティポリシーが適用され、信頼性の確認が可能となります。
選択肢：Cloud Runを使用してカスタムバリデータを作成し、デプロイします。新しい画像がアップロードされたときにバリデーションを実行するEventarcトリガーを有効にします
この選択肢が正しくない理由は以下の通りです。
Cloud RunとEventarcトリガーを使用したバリデーションプロセスはセキュリティ要件を満たしますが、管理オーバーヘッドを増加させます。正解のバイナリ認証はセキュリティポリシーを適用し、信頼できるコンテナイメージのみがデプロイされます。
また、この方法は管理オーバーヘッドも最小限に抑えます。
選択肢：デプロイ時のセキュリティポリシーを実施するために、GKEクラスターで実行するKritisを設定します
この選択肢が正しくない理由は以下の通りです。
もしKritisを設定する途中で問題が生じた場合、解決するための過度な管理オーバーヘッドが発生します。
一方、GKEクラスターでバイナリ認証を構成することは、デプロイ時のセキュリティポリシーを適用することができ、管理オーバーヘッドを最小限に抑えることが可能です。
参考リンク：
https://cloud.google.com/binary-authorization/docs
https://cloud.google.com/kubernetes-engine/docs/how-to/binary-authorization
https://kritis.dev/docs/gke-deployments/
</div></details>

### Q. 問題18: 未回答
あなたは、Google Kubernetes Engine（GKE）上で動作するNode.jsアプリケーションを本番環境でサポートしています。このアプリケーションは、依存するアプリケーションにいくつかのHTTPリクエストを行います。あなたは、どの依存アプリケーションがパフォーマンスの問題を引き起こす可能性があるかを予測したいと考えています。
この要件を満たすために、どうすればよいですか？
1. Node.jsアプリケーションを修正して、依存アプリケーションへのHTTPリクエストとレスポンスタイムをログに記録します。Cloud Loggingを使用して、パフォーマンスが低下している依存アプリケーションを見つけます
2. Google Cloud Operation Suite Debuggerを使用して、各アプリケーション内のロジックの実行をレビューし、すべてのアプリケーションをインストゥルメンテーションします
3. すべてのアプリケーションをGoogle Cloud Operation Suite Traceで計測し、サービス間のHTTPリクエストをレビューします
4. すべてのアプリケーションをGoogle Cloud Operation Suite Profilerで計測します
<details><div>
    答え：3
説明
この問題では、Google Kubernetes Engine（GKE）上で動作するNode.jsアプリケーションに関連するパフォーマンス問題を特定する方法を求めています。また、問題が発生する可能性のある依存アプリケーションを事前に予測したいという要件があります。このため、HTTPリクエストのパフォーマンスを監視し、レビューするための適切なツールを選択する必要があります。このシナリオでは、Google Cloud Operation Suiteの各機能（Trace、Profiler、Debugger、Logging）の特性を理解し、それぞれの適切な使用場面を判断することが重要です。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するマネージドKubernetesサービスです。アプリケーションのデプロイ、スケーリング、また運用を簡単に行うことができます。
Node.jsアプリケーション：非同期イベント駆動のJavaScript環境が必要なアプリケーションの一つです。サーバー側のアプリケーション開発によく使用されます。
Google Cloud Operation Suite Trace：アプリケーションのパフォーマンス問題を捉えるための分析ツールです。HTTPリクエストとレスポンスの遅延を追跡し、アプリケーションのボトルネックを特定するのに役立ちます。
Google Cloud Operation Suite Profiler：Google Cloudのパフォーマンス最適化ツールです。CPU使用率やメモリの使用状況など、アプリケーションの実行時の詳細を提供し、パフォーマンス向上のための洞察を与えます。
Google Cloud Operation Suite Debugger：Google Cloudのツールで、アプリケーションのデバッグ作業を行うのに役立ちます。アプリケーションの実行状況を確認し、問題の特定や修正を行います。
Cloud Logging：Google Cloudのログ管理ツールです。アプリケーションやシステムのログデータを一元的に収集、保存、分析、監視することができます。
正解についての説明：
（選択肢）
・すべてのアプリケーションをGoogle Cloud Operation Suite Traceで計測し、サービス間のHTTPリクエストをレビューします
この選択肢が正解の理由は以下の通りです。
Google Cloud Operation Suite Traceは、アプリケーションに行われるすべてのリクエストのレイテンシデータを収集、分析し、可視化します。これにより、どのサービスがボトルネックになっているかを特定しやすくなります。この具体的な要件では、それぞれの依存アプリケーションに対するHTTPリクエストが特定のパフォーマンス問題を引き起こす可能性があるかどうかを判断するためのデータを提供するため、Operations Suite Traceは理想的なツールです。
また、Google Kubernetes Engineで動作するアプリケーションをサポートするため、Operations Suite TraceはGKEとのネイティブな連携が可能であり、ログ、イベント、メトリクスを容易に取り込むことができます。そのため、この選択肢は要件を適切に満たします。
不正解についての説明：
選択肢：すべてのアプリケーションをGoogle Cloud Operation Suite Profilerで計測します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Profilerは、アプリケーションのランタイムパフォーマンスを最適化するツールで、特定の依存関係がパフォーマンス問題を引き起こしているか確認する機能はありません。
対照的に、Cloud Operation Suite Traceを使用すると、サービス間のHTTPリクエストを追跡し、特定の依存関係がパフォーマンス問題を引き起こしているかどうかを確認できます。
選択肢：Google Cloud Operation Suite Debuggerを使用して、各アプリケーション内のロジックの実行をレビューし、すべてのアプリケーションをインストゥルメンテーションします
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Debuggerはアプリケーションのロジック実行を確認するツールで、HTTPリクエストに関するパフォーマンスの情報を提供しません。
対照的に、Cloud Operation Suite Traceはサービス間のHTTPリクエストを計測・分析し、パフォーマンスの問題を特定する能力を持っています。
選択肢：Node.jsアプリケーションを修正して、依存アプリケーションへのHTTPリクエストとレスポンスタイムをログに記録します。Cloud Loggingを使用して、パフォーマンスが低下している依存アプリケーションを見つけます
この選択肢が正しくない理由は以下の通りです。
アプリケーションの修正は時間とリソースを必要とし、リクエストとレスポンスタイムの追跡を手動で行う必要が生じます。
それに対して、Google Cloud Operation Suite Traceは自動的にHTTPリクエストを計測するので、監視と問題の特定がより効率的です。
参考リンク：
https://cloud.google.com/trace/docs
https://cloud.google.com/logging/docs
https://kubernetes.io/docs/tasks/debug/debug-application/debug-application-introspection/
</div></details>

### Q. 問題19: 未回答
あなたのチームはGoogle Kubernetes Engine（GKE）でマイクロサービスを運用しています。あなたは、顧客を保護し、リリースポリシーを定義するために、エラーバジェットの消費を検出したいと考えています。
この要件を満たすために、どうすればよいですか？
1. マイクロサービスの健全性を測定するために、Anthos Service Meshのメトリクスを使用します
2. SLOを作成します。select_slo_burn_rateにアラートポリシーを作成します
3. メトリクスからSLIを作成します。サービスがパスしない場合は、アラートポリシーを有効にします
4. SLOを作成し、サービスの稼働時間チェックを構成します。サービスが合格しない場合は、アラートポリシーを有効にします
<details><div>
    答え：4
説明
この問題では、Google Kubernetes Engineで運用中のマイクロサービスのエラーバジェットの消費をどのように検出するかが問われています。そのためには、マイクロサービスの運用状況を定量的に把握し、それを基にエラーの発生状況を確認する必要があります。正解と不正解の選択肢を見ると、SLOを作成すること、アラートポリシーを有効にすることなどが含まれています。この問題を解く上で重要なのは、問題の要件を満たすための具体的な方法論を理解することで、それに基づいて選択肢を評価することです。
基本的な概念や原則：
SLO（Service Level Objective）：サービスレベルの目標で、特定のサービスが達成すべき目標を定量的に示したものです。運用品質の管理や、エラーバジェットの消費具体的な指標となります。
稼働時間チェック：サービスの可用性を確認するためのモニタリング機能です。サービスが適切に稼働しているか、定期的にチェックして検知します。
アラートポリシー：特定の状況や条件下で通知を送るためのポリシーです。問題や異常を検出した場合に、対応するためのアクションを定義します。
SLI（Service Level Indicator）：サービスレベルの指標で、特定のサービスがどの程度パフォーマンスを発揮しているかを示すメトリックです。SLO達成のための具体的な評価基準となります。
Anthos Service Mesh：マイクロサービス間の通信を効率的に管理し、監視するGoogle Cloudのサービスです。サービス間のトラフィックフローを視覚化し、制御を可能にします。
正解についての説明：
（選択肢）
・SLOを作成し、サービスの稼働時間チェックを構成します。サービスが合格しない場合は、アラートポリシーを有効にします
この選択肢が正解の理由は以下の通りです。
公認のエラーバジェット管理方法であるSLO（Service Level Objectives）を作成することにより、サービスのパフォーマンスを数値指標で把握し、管理することが可能になります。これがエラーバジェットの消費を検出する手段となります。具体的には、SLOはサービスの信頼性とパフォーマンスを評価するための目標を定義し、これらの目標が達成されないときにリスクが通知され、必要なアクションを取るためのアラートをトリガーします。
また、選択肢中の"サービスの稼働時間チェックを構成します"は、サービスが動作している時間を監視し、許容値を超えた場合にエラーを検出するための方法を示しています。これは、サービス実行の異常を早期にキャッチできる有効な手段です。
最後に、"サービスが合格しない場合は、アラートポリシーを有効にします"は、SLOや稼働時間チェックで定義した基準を満たさなかった場合に、アラートを鳴らすための防御メカニズムを提供します。これにより問題を速やかに検出し、必要な対策を講じることができます。
不正解についての説明：
選択肢：メトリクスからSLIを作成します。サービスがパスしない場合は、アラートポリシーを有効にします
この選択肢が正しくない理由は以下の通りです。
メトリクスからSLIを作成すること自体は誤りではありませんが、エラーバジェットの消費を検出するためにはSLO（Service Level Objective）が必要になります。SLOはSLI（Service Level Indicators）に基づいた目標を設定し、それに対する達成率をエラーバジェットとして管理します。
したがって、エラーバジェットの消費を検出するためには、SLOの設定が必要になります。
選択肢：マイクロサービスの健全性を測定するために、Anthos Service Meshのメトリクスを使用します
この選択肢が正しくない理由は以下の通りです。
Anthos Service Meshのメトリクスはマイクロサービスの健全性を測定するのに役立ちますが、エラーバジェットの消費を検出する直接的な方法ではありません。
一方、SLOとサービス稼働時間のチェックを構成してアラートポリシーを有効にすることで、エラーバジェットの消費を検出し、それに応答できます。
選択肢：SLOを作成します。select_slo_burn_rateにアラートポリシーを作成します
この選択肢が正しくない理由は以下の通りです。
"select_slo_burn_rate"は標準のGoogle Cloud Monitoringメトリックであり、直接エラーバジェットの消費を検出するために使用することはできません。
一方、稼働時間チェックを構成し、その結果に基づいてアラートポリシーを作成することで、サービスの稼働状況を直接的に把握し、必要に応じて対応することが可能です。
参考リンク：
https://cloud.google.com/monitoring/alerts
https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
</div></details>

### Q. 問題20: 未回答
Google Cloud上でのアプリケーションのデプロイ手法を設計しています。デプロイ計画の一環として、ライブトラフィックを使用してアプリケーションの新バージョンのパフォーマンス指標を収集したいと考えています。アプリケーションを起動する前に、本番環境での全負荷に対してテストを行う必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. 継続的デプロイメントでカナリアテストを使用します
2. ローリングアップデートデプロイでカナリアテストを使用します
3. 継続的デプロイメントでシャドーテストを使用します
4. ブルー/グリーンデプロイでA/Bテストを使用します
<details><div>
    答え：3
説明
この問題では、アプリケーションの新バージョンのパフォーマンス指標を収集するためのデプロイ手法を選ぶことが求められています。問題文からは、本番環境で全負荷に対するテストと、ライブトラフィックを用いたテストの収集が求められていることがわかります。そのため、選択肢を選ぶ時には、これらの要件を満たすようなデプロイ手法を選択することが必要です。
基本的な概念や原則：
シャドーテスト：ライブトラフィックを新しいサービスバージョンにコピー（シャドー）し、新しいバージョンがどのように動作するかをテストします。これにより、実際のユーザーは影響を受けず、新しいバージョンに対する実際の負荷の影響をテストできます。
継続的デプロイメント： CI/CDパイプラインの一部で、ソースコードの変更が自動的にテストおよびデプロイされるプロセスです。これにより、新しい修正や機能を迅速かつ頻繁にリリースすることが可能になります。
ブルー/グリーンデプロイ：新しい（グリーン）バージョンのアプリケーションをデプロイし、すべてが問題なく動作することを確認した後でトラフィックを元の（ブルー）バージョンから移行するデプロイ手法です。
A/Bテスト：ユーザーに対して異なるバージョンのアプリケーションを提供し、パフォーマンスを比較する方式です。通常、ユーザーエクスペリエンスの最適化や新機能の有効性を検証するために使用します。
カナリアテスト：新しいバージョンのアプリケーションに対して少量のトラフィックを送り、問題がないことを確認した後で全体にロールアウトするテスト方式です。
ローリングアップデート：新しいバージョンのアプリケーションを段階的に展開していき、徐々にトラフィックを移行する展開手法です。
正解についての説明：
（選択肢）
・継続的デプロイメントでシャドーテストを使用します
この選択肢が正解の理由は以下の通りです。
まず、シャドーテストは、本番環境での新バージョンのアプリケーションのパフォーマンスを評価するための最良の方法の一つです。シャドーテストを行うと、本番環境での実際のトラフィックに基づいてアプリケーションをテストすることができます。これにより、問題が発生した場合でもユーザーに影響を与えずに、新バージョンのアプリケーションのパフォーマンスを確認できます。
また、継続的デプロイメント（Continuous Deployment）は、ソフトウェアのデプロイを自動化し、新しいバージョンを必要な時点で迅速にリリースするための手法です。このアプローチをとることで、テストやリリースプロセスを効率化でき、アプリケーションの品質を保ちつつ、スピーディなデプロイが可能になります。
したがって、シャドーテストと継続的デプロイメントを組み合わせることで、新バージョンのパフォーマンス指標を収集しながら、本番環境での全負荷に対してテストを行うという要件を満たすことができます。
不正解についての説明：
選択肢：ブルー/グリーンデプロイでA/Bテストを使用します
この選択肢が正しくない理由は以下の通りです。
ブルー/グリーンデプロイでA/Bテストを用いると、新旧のバージョンを並行して稼働させる方式を指しますが、これではライブトラフィックの一部をテストに使う事は出来ますが、全負荷のテストは行えません。逆にシャドーテストでは、全てのトラフィックを新旧両方のバージョンに送り結果を比較することが可能です。
選択肢：継続的デプロイメントでカナリアテストを使用します
この選択肢が正しくない理由は以下の通りです。
カナリアテストでは新しいバージョンのアプリケーションに一部のユーザトラフィックを送りますが、全負荷に対するテストは行えません。
一方、シャドーテストでは、本番環境の全トラフィックを新旧のアプリケーションに同時に送るため全負荷テストが可能なため正解となります。
選択肢：ローリングアップデートデプロイでカナリアテストを使用します
この選択肢が正しくない理由は以下の通りです。
カナリアテストは新バージョンを一部のユーザーだけに適用して検証する手法で、全負荷の本番環境テストの要件を満たしません。
一方、シャドーテストは新旧バージョンを並行して稼働させ、同じライブトラフィックを使って性能を測定する方法であり、この質問の要件を満たします。
参考リンク：
https://cloud.google.com/solutions/devops/devops-tech-deployment
https://cloud.google.com/architecture/performing-canary-analyses
https://martinfowler.com/bliki/CanaryRelease.html
</div></details>

### Q. 問題21: 未回答
App Engineで実行され、データストレージにCloudSQLとCloud Storageを使用するウェブアプリケーションをサポートしています。ウェブサイトのトラフィックが短期間で急増した後、すべてのユーザー要求の待ち時間が大幅に増加し、CPU使用量が増加し、アプリケーションを実行しているプロセス数が増加していることに気づきました。また、初期のトラブルシューティングで以下の点が明らかになりました：
- トラフィックの初期スパイク後、負荷レベルは正常に戻りましたが、ユーザーは依然として高い待ち時間を経験しています。
- CloudSQLデータベースからのコンテンツ要求とCloud Storageからの画像要求でも、同じように高い待ち時間が発生しています。
- 待ち時間が増加した前後に、ウェブサイトに変更は加えられていません。
- ユーザーへのエラーの数は増えていません。
今後数日間、ウェブサイトのトラフィックが急増することが予想されるため、ユーザーが待ち時間を経験しないようにしたいと考えています。
この要件を満たすために、どうすればよいですか？
1. アイドル状態のインスタンスが追加されるように、App Engineの設定を変更します
2. アプリケーションをApp EngineからCompute Engineに移動します
3. CloudSQLインスタンスで高可用性を有効にします
4. GCSバケットをMulti-Regionalにアップグレードします
<details><div>
    答え：1
説明
この問題では、App Engineを使用しているウェブアプリケーションのパフォーマンス問題に対処する方法が求められています。問題文によれば、トラフィックスパイクの後もユーザー要求の待ち時間が増加し、その結果、CPU使用率とプロセス数も増大しているとのことです。また、問題の同じ状況がCloudSQLデータベースのコンテンツ要求とCloud Storageからの画像要求でも発生していると明かしています。対策として、App Engineの設定変更やGCSバケットのアップグレード、CloudSQLインスタンスの高可用性有効化、またはプラットフォームの変更など様々な選択肢が提示されていますが、正しい選択肢を選ぶためには、App Engineの負荷分散の仕組みと各サービスのロールと特性を理解しておく必要があります。
基本的な概念や原則：
App Engine：Google Cloudのフルマネージドでスケーラブルなアプリケーションホスティングサービスです。ディフェレンスとメモリなどのリソースを動的にスケーリングする機能があります。
CloudSQL：Google Cloudのフルマネージドリレーショナルデータベースサービスです。MySQL, PostgreSQL, SQL Serverなど複数のデータベースエンジンをサポートします。
Cloud Storage：Google Cloudのオブジェクトストレージサービスです。データを世界中の場所から容易にアクセスできるように保存します。
アイドルインスタンス：App Engineでは、新たなユーザーの要求を待つために予め起動されているインスタンスを指します。これにより新たなユーザーの要求に対するレスポンス時間を短縮できます。
待ち時間：ユーザーが操作を行った後、システムが反応するまでの時間を指します。システムパフォーマンスの一部であり、低いほど良いとされます。
CPU使用率：システムにおけるCPUリソースの使用状況です。高CPU使用率はシステム負荷の増加を示し、パフォーマンス問題の一因となり得ます。
プロセス数：システム内で実行中のプロセス数です。増加するとシステム負荷が高まり、パフォーマンスに影響を与える可能性があります。
正解についての説明：
（選択肢）
・アイドル状態のインスタンスが追加されるように、App Engineの設定を変更します
この選択肢が正解の理由は以下の通りです。
Google App Engineは自動スケーリングをサポートしていますが、その動作は設定による影響を受けます。負荷が減少した後も継続的に高い待ち時間が発生しているという事実は、トラフィックスパイク時に割り当てられたリソースが迅速に縮小され、結果的に待ち時間が増加した可能性を示しています。この問題を解決するためには、App Engineの設定を変更してアイドル状態のインスタンス数を増やすことで対応できます。これは要求のバーストを効率的に処理しながらも、適切なレベルのサービスを提供するための余裕を保つことを可能にします。よって、設定の変更により、急激なトラフィックの増加にすばやく対応するとともに、ユーザーの待ち時間を最小限に抑えることが可能となります。
不正解についての説明：
選択肢：GCSバケットをMulti-Regionalにアップグレードします
この選択肢が正しくない理由は以下の通りです。
問題は待ち時間の増加であり、これはCPU使用量とプロセス数の増加から明らかにアプリケーションのインスタンス管理に関連しています。
一方、GCSバケットをMulti-Regionalにアップグレードすることはデータの可用性と耐久性を向上させるものであり、待ち時間の問題を解決しません。
選択肢：CloudSQLインスタンスで高可用性を有効にします
この選択肢が正しくない理由は以下の通りです。
高可用性を有効にすることは、CloudSQLインスタンスの可用性を高めるためのもので、応答時間の遅延に対する直接的な解決策にはなりません。
また、問題はCloud Storageからの要求でも発生しており、この選択肢はその問題を解決しません。
一方、アイドル状態のインスタンスを追加すると、急激なトラフィック増加にも対応でき、全体のパフォーマンスが改善します。
選択肢：アプリケーションをApp EngineからCompute Engineに移動します
この選択肢が正しくない理由は以下の通りです。
問題の根本原因はApp Engine自体にあるとは限らず、App EngineからCompute Engineに移動するだけで問題が解決するとは限りません。さらにCompute Engineは手動でスケーリングを管理する必要があり、迅速なスケールアップには適しません。対してApp Engineの設定変更でアイドル状態のインスタンスを追加する方が、急激なトラフィック増加に対応するのに適しています。
参考リンク：
https://cloud.google.com/appengine/docs/standard/python/config/appref
https://cloud.google.com/sql/docs/mysql/high-availability
https://cloud.google.com/storage/docs/storage-classes
</div></details>

### Q. 問題22: 未回答
あなたはトラフィックの多いウェブアプリケーションをサポートし、ホームページがタイムリーにロードされるようにしたいと考えています。最初のステップとして、許容可能なページロード時間を100ミリ秒に設定し、ホームページ要求の待ち時間を表すサービスレベルインジケータ（SLI）を実装することにしました。Googleが推奨するSLIの計算方法はどれですか？
1. リクエストの待ち時間を範囲にバケット化し、100ミリ秒でのパーセンタイルを計算します
2. リクエストの待ち時間を範囲にバケット化し、中央値と90パーセンタイルを計算します
3. 100ミリ秒以内にロードされたホームページリクエストの数を数え、ホームページリクエストの総数で割ります
4. 100ミリ秒以内にロードされたホームページリクエストの数を数え、すべてのウェブアプリケーションリクエストの総数で割ります
<details><div>
    答え：3
説明
この問題では、サービスレベルインジケータ（SLI）の計算方法を理解することが重要です。設問は、許容可能なページロード時間を100ミリ秒に設定し、そこからホームページのロード時間を表すSLIを計算する方法を尋ねています。選択肢を見る際には、SLIの計算方法とその対象となるリクエストの範囲、またサイト全体のパフォーマンスを測定するのか、特定のページのパフォーマンスを測定するのか、といった点に注意を払う必要があります。
基本的な概念や原則：
サービスレベルインジケータ（SLI）：特定のサービスのパフォーマンスや可用性を定量的に評価するためのメトリックです。特定の目標を達成するための指標となります。
サービスレベルオブジェクティブ（SLO）：SLIに基づいて設定される目標です。SLIが特定の閾値を下回らないようにすることを目標とします。
ロード時間：ウェブページが完全に表示されるまでの時間です。ユーザー体験に大きな影響を与える重要なパフォーマンスメトリックです。
パーセンタイル計算：データセットの値が全体の何パーセント以下であるかを示す統計的手法です。例えば、90パーセンタイルは、値全体の90%がその値以下であることを示します。
バケット化：データをソートし、特定の範囲またはカテゴリー（バケット）に分けるプロセスです。視覚化や解析が容易になります。
正解についての説明：
（選択肢）
・100ミリ秒以内にロードされたホームページリクエストの数を数え、ホームページリクエストの総数で割ります
この選択肢が正解の理由は以下の通りです。
SLIはサービスレベルの測定方法であり、特定のサービスのレベルを定量的に示します。この問題ではホームページリクエストの待ち時間を表すSLIを実装したいという要件があります。SLIを計算する際には、達成したいサービスレベル目標（ここでの設定値は100ミリ秒以内にロードされるホームページの要求）を測定し、それを全体の要求数で割ることで、それらがどの程度達成されているかを示します。この方法はGoogleが推奨する方法であり、特に高トラフィックのウェブアプリケーションで適用されます。
したがって、100ミリ秒以内にロードされたホームページリクエストの数を計数し、それをホームページリクエストの総数で割ることで、許容可能なページロード時間に対する適用度を数値的に理解することができます。この方法は、ウェブアプリケーションが設定したサービスレベル目標を十分に達成しているかを明確にし、必要に応じてさらなる最適化を行うための基礎となります。
不正解についての説明：
選択肢：リクエストの待ち時間を範囲にバケット化し、100ミリ秒でのパーセンタイルを計算します
この選択肢が正しくない理由は以下の通りです。
リクエストの待ち時間を範囲にバケット化し、100ミリ秒でのパーセンタイルを計算する方法では、具体的なページロード時間としてのSLI（100ミリ秒以内にロードされたかどうか）を直接的に計測せず、各範囲の待ち時間の分布を利用しているため正しくありません。
それに対して、正解の選択肢は100ms以内にロードされたリクエスト数を直接計測し、全リクエスト数で割ることで期待通りのSLIを具体的に把握する方法です。
選択肢：リクエストの待ち時間を範囲にバケット化し、中央値と90パーセンタイルを計算します
この選択肢が正しくない理由は以下の通りです。
この選択肢では待ち時間をバケット化して中央値や90パーセンタイルを計算していますが、それは一定の期間のページロード時間の分布を知るためのものであり、特定の閾値（この場合は100ミリ秒）を満たすリクエストの比率をはかるSLIの目的には適していません。
正解の選択肢のように、特定の閾値を満たすリクエストの比率を直接求めることがSLIとして適切です。
選択肢：100ミリ秒以内にロードされたホームページリクエストの数を数え、すべてのウェブアプリケーションリクエストの総数で割ります
この選択肢が正しくない理由は以下の通りです。
SLIの計算方法として100ミリ秒以内にロードされたホームページリクエストの数を全ウェブアプリケーションリクエストで割る方法では、ホームページ以外のリクエストもカウントされてしまいます。正解はホームページリクエストのみを対象としますから、ウェブ全体のパフォーマンスではなくホームページロード時間のみを正確に測定できます。
参考リンク：
https://cloud.google.com/monitoring/api/metrics_Google Cloud#Google Cloud-loadbalancing
https://cloud.google.com/monitoring/service-level-objectives
https://sre.google/workbook/implementing-slos/
</div></details>

### Q. 問題23: 未回答
あなたは、最近障害が発生したサービスをサポートしています。この障害は、サービスのメモリリソースを使い果たした新しいリリースが原因でした。あなたはリリースのロールバックを成功させ、ユーザへの影響を軽減しました。あなたは現在、この障害のポストモーテムを担当しています。ポストモーテムを作成する際には、サイト信頼性エンジニアリングのプラクティスに従います。
この要件を満たすために、どうすればよいですか？
1. Gitの履歴から、関連するコードのコミットを探します。そのコミットを行ったエンジニアが本番サービスで作業できないようにします
2. 関係するすべてのエンジニアとの個別ミーティングを計画します。誰が新しいリリースを承認し、プロダクションにプッシュしたかを決定します
3. インシデントの原因となった個人を特定するのではなく、インシデントの一因となった原因を特定することに重点を置きます
4. 障害が再発しないようにすることよりも、新機能の開発に集中します
<details><div>
    答え：3
説明
この問題では、サイト信頼性エンジニアリング（SRE）のプラクティスに沿って障害のポストモーテムを実施する方法を求められています。SREのプラクティスの中心的な思想は、障害は個々の人や固有のミスの結果ではなく、システムやプロセスの欠陥の結果として見ることです。選択肢を見る際は、この哲学に従うものを選ぶことを思い出してください。またSREプラクティスは、障害からの学習とシステムの改善に重点を置いているので、選択肢がこれを反映しているか確認します。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：システムの信頼性、スケーラビリティ、効率を向上させるために、ソフトウェアエンジニアリングの原則を適用する方法論です。問題解決と障害対策においては、原因追求よりもシステムの改善に焦点を当てます。
ポストモーテム：障害やインシデントに対するポストモーテムのことです。障害の原因と影響を調査し、再発防止策を立てます。
システムの改善：SREの主要な目標の一つであり、システム障害が発生した際は、個々のミスを指摘するのではなく、システム全体の信頼性を向上させるための方法を探求します。
正解についての説明：
（選択肢）
・インシデントの原因となった個人を特定するのではなく、インシデントの一因となった原因を特定することに重点を置きます
この選択肢が正解の理由は以下の通りです。
インシデントの原因となった個人を特定するのではなく、インシデントの一因となった原因を特定することに重点を置くという発想は、サイト信頼性エンジニアリング（SRE）の中心的なプラクティスである、ブレームレスポストモーテムから導かれています。ブレームレスポストモーテムは、問題の発生について個々の責任を問うことなく、システム全体の障害に対する理解を深めることを目指します。システム全体を見るためには、過去のインシデントにおける失敗の原因を理解し、それを修正し、また同じことが起こらないようにする方がより重要です。つまり、個々のエンジニアや運用者を非難するのではなく、システムの問題を発見・解決して、その回復と予防に取り組むことが求められます。
不正解についての説明：
選択肢：障害が再発しないようにすることよりも、新機能の開発に集中します
この選択肢が正しくない理由は以下の通りです。
サイト信頼性エンジニアリングのプラクティスでは、障害が再発しないようにすることが重要な目標であり、新機能の開発よりも優先するべきです。新機能を開発するよりも、既存のシステムが安定して稼働し続けることを確保することが優先されます。
選択肢：関係するすべてのエンジニアとの個別ミーティングを計画します。誰が新しいリリースを承認し、プロダクションにプッシュしたかを決定します
この選択肢が正しくない理由は以下の通りです。
サイト信頼性エンジニアリングのプラクティスではインシデントの原因追求ではなく、問題が再発しないようなシステムやプロセスの改善に重点を置きます。個々のエンジニアの責任追及ではなく、問題が起きた原因を解明し、それを防止するための施策を考えることが重要となります。
選択肢：Gitの履歴から、関連するコードのコミットを探します。そのコミットを行ったエンジニアが本番サービスで作業できないようにします
この選択肢が正しくない理由は以下の通りです。
問題が起きた際に特定のエンジニアを一方的に非難し、アクセスを制限するのはサイト信頼性エンジニアリングのプラクティスには反します。問題の原因を特定し、その問題が再発しないような改善を提案することが重要なのです。
参考リンク：
https://cloud.google.com/architecture/framework/resilience/reliability
https://cloud.google.com/sre/books
https://sre.google/sre-book/managing-incidents/
</div></details>

### Q. 問題24: 未回答
あなたの組織は最近、アプリケーション開発にコンテナベースのワークフローを採用しました。あなたのチームは、自動化されたビルドパイプラインを通じて、本番環境のKubernetesクラスターに継続的にデプロイされる多数のアプリケーションを開発しています。セキュリティ監査人は、開発者やオペレータが自動テストを回避して、承認なしにコード変更を本番環境にプッシュできることを懸念しています。
承認を強制するために何をすべきですか？
1. Kubernetesクラスター内でバイナリ認証を有効にし、ビルドパイプラインを認証者として設定します
2. プルリクエストの承認が必要な保護ブランチをビルドシステムに設定します
3. アドミッションコントローラを使用して、受信リクエストが承認されたソースから発信されたものであることを確認します
4. Kubernetesのロールベースのアクセス制御（RBAC）を活用して、承認されたユーザーのみにアクセスを制限します
<details><div>
    答え：1
説明
この問題では、開発者やオペレーターが承認なしに本番環境にコード変更をプッシュすることを防ぎ、セキュリティを確保する方策を尋ねています。コード変更を本番環境にプッシュするプロセスが自動化されているため、このプロセスにおいて適切な承認メカニズムを通じてセキュリティを強化する方法を考えるべきです。そのためには、自動化プロセスにおける承認システムとして、それぞれの選択肢の特性を正確に理解することが必要です。
基本的な概念や原則：
バイナリ認証：Google Cloud上のKubernetesクラスターで特定のコンテナイメージのデプロイを制御するためのサービスです。これにより、承認されたソースからの変更だけがデプロイされるよう保証します。
Kubernetesクラスター：Kubernetesは、コンテナ化されたアプリケーションのデプロイ、スケーリング、運用を自動化するオープンソースのオーケストレーションシステムで、Kubernetesクラスターはその基盤となるインフラストラクチャです。
保護ブランチ：ソースコード管理システムにおいて特定の変更を制限するための設定です。例えば、プルリクエストの承認が必要となるよう設定できます。
アドミッションコントローラ：KubernetesのAPIへのリクエストをチェックし、その挙動を制御するプラギンです。リソースの作成、変更、削除等の操作がパスするかどうかをチェックします。
ロールベースのアクセス制御（RBAC）：KubernetesのAPIへの各ユーザーのアクセス権限を制御する手法です。ロール（ロール）を定義し、そのロールに対して一連のアクセス権限を割り当てます。
正解についての説明：
（選択肢）
・Kubernetesクラスター内でバイナリ認証を有効にし、ビルドパイプラインを認証者として設定します
この選択肢が正解の理由は以下の通りです。
まず、バイナリ認証は、Google Cloudにおけるコンテナイメージのデプロイを制御するセキュリティ機能です。これを有効にすると、特定の条件を満たしたコンテナイメージのみがKubernetesクラスターにデプロイされます。これにより、信頼できるソースからのデプロイメントのみが許可され、不正や信頼できないコードのデプロイを未然に防ぐことができます。
また、バイナリ認証は開発者やオペレーターが自動テストを回避して、承認なしにコード変更を本番環境にプッシュできないようにします。
さらに、ビルドパイプラインを認証者として設定することで、CI/CDパイプライン内で生成または変更されたコンテナイメージが信頼できるという保証が得られます。
このように、バイナリ認証を有効にしてビルドパイプラインを認証者とすることで、セキュリティ監査人の懸念を解消し、本番環境でのコード変更に対するリスクを軽減できるため、この選択肢が最適な解答となります。
不正解についての説明：
選択肢：プルリクエストの承認が必要な保護ブランチをビルドシステムに設定します
この選択肢が正しくない理由は以下の通りです。
保護ブランチはコードリポジトリレベルの制御であり、原則として本番環境への直接的なコードプッシュを防ぐことは可能ですが、自動テストの回避に対する具体的な対策とはなりません。
一方、バイナリ認証を有効にすると、ビルドパイプラインを通過したコンテナイメージのみがデプロイされるため、承認の強制及び自動テストの回避の防止に適しています。
選択肢：アドミッションコントローラを使用して、受信リクエストが承認されたソースから発信されたものであることを確認します
この選択肢が正しくない理由は以下の通りです。
アドミッションコントローラは、Kubernetes APIへのリクエストを監視し、ポリシーに基づいてそのリクエストを許可あるいは拒否します。しかし、これでは自動テストを回避し、承認なしに本番環境に変更をPushされることを防ぐ効果は無く、ビルドパイプラインのコントロールには不十分です。
それに対して、バイナリ認証を用いると確実に承認を通すビルドのみをデプロイできます。
選択肢：Kubernetesのロールベースのアクセス制御（RBAC）を活用して、承認されたユーザーのみにアクセスを制限します
この選択肢が正しくない理由は以下の通りです。
Security Auditorの主な懸念は承認なしにコード変更がプッシュされることであり、それはRBACを使ってアクセス制限を設けるだけでは対処できません。
それに対して、バイナリ認証を有効にすると、信頼できるソースからビルドされた正しいバイナリであることを確認し、不正な変更を防ぐことが可能です。
参考リンク：
https://cloud.google.com/binary-authorization/docs
https://cloud.google.com/kubernetes-engine/docs/concepts/binary-authorization
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
</div></details>

### Q. 問題25: 未回答
あなたは、Google Kubernetes Engine（GKE）クラスターのセットへの本番デプロイを管理しています。信頼できるCI/CDパイプラインによって正常にビルドされたイメージだけが本番環境にデプロイされるようにしたいと考えています。
この要件を満たすために、どうすればよいですか？
1. クラスターでCloud Security Scannerを有効にします
2. Kubernetes Engineクラスターをプライベートクラスターとしてセットアップします
3. Artifact Registryの脆弱性分析を有効にします
4. バイナリ認証でKubernetes Engineクラスターをセットアップします
<details><div>
    答え：4
説明
この問題では、CI/CDパイプラインを通じて信頼できるイメージのみが本番環境にデプロイされるようにするための適切なGoogle Cloudの機能やサービスを選ぶ必要があります。GKEクラスターに関連する選択肢に注目し、それぞれが問題の要件をどの程度満たしているかを評価します。特に、"信頼できるイメージのみがデプロイされる"ことを確実にする機能が必要なので、イメージの信頼性を向上すればサービスを絞り込むと良いでしょう。
基本的な概念や原則：
バイナリ認証：Google Cloudのセキュリティ機能の一つで、信頼性の高いビルドプロセスから生成されたコンテナイメージのみがデプロイされるようにします。CI/CDパイプラインと連携して効果を最大化します。
Google Kubernetes Engine：Google CloudのマネージドKubernetesサービスです。複雑なクラスター管理タスクを抽象化し、ユーザーフレンドリーなインターフェースを提供します。
CI/CDパイプライン：コードの変更を自動的にビルド、テスト、デプロイするプロセスです。これにより、素早く安定したリリースが可能になります。
Cloud Security Scanner：Google Cloudが提供するセキュリティスキャンツールです。アプリケーションの脆弱性を自動的に検出します。
Artifact Registryの脆弱性分析：Artifact Registryに格納されているコンテナイメージの脆弱性を分析する機能です。ただし、これはバイナリ認証とは異なるセキュリティ対策です。
プライベートクラスター：Google CloudのVPCネットワーク内部でのみアクセス可能なKubernetesクラスターのことです。これはバイナリ認証とは関係ありません。
正解についての説明：
（選択肢）
・バイナリ認証でKubernetes Engineクラスターをセットアップします
この選択肢が正解の理由は以下の通りです。
Google Cloudのバイナリ認証は信頼性の高いCI/CDパイプラインとコンテナ環境を実現するのに最適なサービスです。このサービスは、既定のポリシーに合致したイメージのみがクラスターにデプロイされる点で頼りになります。これらのポリシーには、ビルドプロセス、対象となるソースコード、テスト手順等が含まれます。つまり、この機能を利用することで、一定の基準を満たさないイメージがデプロイされるのを防止し、CI/CDパイプラインの信頼性を高めることができます。
したがって、クラスターへの本番デプロイを管理する場合に、バイナリ認証でGKEクラスターをセットアップするという選択肢は最適だと言えます。
不正解についての説明：
選択肢：クラスターでCloud Security Scannerを有効にします
この選択肢が正しくない理由は以下の通りです。
Cloud Security Scannerは、Webアプリケーションのセキュリティ欠陥を検出するツールであり、CI/CDパイプラインの制御や、イメージの認証とは無関係です。バイナリ認証では、信頼できるソースからのイメージのみがデプロイされるように制御できます。
選択肢：Artifact Registryの脆弱性分析を有効にします
この選択肢が正しくない理由は以下の通りです。
Artifact Registryの脆弱性分析は、イメージのセキュリティ脆弱性を検出するための機能であり、CI/CDパイプラインを通じて正常にビルドされたイメージのみが本番環境にデプロイされるという要件を満たすものではありません。
一方、バイナリ認証はCI/CDパイプラインによるビルドが正常であることを確認する機能を提供します。
選択肢：Kubernetes Engineクラスターをプライベートクラスターとしてセットアップします
この選択肢が正しくない理由は以下の通りです。
プライベートクラスターとしてKubernetes Engineクラスターをセットアップすることはセキュリティの向上には貢献しますが、それだけではCI/CDパイプラインからのイメージのみを許可するという条件を満たしません。しかしながら、バイナリ認証を用いると信頼できるビルドプロセスから作成されたイメージのみをデプロイすることが可能となります。
参考リンク：
https://cloud.google.com/binary-authorization/docs
https://cloud.google.com/container-registry/docs/getting-started
https://kubernetes.io/docs/concepts/security/pod-security-admission/
</div></details>

### Q. 問題26: 未回答
ある社内アプリケーションの新リリースを、ユーザトラフィックが少ない週末のメンテナンス期間中にデプロイしました。期間終了後、新機能の1つが本番環境で期待通りに動作しないことが判明しました。長時間の停止の後、新しいリリースをロールバックし、修正プログラムをデプロイします。
リカバリまでの平均時間を短縮するためにリリースプロセスを変更し、将来的に長時間の停止を回避できるようにしたいと考えています。
この要件を満たすために、どうすればよいですか？（2つ選択）
1. CIサーバーを設定します。あなたのコードにユニットテスト群を追加し、CIサーバーにコミット時にそれらを実行させ、変更を検証させます
2. 開発者には、リリース前にローカルの開発環境で自動統合テストを実行することを義務付けます
3. CDサーバー経由で新しいコードをリリースするときは、ブルー/グリーンのデプロイメント戦略を採用します
4. 新しいコードをマージする前に、2人の異なるピアにコードの変更をレビューしてもらいます
5. コードリンティングツールを統合して、コードをリポジトリに受け入れる前にコーディング標準を検証します
<details><div>
    答え：1,3
説明
この問題では、本番環境でアプリケーションをリリースした後に新機能が期待通りに動作しなかったことによるダウンタイムが発生したため、ダウンタイムを回避しリカバリ時間を短縮するリリースプロセスの改善が求められています。主にContinuous Integration（CI）やContinuous Deployment（CD）の考え方と、ブルー/グリーンデプロイメントといったデプロイの戦略について理解して、問題文から必要なリリースプロセスの要求を読み取る必要があります。正解選択肢の中からCIとCDのテストとデプロイの戦略に関係する選択肢を選ぶことが重要です。
基本的な概念や原則：
ブルー/グリーンデプロイメント：２つの同一環境（ブルーとグリーン）を使って新旧のシステムを切り替えるデプロイメント手法です。リリースするときはグリーン環境に新しいバージョンをデプロイし、問題がなければトラフィックをグリーンに切り替えます。問題があった場合、すぐにブルーに戻せるためダウンタイムを最小限に抑えることができます。
Continuous Integration（CI）：継続的インテグレーションはコードの変更を頻繁にメインラインにマージする開発手法です。この手法ではユニットテストが重要となり、テストを通過しないコードはマージされません。これにより早期にバグを見つけ出すことができ、大規模なエラーを防ぎます。
ユニットテスト：個々のコードの部品（関数やメソッド）が期待通りに動作するかを確認するテストです。CIサーバー上で実行することで、コミットごとに自動的にコードの品質を保証することができます。
ピアレビュー：他の開発者によるコードのレビューです。これによりコードの品質を保証することができますが、問題の発見は人間に依存しています。
リンティングツール：コードが一定の書式やルールに従っているかを自動的にチェックするツールです。書式の統一や簡単なエラーの検出に有効ですが、複雑なバグの発見には限界があります。
ローカル環境でのテスト：開発者自身が開発環境でテストを行うことです。これにより誤ったコードのコミットを防止できますが、全ての開発者が正しくテストを行うとは限らず、また異なる環境間での差異が生じる可能性があるため、効果には限界があります。
正解についての説明：
（選択肢）
・CDサーバー経由で新しいコードをリリースするときは、ブルー/グリーンのデプロイメント戦略を採用します
・CIサーバーを設定します。あなたのコードにユニットテスト群を追加し、CIサーバーにコミット時にそれらを実行させ、変更を検証させます
この選択肢が正解の理由は以下の通りです。
まず、ブルー/グリーンデプロイメントを使用することで、全く新しい環境（グリーン）に新しいリリースをデプロイし、テストと検証を行います。これが正常に機能すれば、トラフィックを新しい環境に切り替えます。何か問題があればすぐに古い環境（ブルー）に戻すことができるので、ダウンタイムを大幅に短縮できます。
また、CIサーバーを使用すると、開発者が新しいコードをコミットするたびに、ユニットテスト群を自動的に実行し、すぐに潜在的な問題を検知できます。これにより、問題が本番環境に影響を与える前に修正することが可能になります。
また、各変更が期待通りに動作することを検証するため、リリースプロセス中の問題を早期に発見できます。
これらの修正により、問題発生時のリカバリータイムが大幅に短縮され、本番環境での長時間の停止を回避できます。
不正解についての説明：
選択肢：新しいコードをマージする前に、2人の異なるピアにコードの変更をレビューしてもらいます
この選択肢が正しくない理由は以下の通りです。
ピアレビューはコードの品質を保つ重要なプラクティスですが、自動化されたテストやブルー/グリーンデプロイメント戦略のようにシステム全体の動作を確認することはできません。人間のレビューは素早く効率的なリカバリには限界があります。
選択肢：コードリンティングツールを統合して、コードをリポジトリに受け入れる前にコーディング標準を検証します
この選択肢が正しくない理由は以下の通りです。
コードリンティングはコーディング標準を検証するためのツールであり、コードの品質を向上させますが、新機能が期待通りに動作しない問題の特定や、長時間の停止を回避するリリースプロセスの変更には直接寄与しません。
一方、ブルー/グリーンのデプロイメント戦略やユニットテストの追加は、新機能の問題を早期に捉え、停止時間を最小限に抑えることが可能です。
選択肢：開発者には、リリース前にローカルの開発環境で自動統合テストを実行することを義務付けます
この選択肢が正しくない理由は以下の通りです。
開発者がローカルの開発環境で自動統合テストを実行するだけでは、本番環境に変更をデプロイしたときに予想外の問題が発生するリスクを排除することはできません。負荷状況や環境条件が本番と同じでないため、本番での問題を確実に予測できないからです。
参考リンク：
https://cloud.google.com/architecture/creating-cicd-pipelines-on-Google Cloud
https://cloud.google.com/architecture/blue-green-deployments
https://martinfowler.com/bliki/BlueGreenDeployment.html
</div></details>

### Q. 問題27: 未回答
あなたはポストモーテムで、障害に対するアクションアイテムを作成し、割り当てています。障害は終わりましたが、根本的な原因に対処する必要があります。あなたは、チームが迅速かつ効率的にアクションアイテムを処理できるようにしたいと考えています。
アクションアイテムの所有者と共同作業者をどのように割り当てるべきですか？
1. 各項目に複数の所有者を割り当て、チームが迅速に項目に対処できるようにします
2. 事後の責任を問わないために、アイテムに協力者を割り当てますが、個人の所有者は設定しません
3. 各アクション項目に1人のオーナーと、必要な協力者を割り当てます
4. チームリーダはSREチームの責任者であるため、すべてのアクションアイテムのオーナーに任命します
<details><div>
    答え：3
説明
この問題では、障害後のアクションアイテムの所有者と共同作業者の割り当て方について問われています。問題文から、障害が終了し、根本的な原因への対処とアクションアイテムの効率的な処理が必要であることがわかります。その背景を踏まえて、選択肢を見ると、アクションアイテムについて誰が責任を持つべきか、またどのように協力すべきかについての戦略的な選択が求められています。
基本的な概念や原則：
ポストモーテム：障害やインシデントが発生した後に行われる分析で、何が起こったのか、なぜ起こったのか、どのように対処すべきかを詳細にまとめます。
障害対応のアクションアイテム：ポストモーテムに基づいて作成された改善策のリストです。根本原因を解消し、同じ問題が再発するのを防ぐためのアクションを明確に指定します。
アクションアイテムの所有者：特定のアクションアイテムに責任を持つ人物です。アクションの実施と結果の報告がそのロールになります。
協力者：特定のアクションアイテムに関与し、所有者がタスクを正常に完了するのを支援する人物です。
事後の責任：障害発生後、各アクションアイテムの責任を持つ人物やチームを明確にします。これは、アクションの適切な実行と問題の再発防止に役立つ。
正解についての説明：
（選択肢）
・各アクション項目に1人のオーナーと、必要な協力者を割り当てます
この選択肢が正解の理由は以下の通りです。
アクションアイテムの所有権を一人に持たせることの利点は、その個人がアイテムの進行状況を管理し、肩代わりし、責任を負うことが明確になる点です。これにより、そのアクションアイテムがタイムリーに、そして適切に処理される確率が高まります。
ただし、複雑なトピックや重要なアクションアイテムの場合、単独のオーナーだけでは適切な対応が難しい場合があります。そのため、必要な協力者を割り当てることで、オーナーはその道筋で適切なリソースにアクセスでき、課題解決への道筋を効率的かつ迅速に進めることが可能になります。
このように、オーナーと協力者を適切に割り当てることで、チームはアクションアイテムを効率的に処理できるようになります。
不正解についての説明：
選択肢：各項目に複数の所有者を割り当て、チームが迅速に項目に対処できるようにします
この選択肢が正しくない理由は以下の通りです。
各項目に複数の所有者を割り当てると、責任が希薄になり誰が決断を下すべきかが不明確になります。これはアクション項目の効率的な解決を阻む可能性があるため、不適切です。正解選択肢のように1人のオーナーを設定し、責任を明確にする方が効果的です。
選択肢：事後の責任を問わないために、アイテムに協力者を割り当てますが、個人の所有者は設定しません
この選択肢が正しくない理由は以下の通りです。
アクションアイテムに個人の所有者を割り当てないと、誰がプライマリな責任を持つかが不明確となり、アクションアイテムのスケジュール管理や進行状況の確認が困難になります。
それに対し、一つのアクションアイテムに一人のオーナーを割り当てることで、誰が主導し進めるかが明確となり効率的な対応が可能になります。
選択肢：チームリーダはSREチームの責任者であるため、すべてのアクションアイテムのオーナーに任命します
この選択肢が正しくない理由は以下の通りです。
すべてのアクションアイテムのオーナーを一人のチームリーダーにすると、彼がボトルネックとなり、アクションアイテムの処理が遅延する可能性があります。
これに対し、各アクションアイテムに一人のオーナーと必要な協力者を割り当てると、作業の負担が分散され、迅速かつ効率的な処理が可能になります。
参考リンク：
https://cloud.google.com/sre/docs/sre-book/managing-incidents
https://cloud.google.com/sre/docs/sre-book/postmortem-culture
https://landing.google.com/sre/sre-book/chapters/postmortem-culture/
</div></details>

### Q. 問題28: 未回答
あなたの会社は、Google Kubernetes Engine（GKE）でJVMベースのアプリケーションとマイクロサービスアーキテクチャで構築されたeコマースウェブサイトを運営しています。アプリケーションの負荷は日中に増加し、夜間に減少します。運用チームは、夕方のピーク負荷に対応するのに十分なポッドを実行するようにアプリケーションを設定しています。あなたは、負荷に十分なポッドとノードだけを実行することで、スケーリングを自動化したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. Horizontal Pod Autoscalerを構成し、クラスターオートスケーラを有効にします
2. Vertical Pod Autoscalerを構成しますが、ノードプールのサイズは固定にしておきます
3. Vertical Pod Autoscalerを構成し、クラスターオートスケーラを有効にします
4. Horizontal Pod Autoscalerを構成しますが、ノードプールサイズは固定にしておきます
<details><div>
    答え：1
説明
この問題では、非定常な負荷に対応するためのGKEの自動スケーリング設定が求められています。日中の負荷が増えると昼夜で負荷が変動する事象があるため、スケーリングの自動化が必要とされます。さらに、アプリケーションはマイクロサービスアーキテクチャで構築されており、これには多数のポッドとノードが用いられています。したがって、Verticle Pod AutoscalerとHorizontal Pod Autoscalerの違いと、その使用場面を理解する事が問題解決の鍵となります。また、自動スケーリング設定した際のコスト影響を考慮することも重要です。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するKubernetesを実行するためのマネージドサービスです。アプリケーションのデプロイ、スケーリング、管理を行うことができます。
マイクロサービスアーキテクチャ：一つのアプリケーションを、ビジネス機能を果たす独立した小さなサービスの集合体として設計するアーキテクチャの一種です。サービス間は軽量プロトコルで通信し、各サービスは独自にデプロイやスケールすることができます。
Horizontal Pod Autoscaler（HPA）：Kubernetesの機能で、ポッドの数を自動的にスケーリングします。CPU使用率やカスタムメトリクスなどの指標を基に、ポッドのレプリカ数を自動調整します。
Vertical Pod Autoscaler（VPA）：Kubernetesの機能で、ポッドのCPUやメモリ要求を自動的に調整します。これにより、リソース使用量と要求量の差を最小限にし、リソースの無駄遣いを防ぎます。
クラスターオートスケーラ：GKEの機能で、ノードの数を自動的にスケーリングします。負荷に応じてノード数を増やしたり減らしたりすることができます。
正解についての説明：
（選択肢）
・Horizontal Pod Autoscalerを構成し、クラスターオートスケーラを有効にします
この選択肢が正解の理由は以下の通りです。
Google Kubernetes Engine（GKE）では、負荷の変動に基づいて自動的にスケーリングを行うための2つの主要なメカニズム、Horizontal Pod Autoscaler（HPA）とCluster Autoscaler（CA）が提供されています。HPAは、CPU使用率などのパフォーマンス指標に基づいて自動的にポッドの数を増減します。これにより、昼夜で負荷が変動するJVMベースのアプリケーションへの対応が可能です。
一方、Cluster Autoscalerは、ノードレベルでのスケーリングを行います。Cluster Autoscalerは、すぐにスケジュールできないポッドが存在する場合、ノード数を増やします。
また、ジョブが少なくノードが不要になったらノードを削除します。この機能により、夜間などの負荷が低い時間帯に必要以上のリソースを稼働させることなくコストを抑制できます。よって、HPAとCluster Autoscalerを併用することで質問の要件を満たすことができます。
不正解についての説明：
選択肢：Vertical Pod Autoscalerを構成しますが、ノードプールのサイズは固定にしておきます
この選択肢が正しくない理由は以下の通りです。
Vertical Pod Autoscalerはポッドのリソースの使用量を調整しますが、負荷に適応してポッド数を増減させるHorizontal Pod Autoscalerが必要です。
また、ノードプールのサイズを固定にすると、必要に応じてノード数を調整できず、自動スケーリングを実現できません。
選択肢：Vertical Pod Autoscalerを構成し、クラスターオートスケーラを有効にします
この選択肢が正しくない理由は以下の通りです。
Vertical Pod Autoscalerはパフォーマンスを最適化するために個々のポッドのリソース要求を調整しますが、負荷により変動するトラフィックに対しては効果的ではありません。
一方、Horizontal Pod Autoscalerは負荷に応じてポッド数を増減し、日中と夜間の負荷変動に効果的に対応します。
選択肢：Horizontal Pod Autoscalerを構成しますが、ノードプールサイズは固定にしておきます
この選択肢が正しくない理由は以下の通りです。
いくらポッドが自動的に増減されるとしても、ノードプールのサイズが固定の場合、十分なリソースがないと新たなポッドを作成できません。
逆に、負荷が減少したときにも、空いたリソースを解放できずコストが無駄になります。これは、スケーリングを自動化してリソースとコストを最適化するという問題の要件を満たしていません。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
</div></details>

### Q. 問題29: 未回答
Compute Engineの管理対象インスタンスグループにデプロイされたWebアプリケーションを実行しています。すべてのインスタンスにOps Agentがインストールされています。最近、特定のIPアドレスからの不審なアクティビティに気づきました。その特定のIPアドレスからのリクエスト数を最小限の運用オーバーヘッドで表示するように、Cloud Monitoringを構成する必要があります。
この要件を満たすために、どうすればよいですか？
1. Ops Agentにメトリクスレシーバーを設定します
2. ウェブサーバーのログをスクレイピングするスクリプトを作成します。IPアドレスのリクエストメトリクスをCloud Monitoring APIにエクスポートします
3. アプリケーションを更新して、IPアドレス要求メトリクスをCloud Monitoring APIにエクスポートします
4. ログレシーバーでOps Agentを構成します。Bウェブサーバーログをスクレイピングするスクリプトを作成します。IPアドレス要求メトリクスをCloud Monitoring APIにエクスポートします
<details><div>
    答え：4
説明
この問題では、Compute Engine上で走るWebアプリケーションから特定のIPアドレスによる不審なアクティビティを検出した後、どのようにして特定のIPアドレスからのリクエスト数をCloud Monitoringで追跡できるかを問われています。この問題を解く上でエージェントのインストール状況やWebアプリケーションの運用状況を理解することが重要です。また、Cloud Monitoringの利用方法について理解しておく必要があります。不審なアクティビティを検出できる構成としてはログの取得やメトリクスのエクスポートなどが挙げられますが、具体的な手段としてどの選択肢が適しているかを見極めることが大切です。
基本的な概念や原則：
Compute Engine：Google Cloudのインフラストラクチャ構築の核となる、高度にカスタマイズ可能なVMインスタンスを提供するサービスです。
管理対象インスタンスグループ：Google Cloudの機能で、複数の仮想マシン（VM）インスタンスを自動的に管理することが可能なグループです。
Ops Agent：Google Cloudの監視、ロギング、診断エージェントで、一元的な管理と各種パフォーマンス指標の収集が可能です。
Cloud Monitoring：Google Cloudの監視サービスで、リアルタイムのパフォーマンスと診断データを提供します。
ログ：アプリケーションやシステムが行った操作の一覧で、セキュリティ監視や問題解析に利用されます。
スクレイピング：データを自動的に抽出するテクニックで、ログの解析に利用されることがあります。
メトリクス：リソースの使用状況やパフォーマンスを測定する数値です。モニタリングと分析に使用されます。
正解についての説明：
（選択肢）
・ログレシーバーでOps Agentを構成します。Bウェブサーバーログをスクレイピングするスクリプトを作成します。IPアドレス要求メトリクスをCloud Monitoring APIにエクスポートします
この選択肢が正解の理由は以下の通りです。
まず、Ops AgentはGoogle CloudのCompute Engineで稼働するインスタンスにおいて、システムメトリクスとログデータを収集するためのエージェントです。不審なIPからのリクエスト数を追跡する場合、そのログ解析が重要になりますが、Ops Agentのログレシーバーを構成することで、これを行うことが可能になります。
次に、特定のIPアドレスからのリクエスト数を把握するためには、ウェブサーバーログを解析する必要があります。この解析を行うためのスクリプトを作成し、Ops Agentがスクレイピングして情報を収集します。
最後に、収集したデータをCloud Monitoringにエクスポートすることで、特定のIPアドレスからのリクエスト数を一元的に監視し、表示することが可能になります。Cloud Monitoring APIを使用すれば、これらの情報を簡単にエクスポートし、リアルタイムでの監視を可能とします。
したがって、この選択肢が最も適切な解決策となります。
不正解についての説明：
選択肢：ウェブサーバーのログをスクレイピングするスクリプトを作成します。IPアドレスのリクエストメトリクスをCloud Monitoring APIにエクスポートします
この選択肢が正しくない理由は以下の通りです。
不正解の選択肢はLogs ReseiverでOps Agentを構成する部分を無視しています。
ただし、この構成は特定のIPアドレスからのリクエスト数を監視するために重要です。Ops Agentを使用することでシステムやアプリケーションのメトリクスを自動的に収集し、Cloud Monitoringでアクセス可能にすることができます。
選択肢：アプリケーションを更新して、IPアドレス要求メトリクスをCloud Monitoring APIにエクスポートします
この選択肢が正しくない理由は以下の通りです。
アプリケーションを更新するという選択肢は一見合理的に見えますが、運用オーバーヘッドを最小限にするという要件を満たすためには不適切です。既存のアプリケーションを改修することは時間と資源を必要とし、運用オーバーヘッドが増加します。正解選択肢ではOps Agentを利用して監視を行うことで、運用オーバーヘッドを抑えつつ必要な情報収集が可能であるため、より適切な解答となります。
選択肢：Ops Agentにメトリクスレシーバーを設定します
この選択肢が正しくない理由は以下の通りです。
Ops Agentはシステム及びアプリケーションのメトリクスの収集に使用されますが、特定のIPアドレスからのリクエスト数を直接収集する機能は提供していません。
一方、正解の選択肢ではログレシーバーを用いることでWebサーバーログに含まれるIPアドレス情報を収集し解析することが可能です。
参考リンク：
https://cloud.google.com/logging/docs/logs-based-metrics
https://cloud.google.com/logging/docs/agent
https://cloud.google.com/monitoring/api/metrics_agent
</div></details>

### Q. 問題30: 未回答
Google Cloud上で動作する高トラフィックのWebアプリケーションをサポートしています。アプリケーションにエンジニアリング上の変更を加えることなく、ユーザーの視点からアプリケーションの信頼性を測定する必要があります。
この要件を満たすために、どうすればよいですか？（2つ選択）
1. 現在および過去のリクエストログを使用して、顧客とアプリケーションのやり取りを追跡します
2. ウェブプロキシログのみを分析し、各リクエストの応答時間を取得します
3. コードを修正して、ユーザーとの対話のための追加情報を取得します
4. 現在のアプリケーションメトリクスを見直し、必要に応じて新しいメトリクスを追加します
5. アプリケーションを使用するユーザーの使用方法をシミュレートするために、新しい合成クライアントを作成します
<details><div>
    答え：1,5
説明
この問題では、アプリケーションの信頼性をユーザ視点から測定するための解決策を選ぶことが要求されています。2点注意することは、"エンジニアリング上の変更を加えないこと"と"ユーザー視点からの信頼性を測定すること"です。改修が不要で、ユーザ体験を反映した解決策を探すことが重要となります。選択肢を見る際に、アプリケーションの信頼性をどのように客観的、かつユーザ視点で評価できるのかを考える必要があります。
基本的な概念や原則：
合成クライアント：実際のユーザーの行動や使用方法をシミュレートして公開サービスをテストするためのプログラムまたはシステムです。アプリケーションのパフォーマンスや可用性をリアルタイムで監視します。
リクエストログ：サーバーが受け取る全てのリクエストの詳細を記録したログデータです。これを分析することでユーザーとアプリケーションの対話を追跡し、アプリケーションのパフォーマンスの問題点を特定することができます。
アプリケーションの信頼性：システムやアプリケーションが予期せぬ障害やデータ損失などの問題なく、安定して機能する能力のことです。ユーザーエクスペリエンスやビジネスの成功に直接影響を与えます。
アプリケーションのメトリクス：アプリケーションの動作やパフォーマンスを評価するための定量的な指標です。しかし、これを見直すだけでも、また新たなメトリクスを追加するだけでもアプリケーションの信頼性を測定することはできません。
正解についての説明：
（選択肢）
・アプリケーションを使用するユーザーの使用方法をシミュレートするために、新しい合成クライアントを作成します
・現在および過去のリクエストログを使用して、顧客とアプリケーションのやり取りを追跡します
この選択肢が正解の理由は以下の通りです。
まず、新しい合成クライアントを作成することによって、アプリケーションの信頼性をエンドユーザーの視点から測定することができます。合成クライアントは、ユーザーの使用方法をシミュレートするために作られます。つまり、通常のユーザーがアプリケーションを使用する際にどのような操作を行い、どのような類のリクエストを送信するのかといった動きを模倣します。これを用いることによって、アプリケーションの信頼性を実際にユーザーが操作している時の視点から評価することが可能となります。
また、現在および過去のリクエストログを利用することで、ユーザーとアプリケーションとの間の交流を詳細に追跡し分析することが可能となります。これによって、アプリケーションのどこが信頼性を保てているのか、またどこが問題を起こしているのかといった情報を具体的に抽出できます。
したがって、エンジニアリング上の変更を加えることなく、アプリケーションの信頼性をユーザーの視点から評価するためには、合成クライアントの作成とログの分析が有効な手段となります。
不正解についての説明：
選択肢：現在のアプリケーションメトリクスを見直し、必要に応じて新しいメトリクスを追加します
この選択肢が正しくない理由は以下の通りです。
アプリケーションのメトリクスを見直し、新しいメトリクスを追加する行為は、アプリケーションにエンジニアリング上の変更を加える行為になり、問題の要件とは一致しません。正解の選択肢はアプリケーションに対する変更をせずに顧客の視点で信頼性を測定する方法を指しています。
選択肢：コードを修正して、ユーザーとの対話のための追加情報を取得します
この選択肢が正しくない理由は以下の通りです。
問題の要件はアプリケーションにエンジニアリング上の変更を加えることなく信頼性を測定することですが、この選択肢ではコードの修正つまりエンジニアリング上の変更が含まれているためです。正解の選択肢はアプリケーションの変更を必要とせず信頼性を測定する方法を提供しています。
選択肢：ウェブプロキシログのみを分析し、各リクエストの応答時間を取得します
この選択肢が正しくない理由は以下の通りです。
ウェブプロキシログでの分析は、信頼性の測定に一部役立つものの、これだけではユーザー視点のアプリケーションの挙動全体を把握するのが困難です。各リクエストの応答時間を取得することは可能ですが、それが具体的にどのようなユーザー体験をもたらしているのかは明確には把握できません。
一方、合成クライアントを作成し、リクエストログを用いて顧客とアプリケーションとのインタラクションを追跡することで、より幅広い視点からアプリケーションの信頼性を測定することができます。
参考リンク：
https://cloud.google.com/logging
https://cloud.google.com/monitoring
https://cloud.google.com/solutions/devops/devops-tech-monitoring-and-logging
</div></details>

### Q. 問題31: 未回答
大規模なサービス停止が発生し、サービスの全ユーザーが数時間にわたって影響を受けました。数時間のインシデント管理の後、サービスは通常に戻り、ユーザアクセスは回復しました。サイト信頼性エンジニアリングの推奨プラクティスに従って、関連するステークホルダーにインシデントのサマリーを提供する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. 関係者に配布するポストモーテムを作成します
2. 担当エンジニアに、すべてのステークホルダーに謝罪のメールを書くことを義務付けます
3. インシデント状態文書をすべてのステークホルダーに送付します
4. 個々のステークホルダーに電話をかけ、何が起こったかを説明します
<details><div>
    答え：1
説明
この問題では、大規模なサービス停止の発生後にステークホルダーに情報を提供するための最善の方法を選ぶ必要があります。設問に明示的に記されているサイト信頼性エンジニアリング（SRE）の推奨プラクティスについて理解することが重要です。SREの観点から、インシデント後の知見や経験を共有する手段を選択肢から選ぶ必要があります。また、選択肢を見る際には、情報共有の効率性や適切性に着目して評価することが求められます。
基本的な概念や原則：
ポストモーテム：サービスの停止や大きな問題が発生した後に行うレビューのことです。何が起こったのか、どう影響が出たのか、どう解決したのか、今後どう防止するのかを詳細に説明します。さらに、インシデントの原因と対策を共有し、再発防止に役立てます。
サイト信頼性エンジニアリング（SRE）：システムとアプリケーションの信頼性を維持し、障害を迅速に解決するためのエンジニアリングのプラクティスです。エンジニアはプロダクション環境の管理と改善に特化しています。
ステークホルダー：プロジェクトやサービスに関わりのある人や組織のことです。このコンテキストでは、サービスのユーザーや管理者、ビジネスパートナーなどが該当します。彼らはインシデントとその影響について知らされるべきです。
正解についての説明：
（選択肢）
・関係者に配布するポストモーテムを作成します
この選択肢が正解の理由は以下の通りです。
まず、"ポストモーテム"は、サービスの混乱や中断後のインシデント分析のことを指し、これはサイト信頼性エンジニアリングのベストプラクティスの一つとされています。ポストモーテムでは、何が起こったのか、なぜ起こったのか、どのようにそれが解決されたのか、そして同じ問題が再発するのを防ぐためにどのような措置が講じられているのかを詳細に分析し、記述します。
ポストモーテムを作成して関係者に配布することで、理解と学習の機会が促進され、信頼と信憑性が向上します。
また、同じ問題が再発する可能性を減らし、組織全体のレジリエンスを向上させるロールを果たします。
したがって、大規模なサービス停止後にインシデントのサマリーを提供するためには、ポストモーテムを作成し、それを関係者に配布するのが適切な方法となります。
不正解についての説明：
選択肢：個々のステークホルダーに電話をかけ、何が起こったかを説明します
この選択肢が正しくない理由は以下の通りです。
個々のステークホルダーに電話で説明すると、情報が一貫せず伝わらなかったり、説明を繰り返すことで時間がかかる可能性があります。ポストモーテムを作成することで、一貫した情報をステークホルダー全員に効率的に伝えることが可能です。
選択肢：インシデント状態文書をすべてのステークホルダーに送付します
この選択肢が正しくない理由は以下の通りです。
インシデント状態文書はインシデント発生中に随時更新されるものであり、後の振り返りや分析には不適切です。
それに対して、ポストモーテムはインシデント全体の要約、影響、原因、対応、そして今後の改善策をまとめたもので、ステークホルダーに対して詳細な情報を提供するのに適しています。
選択肢：担当エンジニアに、すべてのステークホルダーに謝罪のメールを書くことを義務付けます
この選択肢が正しくない理由は以下の通りです。
担当エンジニアに謝罪メールを書かせるだけでは、インシデントの原因、影響範囲、軽減措置、今後の予防策など、必要な情報が伝わらないため不適切です。
それに対して、ポストモーテムはこれらの情報をまとめ、ステークホルダーに詳細な状況を理解してもらうための有効な方法です。
参考リンク：
https://cloud.google.com/incident-response/docs/postmortem-culture
https://cloud.google.com/iam/docs/creating-managing-service-accounts
https://sre.google/sre-book/managing-incidents/
</div></details>

### Q. 問題32: 未回答
あなたはCIパイプラインを構成しています。CIパイプラインの統合テストのビルドステップでは、プライベートVPCネットワーク内のAPIにアクセスする必要があります。セキュリティチームは、APIトラフィックを公開しないことを要求しています。管理オーバーヘッドを最小化するソリューションを実装する必要があります。
この要件を満たすために、どうすればよいですか？
1. Spinnaker for Google Cloudを使ってプライベートVPCに接続します
2. Cloud Buildをパイプラインランナーとして使用します。APIアクセス用のGoogle Cloud Armorポリシーで外部HTTP(S)ロードバランシングを設定します
3. プライベートVPCに接続するために、Cloud Buildのプライベートプールを使用します
4. Cloud Buildをパイプラインランナーとして使用します。APIアクセス用に内部HTTP(S)ロードバランシングを設定します
<details><div>
    答え：3
説明
この問題では、プライベートVPCネットワーク内のAPIに安全にアクセスする方法と、その際の管理オーバーヘッドを最小化するための方法を探すことが求められています。CIパイプラインの統合テストのビルドステップからAPIにアクセスする必要があること、そしてAPIトラフィックを公開しないというセキュリティ要求があることを理解する必要があります。選択肢を見ると、各選択肢がこれらの要求にどのように対応しているかを問題が評価しています。
基本的な概念や原則：
Cloud Build：Google Cloudの継続的インテグレーション/デリバリー（CI/CD）プラットフォームです。アプリのビルド、テスト、デプロイを自動化します。
API：ソフトウェアアプリケーション間で情報を共有するためのインターフェースです。
プライベートVPC：仮想プライベートクラウド（VPC）は、Google Cloud上で定義されるプライベートネットワークです。プライベートIPアドレススペース、サブネット、ルーティングコントロールを提供します。
プライベートプール：Google Cloud Buildの機能で、特定のネットワークとリソースにアクセスできる専用の実行環境を提供します。
Spinnaker for Google Cloud：Google Cloud上でSpinnakerを簡単にセットアップし、継続的デリバリーを行うためのソリューションです。
内部HTTP(S)ロードバランシング：Google Cloudの内部ロードバランサーです。VPCネットワーク内でのHTTP(S)トラフィックをバランスします。
Google Cloud Armor：Google Cloudの分散型サービス拒否（DDoS）とアプリケーション攻撃防止サービスです。セキュリティルールとポリシーに基づいてトラフィックを制御します。
正解についての説明：
（選択肢）
・プライベートVPCに接続するために、Cloud Buildのプライベートプールを使用します
この選択肢が正解の理由は以下の通りです。
まず、Google CloudのCloud Buildのプライベートプールは、VPCネットワーク内でビルドを実行し、ユーザーのプライベートなIP空間にビルドプロジェクトを次回実行できるようにする機能です。これにより、セキュリティチームのAPIトラフィックを公開しないという要求が満たされます。
また、Cloud Buildのプライベートプールでは、Googleのプライベート通信ネットワークを使用してビルドプロジェクトがGoogle Cloudサービスにアクセスできるようになるため、安全性があります。
さらに、管理オーバーヘッドも最小化できます。これは、Cloud Buildはフルマネージド型のサービスであり、インフラストラクチャの管理やスケーリングについて心配する必要がないからです。
したがって、CIパイプラインでの安全なネットワーク接続と管理の簡素化には、Cloud Buildのプライベートプールの使用が最適です。
不正解についての説明：
選択肢：Spinnaker for Google Cloudを使ってプライベートVPCに接続します
この選択肢が正しくない理由は以下の通りです。
Spinnaker for Google Cloudを利用してプライベートVPCに接続することは可能ですが、この選択肢では管理オーバーヘッドが増大します。Spinnakerは設定も複雑で、メンテナンスも必要になります。
それに対して、Cloud Buildのプライベートプールは、VPC内でのビルドを可能にし、かつ管理オーバーヘッドを減らします。
選択肢：Cloud Buildをパイプラインランナーとして使用します。APIアクセス用に内部HTTP(S)ロードバランシングを設定します
この選択肢が正しくない理由は以下の通りです。
内部ロードバランシングの設定は、公開されていないAPIへアクセスするための一つの手法ですが、管理オーバーヘッドを増大させる可能性があります。
一方、Cloud Buildのプライベートプールを使用すると、管理オーバーヘッドを最小限に抑えながらプライベートVPC内のリソースに安全にアクセスできます。
選択肢：Cloud Buildをパイプラインランナーとして使用します。APIアクセス用のGoogle Cloud Armorポリシーで外部HTTP(S)ロードバランシングを設定します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Armorは、外部HTTP(S)ロードバランシングのセキュリティポリシーを作成・適用するサービスですが、APIトラフィックを公開する問題が解消されません。
一方、Cloud Buildのプライベートプールを使用すれば、パブリックなネットワークを経由せずにプライベートVPCに接続でき、求められる要件を満たします。
参考リンク：
https://cloud.google.com/build/docs/private-pools/private-pools-overview
https://cloud.google.com/build/docs/securing-builds/use-vpc-service-controls
https://cloud.google.com/solutions/setting-up-cloud-build-private-pool-secure-cicd
</div></details>

### Q. 問題33: 未回答
Cloud Monitoringのカスタムダッシュボードをパートナーチームと共有したいと考えています。
この要件を満たすために、どうすればよいですか？
1. ダッシュボードからMQL（Monitoring Query Language）クエリをコピーし、MLクエリをパートナーチームに送信します
2. ダッシュボードのJSON定義をダウンロードし、JSONファイルをパートナーチームに送信します
3. パートナーチームがダッシュボードのコピーを作成できるように、ダッシュボードのURLをパートナーチームに提供します
4. メトリクスをBigQueryにエクスポートします。Looker Studioを使用してダッシュボードを作成し、ダッシュボードをパートナーチームと共有します
<details><div>
    答え：2
説明
この問題では、Cloud Monitoringのカスタムダッシュボードの共有方法について理解しているかが試されています。ここで重要なのは、選択肢を評価する際に、ダッシュボードの表示情報自体を共有するのではなく、再現可能なダッシュボードの定義を共有を目指すという点です。パートナーチームが同じ情報状態を持つカスタムダッシュボードを自身の環境で設定することが目的なため、それに対応した解答を選んでいく必要があります。
基本的な概念や原則：
Cloud Monitoring：Google Cloudのインフラストラクチャ、アプリケーション、サービスのリアルタイムモニタリングを提供するサービスです。カスタムダッシュボードを作成可能で、数値データを視覚化し、アラート通知を設定できます。
カスタムダッシュボード：Cloud Monitoringで作成できるユーザー定義の監視ボードです。ダッシュボード上に表示するデータの選択や視覚化の形式を自由に設定できます。
ダッシュボードのJSON定義：ダッシュボードを構成するための情報を、JSONフォーマットで記述したものです。ダッシュボード上に表示するメトリクスや視覚化の形式を定義します。
BigQuery：大規模なデータセットを迅速に分析するための、フルマネージドなエンタープライズ向けデータウェアハウスサービスです。
Looker Studio：データを探索し、視覚化するためのツールです。BigQueryと組み合わせることで、ダッシュボードを作成し、データを視覚的に理解しやすくします。
MQL（Monitoring Query Language）：Cloud Monitoringで監視情報を抽出するための言語です。具体的な数値データを取り出すクエリを表現できます。
正解についての説明：
（選択肢）
・ダッシュボードのJSON定義をダウンロードし、JSONファイルをパートナーチームに送信します
この選択肢が正解の理由は以下の通りです。
まず、Cloud Monitoringのカスタムダッシュボードを共有する一つの方法は、そのダッシュボードのJSON定義をダウンロードし、それをパートナーチームに送るという手法です。このJSONファイルには、ダッシュボードのレイアウトそして設定が含まれています。"ダッシュボードを管理"画面からダッシュボードのJSONファイルをダウンロードし、それを他の人と共有することで、他のGoogle Cloudプロジェクトで同様のダッシュボードを再作成することが可能になります。
また、この方法を使用すれば、ダッシュボードの設定及び表示内容を正確に反映した状態で他のチームと共有できます。
したがって、パートナーチームは同じような監視を実現し、問題のある場合にすぐに気づくことができます。
このように、ダッシュボードのJSON定義をダウンロードし、それをパートナーチームに送ることで、Cloud Monitoringのカスタムダッシュボードを効率的に共有することができます。
不正解についての説明：
選択肢：パートナーチームがダッシュボードのコピーを作成できるように、ダッシュボードのURLをパートナーチームに提供します
この選択肢が正しくない理由は以下の通りです。
Google CloudのCloud MonitoringはダッシュボードのURLを共有しても、パートナーチームはそれを編集やコピーすることはできません。JSON定義をダウンロードして送ることで、再現性が確保されます。
選択肢：メトリクスをBigQueryにエクスポートします。Looker Studioを使用してダッシュボードを作成し、ダッシュボードをパートナーチームと共有します
この選択肢が正しくない理由は以下の通りです。
Cloud Monitoringのカスタムダッシュボードを共有したいという要件に対して、メトリクスをBigQueryにエクスポートし、Looker Studioで再度ダッシュボードを作成する必要はありません。正解の選択肢であるダッシュボードのJSON定義をダウンロードし、それをパートナーチームに送信することで、より簡潔かつ効率的に共有することが可能です。
選択肢：ダッシュボードからMQL（Monitoring Query Language）クエリをコピーし、MLクエリをパートナーチームに送信します
この選択肢が正しくない理由は以下の通りです。
MQLクエリを共有するだけでは、ダッシュボードの全体像や構造、ビジュアライゼーションの要素等は伝わりません。
それに対して、JSON定義を共有すると、ダッシュボードの設定や表示を完全に再現できるため、正しくダッシュボードを共有できます。
参考リンク：
https://cloud.google.com/monitoring/dashboards/api-dashboard
https://cloud.google.com/monitoring/dashboards/manage-dashboards
https://cloud.google.com/monitoring/mql/export
</div></details>

### Q. 問題34: 未回答
あなたは、Google Kubernetes Engine（GKE）クラスター上で動作する携帯電話ゲームのバックエンドをサポートしています。このアプリケーションは、ユーザーからのHTTPリクエストに対応しています。
あなたは、ネットワークコストを削減するソリューションを実装する必要があります。
この要件を満たすために、どうすればよいですか？
1. VPCをShared VPC Hostプロジェクトとして構成します
2. Google Cloud HTTP Load BalancerをIngressとして設定します
3. Kubernetesクラスターをプライベートクラスターとして設定します
4. スタンダードティアのネットワークサービスを設定します
<details><div>
    答え：2
説明
この問題では、Google Kubernetes Engine（GKE）クラスター上のアプリケーションのネットワークコストの削減が目的です。問題はHTTPリクエストに対応していること、そしてネットワークコストを削減しようとしていることがキーポイントとなります。したがって、選択肢からその目標を実現可能な方法を選択する必要があります。その際に、根本的なネットワーク設定やプロジェクトの構造の変更ではなく、実際にHTTPリクエストの処理方法に関わる選択肢を探すことが重要となります。
基本的な概念や原則：
Google Cloud HTTP Load Balancer：大規模なネットワークトラフィックを高速にルーティングするクラウドベースのロードバランサーです。グローバル及びリージョナルのトラフィックの管理が可能で、Google CloudのIngressとして使用することでネットワークコストを削減できます。
Ingress：Kubernetesのオブジェクトの一つで、クラスターへの外部からのアクセスを管理します。特定のサービスへのルーティングを制御するルールを定義します。
Shared VPC：Google Cloudの機能で、複数のプロジェクト間で同じVPCネットワークを共有できます。しかし、ネットワークコストの削減とは直接関係ありません。
Network Service Tiers：Google Cloudが提供するネットワーク品質の選択肢です。"プレミアムティア"はGoogleのグローバルネットワークを使用し、"スタンダードティア"はISPのネットワークを使用します。
プライベートクラスター：GKEの一部で、クラスター内のすべてのノードがプライベートGoogle Access設定を持つことです。これはネットワークコストの削減とは直接関係ありません。
正解についての説明：
（選択肢）
・Google Cloud HTTP Load BalancerをIngressとして設定します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud HTTP Load Balancerは全球的に分散され、高い可用性とネットワーク性能を提供します。これにより、ユーザーのリクエストが最も近いGoogle Cloudリージョンに自動でルーティングされるため、ネットワーク距離とそれに伴うネットワークコストが大幅に削減されます。
さらに、リクエストはクラスター内のポッドに均等に分散されるため、リソースの効率的な使用が可能になります。
また、Ingressを設定することで、GKEクラスターの負荷を管理し、ユーザートラフィックを適切なサービスにルーティングすることができます。これにより、アプリケーションのスケーラビリティと可用性を保つと共に、ネットワークコストの削減にも寄与します。
したがって、Google Cloud HTTP Load BalancerをIngressとして設定することは、ネットワークコストを削減するための適切な解決策となります。
不正解についての説明：
選択肢：VPCをShared VPC Hostプロジェクトとして構成します
この選択肢が正しくない理由は以下の通りです。
VPCをShared VPC Hostプロジェクトとして構成することは、ネットワークを異なるプロジェクト間で共有する機能ですが、直接的にネットワークコストを削減する効果はありません。
それに対して、Google Cloud HTTP Load BalancerをIngressとして設定することで、トラフィックを効率的に処理し、ネットワークコストを削減することが可能です。
選択肢：スタンダードティアのネットワークサービスを設定します
この選択肢が正しくない理由は以下の通りです。
スタンダードティアのネットワークサービスを設定することは、一部コストを削減できますが、アプリケーションがHTTPリクエストを対応する特性に対して具体的な対策を提供しないため最適な解決策ではありません。
一方、Google Cloud HTTP Load BalancerをIngressとして設定すると、インバウンドリクエストのエンドポイントを効率的に管理し、対応することで通信コストをより削減できるため、正解となります。
選択肢：Kubernetesクラスターをプライベートクラスターとして設定します
この選択肢が正しくない理由は以下の通りです。
プライベートクラスターを設定すると、クラスター内からの通信が内部のネットワーク経由となりますが、ユーザからのHTTPリクエストに応答するためのネットワークコスト削減には直接貢献しません。その点、Google Cloud HTTP Load BalancerをIngressとして設定すると、リクエストが適切に分散され、ネットワークコストを削減できます。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
https://cloud.google.com/kubernetes-engine/docs/how-to/http-load-balancing
https://cloud.google.com/network-tiers/docs/using-network-service-tiers
</div></details>

### Q. 問題35: 未回答
新しいサービスを本番環境にデプロイする必要があります。サービスはマネージドインスタンスグループを使用して自動的にスケールする必要があり、複数のリージョンにデプロイする必要があります。このサービスは各インスタンスに多数のリソースを必要とするため、キャパシティを計画する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. リソース要件が各リージョンで利用可能なプロジェクト枠の範囲内であることを検証します
2. 複数のリージョンにサービスを展開し、内部のロードバランサーを使ってトラフィックをルーティングします
3. Cloud Traceの結果を監視し、最適なサイジングを決定します
4. マネージドインスタンスグループのコンフィギュレーションで、n2-highcpu-96マシンタイプを使用します
<details><div>
    答え：1
説明
この問題では、自動的にスケールするサービスの本番環境へのデプロイとそのためのキャパシティ計画について考慮する必要があります。特に、サービスが各インスタンスに多数のリソースを必要とし、複数のリージョンにデプロイするという要件があるため、リソース管理と配置計画が問題のキーとなります。選択肢を判断する際には、サービスのリソース要件とリージョンごとのリソース容量を確認し、適切なリソース管理が行えるオプションを選択することが重要です。
基本的な概念や原則：
マネージドインスタンスグループ：Compute Engineで提供される機能で、同一の設定で多数のインスタンスをデプロイ、および自動スケーリングすることができます。
リージョン：Google Cloudは世界中に多数のリージョンを配置し、それぞれのリージョンには複数のゾーンが存在します。多地域展開はリージョンを跨って行います。
プロジェクト枠：Google Cloudの各リソースに対して設定されます。プロジェクト枠を超えると、そのリソースの作成や使用がブロックされます。
Cloud Trace：アプリケーションのパフォーマンス問題を診断し、それらの問題を可視化するためのGoogle Cloudのサービスです。
マシンタイプ：Compute Engineで提供される仮想マシンの設定です。CPU、メモリ、ストレージの量などはマシンタイプによります。
ロードバランサー：Google Cloudのサービスであり、入力トラフィックを複数のインスタンス間で自動的に分散する機能を提供します。冗長性と信頼性の向上に寄与します。
正解についての説明：
（選択肢）
・リソース要件が各リージョンで利用可能なプロジェクト枠の範囲内であることを検証します
この選択肢が正解の理由は以下の通りです。
Google Cloudのリソースの利用はプロジェクト全体、そしてリージョンごとにクォータ、つまり上限が設けられています。つまり、新しいサービスをデプロイする際には、そのサービスが必要とするリソースが、各リージョンで利用可能なプロジェクトのクォータ枠内に収まることが必須となります。たとえば、仮にサービスが多くのVMインスタンスを必要とする場合、その数が指定したリージョンのVMインスタンスのクォータを超えてしまうと、デプロイは失敗してしまいます。
したがって、デプロイの前には、サービスが必要とするリソースが利用可能なクォータ枠内に収まるかどうかを確認することが重要となります。この確認作業により、本番環境へのデプロイがスムーズに行え、計画的なキャパシティ管理を実施することが可能となります。
不正解についての説明：
選択肢：Cloud Traceの結果を監視し、最適なサイジングを決定します
この選択肢が正しくない理由は以下の通りです。
Cloud Traceはアプリケーションのパフォーマンスやトラブルシューティングを助けるツールであり、リソースサイジングやキャパシティプランニングツールではありません。
それに対して、正解はデプロイするサービスが各リージョンの利用可能なリソース範囲内で動くか検証することで、サービスの対応キャパシティを確認します。
選択肢：マネージドインスタンスグループのコンフィギュレーションで、n2-highcpu-96マシンタイプを使用します
この選択肢が正しくない理由は以下の通りです。
単にn2-highcpu-96マシンタイプを使用することは、自動スケーリングとリソース計画に直接対応しません。
一方、利用可能なプロジェクト枠内でリソース要件が収まることを検証する選択肢は、必要なリソースを確保し、予期しないリソース不足を防ぐことに繋がります。
選択肢：複数のリージョンにサービスを展開し、内部のロードバランサーを使ってトラフィックをルーティングします
この選択肢が正しくない理由は以下の通りです。
内部ロードバランサーは単一リージョン内でのバランシングのみに使用されます。複数のリージョンにデプロイされるサービスには適しておらず、そのため、この選択肢は要件を満たしません。
一方、正解の選択肢では、リソースの利用可能な範囲を検証し、各リージョンで要件を満たすことを確認します。
参考リンク：
https://cloud.google.com/compute/docs/regions-zones
https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#machine_types
https://cloud.google.com/compute/docs/autoscaler
</div></details>

### Q. 問題36: 未回答
あなたは最近Google Kubernetes Engine（GKE）にアプリケーションをデプロイし、現在アプリケーションの新しいバージョンをリリースする必要があります。新しいバージョンで問題が発生した場合に備えて、アプリケーションを以前のバージョンに即座にロールバックする機能が必要です。どのデプロイモデルを使うべきですか？
1. A/Bテストを実施し、デプロイ完了後も定期的にアプリケーションをテストします
2. ブルー/グリーンのデプロイメントを実行し、デプロイメント完了後に新しいアプリケーションをテストします
3. ローリングデプロイメントを実行し、デプロイメント完了後に新しいアプリケーションをテストします
4. カナリアデプロイメントを実行し、新しいバージョンがデプロイされた後、新しいアプリケーションを定期的にテストします
<details><div>
    答え：2
説明
この問題では、アプリケーションを新バージョンに更新する際のデプロイモデルと、問題が発生したときにすぐに前のバージョンに戻す機能が必要な状況の理解が求められています。提示された選択肢の中から、新バージョンに移行しつつ、同時に即座のロールバックが可能なモデルを選ぶことが重要となります。各デプロイモデルの特性を理解して、その具体的な動作を鑑みれば適切な選択が可能になります。
基本的な概念や原則：
ブルー/グリーンデプロイメント：新旧の環境を並行して立ち上げ、一瞬で切り替える手法です。新バージョンに問題がある場合は、即座に古いバージョンに戻すことが可能です。
Google Kubernetes Engine（GKE）：Google CloudのマネージドKubernetesサービスです。容易にKubernetesクラスターを作成、運用、スケールできます。
ローリングデプロイメント：新旧のバージョンを段階的に入れ替えていくデプロイ手法です。新バージョンの全体的な影響を少しずつ確認できますが、即座のロールバックは困難です。
A/Bテスト：２つまたはそれ以上の異なるバージョンを試し、ユーザーやシステムの反応を比較するテスト手法です。結果に基づいて最適なバージョンを選択します。
カナリアデプロイメント：新バージョンを一部のユーザーやシステムにのみ提供し、問題がなければ全体にロールアウトするデプロイ手法です。新バージョンの影響を徐々に確認します。
正解についての説明：
（選択肢）
・ブルー/グリーンのデプロイメントを実行し、デプロイメント完了後に新しいアプリケーションをテストします
この選択肢が正解の理由は以下の通りです。
まず、ブルー/グリーンデプロイメントモデルでは2つの環境、通常は"ブルー"（現行版）と"グリーン"（新版）を切り替えることで新旧バージョンを瞬時に切り替えることが可能です。これがロールバックを容易にし、新しいバージョンに問題が発生した際のリスクを軽減します。
また、このモデルでは新しいバージョンを本番環境でテストすることが可能なので、リリース前に未発見の問題を検出できます。このようにブルー/グリーンデプロイメントは、新バージョンのアプリケーションリリースと、エラーが発生した際の即座のロールバックを容易にするため、この設問の場面に最適です。
不正解についての説明：
選択肢：ローリングデプロイメントを実行し、デプロイメント完了後に新しいアプリケーションをテストします
この選択肢が正しくない理由は以下の通りです。
ローリングデプロイメントでは、新旧のアプリケーションが一時的に共存するため、全てのポッドに新バージョンが適用されるまで時間がかかる、即座にロールバックするのが難しいです。
それに対して、ブルー/グリーンデプロイメントでは、新旧の環境が完全に分離されているため、問題が発生した際にはすぐに前のバージョンに戻せます。
選択肢：A/Bテストを実施し、デプロイ完了後も定期的にアプリケーションをテストします
この選択肢が正しくない理由は以下の通りです。
A/Bテストは新しいバージョンのアプリケーションと既存バージョンの性能を比較するためのテスト手法であり、問題発生時の即時ロールバック機能とは関係がありません。
対照的に、ブルー/グリーンデプロイメントは、本番環境と完全に同じ条件で新しいバージョンをテストするため、必要に応じて即座に以前のバージョンに戻すことが可能です。
選択肢：カナリアデプロイメントを実行し、新しいバージョンがデプロイされた後、新しいアプリケーションを定期的にテストします
この選択肢が正しくない理由は以下の通りです。
カナリアデプロイメントは新しいバージョンを一部のユーザーにだけ公開し、動作確認をしますが、問題が生じた時に即座にロールバックは困難です。
一方、ブルー/グリーンデプロイメントでは全ユーザーに対して新旧バージョンの切り替えが可能なため、即座のロールバックが可能となります。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment
https://cloud.google.com/architecture/blue-green-deployment-gke
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rollback-to-an-earlier-deployment
</div></details>

### Q. 問題37: 未回答
あなたは、Google Cloud Operation Suite Workspacesを使用して本番環境のGoogle Cloudプロジェクトを監視するための戦略を開発しています。要件の1つは、開発プロジェクトやステージングプロジェクトから誤ったアラートを出さずに、本番環境の問題を迅速に特定して対応できるようにすることです。関連するチームメンバーにGoogle Cloud Operation Suite Workspacesへのアクセス権を提供する際には、最小権限の原則を確実に守りたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. 新しいGoogle Cloudモニタリングプロジェクトを作成し、その中にGoogle Cloud Operation Suiteワークスペースを作成します。本番プロジェクトをこのワークスペースにアタッチします。関連するチームメンバーに、Google Cloud Operation Suiteワークスペースへの読み取りアクセス権を付与します
2. 既存のGoogle Cloudプロダクションプロジェクトを選択し、監視ワークスペースをホストします。このワークスペースにプロダクションプロジェクトをアタッチします。関連するチームメンバーに、Google Cloud Operation Suiteワークスペースへの読み取りアクセス権を付与します
3. 関連するチームメンバーに、すべてのGoogle CloudプロダクションプロジェクトでProject Viewer IAMロールを付与します。各プロジェクト内にGoogle Cloud Operation Suiteワークスペースを作成します
4. 関連するチームメンバーに、すべてのGoogle Cloudプロダクションプロジェクトへの読み取りアクセス権を付与します。各プロジェクト内にGoogle Cloud Operation Suiteワークスペースを作成します
<details><div>
    答え：1
説明
この問題では、Google Cloud Operation Suite Workspacesを使用して監視を行いつつ、不要なアラートを避け、最小権限の原則を適用するための戦略が求められています。その要件を達成するために、適切なProjectとIAMロールの設定、Google Cloud Operation Suiteのワークスペースの作成と紐づけ方に関する理解が求められます。監視対象を限定し、関連メンバーに限定的なアクセス権を付与する方法を考慮すべきです。
基本的な概念や原則：
Google Cloud Operation Suite：Google Cloudの監視、トラブルシューティング、アプリケーションのパフォーマンス改善を実現するための統合監視ツールスイートです。
Workspaces：Google Cloud Operation Suiteの一部で、特定のGoogle Cloudプロジェクトの監視と警告を集中管理するための方法です。一つのWorkspaceに複数のプロジェクトをアタッチすることが可能です。
モニタリングプロジェクト：問題の特定と対応を目的としたプロジェクトです。プロダクション環境に影響を与える可能性のある誤ったアラートを避けるため、本番プロジェクトとは別に設定します。
最小権限の原則：セキュリティのベストプラクティスとして、ユーザーにはそれぞれのロールを遂行するのに必要な最小限の権限だけを付与するべきです。これにより、誤ってシステムに被害を与えるリスクが軽減されます。
読み取りアクセス権：データを表示するだけの権限で、データの変更や削除はできません。管理者は、関連するチームメンバーに対して機密情報を保護しながら必要な情報へのアクセスを許可するためにこれを使用します。
IAMロール：Identity and Access Managementの一部で、特定のアクションを行う権限を持つ一連の権限です。IAMロールを使用することで、ユーザーやサービスアカウントがGoogle Cloudのリソースにどのようにアクセスできるかを制御できます。
正解についての説明：
（選択肢）
・新しいGoogle Cloudモニタリングプロジェクトを作成し、その中にGoogle Cloud Operation Suiteワークスペースを作成します。本番プロジェクトをこのワークスペースにアタッチします。関連するチームメンバーに、Google Cloud Operation Suiteワークスペースへの読み取りアクセス権を付与します
この選択肢が正解の理由は以下の通りです。
まず、新しいGoogle Cloudモニタリングプロジェクトを作成することで、監視対象の本番環境と分離することができます。これにより、開発やステージング環境から誤ったアラートが発生するリスクを排除することが可能になります。
また、新しいプロジェクト内にGoogle Cloud Operation Suiteワークスペースを作成し、本番プロジェクトをこのワークスペースにアタッチすることで、本番環境の監視を一元的に管理することができます。
そして、関連するチームメンバーにGoogle Cloud Operation Suiteワークスペースへの読み取りアクセス権を付与することで、最小権限の原則を確実に守ることができます。読み取りアクセス権を許可することで、チームメンバーはアラートを確認し、本番環境の問題を迅速に特定して対応することが可能になりますが、書き込みなど不必要な操作は制限されるため、セキュリティリスクを最小限に抑えることができます。これらの要素が融合し、正解の選択肢が適切な戦略となる理由を構成しています。
不正解についての説明：
選択肢：関連するチームメンバーに、すべてのGoogle Cloudプロダクションプロジェクトへの読み取りアクセス権を付与します。各プロジェクト内にGoogle Cloud Operation Suiteワークスペースを作成します
この選択肢が正しくない理由は以下の通りです。
まず、すべてのプロジェクトに直接アクセス権を付与する方法は、最小権限の原則に違反します。
また、各プロジェクトで別々のワークスペースを作成すると、本番環境の問題を迅速に特定できなくなり、効率的な監視体制の構築には向いていません。
選択肢：関連するチームメンバーに、すべてのGoogle CloudプロダクションプロジェクトでProject Viewer IAMロールを付与します。各プロジェクト内にGoogle Cloud Operation Suiteワークスペースを作成します
この選択肢が正しくない理由は以下の通りです。
関連するチームメンバーにすべてのプロダクションプロジェクトでProject Viewer IAMロールを付与すると、彼らはプロジェクト全体のリソースを見ることができます。これは最小権限の原則に反します。
また、各プロジェクト内でワークスペースを作成すると、誤ったアラートの問題は解消されません。
選択肢：既存のGoogle Cloudプロダクションプロジェクトを選択し、監視ワークスペースをホストします。このワークスペースにプロダクションプロジェクトをアタッチします。関連するチームメンバーに、Google Cloud Operation Suiteワークスペースへの読み取りアクセス権を付与します
この選択肢が正しくない理由は以下の通りです。
既存の本番プロジェクトで監視ワークスペースをホストすると、本番環境へのアクセスが必要になるため、最小権限の原則が守れません。新しいモニタリングプロジェクトを作り、そこにワークスペースを作る方が、本番環境への影響を避けつつ監視を行うことが可能です。
参考リンク：
https://cloud.google.com/monitoring/workspaces/create
https://cloud.google.com/monitoring/access-control
https://cloud.google.com/iam/docs/understanding-roles
</div></details>

### Q. 問題38: 未回答
あなたのチームは最近、NGINXベースのアプリケーションをGoogle Kubernetes Engine（GKE）にデプロイし、HTTP Google Cloud Load Balancer（GCLB）Ingress経由で公開しました。あなたは適切なサービスレベルインジケータ（SLI）を使用して、アプリケーションのフロントエンドのデプロイをスケールさせたいと考えています。
この要件を満たすために、どうすればよいですか？
1. GKEで垂直ポッドオートスケーラを構成し、クラスターオートスケーラを有効にして、ポッドの拡張に合わせてクラスターをスケールします
2. LivenessプローブとReadinessプローブからの平均応答時間を使用するように、水平ポッドオートスケーラーを設定します
3. NGINX statsエンドポイントを公開し、NGINXデプロイメントによって公開されたリクエストメトリクスを使用するように水平ポッドオートスケーラーを構成します
4. Google Cloud Operation Suiteカスタムメトリクスアダプターをインストールし、GCLBが提供するリクエスト数を使用するように水平ポッドオートスケーラーを構成します
<details><div>
    答え：4
説明
この問題では、Google Kubernetes EngineにデプロイされたNGINXベースのアプリケーションをどのようにスケールするかが問われています。スケーリングの要件を満たすために最も適切なサービスレベルインジケータ（SLI）は何か考慮する必要があります。更にGoogle Cloud Load Balancer（GCLB）の利用、水平ポッドオートスケーラーの設定など、いくつかの補助的な技術要素も示されています。選択肢を見ると、オートスケーラーの設定、メトリクスの監視、応答時間の利用など、複数のアプローチが示されています。これらのアプローチの中から要件を最も正確に満たすものを選ぶことが求められているのです。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するマネージドKubernetesサービスです。Kubernetesのセットアップ、構成、管理の作業を軽減します。
Google Cloud Load Balancer（GCLB）：Google Cloudのスケーラブルで高性能な負荷分散サービスです。HTTP(S), TCP/SSL, UDPなどのトラフィックを分散します。
サービスレベルインジケータ（SLI）：サービスの品質を評価するための一連の指標です。しばしばサービスレベルオブジェクト（SLO）と組み合わせて使用されます。
Google Cloud Operation Suite：Google Cloudの運用管理サービス群です。ログ、メトリクス、トレーシングを統合的に扱うことが可能です。
水平ポッドオートスケーラー：Kubernetesのリソースで、CPU使用率やカスタムメトリクスなどに基づいてポッドの数を自動的にスケールさせます。
LivenessプローブとReadinessプローブ：Kubernetes上のアプリケーションの動作をチェックするための仕組みです。これらの結果に基づいて、Kubernetesはアプリケーションの回復処理を自動化します。
垂直ポッドオートスケーラ：Kubernetesのリソースで、ポッドのCPUやメモリリソースを自動的にスケールさせます。アプリケーションのリソース使用率に対応してパフォーマンスを最適化します。
正解についての説明：
（選択肢）
・Google Cloud Operation Suiteカスタムメトリクスアダプターをインストールし、GCLBが提供するリクエスト数を使用するように水平ポッドオートスケーラーを構成します
この選択肢が正解の理由は以下の通りです。
Google Cloud Operation Suite（以前はStackdriverとして知られていました）は、Google Cloudのアプリケーションやインフラストラクチャの監視、トラブルシューティング、および診断に役立つツールの一連りを提供します。カスタムメトリクスアダプターをインストールすることで、Google Cloud Load Balancer（GCLB）からリクエスト数のようなメトリクスを取得できます。
これらのメトリクスは、最も一般的なフロントエンドのSLI（サービスレベル指標）の一つであるリクエストレートに基づいて、アプリケーションのデプロイをスケールさせるための水平ポッドオートスケーラー（HPA）の構成に使用できます。この方法により、ユーザーへのサービスできる能力を保ちつつ、負荷に応じてスケーリングが自動化され、リソース使用が最適化されます。HPAはその名の通り、ポッドレベルでの自動スケーリングを提供するKubernetesの機能です。これにより、リクエストレートという増減するトラフィックに適応することが可能となります。これらの理由から、この選択肢は正解となります。
不正解についての説明：
選択肢：LivenessプローブとReadinessプローブからの平均応答時間を使用するように、水平ポッドオートスケーラーを設定します
この選択肢が正しくない理由は以下の通りです。
LivenessプローブとReadinessプローブを使って水平ポッドオートスケーラーを構成すると、プローブ自身の結果によりスケーリングされますが、これはユーザがサービスに対する実際のリクエスト状況を基にしたスケーリングではありません。そのため、GCLBのリクエスト数を基に設定するのが適当です。
選択肢：GKEで垂直ポッドオートスケーラを構成し、クラスターオートスケーラを有効にして、ポッドの拡張に合わせてクラスターをスケールします
この選択肢が正しくない理由は以下の通りです。
垂直ポッドオートスケーラはポッドのリソース（CPUやメモリ）を自動的に調整しますが、これはリクエスト数に基づいてスケールするための解決策ではありません。正解はリクエスト数をベースにスケーリングできる水平ポッドオートスケーラーを使うことです。
選択肢：NGINX statsエンドポイントを公開し、NGINXデプロイメントによって公開されたリクエストメトリクスを使用するように水平ポッドオートスケーラーを構成します
この選択肢が正しくない理由は以下の通りです。
NGINX statsエンドポイントはアプリケーションのリクエストメトリクスを取得できますが、この問題で要求されているのはフロントエンドのスケーリングです。そのため、GCLB（フロントエンド）のリクエスト数を使用する正解の選択肢がより適切です。NGINX statsエンドポイントはフロントエンドではなく、アプリケーション自体の情報を提供します。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
</div></details>

### Q. 問題39: 未回答
あなたは、Spinnakerを使用してアプリケーションをデプロイし、パイプラインにカナリアデプロイメントステージを作成しました。アプリケーションには、開始時にオブジェクトをロードするインメモリキャッシュがあります。あなたは、カナリアバージョンと本番バージョンの比較を自動化したいと考えています。
カナリアリリースの分析をどのように構成しますか？
1. カナリアと、過去の本番バージョンのスライディングウィンドウの平均パフォーマンスを比較します
2. カナリアと以前の本番バージョンの新しいデプロイメントを比較します
3. カナリアと現在の本番バージョンの既存のデプロイメントを比較します
4. カナリアと、現在の本番バージョンの新しいデプロイメントを比較します
<details><div>
    答え：4
説明
この問題では、カナリアデプロイメントというリリース戦略を用いてアプリケーションをデプロイする際のベストプラクティスについて問われています。注目すべき点は、アプリケーションが開始時にオブジェクトをロードするインメモリキャッシュが存在するという特性と、カナリアバージョンと本番バージョンの比較を自動化したいとの要求です。選択肢を検討する際には、新旧バージョン間のフェアな比較ができるようなデプロイメントステージの比較設定を考えることが重要となります。
基本的な概念や原則：
Spinnaker：CI/CDパイプラインを管理するためのマルチクラウドプラットフォームです。アプリケーションのテスト、承認、デプロイを効率的に実行します。
カナリアデプロイメント：新しいバージョンの影響を最小限に抑えるためのデプロイメント戦略です。新しいバージョンを一部のユーザーに限定してリリースし、パフォーマンスと影響を評価します。
インメモリキャッシュ：アプリケーションのパフォーマンスを向上させるために使用されるメモリ内のストレージ領域です。頻繁にアクセスされるデータを高速に取得するために使用されます。
新しいデプロイメントとの比較：新規デプロイメントは、一般的に前回のデプロイメントと異なる特性を持つため、新しいバージョンと既存のバージョンを比較して評価することが重要です。
正解についての説明：
（選択肢）
・カナリアと、現在の本番バージョンの新しいデプロイメントを比較します
この選択肢が正解の理由は以下の通りです。
まず、1つの重要な点は、カナリアバージョンが起動時にオブジェクトをロードしてキャッシュするという事実です。それゆえに、既存の稼働中の本番インスタンスとカナリアインスタンスを比較すると、キャッシュが完全に充填されて性能が安定するまでの時間差の影響を受けてしまう可能性があります。そこで、新しくデプロイされた本番バージョンのインスタンスとカナリアバージョンを比較する方が望ましいです。これは、両者とも同時に起動したばかりであり、キャッシュの充填状態がより似ています。
このように、比較の公平性と正確性を保つためには、新しいデプロイメントの本番バージョンとカナリアバージョンを比較するのが最良の選択となります。
不正解についての説明：
選択肢：カナリアと以前の本番バージョンの新しいデプロイメントを比較します
この選択肢が正しくない理由は以下の通りです。
カナリアと以前の本番バージョンの新しいデプロイメントを比較すると、本番環境の現状を把握することが難しくなります。この方法では、現在の本番環境の挙動やパフォーマンスを反映できず、カナリアリリースの分析に必要な現状データが欠けてしまいます。
これに対し、カナリアバージョンと現行本番バージョンを比較することで、最新の本番環境の状況をもとに適切な分析が可能となります。
選択肢：カナリアと現在の本番バージョンの既存のデプロイメントを比較します
この選択肢が正しくない理由は以下の通りです。
現在の本番バージョンの既存デプロイメントとカナリアを比較すると、本番バージョンの既存デプロイメントがキャッシュにより結果が歪む可能性があります。比較するためには、セッションのスタートから終了までの結果を対象とするべきで、そのためには本番バージョンも新たにデプロイする必要があります。
選択肢：カナリアと、過去の本番バージョンのスライディングウィンドウの平均パフォーマンスを比較します
この選択肢が正しくない理由は以下の通りです。
カナリアと過去の本番バージョンのスライディングウィンドウの平均パフォーマンスを比較する方法では、最新の本番バージョンの挙動を適切に反映しきれない可能性があります。
それに対して、カナリアと現在の本番バージョンの新しいデプロイメントを比較することで、直近の実際の挙動との対比が可能となり、より正確なカナリアリリースの分析が可能になります。
参考リンク：
https://cloud.google.com/solutions/continuous-delivery/spinnaker-on-Google Cloud
https://cloud.google.com/architecture/automated-canary-analysis-kayenta-spinnaker
https://spinnaker.io/concepts/
</div></details>

### Q. 問題40: 未回答
あなたは、europe-west2-aゾーンの単一のCompute EngineインスタンスにデプロイされているステートレスWebベースAPIをサポートしています。サービスの可用性に関するサービスレベル指標（SLI）が、指定されたサービスレベル目標（SLO）を下回っています。ポストモーテムにより、APIへのリクエストが定期的にタイムアウトすることが判明しました。タイムアウトの原因は、APIのリクエスト数が多く、メモリが不足していることです。あなたはサービスの可用性を改善したいと考えています。
この要件を満たすために、どうすればよいですか？
1. 他のゾーンに追加のサービスインスタンスをセットアップし、すべてのインスタンス間でトラフィックの負荷分散を行います
2. より多くのメモリを搭載した、よりハイスペックなコンピュートインスタンスにサービスを移行します
3. 他のゾーンに追加のサービスインスタンスをセットアップし、プライマリインスタンスが利用できない場合のフェイルオーバーとして使用します
4. 指定されたSLOを、測定されたSLIと一致するように変更します
<details><div>
    答え：1
説明
この問題では、負荷分散を用いてCompute Engine上のWebベースAPIの可用性を改善する方法を求めています。問題では、APIへのリクエストが多すぎてメモリが不足し、リクエストのタイムアウトが発生していることが明らかにされています。セットアップ、リソースのスケーリング、そして負荷分散の手法がこの状況を改善するための選択肢となりますが、ソリューションを選ぶためにはその適切な応用が重要です。負荷分散を導入することで、メモリが足りなくなる問題を緩和あり、良好なパフォーマンスを維持することができます。
基本的な概念や原則：
Compute Engine：Google Cloudの仮想マシンを提供するサービスです。アプリケーションを柔軟にデプロイでき、スケーラビリティと効率性を実現します。
ステートレス：アプリケーションがユーザーセッション情報を保持しない状態を指します。スケーリングや冗長性の展開に役立つ原則です。
サービスレベル指標（SLI）：サービスのパフォーマンスや可用性の計測基準です。この数値は、サービスレベル目標（SLO）と比較され、サービスの品質を評価するために使用されます。
サービスレベル目標（SLO）：サービスが達成すべき品質やパフォーマンスの目標です。これは、サービスプロバイダーと顧客の間で契約され、SLIと比較されます。
負荷分散：トラフィックやワークロードを複数のサーバーまたはリソースに分散する技術です。これにより、高可用性やレスポンスタイムの改善が期待できます。
ゾーン：Google Cloudのリージョン内のデータセンターの場所を指します。ゾーン間での負荷分散は、サービスの可用性を高めるのに有効です。
メモリ不足：コンピューティングリソース（仮想マシン等）が、動作に必要なメモリ（RAM）を十分に持っていない状態です。これは、パフォーマンスに大きな影響を及ぼす可能性があります。
正解についての説明：
（選択肢）
・他のゾーンに追加のサービスインスタンスをセットアップし、すべてのインスタンス間でトラフィックの負荷分散を行います
この選択肢が正解の理由は以下の通りです。
まず、タイムアウトの主な原因は、APIリクエストの数が多くメモリが不足することにあると報告されています。単一のCompute Engineインスタンスでは、この負荷を扱う能力が限られているため、サービスの可用性が目標を満たせない状況が生じています。そのため、更なるリソース、つまり更なるインスタンスの追加が必要です。
また、複数ゾーンにインスタンスを設置することで単一のゾーンに依存しない冗長性も持つことができ、これによりサービスの可用性も向上します。
さらに、トラフィックの負荷分散を行うことで、各インスタンスにリクエストが均等に配分され、メモリの不足によるタイムアウトが軽減されることが期待できます。
以上の理由から、追加のインスタンスをセットアップし、トラフィックの負荷分散を行うことが、サービスの可用性を改善するための適切な解決策となります。
不正解についての説明：
選択肢：指定されたSLOを、測定されたSLIと一致するように変更します
この選択肢が正しくない理由は以下の通りです。
指定されたSLOを変更するというアプローチはシステムの性能問題を解決しません。実際の問題は、負荷が高くなるとリクエストがタイムアウトすることであり、これは単にSLOの目標を変更することでは修正できません。正しいアプローチは、負荷分散を行いシステムリソースの使用を最適化することです。
選択肢：より多くのメモリを搭載した、よりハイスペックなコンピュートインスタンスにサービスを移行します
この選択肢が正しくない理由は以下の通りです。
より多くのメモリを搭載したコンピュートインスタンスに移行すると、確かに現在の問題を解決できますが、トラフィックの増大による同様の問題が再発する可能性があります。
一方で、正解の選択肢として複数ゾーンにサービスインスタンスを分散し負荷を分散させる方法は、サービスの拡張性と可用性を高めます。
選択肢：他のゾーンに追加のサービスインスタンスをセットアップし、プライマリインスタンスが利用できない場合のフェイルオーバーとして使用します
この選択肢が正しくない理由は以下の通りです。
フェイルオーバー設定では、主要なインスタンスが利用できない場合のみ追加のサービスインスタンスが活用されます。しかし、問題は可用性とパフォーマンスであり、すべてのインスタンス間で負荷分散を行うことでこれを解決する必要があります。なので、フェイルオーバーとして使用する選択肢は不適切です。
参考リンク：
https://cloud.google.com/load-balancing/docs/load-balancing-overview
https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources
https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances
</div></details>

### Q. 問題41: 未回答
最近、あるeコマースアプリケーションをGoogle Cloudに移行しました。次のピークトラフィックシーズンに向けてアプリケーションを準備する必要があります。あなたは、Googleが推奨するプラクティスに従いたいと考えています。
繁忙期に備えて、まず何をすべきですか？
1. アプリケーションの負荷テストを行い、スケーリングのためのパフォーマンスをプロファイリングします
2. アプリケーションの基盤となるインフラストラクチャのTerraform設定を作成し、追加リージョンに素早くデプロイします
3. アプリケーションをCloud Runに移行し、オートスケーリングを使用します
4. 昨年に使用されたコンピュートパワーの追加を事前にプロビジョニングし、成長を期待します
<details><div>
    答え：1
説明
この問題では、ピークトラフィックシーズンに向けてeコマースアプリケーションをどのように準備するかを問われています。選択肢と設問をよく理解し、Googleが推奨しているベストプラクティスに沿った手段を選ぶことが重要です。つまり、単にリソースを増やすだけでなく、実際のクラウド環境でアプリケーションの挙動やパフォーマンスを慎重に評価する策が求められます。
基本的な概念や原則：
負荷テスト：システムやアプリケーションが最大容量の状態で問題なく機能するかを確認するためのテストです。パフォーマンスと信頼性の問題を特定するために用いられます。
パフォーマンスプロファイリング：アプリケーションの実行時間やメモリ消費を詳細に解析することで、ボトルネックや最適化の余地を見つけ出すプロセスです。
スケーリング：ワークロードの需要に応じてシステムリソースを増減させる方法です。パフォーマンスを保持しつつコストを最適化するために重要です。
Cloud Run：コンテナ化されたアプリケーションをフルマネージドで実行するサービスです。自動スケーリングの機能を提供しますが、全てのアプリケーションへの適応は保証されません。
Terraform：インフラストラクチャをコードとして管理し、バージョン管理と再利用を可能にするツールです。迅速なデプロイやマルチクラウド対応を実現しますが、必ずしもパフォーマンス最適化に直結するわけではありません。
プロビジョニング：システムが必要とするリソースを配置し、管理するプロセスです。需要に応じたリソース割り当てが重要となりますが、過度なプロビジョニングはコスト増加を招く可能性があります。
正解についての説明：
（選択肢）
・アプリケーションの負荷テストを行い、スケーリングのためのパフォーマンスをプロファイリングします
この選択肢が正解の理由は以下の通りです。
まず、ピークトラフィックシーズンを迎える前に、アプリケーションの負荷テストを実施することは、アプリケーションの性能を確認し、負荷増加に対する準備度を測るために重要です。負荷テストを通じてアプリケーションの弱点を発見し、その改善に取り組むことでピーク時のパフォーマンス低下やシステムの障害を防ぐことができます。
また、パフォーマンスのプロファイリングは、システムのリソース使用率、応答時間、スループットなどを監視し解析することで、アプリケーションのスケーリングの需要と方向を理解するのに役立ちます。具体的には、求められるパフォーマンスを維持するために、具体的な負荷レベルでどの程度のリソースが必要であるかを把握することができます。
したがって、負荷テストとパフォーマンスのプロファイリングは、繁忙期にアプリケーションをスムーズに運用し、ユーザー体験を維持するうえで、適切な最初の手順と言えます。
不正解についての説明：
選択肢：アプリケーションをCloud Runに移行し、オートスケーリングを使用します
この選択肢が正しくない理由は以下の通りです。
アプリケーションをCloud Runに移行し、オートスケーリングを使用するのは可能な解決策ですが、最初に実行すべきなのは負荷テストとパフォーマンスプロファイリングです。これは、これがアプリケーションがピークトラフィックに対応可能かどうかを判断するための重要な情報を提供し、その後のスケーリング戦略を決定するのに役立つからです。
選択肢：アプリケーションの基盤となるインフラストラクチャのTerraform設定を作成し、追加リージョンに素早くデプロイします
この選択肢が正しくない理由は以下の通りです。
与えられた問題の目的はピークトラフィック時にアプリケーションが適切に動作することを確認することであり、そのためには負荷テストとパフォーマンスのプロファイリングが必要です。たとえ新たに追加リージョンにデプロイできるTerraform設定を作成したとしても、アプリケーションがピークトラフィックに適応する能力を評価するものではありません。
選択肢：昨年に使用されたコンピュートパワーの追加を事前にプロビジョニングし、成長を期待します
この選択肢が正しくない理由は以下の通りです。
昨年のコンピュートパワーを単純にプロビジョニングしてしまうと、過剰なリソースの確保や予測に合わないパフォーマンスが出る可能性があります。そのため、具体的な負荷テストとパフォーマンスプロファイリングを行い、必要なスケーリングを正確に把握するのが適切です。
参考リンク：
https://cloud.google.com/architecture/capacity-planning-for-compute-engine
https://cloud.google.com/solutions/testing-and-performance-tuning-an-app-engine-standard-environment
https://cloud.dev/posts/how-to-get-started-with-load-testing-using-locust-on-google-cloud
</div></details>

### Q. 問題42: 未回答
Cloud Run上で動作するアプリケーションがあります。品質保証チームに手動テストを実行させながら、ライブの本番トラフィックを使用してアプリケーションの新バージョンをテストしたいと考えています。新バージョンのテスト中に発生する可能性のある問題の影響を制限し、必要に応じて以前のバージョンのアプリケーションにロールバックできなければなりません。
新バージョンをどのようにデプロイしますか？（2つ選択）
1. 新しいアプリケーションバージョンをデプロイし、--no-trafficオプションを使用します。本番環境のトラフィックをリビジョンのURLにルーティングします
2. 新しいCloud Runリビジョンをタグなしでデプロイし、--no-trafficオプションを使用します
3. 新しいCloud Runリビジョンをタグ付きでデプロイし、--no-trafficオプションを使用します
4. 新しいアプリケーションバージョンをデプロイし、トラフィックを新しいバージョンに分割します
5. アプリケーションを新しいCloud Runサービスとしてデプロイします
<details><div>
    答え：3,4
説明
この問題では、Cloud Run上で動作するアプリケーションの新バージョンを本番環境でテストし、同時に旧バージョンへのロールバックが可能なデプロイ方法を求めています。問題の点は、新バージョンに対するトラフィックを制御しつつ、問題が発生した場合にも旧バージョンに素早く戻すことができるようにすることです。したがって、デプロイオプションを選択する際には、これらを満たすものを選ぶ必要があります。また同時に異なるバージョンを複数扱う必要があるため、各バージョンの管理も重要な要素となります。
基本的な概念や原則：
Cloud Run：コンテナ化されたアプリケーションをフルマネージドで実行するGoogle Cloudのサービスです。スケールアップとスケールダウンの自動管理が可能です。
リビジョン：Cloud Runのアプリケーションが更新されるたびに作成される、アプリケーションの特定のバージョンを指します。
タグ付きデプロイ：新しいリビジョンを作成し、そのリビジョンに固有の名前を割り当てるデプロイ方法です。トラフィックの分割やロールバックを容易に行うことができます。
--no-trafficオプション：Cloud Runのデプロイコマンドで使用するオプションです。新しいリビジョンを作成する際に、自動的にトラフィックのルーティングを行わないように設定します。
トラフィックの分割：複数のアプリケーションバージョン（リビジョン）へのトラフィックを制御する機能です。テストやステージング環境で新バージョンの挙動を確認するのに使用します。
ロールバック：問題が発生した場合に、以前のバージョンに戻すことを指します。Cloud Runのタグ付きデプロイを使用することで簡易的にロールバックが可能です。
正解についての説明：
（選択肢）
・新しいCloud Runリビジョンをタグ付きでデプロイし、--no-trafficオプションを使用します
・新しいアプリケーションバージョンをデプロイし、トラフィックを新しいバージョンに分割します
この選択肢が正解の理由は以下の通りです。
まず、"新しいCloud Runリビジョンをタグ付きでデプロイし、--no-trafficオプションを使用する"ことは、新バージョンをライブ環境に展開する際の問題を未然に防止します。これにより、新バージョンがライブの本番トラフィックを受けることなく、品質保証チームが手動テストを行うことができます。
その上で、問題がないと確認された後、"新しいアプリケーションバージョンをデプロイし、トラフィックを新しいバージョンに分割する"ことで、本番トラフィックを新旧のバージョン間で分散できます。これにより、新バージョンで予期せぬ問題が発生した場合でも影響を制限し、トラフィックの割合を調整することで迅速に前のバージョンにロールバックすることが可能です。このような段階的なデプロイ戦略は、ユーザーエクスペリエンスへのネガティブな影響を最小限に抑えつつ新バージョンのリリースを管理するための最適なアプローチとなります。
不正解についての説明：
選択肢：アプリケーションを新しいCloud Runサービスとしてデプロイします
この選択肢が正しくない理由は以下の通りです。
新しいCloud Runサービスとしてデプロイすると、新旧のバージョンの間でトラフィックの分割やロールバックが容易には出来なくなってしまいます。ライブの本番トラフィックを使用したテストとロールバックのためには、同一のサービス内でのリビジョン管理が適しています。
選択肢：新しいCloud Runリビジョンをタグなしでデプロイし、--no-trafficオプションを使用します
この選択肢が正しくない理由は以下の通りです。
新しいリビジョンをタグなしでデプロイすると、品質保証チームが指定された新しいバージョンに直接アクセスすることが難しくなります。タグはリビジョンの特定と疎通を容易にするため、新しいリビジョンのテストには不可欠な機能です。
選択肢：新しいアプリケーションバージョンをデプロイし、--no-trafficオプションを使用します。本番環境のトラフィックをリビジョンのURLにルーティングします
この選択肢が正しくない理由は以下の通りです。
この選択肢では、--no-trafficオプションを使用しながら全ての本番トラフィックを新リビジョンにルーティングすると提案しています。しかし、これは本番環境のトラフィックに問題が発生する恐れがあります。品質検証中には新バージョンに対するトラフィックを限定的にし、問題が発生した場合にロールバックできるようにするべきです。
参考リンク：
https://cloud.google.com/run/docs/managing/traffic
https://cloud.google.com/run/docs/rolling-out-changes
https://cloud.google.com/run/docs/testing-new-revisions
</div></details>

### Q. 問題43: 未回答
あなたの組織では、複数のGoogle Cloudプロジェクトからのすべてのアプリケーションログを、中央のCloud Loggingプロジェクトに保存しています。セキュリティチームは、各プロジェクトチームはそれぞれのログしか閲覧できず、運用チームだけがすべてのログを閲覧できるというルールを強制したいと考えています。コストを最小限に抑えながら、セキュリティチームの要件を満たすソリューションを設計する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. 各プロジェクトチームにIDおよびIAMロールを作成し、個々のGoogle Cloudプロジェクトで _Defaultログビューへのアクセスを制限します。中央ログプロジェクトの運用チームにビューアアクセスを許可します
2. プロジェクトチームごとにログビューを作成し、各プロジェクトチームのアプリケーションログのみを表示します。運用チームに、中央ログプロジェクトの _AllLogsビューへのアクセス権を与えます
3. 各プロジェクトチームに、中央ログプロジェクトのプロジェクト_デフォルトビューへのアクセス権を付与します。中央ロギングプロジェクトで、運用チームにトギングビューアへのアクセス権を付与します
4. 各プロジェクトチームのBigQueryテーブルにログをエクスポートします。プロジェクトチームにテーブルへのアクセス権を付与します。ログのライターに、中央ログプロジェクトのオペレーションチームへのアクセス権を付与します
<details><div>
    答え：2
説明
この問題では、複数のGoogle Cloudプロジェクトからのアプリケーションログを中央のCloud Loggingプロジェクトに保存し、セキュリティ要件に合ったルール設計をコストを最小限に抑えて行う手法が求められています。プロジェクトチームは各自のログのみを閲覧可能で、運用チームのみが全ログを閲覧可とする要件について考えなければなりません。これを達成するためには、Google Cloudのアクセス管理とログビューの機能を理解し適切に設定できることが重要です。エクスポートやビューの作成、アクセス権の付与などがキーファクターとなります。
基本的な概念や原則：
Cloud Logging：Google Cloudのロギングサービスです。アプリケーションのログを中央化して収集、分析、可視化することができます。
ログビュー：Cloud Loggingの特定のログエントリへのアクセスを制御する機能です。特定のプロジェクトやリソースのみのログを表示するビューを作成できます。
IAM（Identity and Access Management）：Google Cloudのユーザーやサービスアカウントへのリソースアクセスを管理するサービスです。特定のロールを付与して、リソースへのアクセスを制御します。
ロールとアクセス権：IAMでは、ロールを付与することでユーザーやサービスアカウントのアクセス権を制御します。ロールは一連の許可を持つことで、どの種類の操作が許されるかを示します。
BigQuery：Google Cloudのビッグデータ分析サービスです。一般的には、ロギングデータをエクスポートするための選択肢の一つですが、ロギングの閲覧権限を制御する目的には適していません。
正解についての説明：
（選択肢）
・プロジェクトチームごとにログビューを作成し、各プロジェクトチームのアプリケーションログのみを表示します。運用チームに、中央ログプロジェクトの _AllLogsビューへのアクセス権を与えます
この選択肢が正解の理由は以下の通りです。
まず、ログビュー機能を使うことによって、異なるGoogle Cloudプロジェクトからのすべてのアプリケーションログを一箇所で閲覧でき、特定のログ情報のみをフィルタリングして表示することが可能となります。
また、これによってプロジェクトチームごとにログビューを作成し、各プロジェクトチームのアプリケーションログのみを表示することが可能となります。
さらに、Cloud Loggingにおける _AllLogsビューは、すべてのログを閲覧することができるビューで、これへのアクセス権を運用チームに与えることにより、運用チームだけがすべてのログを閲覧できる要件を満たすことができます。
最後に、このアプローチは、Cloud Loggingのビュー機能を活用するだけであり、追加のコストをほとんど発生させることなくセキュリティチームの要件を満たすため、コストを最小限に抑えることができます。
不正解についての説明：
選択肢：各プロジェクトチームに、中央ログプロジェクトのプロジェクト_デフォルトビューへのアクセス権を付与します。中央ロギングプロジェクトで、運用チームにトギングビューアへのアクセス権を付与します
この選択肢が正しくない理由は以下の通りです。
各プロジェクトチームに中央ログプロジェクトのプロジェクト_デフォルトビューへのアクセス権を付与すると、他のプロジェクトのログも閲覧が可能となり、要件が満たせません。適切なビューを作成してアクセス権を制御する必要があります。
選択肢：各プロジェクトチームにIDおよびIAMロールを作成し、個々のGoogle Cloudプロジェクトで _Defaultログビューへのアクセスを制限します。中央ログプロジェクトの運用チームにビューアアクセスを許可します
この選択肢が正しくない理由は以下の通りです。
IAMロールを使ってログの閲覧を制限することは可能ですが、各プロジェクトチームごとにIDとIAMロールを作成することは管理が複雑化し、コストを抑えるという目的に反します。
逆に、ログビューを利用すればログの閲覧を柔軟に制御でき、尚かつ、コストも抑えられます。
選択肢：各プロジェクトチームのBigQueryテーブルにログをエクスポートします。プロジェクトチームにテーブルへのアクセス権を付与します。ログのライターに、中央ログプロジェクトのオペレーションチームへのアクセス権を付与します
この選択肢が正しくない理由は以下の通りです。
各プロジェクトチームのBigQueryテーブルにログをエクスポートすると、アドミニストレーションの複雑性が増すだけでなく、エクスポートに時間とコストがかかります。これに比べ、ログビューを作成する方が直接Cloud Loggingを使用しコスト効率が良く、制御も容易であるため適切です。
参考リンク：
https://cloud.google.com/logging/docs/view/overview
https://cloud.google.com/logging/docs/iam
https://cloud.google.com/logging/docs/logs-views-access-control
</div></details>

### Q. 問題44: 未回答
あなたは、明確に定義されたサービスレベル目標（SLO）を持つサービスをサポートしています。過去6ヶ月間、あなたのサービスは常にSLOを達成し、顧客満足度は常に高いです。サービスの運用タスクのほとんどは自動化されており、頻繁に発生する反復タスクはほとんどありません。サイトの信頼性エンジニアリングのベストプラクティスに従いながら、信頼性と展開速度のバランスを最適化したいと考えています。
この要件を満たすために、どうすればよいですか？（2つ選択）
1. サービスの展開速度やリスクを高めます
2. サービスレベル指標（SLI）の実施方法を変更し、カバー率を高めます
3. サービスのSLOをもっと厳しくします
4. エンジニアリングの時間を、より信頼性を必要とする他のサービスに振り向けます
5. 製品チームに、新機能よりも信頼性作業を優先させます
<details><div>
    答え：1,4
説明
この問題では、サービスが安定しておりかつ顧客満足度も高い状況で、信頼性と展開速度のバランスを最適化する方法を尋ねています。信頼性は確保され自動化も達成されているので、そのまま維持しつつ展開速度を高める、もしくは他の信頼性が必要とされるサービスへ人的資源を移すという選択肢が考えられます。ただし、すでに適切な範囲でSLOが達成されているので、不必要にSLOを厳しくするなど、既存のバランスを崩す必要はありません。
基本的な概念や原則：
サービスレベル目標（SLO）：サービスの可用性や性能について定量的に定義された目標です。これを達成することで、顧客満足度を高めます。
信頼性：システムまたはコンポーネントが指定された期間中に適切に機能する能力です。これはサービスの品質を保証するための重要な要素です。
自動化：作業プロセスを自動化することで作業の効率性や精度を向上させ、反復作業を削減するための行為またはプロセスです。
サービス展開速度：新しい機能や修正をユーザーに提供する速度です。これは製品の競争力を維持するために重要です。
サイトの信頼性エンジニアリング：ソフトウェアシステムの信頼性と効率性を向上させるための工学的アプローチです。これはシステムのライフサイクル全体を通じて信頼性を保証することを目指します。
サービスレベル指標（SLI）：SLOを達成するために測定される指標です。これはサービスのパフォーマンスを定量的に評価するために使用されます。
正解についての説明：
（選択肢）
・サービスの展開速度やリスクを高めます
・エンジニアリングの時間を、より信頼性を必要とする他のサービスに振り向けます
この選択肢が正解の理由は以下の通りです。
まず、"サービスの展開速度やリスクを高める"が優れた選択となる理由は、過去6ヶ月間のSLO達成と高い顧客満足度があるため、これらの指標が維持される限りサービスが一定のリスクを取り入れても安全性は維持されると見込まれます。それにより、新たな機能をより迅速に展開して顧客価値を提供することが可能になります。
次に、"エンジニアリングの時間を、より信頼性を必要とする他のサービスに振り向ける"が適切な選択となる理由は、既存のサービスが自動化されて反復タスクが少なく、リソースを他のタスクに再配分する余裕があるためです。サイト信頼性エンジニアリング（SRE）の原則に従えば、信頼性が既に高いサービスにリソースを割くよりも、信頼性向上が必要な他のサービスにエンジニアリングの時間を振り向けることで全体の信頼性を高めることができます。これにより、組織全体の信頼性と展開速度のバランスを最適化することが可能になります。
不正解についての説明：
選択肢：サービスのSLOをもっと厳しくします
この選択肢が正しくない理由は以下の通りです。
サービスのSLOを厳しくすることは、信頼性と展開速度のバランスを最適化する目的に対して必ずしも貢献しません。現状、サービスは常にSLOを達成し、顧客満足度も高いため、むしろSLOを厳しくすることは過剰な信頼性追求となり、新機能の展開速度を妨げる可能性があります。
選択肢：製品チームに、新機能よりも信頼性作業を優先させます
この選択肢が正しくない理由は以下の通りです。
製品チームに新機能よりも信頼性作業を優先させると、展開速度が低下し、適切なバランスを保つことが出来ません。既にSLOを常に達成し、顧客満足度が高いので、製品の進化と展開速度を優先し、高めるべきでしょう。
選択肢：サービスレベル指標（SLI）の実施方法を変更し、カバー率を高めます
この選択肢が正しくない理由は以下の通りです。
SLIの実施方法を変更しカバー率を高める手法は、すでにSLOを達成し高い顧客満足度を得ており、反復タスクもないサービスには無用です。課題がなく非効率もないこの状況でSLIを変更すれば、不必要な作業とリスクを増やす結果に繋がります。それよりサービスの展開速度やリスクを高めたり、他のサービスにエンジニアリングの時間を振り分けることが適切です。
参考リンク：
https://cloud.google.com/architecture/devops/devops-measurement-balancing-change-velocity-reliability
https://cloud.google.com/architecture/devops/devops-tech-skills-culture-and-processes#create_a_culture_of_continuous_improvement
https://sre.google/books/building-secure-reliable-systems/
</div></details>

### Q. 問題45: 未回答
あなたのチームはすべてのCI/CDパイプラインにCloud Buildを使用しています。Google Kubernetes Engine（GKE）に新しいイメージをデプロイするために、Cloud Build用のkubectlビルダーを使用したいと考えています。開発工数を最小限に抑えながら、GKEへの認証を行う必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. Cloud Buildでサービスアカウントの認証情報を取得する別のステップを作成し、kubectlに渡します
2. Cloud BuildサービスアカウントにContainer Developerロールを割り当てます
3. cloudbuild.yamlファイルでCloud BuildのContainer Developerロールを指定します
4. Container Developerロールを持つ新しいサービスアカウントを作成し、それを使用してCloud Buildを実行します
<details><div>
    答え：2
説明
この問題では、Cloud Buildを使用してGKEに新しいイメージをデプロイする際の認証方法について問われています。開発工数を最小限に抑えるとの要求があるので、最もシンプルかつ効率的な認証方法を選択肢の中から探すことが求められます。その際、Cloud BuildやGKEといったGoogle Cloudサービスの認証方法の理解が重要となります。また、選択肢中の"Cloud BuildのContainer Developerロールを指定する"新しいサービスアカウントを作成する"サービスアカウントの認証情報を取得するステップを作成する"などの表現からも、認証方法の理解度が試されています。
基本的な概念や原則：
Cloud Build：Google CloudのフルマネージドCI/CDプラットフォームです。ソースコードからコンテナをビルドし、テスト、デプロイするためのプラットフォームです。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）を組み合わせた開発プロセスです。コードの変更を自動的にビルド、テスト、デプロイすることで、高速かつ安定したソフトウェアリリースを支援します。
kubectl：Kubernetesクラスターを制御するためのコマンドラインツールです。kubectlビルダーはCloud Buildで利用可能なツールで、このツールを使うことでKubernetesの管理操作をCI/CDパイプライン内で行えます。
Google Kubernetes Engine（GKE）：Google Cloud上でマネージドKubernetesを実行するためのサービスです。アプリケーションのスケーリングや運用が容易になります。
Cloud Buildサービスアカウント：Cloud BuildがGoogle Cloudプロジェクト内で行動するためのサービスアカウントです。Google Cloudリソースとのやり取りのための適切な権限が必要です。
Container Developerロール：GKEクラスターのコンテナ（ポッド）を読み書きする権限を持つロールです。GKEに新しいイメージをデプロイするにあたり、このロールをCloud Buildサービスアカウントに割り当てることが必要です。
正解についての説明：
（選択肢）
・Cloud BuildサービスアカウントにContainer Developerロールを割り当てます
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud BuildはCI/CDパイプラインを実行するためにサービスアカウントを使用し、そのサービスアカウントは認証と認可に使用されます。サービスアカウントにロールを割り当てることで、それらのアカウントに対して特定のリソースアクセス権を与えることができ、必要な操作をKubernetes Engine上で行うことができます。
開発工数を最小限に抑えるためには、サービスアカウントに最適なロールを割り当てることが重要です。このケースでは、"Container Developer"ロールが最も適しています。このロールを割り当てると、Cloud BuildはGKEに対して認証し、新しいイメージのデプロイなどの操作を行うことができます。"Container Developer"ロールは、特定のGKEクラスターにデプロイするために必要な最低限の権限を提供します。これにより、セキュリティのベストプラクティスに従いながら、開発工数を必要最低限に抑えることが可能となります。
そのため、GKEへの認証を行う必要があり、開発工数を最小限に抑えながら、Cloud Build用のkubectlビルダーを使用したいのであれば、Cloud BuildサービスアカウントにContainer Developerロールを割り当てるのが最適です。
不正解についての説明：
選択肢：cloudbuild.yamlファイルでCloud BuildのContainer Developerロールを指定します
この選択肢が正しくない理由は以下の通りです。
cloudbuild.yamlファイル内でロールを指定するという手法は存在しません。ロールの割り当てはIAMによって行われ、Cloud BuildサービスアカウントにContainer Developerロールを付与することでGKEへの認証を行うのが適切な手段です。
選択肢：Container Developerロールを持つ新しいサービスアカウントを作成し、それを使用してCloud Buildを実行します
この選択肢が正しくない理由は以下の通りです。
新しいサービスアカウントを作成すると管理が煩雑になり、開発工数を無駄に増やす可能性があります。
一方、既存のCloud BuildサービスアカウントにContainer Developerロールを割り当てることで、追加の管理や設定なしでGKEへの認証を行うことができます。
選択肢：Cloud Buildでサービスアカウントの認証情報を取得する別のステップを作成し、kubectlに渡します
この選択肢が正しくない理由は以下の通りです。
Cloud Buildでサービスアカウントの認証情報を取得するステップを作成すると、開発工数が増えてしまいます。
一方、Cloud BuildサービスアカウントにContainer Developerロールを割り当てることで、認証を行いながら開発工数も最小限に抑えることが可能です。
参考リンク：
https://cloud.google.com/build/docs/deploying-builds/deploy-gke
https://cloud.google.com/iam/docs/understanding-roles#kubernetes-engine-roles
https://kubernetes.io/docs/tasks/tools/install-kubectl/
</div></details>

### Q. 問題46: 未回答
あなたのチームは3つのGoogle Kubernetes Engine（GKE）環境（開発、ステージング、本番）にアプリケーションをデプロイしています。あなたはGitHubリポジトリをソースとして使用しています。3つの環境の一貫性を確保する必要があります。Googleが推奨するプラクティスに従って、これらの環境のすべてのGKEクラスターにネットワークポリシーとロギングDaemonSetを適用してインストールしたいと考えています。
この要件を満たすために、どうすればよいですか？
1. Cloud Buildを使用して、ネットワークポリシーとDaemonSetをレンダリングおよびデプロイします。ポリシーコントローラーをセットアップして、3つの環境の設定を実施します
2. Google Cloud Deployを使ってDaemonSetをデプロイし、Policy Controllerを使ってネットワークポリシーを設定します。Cloud Monitoringを使用して、リポジトリ内のソースからのドリフトを検出し、Cloud Functionsを使用してドリフトを修正します
3. Cloud Buildを使用して、ネットワークポリシーとDaemonSetをレンダリングしてデプロイします。Config Syncをセットアップして、3つの環境の設定を同期します
4. Google Cloud Deployを使用して、ネットワークポリシーとDaemonSetをデプロイします。Cloud Monitoringを使用して、リポジトリ内のソースからネットワークポリシーとDaemonSetがドリフトした場合にアラートをトリガーします
<details><div>
    答え：1
説明
この問題では、3つの異なるGoogle Kubernetes Engine（GKE）環境（開発、ステージング、本番）の一貫性を保つための使用サービスとその操作方法が主な問いとなっています。あるGithubリポジトリをソースにして、各環境にネットワークポリシーとロギングDaemonSetを導入したいという要件があります。ここで注目すべきは、統一的な設定導入をどのように行うか、そしてその統一性をどのように保つかです。それらに対する適切なGoogle Cloudのサービスを選択し、それが各環境の一貫性を確保する手段となるかを理解することが求められています。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するマネージドKubernetesサービスです。GKEを使用すると、Kubernetesクラスターの設定、運用、スケーリングを効率的に行うことができます。
DaemonSet：Kubernetesのリソースの一種で、クラスター内のすべてのノード上にポッドを自動で配置します。
ネットワークポリシー：Kubernetesのリソースで、ポッドの間のネットワーク通信を制御するための仕組みです。
Cloud Build：Google Cloudのサービスで、CI/CDパイプラインを作ることができます。ソースコードのビルドとテスト、デプロイを自動化します。
ポリシーコントローラー：一貫したポリシーをKubernetesクラスターに適用するためのツールです。環境間で設定の一貫性を維持するのに役立ちます。
Google Cloud Deploy：Google Cloudのサービスで、信頼性の高いデプロイをするためのツールです。しかし、この問題の要件には適していません。
Cloud Monitoring：Google Cloudの監視サービスで、リソースとアプリケーションのパフォーマンスを追跡し、問題を検出してアラートを生成します。ですが、この問題の要件には適していません。
正解についての説明：
（選択肢）
・Cloud Buildを使用して、ネットワークポリシーとDaemonSetをレンダリングおよびデプロイします。ポリシーコントローラーをセットアップして、3つの環境の設定を実施します
この選択肢が正解の理由は以下の通りです。
まず、3つのGKE環境に対してのネットワークポリシーとロギングDaemonSetの一貫性を保つために、Cloud Buildを使用してこれらをレンダリングおよびデプロイするのは一般的なプラクティスです。Cloud Buildを使用することで、各GKE環境に対して一貫性のある設定を適用することが可能です。これは自動化され、リポジトリの変更に基づいてトリガーされるため、手動でのエラーやばらつきを減らすことができます。
また、ポリシーコントローラーをセットアップすることで、3つの環境の設定の一貫性を維持することができます。これは要件と一致し、Google Cloudの推奨するベストプラクティスに従っています。
したがって、この選択肢が最適なものです。
不正解についての説明：
選択肢：Google Cloud Deployを使用して、ネットワークポリシーとDaemonSetをデプロイします。Cloud Monitoringを使用して、リポジトリ内のソースからネットワークポリシーとDaemonSetがドリフトした場合にアラートをトリガーします
この選択肢が正しくない理由は以下の通りです。
Google Cloud Deployはアプリケーションのデプロイに使用されますが、ネットワークポリシーやDaemonSetの適用を自動化するためには、Cloud BuildのようなCI/CDツールが適しています。
また、Cloud Monitoringはシステムの監視には最適ですが、設定のドリフトを検出するのはポリシーコントローラーのロールです。
選択肢：Google Cloud Deployを使ってDaemonSetをデプロイし、Policy Controllerを使ってネットワークポリシーを設定します。Cloud Monitoringを使用して、リポジトリ内のソースからのドリフトを検出し、Cloud Functionsを使用してドリフトを修正します
この選択肢が正しくない理由は以下の通りです。
Cloud Monitoringは、環境の一貫性を維持するための直接的な方法ではなく、ドリフトの検出に用いるものであり、修正は手動です。
さらに、Cloud Functionsはドリフトを修正するための適切なツールではなく、一貫性の維持のためにはCloud Buildとポリシーコントローラーの使用が推奨されています。
選択肢：Cloud Buildを使用して、ネットワークポリシーとDaemonSetをレンダリングしてデプロイします。Config Syncをセットアップして、3つの環境の設定を同期します
この選択肢が正しくない理由は以下の通りです。
Config SyncはKubernetesの設定を同期するもので、実際のデプロイメント手順を行うものではありません。そのため、ネットワークポリシーやDaemonSetを実際にデプロイするための実行エンジンとしては不適切です。この場合、Cloud Buildがデプロイの実行を担当し、ポリシーコントローラーが設定の一貫性を確保することが適切です。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/policy-controller
https://cloud.google.com/cloud-build/docs
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-tenancy-overview
</div></details>

### Q. 問題47: 未回答
あなたの組織では、既存のサービスに対するすべての変更を承認するために、変更諮問委員会（CAB）を設置しています。あなたは、ソフトウェアデリバリパフォーマンスへの悪影響を排除するために、このプロセスを改訂したいと考えています。
この要件を満たすために、どうすればよいですか？（2つ選択）
1. 開発者には自分の変更をマージさせるが、チームのデプロイメントプラットフォームでは、問題が発見された場合に変更をロールバックできるようにします
2. チームの開発プラットフォームで、開発者が変更の影響について迅速にフィードバックを得られるようにします
3. より大規模だが頻度の低いソフトウェアリリースをするためにバッチ変更を行います
4. 開発から配備までの継続的な監視を確実にするために、CABをシニアマネジャーに置き換えます
5. コードチェックイン時に実施され、自動テストによってサポートされる、個々の変更に対するピアレビューベースのプロセスに移行します
<details><div>
    答え：2,5
説明
この問題では、変更諮問委員会（CAB）プロセスの改訂について考えます。問題の目的は、ソフトウェアデリバリパフォーマンスの低下を防ぐための新しいプロセスを推進することです。この要件を達成するために、開発者が迅速にフィードバックを得られる環境を整えて、個々の変更に対して自動テストとピアレビューを実施することが重要です。それにより、開発プロセス全体のスムーズな運用が可能となります。
基本的な概念や原則：
ピアレビュー：プログラムのソースコードの確認を他の開発者に依頼するプロセスのことです。コードの品質を確保し、エラーや問題を早期に発見する目的で行われます。
自動テスト：ソフトウェアのテスト工程を自動化する手法です。バグの検出、機能の検証、パフォーマンスの評価などを自動的に行います。開発者がコードチェックイン時に実行し、変更が問題を引き起こす可能性があるかどうかを確認します。
開発プラットフォーム：アプリケーションを開発するためのツールや環境のことです。インテレキチャルプロパティ管理、チームのコラボレーション、開発プロセスの自動化などが提供されます。
フィードバック：開発者がコードの変更の影響を把握するための情報のことです。早期にフィードバックを得ることで、問題の修正や品質の改善が容易になります。
正解についての説明：
（選択肢）
・コードチェックイン時に実施され、自動テストによってサポートされる、個々の変更に対するピアレビューベースのプロセスに移行します
・チームの開発プラットフォームで、開発者が変更の影響について迅速にフィードバックを得られるようにします
この選択肢が正解の理由は以下の通りです。
まず、変更諮問委員会（CAB）は変更を遅延させ、開発の官僚化を引き起こす可能性があります。ピアレビューベースのプロセスは、速度と反応性に重点を置いたアプローチであり、開発者同士が直接コードの質をチェックし合います。これらのレビューはコードチェックイン時に行われ、自動テストによってサポートされることで、迅速かつ正確なフィードバックが可能になります。これにより、変更が早期に検出され、修正が必要な場合はすぐに修正することができます。
また、開発者が影響を迅速に把握できるようにすることは、変更がシステム全体に及ぼす影響を理解し、必要な修正を迅速に行えるようにするために重要です。これは迅速なフィードバックループを確立し、開発者が自分の作業に関連する問題を早期に特定して対処することを可能にします。これにより、開発の速度が向上し、最終的にはソフトウェアの配信パフォーマンスを改善することにつながります。
不正解についての説明：
選択肢：開発から配備までの継続的な監視を確実にするために、CABをシニアマネジャーに置き換えます
この選択肢が正しくない理由は以下の通りです。
シニアマネジャーにCABを置き換えることは、ソフトウェアデリバリパフォーマンスへの悪影響を排除する目指す要件を解決する策ではありません。
逆に、管理レベルが上がることで承認プロセスが遅くなり、デリバリーの速度を阻害する恐れがあります。
それに対して、正解の選択肢は、自動テストや開発者のフィードバックにより、より効率的で迅速な変更プロセスを提案しています。
選択肢：開発者には自分の変更をマージさせるが、チームのデプロイメントプラットフォームでは、問題が発見された場合に変更をロールバックできるようにします
この選択肢が正しくない理由は以下の通りです。
開発者に変更をマージさせてから問題を探し、それをロールバックすることは、問題が本番環境で発生するまで課題を特定できない可能性があります。これは、問題発見の遅延をもたらしソフトウェアのデリバリーパフォーマンスを低下させる可能性があります。
正解の選択肢は、変更の前段階でレビューとフィードバックを組み込んでおり、より効果的に問題を予防します。
選択肢：より大規模だが頻度の低いソフトウェアリリースをするためにバッチ変更を行います
この選択肢が正しくない理由は以下の通りです。
より大規模で頻度の低いソフトウェアリリースは、ソフトウェアのデリバリーパフォーマンスを改善するという目標に反します。
逆に、大規模な変更は意図しない影響を及ぼす可能性を高め、問題が発生した際のデバッグも困難になります。
正解の選択肢は、小さな変更を頻繁に行い、すぐにフィードバックを取得することで、より効率的な改善を目指しています。
参考リンク：
https://cloud.google.com/solutions/devops/devops-tech-changing-culture
https://cloud.google.com/solutions/devops/devops-process-improving-change-review
https://cloud.google.com/solutions/automating-at-scale-ci-cd-with-github-actions-and-google-cloud
</div></details>

### Q. 問題48: 未回答
Google Cloudにデプロイされたアプリケーションのフロントエンド層を設定しています。フロントエンド層はnginxでホストされ、Envoyベースの外部HTTP(S) ロードバランサーを前面に配置したマネージドインスタンスグループを使用してデプロイされます。このアプリケーションは完全にeurope-west2リージョン内にデプロイされ、イギリスを拠点とするユーザーにのみサービスを提供しています。最も費用対効果の高いネットワーク層とロードバランシング構成を選択する必要があります。
何を使うべきですか？
1. グローバルロードバランサーを備えたプレミアムティア
2. リージョンロードバランサーを備えたスタンダードティア
3. リージョンロードバランサー付きプレミアムティア
4. グローバルロードバランサーを備えたスタンダードティア
<details><div>
    答え：2
説明
この問題では、効率的なネットワーク層とロードバランシング設定を選択するための要素に注意を払って読む必要があります。アプリケーションは完全に一つのリージョン（europe-west2）で稼働しており、顧客層も限定的（イギリスに拠点を持つユーザー）であるという事実は重要な指標です。これらの要素を元に選択肢を評価することで、最もコスト効果の高いソリューションを判断することができます。
基本的な概念や原則：
リージョンロードバランサー：特定の地理的リージョン内でネットワークトラフィックを分散させるためのロードバランサーです。単一リージョン内での負荷分散に適しています。
スタンダードティア：Google Cloudのネットワーキングプロダクトの価格設定モデルの一つで、低コストを実現します。国内リージョンまたは同じ大陸のリージョン間の通信に対して最適化されています。
プレミアムティア：Google Cloudのネットワーキングプロダクトの価格設定モデルの一つで、選択的なネットワークパスとグローバルでの接続を提供します。これは高品質のサービスを求めるグローバルなリーチを持つユーザーにとって最適です。
グローバルロードバランサー：世界中の複数の地理的リージョンでネットワークトラフィックを分散するためのロードバランサーです。複数のリージョンにまたがる負荷分散に適しています。
正解についての説明：
（選択肢）
・リージョンロードバランサーを備えたスタンダードティア
この選択肢が正解の理由は以下の通りです。
まず、アプリケーションはイギリスのユーザーを対象としており、リージョン内で完全にデプロイされています。この要件には、リージョナルロードバランサーが最も適しています。これは、リージョナルロードバランサーは特定のリージョンでのトラフィックの分散に最適化されています。これは、拠点を同じリージョンに限定したユーザーベースに対して、最も効率的なパフォーマンスを提供します。
また、スタンダードティアが選ばれる理由は、それが最も費用対効果が高く、基本的なネットワークニーズを満たすからです。プレミアムティアはより多くの機能を提供しますが、それらの機能がこの特定のケースでは必要ないため、スタンダードティアの方が適切です。これらの理由から、リージョンロードバランサーを備えたスタンダードティアの採用が、このシナリオで最も費用対効果の高い選択となります。
不正解についての説明：
選択肢：グローバルロードバランサーを備えたプレミアムティア
この選択肢が正しくない理由は以下の通りです。
グローバルロードバランサーを備えたプレミアムティアは、全世界規模での負荷分散を可能にしますが、今回のシナリオでは全てのユーザーがイギリスに集中するため必要ありません。
それに対して、リージョンロードバランサーを使用すれば特定のリージョン内での負荷分散が可能で、より費用対効果が高くなります。
選択肢：リージョンロードバランサー付きプレミアムティア
この選択肢が正しくない理由は以下の通りです。
プレミアムティアは、Googleの広範な国際ネットワークを使用することが特徴ですが、このアプリケーションは完全にeurope-west2リージョン内で稼働し、イギリスのユーザーのみにサービスを提供するため、この広範なネットワークは不要です。そのため、スタンダードティアの使用が最も費用対効果が高いとなります。
選択肢：グローバルロードバランサーを備えたスタンダードティア
この選択肢が正しくない理由は以下の通りです。
グローバルロードバランサーは複数のリージョン間でのトラフィック分散に利用されますが、問題文の状況ではアプリケーションは一つのリージョン（europe-west2）にデプロイされ、ユーザーも固定されています。そのためリージョンロードバランサーが適切で、費用対効果が高いと言えます。
参考リンク：
https://cloud.google.com/net-services/docs/network-tier
https://cloud.google.com/load-balancing/docs/load-balancing-overview
https://cloud.google.com/compute/docs/instance-groups/#managed_instance_groups
</div></details>

### Q. 問題49: 未回答
あなたの組織は最近、アプリケーション開発にコンテナベースのワークフローを採用しました。あなたのチームは、自動化されたビルドパイプラインを通じて本番環境に継続的にデプロイされる多数のアプリケーションを開発しています。最近のセキュリティ監査で、本番環境にプッシュされるコードには脆弱性が含まれている可能性があり、仮想マシン（VM）の脆弱性に関する既存のツールはコンテナ化された環境にはもはや適用されないことがチームに警告されました。パイプラインを通じて実行されるすべてのコードのセキュリティとパッチレベルを確保する必要があります。
この要件を満たすために、どうすればよいですか？
1. コンテナ分析を設定し、一般的な脆弱性と暴露をスキャンしてレポートします
2. ビルドパイプラインのコンテナを、リリース前に常にアップデートするように設定します
3. 既存のオペレーティングシステムの脆弱性ソフトウェアを、コンテナ内に存在するように再構成します
4. コンテナの作成に使用されるDockerファイルに対する静的コード解析ツールを実装します
<details><div>
    答え：1
説明
この問題では、アプリケーション開発にコンテナベースのワークフローを採用している組織が、本番環境にデプロイされるコードのセキュリティとパッチレベルを確保する方法を問われています。脆弱性スキャンツールがコンテナ環境に対応していないことが問題となっており、コンテナのセキュリティに特化した解決策が求められています。その際、既存のVM向けのツールを適用することや、開発プロセスの更新だけではなく、コンテナ独特の脆弱性を考慮したアプローチが重要です。
基本的な概念や原則：
コンテナベースのワークフロー：アプリケーションの開発、デプロイ、実行をコンテナ化された環境で行う手法です。環境の一貫性を保つため、アプリケーションのパフォーマンスと信頼性を向上させます。
コンテナ分析：Google Cloudの機能で、脆弱性スキャンを行うことでコンテナの安全性を評価し、レポートします。これにより、未解決の脆弱性を特定し、パッチを適用するタイミングを判断できます。
ビルドパイプライン：ソースコードの変更を自動的にビルド、テスト、デプロイする開発プロセスです。これにより、新しいまたは更新されたコードを迅速にリリースします。
脆弱性ソフトウェアの再構成：既存の脆弱性ソフトウェアをコンテナ環境に適用するための再構成ですが、コンテナ環境への適用は困難であり望ましくありません。コンテナ専用の脆弱性スキャンツールの使用が推奨されます。
静的コード解析ツール：ソースコードを分析して脆弱性を特定するツールです。しかし、これだけではランタイム環境の脆弱性までは検出できません。
正解についての説明：
（選択肢）
・コンテナ分析を設定し、一般的な脆弱性と暴露をスキャンしてレポートします
この選択肢が正解の理由は以下の通りです。
まず、Google Cloudのコンテナ分析は、ビルドパイプラインに組み込むことが可能で、イメージの脆弱性を自動的にスキャンできます。この結果、アプリケーションが本番環境にデプロイされる前に問題が特定され、適切な対策が講じられます。
また、新たに発見された脆弱性についても、既存のコンテナイメージに自動的に反映されます。これにより、開発者や運用チームが迅速に脆弱性の情報を入手し、対処することが可能です。
さらに、コンテナ分析はレポートを生成するので、セキュリティ監査の要件を満たし、組織全体のセキュリティポスチャの向上に対して証明可能な価値を提供します。
したがって、この問題の需要を満たすために最適な選択肢は、コンテナ分析を設定し、一般的な脆弱性と暴露をスキャンしてレポートすることです。
不正解についての説明：
選択肢：ビルドパイプラインのコンテナを、リリース前に常にアップデートするように設定します
この選択肢が正しくない理由は以下の通りです。
ビルドパイプラインのコンテナを常にアップデートすることは、セキュリティとパッチレベルを確保するための手つかずの確実な方法ではありません。アップデートでも、新たに脆弱性が紛れ込む可能性があります。正解の選択肢の"コンテナ分析"は、具体的に脆弱性をスキャンしてレポートすることができ、より確実な対策を提供します。
選択肢：既存のオペレーティングシステムの脆弱性ソフトウェアを、コンテナ内に存在するように再構成します
この選択肢が正しくない理由は以下の通りです。
既存のオペレーティングシステムの脆弱性ソフトウェアをコンテナ内に存在するように再構成すると、コードのセキュリティとパッチレベルを確保するための網羅的な解決策にはなりません。特に、コンテナ環境で実行されている全てのコードの脆弱性をチェックする能力を持つわけではないからです。比較すると、コンテナ分析はコンテナイメージの脆弱性を網羅的にスキャンでき、対応策を提示するため、適切な選択肢となります。
選択肢：コンテナの作成に使用されるDockerファイルに対する静的コード解析ツールを実装します
この選択肢が正しくない理由は以下の通りです。
Dockerファイルの静的解析ツールはサービスの脆弱性を検出するには不十分です。これは主にソースコードの潜在的な問題を見つけるためのもので、ビルドパイプライン全体やランタイムの脆弱性を確認するためのものではありません。
また、コンテナ分析は全体的なセキュリティとパッチレベルを管理する機能を持っています。
参考リンク：
https://cloud.google.com/container-analysis/docs/getting-started
https://cloud.google.com/container-analysis/docs/container-scanning-overview
https://cloud.google.com/architecture/best-practices-for-operating-containers
</div></details>

### Q. 問題50: 未回答
Webベースのアプリケーションの新機能を本番環境にデプロイする準備が整いました。Google Kubernetes Engine（GKE）を使用して、Webサーバーポッドの半分に段階的なロールアウトを行いたいと考えています。
この要件を満たすために、どうすればよいですか？
1. NoExecuteでノードテイントを使用します
2. パーティショニングされたローリングアップデートを使用します
3. パラレルポッド管理ポリシーでステートフルセットを使用します
4. デプロイメント仕様でレプリカセットを使用します
<details><div>
    答え：2
説明
この問題では、次の新機能を段階的に本番環境にデプロイしたいと考えているWebベースのアプリケーションの状況を理解することが必要です。この目的を達成するための方法について問われています。選択肢を検討するときは、Google Kubernetes Engine（GKE）を使ってWebサーバーポッドの半分にどのように段階的な更新を行うのかという点に焦点を当てることが重要です。選択肢はすべてKubernetesやGKEの機能に関するものですが、それぞれの機能が適切なロールアウト方法を提供できるかどうかを考える必要があります。
基本的な概念や原則：
パーティショニングされたローリングアップデート：新しいバージョンのアプリケーションを段階的にデプロイする手法です。元のポッドの一部をダウンして新しいバージョンのポッドを起動し、問題がなければ全てのポッドを新しいバージョンに更新することで、効果的なロールアウトやロールバックを実行します。
Google Kubernetes Engine（GKE）：Kubernetesクラスターのデプロイと管理を行うGoogle Cloudのサービスです。アプリケーションのスケーリングとロールアウト機能を提供します。
ノードテイント：Kubernetesの機能で、特定のノードにスケジュールされるポッドの制御を可能にします。ただし、デプロイメントの更新とは直接関連しません。
レプリカセット：Kubernetesのオブジェクトで、指定された数のポッドレプリカが実行されていることを保証します。ただし、回答として不適切です。
ステートフルセット：Kubernetesのオブジェクトで、順序付けられた、安定したネットワーク識別子を持つ一連のポッドを管理します。多くの場合、パラレルポッド管理ポリシーと共に使用されますが、段階的なロールアウトの答えとしては不適切です。
正解についての説明：
（選択肢）
・パーティショニングされたローリングアップデートを使用します
この選択肢が正解の理由は以下の通りです。
Google Kubernetes Engine（GKE）のパーティショニングされたローリングアップデートは、一度にすべてのポッドを更新するのではなく、指定した割合または数のポッドを一度に更新することができます。これにより、新機能を段階的にロールアウトし、障害発生時には影響を最小限に抑えることができます。
さらに、段階的なロールアウトでは新機能のパフォーマンスやユーザーフィードバックを逐次監視しながら進めることが可能になり、問題が生じた場合は迅速にロールバックすることも可能です。これは、新機能を安全に投入することが求められる本番環境において、中断時間を最小限に抑えつつ、新しいバージョンへの移行をスムーズに進めるために不可欠な機能です。
したがって、GKEのパーティショニングされたローリングアップデートが、Webサーバーポッドの半分に対する段階的ロールアウトを行うための適切な方法となります。
不正解についての説明：
選択肢：NoExecuteでノードテイントを使用します
この選択肢が正しくない理由は以下の通りです。
NoExecuteでノードテイントを使用すると、該当ノードは指定のポッドを撃退しますが、これはWebサーバーポッドの半分に段階的なロールアウトを行う要件を満たしません。
一方、パーティショニングされたローリングアップデートを使用すれば、徐々に更新を適用することが可能で、要件に合致します。
選択肢：デプロイメント仕様でレプリカセットを使用します
この選択肢が正しくない理由は以下の通りです。
デプロイメント仕様のレプリカセットでは段階的なロールアウトはできません。レプリカセットを使用すると一度にすべてのポッドが更新されます。
それに対して、パーティショニングされたローリングアップデートを使用すると、指定した割合のポッドを一度に更新することができ、段階的なロールアウトに対応しています。
選択肢：パラレルポッド管理ポリシーでステートフルセットを使用します
この選択肢が正しくない理由は以下の通りです。
ステートフルセットは状態を保持する必要があるアプリケーションのデプロイに使用しますが、新機能の段階的なロールアウトには適していません。
一方、パーティショニングされたローリングアップデートは、Webサーバーポッドの半分で新機能のデプロイを行う要件を満たします。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment#rolling_updates
https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment
</div></details>

## 3
### Q. 問題1: 未回答
あなたは、大きなファイルを提供するn2-standard-2 Compute Engineインスタンスを使用するサービスを監視しています。ユーザーからダウンロードが遅いと報告されています。Cloud Monitoringダッシュボードは、VMがピークネットワークスループットで実行されていることを示しています。あなたはネットワークのスループットパフォーマンスを改善したいと考えています。
この要件を満たすために、どうすればよいですか？
1. VMのマシンタイプをn2-standard-8に変更します
2. Ops Agentを導入して、追加のモニタリングメトリクスをエクスポートします
3. VMにネットワークインターフェイスコントローラ（NIC）を追加します
4. Cloud NATゲートウェイをデプロイし、VMのサブネットにゲートウェイをアタッチします
<details><div>
    答え：1
説明
この問題では、Compute Engineインスタンスのネットワークスループットの改善を求められています。ユーザからの遅いダウンロードの報告と、Cloud Monitoringダッシュボードからのピークネットワークスループットの情報を基に動作アプローチを考える必要があります。選択肢を見る時には、それぞれがスループット改善にどのように寄与するか、またCompute Engineインスタンスのパフォーマンスとどのような関係があるかを理解することが重要です。
基本的な概念や原則：
Compute Engineインスタンス：Google Cloudのインフラーストラクチャサービスで、仮想マシンを計算リソースとして提供します。インスタンスのタイプによりコンピューティングパワーやネットワーク性能が異なります。
n2-standard：Compute Engineインスタンスの一種で、第二世代の一般的な目的のインスタンスです。vCPUとメモリのバランスが良好で、多種多様なワークロードに対応します。
ネットワークスループット：ネットワークの転送能力を示す指標で、一定時間内に転送できるデータの量を指します。Compute Engineのインスタンスタイプにより最大ネットワークスループットが異なります。
Cloud Monitoring：Google Cloudの監視サービスで、アプリケーションとシステムのヘルスステータスを監視し、パフォーマンスの課題を特定します。
ネットワークインターフェイスコントローラ（NIC）：ネットワークに接続するためのハードウェアデバイスです。ただし、NICの追加はネットワークスループットの向上には直接寄与しません。
Cloud NAT：Google CloudのNAT（Network Address Translation）サービスですが、ネットワークスループットの向上には寄与しません。
Ops Agent：Google Cloudのログ収集とメトリクスのエクスポートを行うためのエージェントです。モニタリングの詳細度を向上させることができますが、ネットワークスループットを直接改善するものではありません。
正解についての説明：
（選択肢）
・VMのマシンタイプをn2-standard-8に変更します
この選択肢が正解の理由は以下の通りです。
Compute Engineインスタンスのネットワークスループットは、マシンタイプとそのサイズに関連付けられています。n2-standard-2とn2-standard-8の主な相違点は、利用可能なvCPUの数量であり、このvCPUの数が増えるとネットワークスループットも増加します。
したがって、ネットワーク性能を向上させたい場合、より大きなマシンタイプに移行するのが良い選択であると言えます。
具体的には、n2-standard-2からn2-standard-8に変更することで、大きなファイルをダウンロードする際のスループットが増加し、結果としてユーザー体験が向上します。Cloud MonitoringダッシュボードがVMのピークネットワークスループットで実行されていることを示しているので、マシンタイプをn2-standard-8に変更すると、スループットが増加し、結果としてダウンロードのパフォーマンスが向上すると予測できます。
不正解についての説明：
選択肢：VMにネットワークインターフェイスコントローラ（NIC）を追加します
この選択肢が正しくない理由は以下の通りです。
ネットワークインターフェイスコントローラ（NIC）を追加してもインスタンスのネットワークスループットは改善しません。なぜならスループットはVMのタイプによって定まるためです。反対に、n2-standard-8に変更すると、スループットが増加するためネットワークパフォーマンスが改善します。
選択肢：Cloud NATゲートウェイをデプロイし、VMのサブネットにゲートウェイをアタッチします
この選択肢が正しくない理由は以下の通りです。
Cloud NATゲートウェイは、プライベートIPアドレスを使用するマシンとインターネットや他のGoogleサービスとの通信を可能にしますが、これはネットワークスループットを改善するものではありません。
それに対して、VMのマシンタイプをn2-standard-8に変更することで、ネットワークスループットの上限が上昇しパフォーマンス改善が期待できます。
選択肢：Ops Agentを導入して、追加のモニタリングメトリクスをエクスポートします
この選択肢が正しくない理由は以下の通りです。
Ops Agentを導入して追加のモニタリングメトリクスをエクスポートすることは、ネットワークのスループットパフォーマンスを改善する直接的な解決策にはなりません。主にモニタリングとトラブルシューティングが主な目的であり、パフォーマンス自体を向上させるわけではありません。
一方、VMのマシンタイプをn2-standard-8に変更することは、ネットワークのスループットパフォーマンスを直接的に向上させる効果的な手法です。これは、マシンタイプが大きいほどネットワークパフォーマンスも向上するためです。
参考リンク：
https://cloud.google.com/compute/docs/machine-types#n2_machine_types
https://cloud.google.com/compute/docs/network-throughput#throughoutput_caps
https://cloud.google.com/vpc/docs/create-use-multiple-interfaces
<details><div>

### Q. 問題2: 未回答
Google Cloud上でネイティブにCI/CDパイプラインを構成しています。プリプロダクションのGoogle Kubernetes Engine（GKE）環境でのビルドを、プロダクションのGKE環境に昇格させる前に自動的にロードテストさせたいとします。このテストに合格したビルドだけが本番環境にデプロイされるようにする必要があります。あなたは、Googleが推奨するプラクティスに従いたいと考えています。
バイナリ認証を使ってこのパイプラインをどのように構成すればよいですか？
1. Cloud Key Management Service（Cloud KMS）に保存された秘密鍵と、Kubernetes Secretとして保存されたサービスアカウントのJSONキーを使用して、負荷テストに合格したビルドの認証を作成します
2. Cloud Key Management Service（Cloud KMS）に保存された鍵を使用して、品質保証の主任エンジニアが証明書に署名することを要求することにより、負荷テストに合格したビルドの証明書を作成します
3. Workload Identityで認証されたCloud Key Management Service（Cloud KMS）に保存された秘密鍵を使用して、負荷テストに合格したビルドの認証を作成します
4. 負荷テストに合格したビルドの証明書を作成し、リード品質保証エンジニアが個人の秘密鍵を使用して証明書に署名することを義務付けます
<details><div>
    答え：3
説明
この問題では、Google Cloud上でCI/CDパイプラインを構成し、ロードテストを経たビルドのみを本番環境にデプロイする方法について考えます。具体的には、Google Kubernetes Engine（GKE）の本番前環境でのビルドをロードテストしてから本番環境に昇格する際、これらのビルドを自動的に認証でき、かつGoogleが推奨するプラクティスに従った方法を理解する必要があります。また、バイナリ認証の使用について理解が必要となります。選択肢からは、Cloud Key Management Service（Cloud KMS）やWorkload Identity、及びそれらの秘密鍵の使用についても考慮する必要があります。また、各選択肢の組み合わせを見て、どのアプローチがセキュリティと効率性を最も高めるかを理解する必要があります。
基本的な概念や原則：
CI/CDパイプライン：継続的インテグレーションと継続的デリバリーを組み合わせた開発プロセスです。ソースコードの変更を自動的にビルド、テスト、デプロイします。
Google Kubernetes Engine（GKE）：Google Cloudの管理型Kubernetesサービスです。クラスター管理タスクを自動化することで、開発者はアプリケーションのデプロイに専念できます。
バイナリ認証：Google Cloudのサービスで、開発者が信頼されたソースからのコンテナイメージのみをデプロイできるようにします。
Workload Identity：GKEのサービスアカウントとGoogle Cloudのサービスアカウントを紐付けます。これにより、GKEのワークロードがGoogle CloudのAPIに安全にアクセスできます。
Cloud Key Management Service（Cloud KMS）：暗号鍵をクラウドで管理して、暗号化されたデータを保護するサービスです。
Kubernetes Secret：Kubernetesのオブジェクトで、パスワード、OAuthトークン、SSHキーなどの機密情報を保存し、管理します。これらの情報はデプロイメントやポッドといった他のKubernetesオブジェクトから使用できます。
リード品質保証エンジニア：品質保証チームのリーダーポジションであり、テスト戦略の立案や実施、テスト結果の分析などを行います。しかし、彼らの個人キーを使用した署名作業は推奨されないプラクティスです。
正解についての説明：
（選択肢）
・Workload Identityで認証されたCloud Key Management Service（Cloud KMS）に保存された秘密鍵を使用して、負荷テストに合格したビルドの認証を作成します
この選択肢が正解の理由は以下の通りです。
まず、Googleの推奨するプラクティスでは、バイナリ認証を使用して、ビルドが特定の基準を満たすかどうかを検証いたします。このパイプラインでは、負荷テストの成功がその基準となります。
従って、負荷テストに合格したビルドのために認証を作成する方法が必要です。
次に、認証を作成するためには秘密鍵が必要となります。この秘密鍵はGoogleのCloud Key Management Service（Cloud KMS）に保存され、Workload Identityを使用して認証されます。Workload Identityは、サービス間の認証を容易にするGoogle Cloudの機能であり、秘密鍵を安全に管理します。
最後に、この認証が作成された後、バイナリ認証はその認証を使用して、ビルドがテスト基準を満たしていることを検証し、満たす場合にのみビルドを本番環境にデプロイします。
したがって、この選択肢はGoogleの推奨するプラクティスに従っています。
不正解についての説明：
選択肢：負荷テストに合格したビルドの証明書を作成し、リード品質保証エンジニアが個人の秘密鍵を使用して証明書に署名することを義務付けます
この選択肢が正しくない理由は以下の通りです。
リードエンジニアが秘密鍵を使用して署名するという手順は、Googleの推奨するプラクティスとは異なります。Workload Identityを使用したCloud KMSによる自動化された署名が推奨され、人間が直接関与するような手順はエラーやタイムロスの原因となります。
選択肢：Cloud Key Management Service（Cloud KMS）に保存された秘密鍵と、Kubernetes Secretとして保存されたサービスアカウントのJSONキーを使用して、負荷テストに合格したビルドの認証を作成します
この選択肢が正しくない理由は以下の通りです。
Kubernetes Secretとして保存されたサービスアカウントのJSONキーの使用は、Googleの推奨するプラクティスに反しています。これはセキュリティリスクを増大させる可能性があります。
代わりに、特権を最小限に抑え、よりセキュアなWorkload Identityを使用すべきです。
選択肢：Cloud Key Management Service（Cloud KMS）に保存された鍵を使用して、品質保証の主任エンジニアが証明書に署名することを要求することにより、負荷テストに合格したビルドの証明書を作成します
この選択肢が正しくない理由は以下の通りです。
手動で証明書に署名を依頼しても、ビルドが負荷テストに合格したかどうかを自動的に検証できないため、CI/CDパイプラインの自動化や効率化には向いていません。
これに対し、Workload Identityを使用すれば、負荷テストの結果に基づいた自動的な署名が可能となります。
参考リンク：
https://cloud.google.com/binary-authorization/docs/key-concepts
https://cloud.google.com/binary-authorization/docs/creating-attestors
https://cloud.google.com/kubernetes-engine/docs/how-to/binary-authorization
<details><div>

### Q. 問題3: 未回答
Compute Engine上でアプリケーションを実行し、Google Cloud Operation Suiteを通してログを収集しています。あなたは、個人を特定できる情報（PII）が特定のログエントリフィールドに漏洩していることを発見しました。すべてのPIIエントリはuserinfoというテキストで始まります。これらのログエントリを後で確認できるように安全な場所にキャプチャし、Cloud Loggingへの漏洩を防ぎたいとします。
あなたはこの要件を満たすために、どうすればよいですか？
1. oogle Cloud Operation SuiteエージェントでFluentdフィルタープラグインを使い、userinfoを含むログエントリーを削除し、エントリーをCloud Storageバケットにコピーします
2. Google Cloud Operation SuiteエージェントでFluentdフィルタープラグインを使い、userinfoを含むログエントリーを削除し、userinfoにマッチする高度なログフィルターを作成し、Google Cloud Operation SuiteコンソールでCloud Storageをシンクとしてログエクスポートを設定します
3. userinfoにマッチする基本的なログフィルターを作成し、Google Cloud Operation SuiteコンソールでCloud Storageをシンクとしてログエクスポートを設定します
4. userinfoに一致する高度なログフィルターを作成し、Google Cloud Operation SuiteコンソールでCloud Storageをシンクとしてログエクスポートを構成し、userinfoをフィルターとしてログ除外を構成します
<details><div>
    答え：1
説明
この問題では、Compute Engine上で動作していて、Google Cloud Operation Suiteを通じてログを収集するアプリケーションを考慮しています。特定のログエントリーフィールドに個人を特定できる情報（PII）が漏洩してしまっており、対策が求められています。注意すべきは、これらのPIIエントリはすべて'userinfo'というテキストで始まるという情報です。問題は、どのようにこれらを安全にキャプチャし、Google Cloud Loggingへの漏洩を防ぐかという点です。この問題を解決するために、Google Cloud Operation Suiteのエージェントやフィルタリングプラグイン、ログエクスポートといった概念が重要となります。
基本的な概念や原則：
Google Cloud Operation Suite：Google Cloudの監視ツール群です。Cloud Logging, Cloud Monitoringなどが含まれ、アプリケーションのパフォーマンス、ログ、リアルタイム分析などを管理します。
Fluentd：オープンソースのデータコレクターで、ログを統一的に管理するためのツールです。フィルタープラグインなど多種多様なプラグインが提供され、ログの収集、フィルタリング、転送を柔軟に行うことができます。
個人を特定できる情報（PII）：個人の識別や連絡などの情報を表します。プライバシー法により、保護されている情報が含まれます。
Cloud Logging：Google Cloud Operation Suiteの一部で、アプリケーションとシステムのログデータを一元的に管理するサービスです。
Cloud Storage：容量を気にせず、データを蓄積できるGoogle Cloudのストレージサービスです。データの保管やバックアップ、アーカイブなど、さまざまな利用シナリオに対応しています。
ログエクスポート：操作のログ情報のエクスポートのことです。異なるストレージサービスにデータをエクスポートし、取り出しや分析を可能にします。
ログフィルタ：特定の検索条件に一致するログデータのみを取り出すためのツールです。エラーや特定の操作など、重要な情報を効率的に抽出することができます。
正解についての説明：
（選択肢）
・Google Cloud Operation SuiteエージェントでFluentdフィルタープラグインを使い、userinfoを含むログエントリーを削除し、エントリーをCloud Storageバケットにコピーします
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Operation SuiteエージェントはCompute Engineインスタンスからログを収集しCloud Loggingへ送信します。Fluentdフィルタープラグインを使用すると、エージェントがログエントリーを処理して送信する前にログをハンドリングすることができます。Fluentdフィルタープラグインにより、特定のパターンを含むログエントリーを削除することが可能となり、これによりPII情報がCloud Loggingに送信されるのを防ぐことができます。
また、Fluentdはログを他のデータストレージ、この場合はCloud Storageにコピーする機能も持っています。そのため、削除前のログエントリーを保存し、後から確認できるようにすることが可能です。削除したログエントリーを保存するため、PII情報を保持しつつ、セキュリティ上の漏洩を防げます。これらの理由から、この選択肢が最適な解答となります。
不正解についての説明：
選択肢：userinfoにマッチする基本的なログフィルターを作成し、Google Cloud Operation SuiteコンソールでCloud Storageをシンクとしてログエクスポートを設定します
この選択肢が正しくない理由は以下の通りです。
基本的なログフィルターを使用しても、該当するログエントリーはCloud Loggingから削除されず、依然として漏洩リスクが存在します。Fluentdフィルタープラグインを使用することで、適切にログエントリーを削除し、安全にエントリーをCloud Storageバケットにコピーすることが可能です。
選択肢：userinfoに一致する高度なログフィルターを作成し、Google Cloud Operation SuiteコンソールでCloud Storageをシンクとしてログエクスポートを構成し、userinfoをフィルターとしてログ除外を構成します
この選択肢が正しくない理由は以下の通りです。
高度なログフィルターとログ除外の構成は、userinfoを含むログエントリをフィルタリングし除外しますが、これらがCloud Storageにコピーされる保証はありません。
一方、Fluentdフィルタープラグインでは、userinfoを含むエントリを削除し、同時にエントリをCloud Storageにコピーすることが可能です。
選択肢：Google Cloud Operation SuiteエージェントでFluentdフィルタープラグインを使い、userinfoを含むログエントリーを削除し、userinfoにマッチする高度なログフィルターを作成し、Google Cloud Operation SuiteコンソールでCloud Storageをシンクとしてログエクスポートを設定します
この選択肢が正しくない理由は以下の通りです。
フィルタープラグインでPIIデータを削除した後に、userinfoをマッチさせるフィルターを作成するのはピント外れです。PIIデータは既に削除されているため、このフィルターは結果に影響を与えません。そのため、マッチングフィルターの作成は不要なステップで、正解の解答と比べて効率的でもないです。
参考リンク：
https://cloud.google.com/logging/docs/agent/logging/configuration
https://cloud.google.com/logging/docs/exclusions
https://www.fluentd.org/guides/recipes/google-cloud-platform-logging
<details><div>

### Q. 問題4: 未回答
あなたの組織はHelmを使用してコンテナ化されたアプリケーションをパッケージ化しています。アプリケーションはパブリックチャートとプライベートチャートの両方を参照しています。セキュリティチームは、パブリックなHelmリポジトリを依存関係として使用することはリスクであると指摘しました。ネイティブのアクセス制御とVPC Service Controlsを使用して、すべてのチャートを統一的に管理したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. Artifact Registryを使用して、公開および非公開チャートをOCIフォーマットで保存します
2. Google Kubernetes Engine（GKE）で動作するHelmチャートリポジトリサーバを設定し、ストレージバックエンドとしてCloud Storage bucketを使用します
3. Gitリポジトリを使って、パブリックチャートとプライベートチャートを保存します。Cloud Storageバケットにリポジトリの内容を同期するようにCloud Buildを設定します。Helmリポジトリとしてhttps://[bucket].storage-googleapis.com/[helmchart]を使用して、Helmをバケットに接続します
4. GitHub EnterpriseとGoogle WorkspaceをIDプロバイダとして使用することで、パブリックチャートとプライベートチャートを保存できます
<details><div>
    答え：1
説明
この問題では、Helmチャートを一元的に管理し、セキュリティを確保する手段が求められています。パブリックなチャートとプライベートチャートの両方を取り扱い、その依存関係を管理する一方で、セキュリティチームから指摘されたリスクを排除する必要があります。そのためには、ネイティブのアクセス制御とVPC Service Controlsを導入し、チャートを統一的に管理する方法を探らなければなりません。用意されている選択肢からは、チャートを管理するための具体的なプラットフォームや方法が提示されていますが、ここではGoogle Cloudの特定のサービスを適切に利用することが重要となります。
基本的な概念や原則：
Artifact Registry：Google Cloudの統合アーティファクト管理サービスです。これにより、Dockerコンテナイメージや言語ベースのパッケージなどのソフトウェアアーティファクトを一元化して管理できます。
Helm：Kubernetesのパッケージマネージャーです。事前に設定されたKubernetesリソースの集合であるHelmチャートを簡単にインストール、パッケージ化することが可能です。
OCI形式：Open Container Initiative（OCI）が策定したコンテナイメージの標準フォーマットです。コンテナランタイムやパッケージ管理システム間での互換性を促進します。
VPC Service Controls：Google Cloud上のサービス間ネットワークデータリークやデータエクスフィルトレーションを防ぐセキュリティサービスです。
ネイティブのアクセス制御：Google CloudのIAMロールを利用したリソースへの対アクセスを制限する仕組みです。ロールベースのアクセス制御（RBAC）を提供します。
GitHub Enterprise：プロプライエタリなホスティングサービスで、企業向けにプライベートリポジトリを提供します。公開リポジトリと比較すると、制御とプライバシーの要件を満たす柔軟性があります。
Git：バージョン管理システムです。複数の開発者が1つのプロジェクトに対して同時に作業を進められます。効率的なコード開発とバージョン管理が可能です。
正解についての説明：
（選択肢）
・Artifact Registryを使用して、公開および非公開チャートをOCIフォーマットで保存します
この選択肢が正解の理由は以下の通りです。
まず、Artifact RegistryはGoogle Cloudが提供するパッケージ管理サービスで、ネイティブなアクセス制御を提供します。これは異なるチャートへのアクセスを統一的に管理する要件を満たします。
また、VPC Service Controlsもサポートされており、仮想ネットワークレベルの保護を提供します。
さらに、Artifact RegistryはOCI（Open Container Initiative）フォーマットをサポートしています。これにより、Helmチャートをパッケージとして管理でき、公開及び非公開チャートに一貫性を持たせることが可能になります。よって、セキュリティレベルの向上と一貫したチャート管理を実現するためにArtifact Registryの使用が推奨されます。
不正解についての説明：
選択肢：GitHub EnterpriseとGoogle WorkspaceをIDプロバイダとして使用することで、パブリックチャートとプライベートチャートを保存できます
この選択肢が正しくない理由は以下の通りです。
GitHub EnterpriseとGoogle WorkspaceがIDプロバイダとして有効であろうと、これらの組み合わせは統一的にチャートを管理するものではなく、チャートのアクセス制御やVPC Service Controlsの要件に対して解決策を提供しません。
それに対して、Artifact Registryはアクセス制御とVPC Service Controlsの両方を利用し、Helmチャートの統一管理を提供します。
選択肢：Gitリポジトリを使って、パブリックチャートとプライベートチャートを保存します。Cloud Storageバケットにリポジトリの内容を同期するようにCloud Buildを設定します。Helmリポジトリとしてhttps://[bucket].storage-googleapis.com/[helmchart]を使用して、Helmをバケットに接続します
この選択肢が正しくない理由は以下の通りです。
GitリポジトリとCloud Storageバケットを用いても、統一的な管理が困難であり、ネイティブのアクセス制御とVPC Service Controlsのフル活用は難しいと言えます。
一方、Artifact Registryを使えば、公開および非公開のHelmチャートを統一的かつセキュアに管理できます。
選択肢：Google Kubernetes Engine（GKE）で動作するHelmチャートリポジトリサーバを設定し、ストレージバックエンドとしてCloud Storage bucketを使用します
この選択肢が正しくない理由は以下の通りです。
GKEで動作するHelmチャートリポジトリサーバを設定すると、VPC Service Controlsのようなネイティブのアクセス制御を利用することが難しくなります。
また、Artifact Registryを使うことで、公開・非公開チャートを統一的に管理でき、セキュリティリスクを軽減できます。
参考リンク：
https://cloud.google.com/artifact-registry/docs/overview
https://cloud.google.com/artifact-registry/docs/helm/manage-helm-charts
https://helm.sh/docs/topics/registries/
<details><div>

### Q. 問題5: 未回答
インフラストラクチャDevOpsエンジニアのチームが成長し、インフラストラクチャを管理するためにTerraformを使い始めています。コードのバージョン管理を実装し、他のチームメンバーとコードを共有する方法が必要です。
この要件を満たすために、どうすればよいですか？
1. オブジェクトバージョニングを使ってTerraformのコードをCloud Storageのバケットに保存します。チームメンバー全員にバケットへのアクセス権を付与し、ファイルをダウンロードできるようにします
2. TerraformのコードをGoogle Driveの共有フォルダに保存し、チームメンバー全員のコンピューターに自動的に同期させます。新しいバージョンを識別できるような命名規則でファイルを整理します
3. Terraformコードをネットワーク共有フォルダに格納し、各バージョンリリースごとに子フォルダを作成します。全員が異なるファイルで作業するようにします
4. Terraformのコードをバージョン管理システムに保存します。新しいバージョンをプッシュし、マスターとマージする手順を確立します
<details><div>
    答え：4
説明
この問題では、インフラストラクチャ管理のためにTerraformを使用しているDevOpsエンジニアのチームが、コードのバージョン管理と共有をどのように実現すべきかが問われています。ここで重要なのは、各メンバーが同じバージョンのコードにアクセスし、それらを適切に管理・更新できる環境を作ることです。また、選択肢からは、バージョン管理システムの利用、ネットワーク共有フォルダの使用、Cloud Storageのバケットへの保存、Google Driveへのアップロードなど、さまざまな方法が提示されており、それぞれの利点と欠点を理解し最適な選択を行うことが求められます。
基本的な概念や原則：
Terraform：インフラストラクチャをコードで管理するためのオープンソースのツールです。クラウドサービス、オンプレミスのリソースなど、多様なインフラを一元的に管理することができます。
バージョン管理システム：ソフトウェアのバージョンを管理するためのツールです。コードの変更履歴を保存し、必要に応じて以前のバージョンに戻すことが可能です。また、複数の人が同時に作業しても衝突を防ぐことができます。
マージ：バージョン管理システムで、複数のブランチから変更を取り込んで新しいバージョンを作成する操作です。コンフリクトが発生した場合は適切に解消する必要があります。
ネットワーク共有フォルダ：複数の人が共有して利用できるフォルダです。ただし、バージョン管理や変更履歴のトラッキングは基本的にはサポートされていません。
オブジェクトバージョニング：Cloud Storageなどのオブジェクトストレージサービスが提供する機能で、オブジェクトの異なるバージョンを保存し、必要に応じて元に戻すことが可能です。
Google Drive：Googleが提供するCloud Storageサービスです。ファイルの共有や同期が容易ですが、バージョン管理や変更履歴の確認は限定的です。
正解についての説明：
（選択肢）
・Terraformのコードをバージョン管理システムに保存します。新しいバージョンをプッシュし、マスターとマージする手順を確立します
この選択肢が正解の理由は以下の通りです。
まず、Terraformのコードをバージョン管理システムに保存することで、変更履歴を追跡することが可能になります。これにより、以前のバージョンに簡単に戻すことも可能になりますし、発生した問題がどの変更によって引き起こされたのかを素早く把握することも可能になります。これによって、問題の特定と修正が容易になります。
また、新しいバージョンをプッシュしてマスターとマージするという手順を確立することで、複数の開発者が同時に作業を行う場合でも、全体としての整合性を維持することが可能となります。新しい特性を追加または既存の特性を変更するための作業は、分離されたブランチで行い、その変更が問題ないと確認できた時点でマスターブランチにマージするという流れを作ることで、互いの作業が干渉し合うことなく進めることができます。
したがって、Terraformのコードをバージョン管理システムに保存し、定められた手順に従って新しいバージョンをマスターとマージすることで、インフラストラクチャのバージョン管理と、チームでの効率的な共有と協調作業が可能となります。
不正解についての説明：
選択肢：Terraformコードをネットワーク共有フォルダに格納し、各バージョンリリースごとに子フォルダを作成します。全員が異なるファイルで作業するようにします
この選択肢が正しくない理由は以下の通りです。
ネットワーク共有フォルダはバージョン管理には適しておらず、コードの変更履歴の追跡やコンフリクトの解決が困難です。これに対してバージョン管理システムはこれらの問題を解決し、チーム作業に適しています。
選択肢：オブジェクトバージョニングを使ってTerraformのコードをCloud Storageのバケットに保存します。チームメンバー全員にバケットへのアクセス権を付与し、ファイルをダウンロードできるようにします
この選択肢が正しくない理由は以下の通りです。
オブジェクトバージョニングを用いたCloud Storageのバケットでのコード共有は、コードのバージョン管理を実装しやすいバージョン管理システムと比べて、共有や更新の管理が困難であり、複数人での作業の効率性が低くなります。
それに対して、バージョン管理システムの利用は、効率的かつ一貫性のあるコード管理を可能にします。
選択肢：TerraformのコードをGoogle Driveの共有フォルダに保存し、チームメンバー全員のコンピューターに自動的に同期させます。新しいバージョンを識別できるような命名規則でファイルを整理します
この選択肢が正しくない理由は以下の通りです。
Google Driveはファイルのバージョン管理に有用ですが、コードのバージョン管理やマージ、差分の確認など開発作業に必要な機能を十分に提供できません。
また、命名規則によるバージョン管理は人為的なエラーが発生しやすく、Terraformコードの管理には適していません。
参考リンク：
https://cloud.google.com/source-repositories/docs
https://cloud.google.com/architecture/managing-infrastructure-as-code
https://www.terraform.io/docs/cli/index.html
<details><div>

### Q. 問題6: 未回答
機密情報にアクセスする必要のあるアプリケーションを導入しようとしています。この情報は暗号化され、万が一侵害が発生した場合でも漏洩のリスクが最小限に抑えられるようにする必要があります。
この要件を満たすために、どうすればよいですか？
1. 暗号鍵をCloud Key Management Service（KMS）に保管し、鍵を頻繁にローテーションします
2. アプリケーションをシングルサインオン（SSO）システムと統合し、アプリケーションにシークレットを公開しません
3. アプリケーションのインスタンスごとに複数のバージョンのシークレットを生成する継続的ビルドパイプラインを活用します
4. 暗号化された構成管理システムを介して、インスタンス作成時にシークレットを注入します
<details><div>
    答え：1
説明
この問題では、機密情報にアクセスするアプリケーションの導入とその情報漏洩リスクの最小化が中心的なテーマとなっています。解答する際には、情報漏洩防止のために最も適したセキュリティの手法、特に暗号化と鍵管理に焦点を合わせる必要があります。特に鍵の保管とローテーションの方法によって、情報が侵害された場合でも被害を最小限に抑えるための策が問われています。選択肢の中で最もセキュアで適切な鍵管理の手法を選ぶことが重要です。
基本的な概念や原則：
Cloud Key Management Service（KMS）：対称鍵暗号化と非対称鍵暗号化を提供するクラウドベースのサービスです。暗号化キーの生成、使用、破棄、管理、およびローテーションを容易に行えます。
暗号化：データを保護するための手段の一つで、利用者以外の者がデータを理解できないような形式に変換する手法です。暗号化されたデータは、適切な鍵を持つ利用者だけが元の形式に戻すことができます。
キーローテーション：定期的に暗号化キーを新しいものと交換することです。これにより、旧キーによる不正アクセスを防ぐことができます。
構成管理システム：ソフトウェアとハードウェアの設定を管理し、一貫性と再現性を保つためのシステムです。
シングルサインオン（SSO）：複数のアプリケーションまたはサービスに対して、一度の認証でアクセスできるようにするシステムです。ユーザー管理やパスワード管理を効率化します。
ビルドパイプライン：コードの変更を自動的にビルド、テスト、デプロイするプロセスです。デプロイの効率化やエラー発見の早期化に寄与します。
正解についての説明：
（選択肢）
・暗号鍵をCloud Key Management Service（KMS）に保管し、鍵を頻繁にローテーションします
この選択肢が正解の理由は以下の通りです。
Google CloudのKey Management Service（KMS）は、機密情報を暗号化し、それを安全に管理するためのプラットフォームです。KMSを用いることで、機密情報に対するアクセス制御を強化し、暗号鍵のライフサイクル全体の管理が可能になります。更に、KMSの特徴として、独自の暗号鍵を作成し、それを使用してデータを暗号化・復号化することが可能で、これにより暗号化されたデータが不適切にアクセスされるリスクを軽減できます。
また、KMSには鍵ローテーション機能も存在します。鍵ローテーションとは、定期的に暗号化用の鍵を新しくすることで、それにより鍵が漏洩しても新しい鍵に切り替わるまでの窓口を短くし、漏洩リスクを最小化します。以上の理由から、この選択肢が適切な対策と言えます。
不正解についての説明：
選択肢：暗号化された構成管理システムを介して、インスタンス作成時にシークレットを注入します
この選択肢が正しくない理由は以下の通りです。
構成管理システムでシークレットを注入する手法は、管理が難しく鍵のローテーションができないため安全性に欠けます。
一方、Cloud KMSを使用すると、鍵の管理とローテーションが容易く、万が一の侵害が発生した場合でも安全性が確保できます。
選択肢：アプリケーションをシングルサインオン（SSO）システムと統合し、アプリケーションにシークレットを公開しません
この選択肢が正しくない理由は以下の通りです。
シングルサインオン（SSO）システムとの統合はユーザー認証やアクセス管理に役立ちますが、機密情報の暗号化やその鍵の管理を提供しません。
一方、Cloud KMSは暗号鍵の管理とローテーションを行い、機密情報を適切に保護するための機能を提供します。
選択肢：アプリケーションのインスタンスごとに複数のバージョンのシークレットを生成する継続的ビルドパイプラインを活用します
この選択肢が正しくない理由は以下の通りです。
インスタンスごとに複数バージョンのシークレットを生成する継続的ビルドパイプラインは、状況によりシークレット管理が複雑になり、安全性を保証するための最善の方法ではありません。
一方、Cloud Key Management Serviceを使用すると、中心化と管理が容易になり、鍵の頻繁なローテーションにより安全性が確保できます。
参考リンク：
https://cloud.google.com/kms/docs
https://cloud.google.com/kms/docs/rotating-keys
https://cloud.google.com/security/encryption-at-rest/default-encryption
<details><div>

### Q. 問題7: 未回答
セキュリティのシフトレフトを目指すあなたの会社のイニシアチブの一環として、InfoSecチームは全チームに対し、すべてのGoogle Kubernetes Engine（GKE）クラスターにガードレールを実装し、信頼され承認されたイメージのデプロイのみを許可するよう求めています。あなたは、セキュリティのシフトレフトというInfoSecチームの目標を満たす方法を決定する必要があります。
この要件を満たすために、どうすればよいですか？
1. Artifact Registryでコンテナ分析を有効にし、コンテナイメージの一般的な脆弱性と暴露（CVE）をチェックします
2. 実行中のポッドの脆弱性を監視するために、FalcoまたはTwistlockをGKEにデプロイします
3. IAM（Identity and Access Management）ポリシーを構成して、GKEクラスターに最小権限モデルを作成します
4. バイナリ認証を使用して、CI/CDパイプラインでイメージを認証します
<details><div>
    答え：4
説明
この問題では、セキュリティのシフトレフトという概念とその具現化を理解することがキーとなります。シフトレフトは開発プロセスの早い段階でセキュリティを取り入れることを意味し、すべてのGKEクラスターにおいて信頼できるイメージのみデプロイするという要求が出されています。選択肢を見るときには、この目指すべき状態を満たす手段に焦点を当て、GKEのクラスターレベルで操作するよりも早い段階、つまりCI/CDパイプラインのステージでセキュリティを担保する選択肢を探すべきです。
基本的な概念や原則：
バイナリ認証：Google Kubernetes Engine（GKE）環境でのコンテナイメージの実行を制御するための運用時間のセキュリティ機能です。信頼されたソースからのイメージのみがデプロイできるようにします。
セキュリティのシフトレフト：セキュリティの考慮をプロジェクトの初期段階に持ち込むアプローチです。開発プロセスの初期段階でセキュリティ問題を検出し、修正します。
Artifact Registry：Google Cloudのパッケージ管理サービスで、Dockerコンテナイメージや言語ベースのパッケージを保存、管理します。ワークロードのセキュリティと信頼性を向上させるために使用します。
IAM（Identity and Access Management）：Google Cloudリソースへの認証と認可を管理するサービスです。IAMポリシーを使って、特定のユーザーが特定のリソースに対して何をすることができるかをコントロールします。
Falco、Twistlock：Kubernetesセキュリティのツールです。実行中のポッドの脆弱性を監視し、不正な行為を検出します。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）を組み合わせた開発プロセスです。コードの変更を自動的にビルド、テスト、デプロイします。
正解についての説明：
（選択肢）
・バイナリ認証を使用して、CI/CDパイプラインでイメージを認証します
この選択肢が正解の理由は以下の通りです。
まず、バイナリ認証はGoogle Cloudのサービスで、信頼できるイメージのみがデプロイされることを保証します。これは、環境に対する全てのイメージデプロイを制御し、未承認や不明なソースからのデプロイを防ぎます。そのため、InfoSecチームが求めている"信頼され承認されたイメージのデプロイのみを許可する"という要件を満たすことができます。
次に、バイナリ認証はCI/CDパイプライン（継続的インテグレーション/継続的デリバリー）と統合され、デプロイプロセス中にイメージが信頼できるかどうかを自動的に検証します。これにより、セキュリティのシフトレフトを進めることができます。セキュリティのシフトレフトとは、開発の初期段階からセキュリティを考慮に入れるアプローチのことで、この早期に問題を特定することでリスクを軽減し、修正費用を削減することができます。
不正解についての説明：
選択肢：Artifact Registryでコンテナ分析を有効にし、コンテナイメージの一般的な脆弱性と暴露（CVE）をチェックします
この選択肢が正しくない理由は以下の通りです。
Artifact Registryでのコンテナ分析はイメージの脆弱性をチェックする機能ですが、特定のイメージのデプロイを制限するガードレールを実装する機能はありません。
一方、バイナリ認証を使用すれば、信頼され承認されたイメージのみのデプロイを実現できます。
選択肢：IAM（Identity and Access Management）ポリシーを構成して、GKEクラスターに最小権限モデルを作成します
この選択肢が正しくない理由は以下の通りです。
IAMポリシーを配置してGKEクラスターに最小権限モデルを作成するという手法は、認証やアクセス制御には効果的ですが、特定の信頼され承認されたイメージのデプロイのみを許可するという具体的な要件を実現するには対応していません。
一方、バイナリ認証はCI/CDパイプライン内で特定のイメージが期待するように署名され証明されたものであるかを確認し、その結果次第でGKE上へのデプロイを控えることが可能です。これにより、問題の要件を概念的にも技術的にも満たすことができます。
選択肢：実行中のポッドの脆弱性を監視するために、FalcoまたはTwistlockをGKEにデプロイします
この選択肢が正しくない理由は以下の通りです。
FalcoやTwistlockは、既にデプロイされたポッドの脆弱性を監視するツールですが、この問題はあらかじめ承認されたイメージのみデプロイできるようにする点に重点が置かれています。
したがって、正解のバイナリ認証を用いてCI/CDパイプラインでイメージが認証されるべきです。
参考リンク：
https://cloud.google.com/binary-authorization/docs
https://cloud.google.com/kubernetes-engine/docs/concepts/binary-authorization
https://cloud.google.com/artifact-registry/docs/container-analysis
<details><div>

### Q. 問題8: 未回答
パブリックIPアドレスを持つCompute Engineインスタンス上で実行される新しいアプリケーションのためにCloud Loggingを構成しています。ユーザー管理サービスアカウントがインスタンスにアタッチされています。必要なエージェントがインスタンス上で実行されていることを確認しましたが、Cloud Loggingでインスタンスからのログエントリが表示されません。Googleが推奨するプラクティスに従って問題を解決したいと考えています。
この要件を満たすために、どうすればよいですか？
1. デフォルトのCompute Engineサービスアカウントを使用するようにインスタンスを更新します
2. サービスアカウントにLogs Writerロールを追加します
3. サービスアカウントキーをエクスポートし、そのキーを使用するようにエージェントを設定します
4. インスタンスがあるサブネットでプライベートGoogleアクセスを有効にします
<details><div>
    答え：2
説明
この問題では、Cloud Loggingとサービスアカウントの権限に関する理解が必要となります。Compute EngineインスタンスからCloud Loggingにログエントリが表示されない状況が提示されており、設問はその解決法を尋ねています。Googleの推奨するプラクティスを用いて問題を解決する必要があります。そのため、選択肢を評価する際には、Google Cloudのサービスアカウントと権限の管理、特にログ管理に関わるベストプラクティスに基づいた選択をすることが求められています。
基本的な概念や原則：
Compute Engineインスタンス：Google Cloudの仮想マシン（VM）です。Compute Engineは、VMインスタンスを作成し、管理するためのサービスです。
Cloud Logging：Google Cloudのログ管理サービスで、アプリケーション、システム、ユーザーの行動などのログデータを取り込み、管理するためのものです。
サービスアカウント：アプリケーションや仮想マシン（VM）インスタンスがGoogle Cloudリソースと通信するための資格情報を提供する特殊な種類のGoogleアカウントです。
Logs Writerロール：Google Cloud Logging APIにログエントリを作成する権限を提供します。"roles/logging.logWriter"とも呼ばれます。
プライベートGoogleアクセス：Compute EngineインスタンスがプライベートIPアドレスを使用してGoogle Cloud APIを呼び出すことを可能にします。インスタンスはパブリックIPアドレスを持っていなくてもこれらのAPIを呼び出すことができます。
正解についての説明：
（選択肢）
・サービスアカウントにLogs Writerロールを追加します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloudにおいて、サービスアカウントはアプリケーションに代わってGoogle Cloud APIとやり取りを行うためのエンティティです。サービスアカウントにはロールが割り当てられており、そのロールに応じた権限がサービスアカウントに付与されます。
この問題では、インスタンスからCloud Loggingでログエントリが表示できないという課題があります。Cloud Loggingでログエントリを表示するためには、ログの書き込みに必要な権限をもったサービスアカウントが必要となります。ここで重要なロールがLogs Writerです。Logs Writerロールはログエントリを作成し、Cloud Loggingに書き込む権限を与えます。
したがって、ユーザーが管理するサービスアカウントにLogs Writerロールを追加すれば、問題は解決します。
不正解についての説明：
選択肢：サービスアカウントキーをエクスポートし、そのキーを使用するようにエージェントを設定します
この選択肢が正しくない理由は以下の通りです。
サービスアカウントキーをエクスポートするという手法は、必要な認証や許可を求める方法ではなく、セキュリティリスクが高まる可能性があります。正解の選択肢が示すように、適切なロールをサービスアカウントに追加することで、エージェントがCloud Loggingにログを書き出す権限を適切に与えることが推奨されます。
選択肢：デフォルトのCompute Engineサービスアカウントを使用するようにインスタンスを更新します
この選択肢が正しくない理由は以下の通りです。
デフォルトのCompute Engineサービスアカウントを使用するように更新するだけでは、ログエントリがCloud Loggingに送信されない問題は解決しません。ログ書き込みrequires Logs Writerロールが必要なため、正解はサービスアカウントにLogs Writerロールを追加することです。
選択肢：インスタンスがあるサブネットでプライベートGoogleアクセスを有効にします
この選択肢が正しくない理由は以下の通りです。
プライベートGoogleアクセスは内部IPアドレスからGoogle APIやサービスにアクセスするための設定ですが、今回の問題ではインスタンスはパブリックIPを持っています。
それに対して、正答のLogs WriterロールはCloud Loggingにログを書き込むための権限です。
参考リンク：
https://cloud.google.com/logging/docs/agent/authorization
https://cloud.google.com/iam/docs/understanding-roles#logging-roles
https://cloud.google.com/logging/docs/setup/compute
<details><div>

### Q. 問題9: 未回答
Cloud Buildを使って新しいDockerイメージをビルドし、Docker HubにプッシュするCI/CDパイプラインを持っています。コードのバージョン管理にはGitを使用しています。Cloud BuildのYAML設定を変更した後、パイプラインによって新しい成果物がビルドされていないことに気づきました。サイト信頼性エンジニアリングのプラクティスに従って問題を解決する必要があります。
この要件を満たすために、どうすればよいですか？
1. Docker Hubの代わりにContainer Registryで成果物をプッシュするようにCIパイプラインを変更します
2. 設定YAMLファイルをCloud Storageにアップロードし、Error Reportingを使って問題を特定して修正します
3. CIパイプラインを無効にし、手作業でのビルドと成果物のプッシュに戻します
4. 以前のCloud Build Configurationファイルと現在のCloud Build ConfigurationファイルのGit比較を実行し、バグを見つけて修正します
<details><div>
    答え：4
説明
この問題では、Cloud Buildを活用したCI/CDパイプラインでのトラブルシューティングが求められています。DockerイメージのビルドとDocker Hubへのプッシュが行われず、Cloud BuildのYAML設定に変更が加えられた後の状況を手がかりとします。さらに、故障解析にはサイト信頼性エンジニアリングのプラクティスを適用する必要があります。そのため、なぜ新しい成果物がビルドされていないのかを特定し、それを修正するための適切な手段を選択することが求められています。
基本的な概念や原則：
Cloud Build：Google Cloudの持続的インテグレーションとデリバリーサービスです。ソースコードからコンテナイメージやアプリケーションをビルド、テスト、デプロイします。
Docker Hub：Dockerの公式レジストリサービスで、Dockerイメージの保存と共有が可能です。
CI/CDパイプライン：ソフトウェア開発で用いられる手法で、開発者の変更を本番環境に自動的に反映させる仕組みです。
Git：分散型のバージョン管理システムです。コードの変更履歴を追跡し、複数人の開発者が共同で作業するのに役立ちます。
サイト信頼性エンジニアリング（SRE）：システムやサービスの信頼性・スケーラビリティ・効率性を向上させるためのエンジニアリング分野です。自動化とシステム思考を活用して問題の解決と予防を行います。
Cloud Storage：Google Cloudのオブジェクトストレージサービスです。設定ファイルやデータの保存に用いられます。
Error Reporting：Google Cloudのエラー管理とトラッキングサービスです。実行中のアプリケーションで発生したエラーを自動的に追跡・通知します。
正解についての説明：
（選択肢）
・以前のCloud Build Configurationファイルと現在のCloud Build ConfigurationファイルのGit比較を実行し、バグを見つけて修正します
この選択肢が正解の理由は以下の通りです。
サイト信頼性エンジニアリング（SRE）の一つの原則は、変更によるインシデントを詳細に調査し、それが再発しないようにすることです。ここで指摘されている問題は、Cloud Buildの設定（YAMLファイル）の変更を行った直後に新しい成果物がビルドされなくなったことです。
したがって、この問題が発生した直接的な原因は、その変更にある可能性が高いです。このため、以前のCloud Build Configurationファイルと現在のCloud Build Configurationファイルを比較して変更点を明らかにすることは最適で、バージョン管理ツールであるGitが比較作業を容易に行えるため有用です。その結果、直近の変更により問題が発生したバグを特定し、修正することが可能となります。このようなアプローチは、SREのプラクティスに従った問題解決手段といえます。
不正解についての説明：
選択肢：CIパイプラインを無効にし、手作業でのビルドと成果物のプッシュに戻します
この選択肢が正しくない理由は以下の通りです。
CIパイプラインを無効にし手作業でビルド、プッシュに戻すという方法は、問題を解決するというよりも回避するという行為であり、バグの特定や修正が進まず問題の本質解決になりません。
一方、正しい選択肢はバグを特定し修正することで問題の根本的な解決を目指しています。
選択肢：Docker Hubの代わりにContainer Registryで成果物をプッシュするようにCIパイプラインを変更します
この選択肢が正しくない理由は以下の通りです。
問題はYAML設定の変更によって新しい成果物がビルドされていない事に起因しています。Docker HubからContainer Registryへの切り替えは問題解決に寄与せず、正しく設定が行われているかの確認が必要です。
選択肢：設定YAMLファイルをCloud Storageにアップロードし、Error Reportingを使って問題を特定して修正します
この選択肢が正しくない理由は以下の通りです。
Cloud StorageとError Reportingを用いて設定の問題を特定する手法は、この状況には適していません。これは、エラーリポーティングがCI/CDパイプラインの問題を特定するための適切なツールではないからです。最も有効な手法は、Gitを使用して設定変更を追跡し、問題を特定することです。
参考リンク：
https://cloud.google.com/build/docs/building/build-containers
https://cloud.google.com/error-reporting/docs
https://cloud.google.com/sre/docs/sre-book/managing-change
<details><div>

### Q. 問題10: 未回答
あなたは、明確に定義されたサービスレベル目標（SLO）を持つ大規模なサービスをサポートしています。開発チームはサービスの新しいリリースを週に何度もデプロイします。
重大なインシデントが原因でサービスがSLOを逸脱した場合、開発チームは機能開発からサービスの信頼性向上に重点を移す必要があります。
重大インシデントが発生する前に何をすべきですか？
1. 開発チームと交渉し、リリース頻度を週1回以下にします
2. Jenkinsパイプラインにプラグインを追加し、サービスがSLOから外れるたびに新しいリリースが行われないようにします
3. 新機能のリリースよりもサービスの信頼性を常に優先するよう、製品チームと交渉します
4. すべてのサービス関係者と協力して、適切なエラーバジェット方針を策定します
<details><div>
    答え：4
説明
この問題では、大規模なサービスの運用管理に関して問われています。指定されたサービスレベル目標（SLO）を達成するために、重大なインシデントが発生する前に何をすべきかについて考える必要があります。また、開発チームが頻繁に新しいリリースをデプロイする状況と、インシデントが起きた場合には信頼性向上に注力するという要件にも注意が必要です。この状況を解決するための適切な方策を選択肢から選びましょう。選択肢を評価する際には、SLOを尊重しつつ、開発チームの生産性を維持するバランスを見つけることが求められます。
基本的な概念や原則：
サービスレベル目標（SLO）：サービスの運用目標の一つで、サービスが達成すべき品質を特定の指標で表したものです。
エラーバジェット：サービスレベル目標（SLO）と実際のサービスレベル指標（SLI）との差（すなわち、許容されるエラーの割合）を指す概念です。エラーバジェットが耗尽すると、開発チームは新機能の追加よりも信頼性の向上に参加する必要があります。
エラーバジェット方針：エラーバジェットの管理と消費をどのように行うかを定めた方針です。サービスの各ステークホルダーと共に策定することが望ましいです。
リリース頻度：新しい機能や修正を実装する頻度のことです。頻繁なリリースは新しい機能を迅速に提供する利点がありますが、リスクの増加やエラーバジェットの消費速度を加速させる可能性があります。
信頼性：システムが特定の期間にわたって、かつ特定の条件下で正常に機能する確率のことを指します。サービスの信頼性の向上は、エラーバジェットの管理と密接に関連しています。
正解についての説明：
（選択肢）
・すべてのサービス関係者と協力して、適切なエラーバジェット方針を策定します
この選択肢が正解の理由は以下の通りです。
まず、エラーバジェットとは、サービスがSLOを守りつつ許容できる障害レベルの量を定義したものです。これはSLOを逸脱したと判定される前に、どの程度のエラーが許容されるかを決定します。大規模なサービス、特に頻繁に新しいリリースをデプロイするサービスでは、エラーバジェットの策定が重要となります。
エラーバジェット方針は、開発チーム、サービスの運用担当者、その他すべての関係者が協力して策定するべきです。これは、すべての関係者がサービスの目標と信頼性について合意し、共通の理解を持つことを可能にします。
したがって、協力してエラーバジェット方針を策定することは、重大インシデントが発生する前の必要な行動となります。
不正解についての説明：
選択肢：新機能のリリースよりもサービスの信頼性を常に優先するよう、製品チームと交渉します
この選択肢が正しくない理由は以下の通りです。
常にサービスの信頼性を優先すると、新機能のリリースが停滞してしまい、ビジネスの進展に影響を及ぼす可能性があります。エラーバジェット方針を策定することで、信頼性と機能開発のバランスを適切に保つことが可能となります。
選択肢：開発チームと交渉し、リリース頻度を週1回以下にします
この選択肢が正しくない理由は以下の通りです。
リリース頻度を週1回以下に制限するアプローチは制約的であり、開発の効率とイノベーションを妨げる可能性があります。
逆に、エラーバジェットの策定はサービスの信頼性とリリースのペースをバランス良く維持する効果的な手段です。
選択肢：Jenkinsパイプラインにプラグインを追加し、サービスがSLOから外れるたびに新しいリリースが行われないようにします
この選択肢が正しくない理由は以下の通りです。
Jenkinsパイプラインにプラグインを追加し、SLOから外れるたびに新しいリリースが行われないようにするアプローチは、事後的な対応となり、事前の対策を効果的に行うのに役立ちません。エラーバジェット方針を策定して事前に対策を練ることが適切な戦略です。
参考リンク：
https://cloud.google.com/architecture/framework
https://cloud.google.com/sre/docs/sre-book/implementing-slos
https://cloud.google.com/blog/products/Google Cloud/sre-fundamentals-slis-slas-and-slos
<details><div>

### Q. 問題11: 未回答
あなたの会社のGoogle Cloudリソース階層には、本番用、テスト用、開発用のフォルダがあります。サイバーセキュリティチームは、セキュリティ問題の特定と解決を加速するために、自社のGoogle Cloudセキュリティ体制を見直す必要があります。すべてのプロジェクトからGoogle Cloudサービスによって生成されたログを本番用フォルダ内にのみ集中させ、アラートとほぼリアルタイムの分析を可能にする必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. Pub/Subトピックを宛先として使用する、本番フォルダに関連付けられた集約ログシンクを作成します
2. Cloud Loggingバケットを宛先として使用する、本番フォルダに関連付けられた集約ログシンクを作成します
3. 中央のCloud Monitoringワークスペースを作成し、関連するすべてのプロジェクトを添付します
4. Workflows APIを有効にし、すべてのログをCloud Loggingにルーティングします
<details><div>
    答え：1
説明
この問題では、Google Cloudのリソース階層内の各プロジェクトからログを集約し、セキュリティのほぼリアルタイム分析を可能にする方法を尋ねています。特に、そのログを本番用フォルダ内に集中させることが重要とされています。このような要件を満たすには、Google Cloudの各サービス、特にログ関連の機能を理解して適切に活用することが求められます。そのために最適なサービスやロギングの方法を選択することが重要であり、それが正当な選択肢に結びつきます。また、選択肢の中には、特定の目的でセットアップされたストレージやピアツーピーサービスなど、不適切なサービスを用いた解答例も含まれているため、その辺りの理解も重要となります。
基本的な概念や原則：
Google Cloudリソース階層：Google Cloudプロジェクトの組織構造で、最上位には組織、その下にはフォルダ、プロジェクト、そしてリソースが配置されます。
フォルダ：Google Cloudのリソース階層の一部で、組織をさらにカテゴライズして管理することができます。本番、テスト、開発などの用途で使用されます。
集約ログシンク：特定のリソース階層に関連付とされたlogエントリを収集する設定です。これを使用すると、複数のプロジェクトからのログを集約して解析することが可能になります。
Google Cloud Pub/Sub：Google Cloudのメッセージングサービスであり、ログデータなどをリアルタイムで送受信するためのサービスです。
Cloud Logging：Google Cloudのログ管理サービスで、Google Cloudプロジェクトのログエントリーを集約、表示、解析するためのツールです。
Cloud Monitoring：Google Cloudの監視サービスで、アプリケーションやインフラストラクチャのパフォーマンスを監視します。しかし、リアルタイムのログ情報のように高頻度で更新されるデータを扱うには制限があります。
Workflows API：Google Cloudの統合サービスで、複数のGoogle Cloudサービスを一つの自動化フローとして組み合わせることが可能です。しかし、リアルタイムで大量のログ情報を取り扱うことは得意でないです。
正解についての説明：
（選択肢）
・Pub/Subトピックを宛先として使用する、本番フォルダに関連付けられた集約ログシンクを作成します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloudでは、Cloud Loggingを使ってログを集約して分析することが可能です。この際、ログシンクを作成することで、指定した条件にマッチするログエントリを自動的にエクスポートできます。このシンクは異なるリソースヒエラルキーレベル（組織、フォルダ、プロジェクト）で作成でき、親のヒエラルキーレベルで定義したシンクはその下位の子のレイヤーに継承されます。
したがって、本番フォルダにシンクを作成することで、そのフォルダ内のすべてのプロジェクトから生成されるログを集約することができます。
次に、Pub/Subトピックをシンクのエクスポート先とすることで、集約したログをニアリアルタイムで解析できます。Pub/Subは高度なスケーリング性を備えたメッセージングサービスであり、発行されたメッセージをすぐに利用可能にします。この特性を利用することで、セキュリティアラートの即時性と、リアルタイム分析の要件を満たすことができます。
不正解についての説明：
選択肢：Workflows APIを有効にし、すべてのログをCloud Loggingにルーティングします
この選択肢が正しくない理由は以下の通りです。
Workflows APIは、一連のステップを実行するために使用されますが、リアルタイムの分析やアラートを生成するためには適していません。
代わりに、Pub/Subトピックを使用すると、ログを集約してリアルタイムに分析し、アラートを設定することが可能になります。
選択肢：中央のCloud Monitoringワークスペースを作成し、関連するすべてのプロジェクトを添付します
この選択肢が正しくない理由は以下の通りです。
Cloud Monitoringはモニタリングとアラートに最適化されているものの、ログを集中させることはできません。
一方、Pub/Subトピックを宛先として使用する集約ログシンクはログを集中化し、ほぼリアルタイムの分析を可能にするという要件に適合します。
選択肢：Cloud Loggingバケットを宛先として使用する、本番フォルダに関連付けられた集約ログシンクを作成します
この選択肢が正しくない理由は以下の通りです。
Cloud Loggingバケットを宛先とすると、ほぼリアルタイムの分析を行うのが困難になります。
一方、Pub/Subトピックを利用すれば、Pub/Subの即時メッセージング機能により、データをほぼリアルタイムで集約、分析することが可能です。
参考リンク：
https://cloud.google.com/logging/docs/aggregated-exports
https://cloud.google.com/pubsub/docs
https://cloud.google.com/logging/docs/storage
<details><div>

### Q. 問題12: 未回答
あなたはグローバルな組織で働いており、限られたエンジニアリングリソースで99%の可用性を目標にサービスを運営しています。
当月のサービスの可用性は99.5%でした。あなたは、サービスが定義された可用性目標を満たし、今後の新機能の立ち上げを含むビジネスの変化に対応できることを保証しなければなりません。
また、運用コストを最小限に抑えながら、技術的負債を減らす必要があります。あなたは、Googleが推奨するプラクティスに従いたいと考えています。
この要件を満たすために、どうすればよいですか？
1. 反復作業を自動化することで、労力を特定し、測定し、排除します
2. サービスレベルの可用性に対してエラーバジェットを定義し、残りのエラーバジェットを最小限に抑えます
3. 利用可能なエンジニアを機能バックログに割り当てる一方で、サービスが目標可用性内に収まるようにします
4. サービスにコンピュートリソースを追加することで、サービスにN+1冗長性を追加します
<details><div>
    答え：2
説明
この問題では、限られたリソースと可用性目標の達成に対応する方法として、Googleが推奨する最良のプラクティスを選ぶ必要があります。事前にGoogleの推奨プラクティスについて把握していることが役立ちます。可用性目標の達成、新機能の立ち上げへの対応、運用コストと技術的負債の削減という複数の要件も考慮して、各選択肢がこれらの要件をどのように満たすかを評価しなければなりません。選択肢を検討する際には、それぞれが全ての要件をどの程度満たすかを評価することが重要です。
基本的な概念や原則：
エラーバジェット：サービスレベル目標（SLO）から導き出される、予期せぬエラーやダウンタイムに対する許容量のことです。これを通じて、サービスの品質と開発速度をバランスすることが可能になります。
サービスレベル目標（SLO）：サービスの可用性、レイテンシ、スループットなど、サービスを評価する基準のことです。SLOは顧客の期待やビジネス要件に基づいて定められます。
N+1冗長性：システムのコンポーネントに対する冗長設計の一種で、システムの正常な運用に必要なコンポーネントに対して一つ以上の予備を確保することです。しかし、これには追加のコストが発生します。
作業の自動化：繰り返し行われるタスクを自動化することで、エラーレートを減らし、効率を向上させる手法です。しかし、自動化自体には開発とテストのコストがかかります。
技術的負債：システムが持つ欠陥や改善のための要件に対する債務のようなもので、これを減らすことで長期的なオペレーションの効率性を向上させることができます。
正解についての説明：
（選択肢）
・サービスレベルの可用性に対してエラーバジェットを定義し、残りのエラーバジェットを最小限に抑えます
この選択肢が正解の理由は以下の通りです。
まず、サービスレベルの可用性に対してエラーバジェットを定義するとは、サービスが許容することができるダウンタイムの最大量を定義することです。指定された可用性目標（例えば99%）から算出され、目標を上回った場合（例えば99.5%）、その差分がエラーバジェットとなります。これにより、システムがどの程度のエラーを許容できるのかを具体的に示すことができます。
残りのエラーバジェットを最小限に抑えるとは、限られたエラーバジェット内で新機能の立ち上げやシステムの改善を行うことを意味します。これにより、技術的負債を減らしながらも、ビジネスの変化に対応することが可能になります。
このアプローチは、Googleの推奨するプラクティスに含まれていて、運用コストを最小限に抑えることも可能です。つまり、この選択肢は問題の要求を満たしており、正解となります。
不正解についての説明：
選択肢：サービスにコンピュートリソースを追加することで、サービスにN+1冗長性を追加します
この選択肢が正しくない理由は以下の通りです。
コンピュートリソースを追加することで冗長性を確保するのは一見有効に思えますが、ビジネスの変化に対応するという要件や運用コストを最小限に抑えるという要件には対応していません。
それに対して、エラーバジェットを定義することにより、適切なトレードオフを達成しながらサービスレベルの可用性を維持できます。
選択肢：反復作業を自動化することで、労力を特定し、測定し、排除します
この選択肢が正しくない理由は以下の通りです。
反復作業の自動化は労力の節約と技術的負債の減少に寄与しますが、可用性目標の達成や新機能の立ち上げへの対応保証を直接的には支えません。
それに対して、エラーバジェットの定義と最小化はサービスレベルの可用性を管理する効果的な手法であり、目標達成の具体的な指標となります。
選択肢：利用可能なエンジニアを機能バックログに割り当てる一方で、サービスが目標可用性内に収まるようにします
この選択肢が正しくない理由は以下の通りです。
利用可能なエンジニアを機能バックログに割り当てる一方で、サービスが目標可用性内に収まるようにすると、新機能の立ち上げと技術的負債の減少という要件を同時に満たすことが難しくなります。
逆に、エラーバジェットを定義することで、新機能導入の影響を計算し、サービス目標を満たすのに適切なリソースを割り当てることができます。
参考リンク：
https://cloud.google.com/architecture/framework/operational-excellence/sre-change-management
https://cloud.google.com/sre/docs/sre-book/managing-risk-with-error-budgets
https://sre.google/books/service-level-objectives/chapter/operating-level-agreements/
<details><div>

### Q. 問題13: 未回答
あなたの会社では、CI/CDのためにGoogle Cloud VMインスタンス上で動作するJenkinsを使っています。あなたは、Terraformを使用することで、Infrastructure as Code Automationを使用するように機能を拡張する必要があります。Terraform JenkinsインスタンスがGoogle Cloudリソースを作成する権限があることを確認する必要があります。あなたは、Googleが推奨するプラクティスに従いたいと考えています。
この要件を満たすために、どうすればよいですか？
1. Jenkins VMインスタンスに、適切なIAM（Identity and Access Management）権限を持つサービスアカウントがアタッチされていることを確認します
2. Terraformインスタンス専用のサービスアカウントを作成します。シークレットキーの値をダウンロードし、JenkinsサーバのGOOGLE_CREDENTIALS環境変数にコピーします
3. Terraformコマンドを実行する前に、Jenkinsのステップとしてgcloud auth application-default loginコマンドを追加します
4. Secret Managerが認証情報を取得できるように、Terraformモジュールを使用します
<details><div>
    答え：1
説明
この問題では、Google Cloudの仮想マシンインスタンス上で稼働するJenkinsがTerraformを介してGoogle Cloudリソースの作成を可能にすることが求められています。その際にはGoogleが推奨するプラクティスに従うことが求められています。その要点としては、サービスアカウントの適切な使用やIAM権限の管理が挙げられます。また、認証情報やシークレットキーの管理も重要な要素となります。これらの要素を考慮に入れつつ、最適な解答選択肢を選びます。
基本的な概念や原則：
サービスアカウント：Google Cloudでアプリケーションやサービスが他のGoogleサービスと認証され、通信できるようにするための特殊なアカウントです。IAMポリシーによって権限を与えられます。
IAM（Identity and Access Management）：Google Cloudにおける認証と権限管理のフレームワークです。ユーザー、グループ、サービスアカウントへのリソースへのアクセスを制御できます。
Terraform：Infrastructure as Code（IaC）を実現するためのオープンソースのツールです。クラウドリソースの作成、変更、バージョン管理を自動化できます。
Secret Manager：Google Cloudのセキュアなストレージサービスで、APIキーやパスワードなどの機密情報を保存、管理できます。
Jenkins：オープンソースのCI/CDツールです。ビルド、テスト、デプロイなどの自動化をサポートします。
gcloud auth application-default loginコマンド：Google Cloud SDKのコマンドで、アプリケーションデフォルトの認証情報を取得します。ただし、プロダクション環境ではこの方法は推奨されていません。
正解についての説明：
（選択肢）
・Jenkins VMインスタンスに、適切なIAM（Identity and Access Management）権限を持つサービスアカウントがアタッチされていることを確認します
この選択肢が正解の理由は以下の通りです。
Google Cloudでは、Googleが推奨するプラクティスとして、インスタンスに操作権限を与えるべき相手（この場合はTerraform Jenkinsインスタンス）に対してサービスアカウントを用いることが推奨されています。サービスアカウントはアプリケーションやサービスがGoogle Cloudリソースに安全にアクセスするための特別な種類のアカウントです。
また、これらのサービスアカウントに対しては、必要な操作のみを許可する最小権限の原則に基づきIAM権限を付与することが推奨されています。
したがって、Jenkins VMインスタンスに適切なIAM権限を持つサービスアカウントをアタッチすることで、Terraform JenkinsインスタンスがGoogle Cloudリソースを作成する権限を確保することができます。以上から、この選択肢が要件を満たす最適な方法となります。
不正解についての説明：
選択肢：Secret Managerが認証情報を取得できるように、Terraformモジュールを使用します
この選択肢が正しくない理由は以下の通りです。
要件は、Terraform Jenkinsインスタンスが適切な権限を有していること確認することで、Secret Managerを使用すると認証情報の取得が可能になりますが、その使用が要件を満たすわけではありません。プラクティスとしては、サービスアカウントを通じて適切なIAM権限をアタッチするのが推奨されています。
選択肢：Terraformインスタンス専用のサービスアカウントを作成します。シークレットキーの値をダウンロードし、JenkinsサーバのGOOGLE_CREDENTIALS環境変数にコピーします
この選択肢が正しくない理由は以下の通りです。
Googleの推奨するプラクティスでは、シークレットキーをダウンロードし直接環境変数にコピーするのではなく、サービスアカウントを用いて認証を行います。これは、シークレットキーの取り扱いはリスクが高く、セキュリティの観点から推奨されないからです。正解の選択肢であるIAM権限を持つサービスアカウントを利用する方が、セキュリティが確保されます。
選択肢：Terraformコマンドを実行する前に、Jenkinsのステップとしてgcloud auth application-default loginコマンドを追加します
この選択肢が正しくない理由は以下の通りです。
gcloud auth application-default loginコマンドを使用すると、ユーザーアカウントを使ってGoogle Cloudサービスに対する認証を行いますが、これは本番環境では適切な方法ではありません。セキュリティ上の理由から、サービスアカウントを介した認証が推奨されます。そのため、Jenkins VMインスタンスに適切なIAM権限を持つサービスアカウントをアタッチする方法が必要です。
参考リンク：
https://cloud.google.com/iam/docs/service-accounts
https://cloud.google.com/docs/terraform
https://www.terraform.io/docs/providers/google/guides/getting_started.html
<details><div>

### Q. 問題14: 未回答
あなたの会社では、Pub/Sub、App Engineスタンダード環境、Goで書かれたアプリケーションを使用して、IoTデータを大規模に処理しています。ピーク負荷時にパフォーマンスが一貫して低下することに気づきました。ワークステーションではこの問題を再現できませんでした。本番環境でアプリケーションを継続的に監視し、コード内の遅いパスを特定する必要があります。パフォーマンスへの影響と管理オーバーヘッドを最小限に抑えたいと考えています。
この要件を満たすために、どうすればよいですか？
1. アプリケーションインスタンスに対して、goツールのpprofコマンドを定期的に実行します。フレームグラフを使って結果を分析します
2. Cloud Profilerを設定し、アプリケーションでcloud.google.com/go/profilerライブラリを初期化します
3. Compute Engineに継続的なプロファイリングツールをインストールします。プロファイリングデータをツールに送信するようにアプリケーションを設定します
4. Cloud Monitoringを使用して、App EngineのCPU使用率メトリックを評価します
<details><div>
    答え：2
説明
この問題では、大量のIoTデータを処理するアプリケーションのパフォーマンス低下問題を診断する方法について問われています。本番環境でのパフォーマンスの問題を特定し、それがコードのどの部分から引き起こされているかを明らかにしなければなりません。ピーク負荷時にパフォーマンスが低下し、ワークステーションで遅延の原因を再現できないことから、本番環境でのみ発生する問題である可能性があります。また、パフォーマンスへの影響と管理オーバーヘッドを最小限に抑えたいという要件も考慮する必要があります。したがって、適切なツールやサービスを用いて、本番環境でシステムの継続的な監視と評価を行う方法を選ぶことが求められます。
基本的な概念や原則：
Pub/Sub：Google Cloud'sリアルタイムメッセージングサービスです。大量のデータをプロデューサーからコンシューマーへ迅速に配信します。IoTなどの大規模なデータ処理に使用されます。
App Engineスタンダード環境：Google Cloudのフルマネージドなアプリ開発とホスティングプラットフォームです。自動スケーリング、負荷分散、セキュリティなどのメリットを持ちます。
Go言語：シンプルさと効率性に優れた静的型付けのコンパイル言語です。スケーラブルなシステム開発に向いています。
Cloud Profiler：Google Cloudのアプリケーションパフォーマンス管理ツールです。コードレベルのアプリケーションパフォーマンスを視覚化し、ボトルネックを解析します。パフォーマンスやオーバーヘッドへの影響はほとんどありません。
Cloud Monitoring：Google Cloudのモニタリングサービスです。アプリケーションのパフォーマンスメトリックを収集、視覚化、警告します。しかし、深いコードレベルの洞察には向いていません。
Compute Engine：Google Cloudの仮想マシンをホストするIaaSサービスです。しかし、既存のプロファイリングツールを持っていないため、適切なツールを自身で設定する必要があります。
pprof：Go言語のパフォーマンス分析ツールです。CPU使用率、メモリ使用量、ネットワーク使用量などを可視化します。ただし、定期的に手動実行と分析が必要で、管理オーバーヘッドが増えます。
正解についての説明：
（選択肢）
・Cloud Profilerを設定し、アプリケーションでcloud.google.com/go/profilerライブラリを初期化します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Profilerは、アプリケーションを実行中の監視とパフォーマンス解析を行うためのツールです。これを設定することで、本番環境においてアプリケーションの遅いパスを特定することが可能になります。
さらに、cloud.google.com/go/profilerライブラリを初期化することで、Goで書かれたアプリケーションの実行時間とリソース利用の情報を収集できます。これにより、パフォーマンスの低下している箇所を特定し、無駄なリソース利用を防ぎ、最適化するための情報を得ることができます。
また、Cloud Profilerは長期的なパフォーマンストラッキングを提供するため、将来のパフォーマンス問題を迅速に特定するための貴重な情報が得られます。
最後に、Cloud Profilerは非侵襲的であり、パフォーマンスへの影響と管理オーバーヘッドをほとんど増やしません。
したがって、この選択肢は問題の要件を満たす最適な解決策です。
不正解についての説明：
選択肢：Cloud Monitoringを使用して、App EngineのCPU使用率メトリックを評価します
この選択肢が正しくない理由は以下の通りです。
Cloud Monitoringを使用してApp EngineのCPU使用率メトリックを評価することは、システム全体のリソース使用状況を理解する助けにはなりますが、コード内の特定の遅いパスを明らかにするには不十分です。
一方、Cloud Profilerはアプリケーションの実行時間情報を可視化し、高負荷の箇所を特定するための適切なツールです。
選択肢：Compute Engineに継続的なプロファイリングツールをインストールします。プロファイリングデータをツールに送信するようにアプリケーションを設定します
この選択肢が正しくない理由は以下の通りです。
問題のアプリケーションはApp Engineスタンダード環境で実行されているため、Compute Engineにプロファイリングツールをインストールすることは役立たない上、管理が煩雑になります。正解選択肢であるCloud Profilerの使用が、パフォーマンスへの影響と管理オーバーヘッドを最小限にしつつ要件を満たす解決策となります。
選択肢：アプリケーションインスタンスに対して、goツールのpprofコマンドを定期的に実行します。フレームグラフを使って結果を分析します
この選択肢が正しくない理由は以下の通りです。
goツールのpprofコマンドを定期的に実行するとパフォーマンスに影響を及ぼし、管理オーバーヘッドが増大します。しかも、この方法では継続的な監視が困難です。
一方、Cloud Profilerは監視を簡単に行え、パフォーマンスへの影響と管理オーバーヘッドを最小限に抑えることができます。
参考リンク：
https://cloud.google.com/profiler/docs/profiling-go
https://cloud.google.com/appengine/docs/standard/go111/runtime#profiling
https://pkg.go.dev/cloud.google.com/go/profiler
<details><div>

### Q. 問題15: 未回答
あなたの会社は、ホリデーショッピングシーズンに、オンライン小売業者の大規模なマーケティングイベントを計画しています。あなたのウェブアプリケーションは、短期間に大量のトラフィックを受けることが予想されます。あなたは、イベント中の潜在的な障害に備えて、アプリケーションを準備する必要があります。
この要件を満たすために、どうすればよいですか？（2つ選択）
1. Cloud Monitoringで関連するシステムメトリクスが取得されていることを確認し、関心のあるレベルでアラートを作成します
2. 容量増加の要件を検討し、必要なクォータ管理を計画します
3. Anthosサービスメッシュをアプリケーションに設定し、トポロジーマップ上の問題を特定します
4. サービスのレイテンシを監視し、平均レイテンシパーセンタイルを求めます
5. アプリケーションが経験するすべての一般的な障害に対して、Cloud Monitoringでアラートを作成します
<details><div>
    答え：1,2
説明
この問題では、"大量のトラフィックを短期間で受けることが予想されるウェブアプリケーションの準備"が重要なテーマです。特に、適切な監視とアラートの設定、そして予想トラフィックに対処できるように適切なリソースクォータを確保することが求められている点に注目しましょう。各選択肢を見て、これらの要件を満たす最善の措置かどうかを評価してください。
基本的な概念や原則：
Cloud Monitoring：Google Cloudの監視サービスです。システムのメトリクスを取得し、アラートを作成することができます。これにより、システムのパフォーマンスを監視し、問題が発生した場合に速やかに対応することが可能となります。
クォータ管理：Google Cloudのサービスでは、リソースの利用に対する上限（クォータ）が設定されています。これはシステムの安定性を保ち、リソースの公平な分配を行うためのものです。大量のトラフィックなど、予期せぬ増加が見込まれる場合には、クォータ管理を適切に行うことが重要です。
Anthos Service Mesh：マイクロサービス間の通信を可視化し、制御し、セキュリティを強化するためのサービスです。しかし、トラフィックの急増に対しては直接対応しません。
レイテンシ監視：システムのパフォーマンス管理の一環で、サービスの応答時間を監視することです。しかし、単体で行うだけでは、大量のトラフィックによる障害に備える対策にはなりません。
正解についての説明：
（選択肢）
・Cloud Monitoringで関連するシステムメトリクスが取得されていることを確認し、関心のあるレベルでアラートを作成します
・容量増加の要件を検討し、必要なクォータ管理を計画します
この選択肢が正解の理由は以下の通りです。
最初に、Cloud Monitoringはシステムの全体的なヘルスステータスを測定し、監視するためのツールで、パフォーマンスメトリクスの収集や視覚化、アラートの設定などが可能です。これを活用することで、突然のトラフィックの増加によって生じるであろう問題を早期に検出し、対策を講じることが可能となります。アラートを適切なレベルで設定し、システムの異常をすばやく検知することで、アプリケーションのダウンタイムを最小化することが可能になります。
また、大規模なイベント前には、使用するリソースの容量の増加を予測し、必要なクォータを事前に計画することが極めて重要です。Google Cloudの各サービスは、ユーザーを保護し、リソースの濫用を防ぐためにクォータ制限を設けています。
したがって、大量のトラフィックを見込む場合には、サービスの使用量がこれらの制限を超えないように、早期にクォータの調整申請を行い、必要な容量を確保しておくべきです。これにより、イベント中に予期しないサービスの中断を防ぐことができます。
不正解についての説明：
選択肢：Anthosサービスメッシュをアプリケーションに設定し、トポロジーマップ上の問題を特定します
この選択肢が正しくない理由は以下の通りです。
Anthos Service Meshを設定することでの監視と問題識別は有効な手段ですが、問題点はその設定に相当な時間と専門知識が必要であることです。正解選択肢のCloud Monitoringとクォータ管理は、即座の対策として適切な方法を提供します。
選択肢：サービスのレイテンシを監視し、平均レイテンシパーセンタイルを求めます
この選択肢が正しくない理由は以下の通りです。
サービスのレイテンシを監視し、平均レイテンシパーセンタイルを求めることは必要な監視活動の一部ではありますが、それだけでは大量のトラフィックを受けるイベントを準備するという全体的な要件を満たしません。適切なメトリクスの監視とアラート作成、さらにクォータ管理の計画性が必要です。
選択肢：アプリケーションが経験するすべての一般的な障害に対して、Cloud Monitoringでアラートを作成します
この選択肢が正しくない理由は以下の通りです。
アプリケーションが経験するすべての一般的な障害に対してアラートを設定するのは適切ではありません。これは、無関係な障害やエラーに対する多数のアラートが適切なアラートの判断を困難にする恐れがあります。
代わりに、特に重要と考えられるシステムメトリクスに対するアラートを設定するのが最適です。
参考リンク：
https://cloud.google.com/monitoring/docs
https://cloud.google.com/docs/quota
https://cloud.google.com/architecture/framework/resilience-planning
<details><div>

### Q. 問題16: 未回答
あるアプリケーションを大規模なStandard Google Kubernetes Engine（GKE）クラスターにデプロイしました。アプリケーションはステートレスで、複数のポッドが同時に実行されます。アプリケーションは一貫性のないトラフィックを受け取ります。トラフィックの変化に関係なくユーザーエクスペリエンスが一貫性を保ち、クラスターのリソース使用量が最適化されるようにする必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. Horizontal Pod Autoscalerを構成します
2. 垂直ポッドオートスケーラーを構成します
3. ノードプールでクラスターオートスケーリングを設定します
4. デプロイメントをスケジュール通りにスケールするcronジョブを設定します
<details><div>
    答え：1
説明
この問題では、アプリケーションの特性と目標を理解することが求められています。問題文では、アプリケーションがステートレスであること、複数のポッドで同時に稼働すること、不規則なトラフィックを受け取ること、そしてユーザーエクスペリエンスの一貫性とリソースの最適化が重要であることが示されています。これらの情報に基づき、適切なオートスケーリング戦略を選択する必要があります。ただし、単に範囲内で最も適切な選択肢を選ぶのではなく、それが問題に示された要件をどのように満たすかを考えることが重要です。
基本的な概念や原則：
Horizontal Pod Autoscaler（HPA）：Kubernetesの機能の1つで、ポッドの数を動的にスケーリングすることでワークロードの需要に応じたリソースの使用を可能にします。
ステートレスアプリケーション：ユーザーセッション間で状態を保存しないアプリケーション。スケーリングや冗長性を実現しやすいです。
トラフィックの変動：アプリケーションへのリクエスト量が時間や状況によって変化します。オートスケーリングの設定はこれを考慮に入れるべきです。
Cronジョブ：定時に実行されるタスクを設定する仕組み。しかし、変動するトラフィックに対し一定の時間にスケジューリングすることは適切な対応とは言えません。
垂直ポッドオートスケーラー：個々のポッドのリソース（CPUやメモリ）を自動的に調整するKubernetesの機能。しかし、一貫性のないトラフィックに対してはポッド数そのものを自動的に調整するHPAの方が適切です。
ノードプール：GKEクラスター内のノードのグループ。ノードプールでクラスターオートスケーリングを設定することも可能ですが、要件はポッドの自動スケーリングを求めているため不正解です。
正解についての説明：
（選択肢）
・Horizontal Pod Autoscalerを構成します
この選択肢が正解の理由は以下の通りです。
まず、Horizontal Pod Autoscaler（HPA）は、Google Kubernetes Engine（GKE）で提供される、ポッドの数を自動的にスケールインまたはスケールアウトする機能です。負荷や需要に基づいて、自動的にポッドの数を増やしたり減らしたりします。これにより、一貫性のないトラフィックに対応することが可能になります。つまり、トラフィックが増えたときにはポッドの数を増やし、トラフィックが減ったときにはポッドの数を減らすことで、ユーザーエクスペリエンスを一貫性を保つことが可能です。
また、使用しないポッドを除去することにより、クラスターのリソース使用量を最適化することも可能です。
したがって、HPAの構成は、任意のトラフィックに対して一貫性のあるユーザーエクスペリエンスとリソースの最適化を同時に達成するのに適した選択肢となります。
不正解についての説明：
選択肢：デプロイメントをスケジュール通りにスケールするcronジョブを設定します
この選択肢が正しくない理由は以下の通りです。
cronジョブでスケジュール通りのスケールは、決まったタイミングの負荷に対応可能ですが、一貫性のないトラフィックパターンには適応できません。正解のHorizontal Pod AutoscalerはCPU使用率などのメトリクスに基づき動的にポッド数を調節し、変動するトラフィックに適応します。
選択肢：垂直ポッドオートスケーラーを構成します
この選択肢が正しくない理由は以下の通りです。
垂直ポッドオートスケーラーはポッドのリソース（CPUやメモリー等）の使用率を最適化する目的で利用しますが、一貫性のないトラフィックに対してユーザーエクスペリエンスを一貫性を保つためには、Horizontal Pod Autoscalerのように、需要に応じてポッドの数を増減させることが必要となります。
選択肢：ノードプールでクラスターオートスケーリングを設定します
この選択肢が正しくない理由は以下の通りです。
クラスターオートスケーリングはノードの数を調節しますが、トラフィックの一貫性を保つためには、ポッド自体の数を動的に調節する必要があります。そのロールを果たすのがHorizontal Pod Autoscalerで、トラフィックに合わせてポッドの数を自動的にスケーリングします。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
<details><div>

### Q. 問題17: 未回答
Terraformを使用して、Google Cloud環境にデプロイされたアプリケーションを管理します。アプリケーションは、マネージドインスタンスグループによってデプロイされたインスタンス上で実行されます。Terraformコードは、CI/CDパイプラインを使用してデプロイされます。マネージドインスタンスグループで使用されるインスタンステンプレートのマシンタイプを変更すると、パイプラインはTerraform適用ステージで失敗し、次のエラーメッセージが表示されます。
インスタンステンプレートの削除待機中にエラーが発生しました:instance_templateリソース 'project/my-project/global/instanceTemplates/cloud-jp-it-20231125' はすでに 'projects/my-project/regions/us-central1/instanceGroupManagers/my-migによって使用されています '
インスタンステンプレートを更新し、アプリケーションの中断とパイプラインの実行数を最小限に抑える必要があります。
この要件を満たすために、どうすればよいですか？
1. 新しいインスタンステンプレートを追加し、新しいインスタンステンプレートを使用するようにマネージドインスタンスグループを更新し、古いインスタンステンプレートを削除します
2. マネージドインスタンスグループを削除し、インスタンステンプレートの更新後に再作成します
3. Terraformステートファイルからマネージドインスタンスグループを削除し、インスタンステンプレートを更新し、マネージドインスタンスグループを再インポートします
4. インスタンステンプレートのライフサイクルブロックで、create_before_destroyメタ引数をtrueに設定します
<details><div>
    答え：4
説明
この問題では、Terraformを使用してGoogle Cloud環境にデプロイされたアプリケーションを管理し、特にマネージドインスタンスグループを使用してデプロイされたインスタンステンプレートの更新方法について問われています。問題文から、インスタンステンプレートが更新された際にTerraformの適用ステージでエラーが出ているという情報が与えられているので、それを解消する方法を考えることが肝要です。また、アプリケーションの中断とパイプラインの実行数を最小限に抑える必要があるとされているため、極力ダウンタイムをなくし、冗長な操作を避ける解決策を選択することが重要です。
基本的な概念や原則：
Terraform：インフラストラクチャをコード化し、バージョン管理しながら安全かつ効率的に管理できるツールです。Google Cloudはもちろん、多様なプロバイダーと連携が可能です。
マネージドインスタンスグループ：Google Cloudで提供されるサービスの一つで, 自動スケーリング、負荷分散、自動ヒール等の機能があります。
インスタンステンプレート：Google Cloudで使用される設定の一部を再利用可能な形で保存しておく仕組みです。これにより、設定の再利用と一貫性が保たれます。
CI/CDパイプライン：コードの変更を自動的にビルド、テスト、デプロイするプロセスです。ユーザーからのフィードバック応答時間を大幅に短縮し、より頻繁かつ安全にアップデートが可能にします。
create_before_destroyメタ引数：Terraformのライフサイクル機能の一部で、この引数がtrueに設定されていると新しいリソースが先に作成され、その後で古いリソースが削除されます。つまり、この引数をtrueに設定することで新旧のリソースが一時的に共存でき、ダウンタイムを防ぐことができます。
Terraformステートファイル：Terraformがリソースの現在の状態を記録・保存するファイルです。正確にリソースの追加や削除を管理するために重要です。
正解についての説明：
（選択肢）
・インスタンステンプレートのライフサイクルブロックで、create_before_destroyメタ引数をtrueに設定します
この選択肢が正解の理由は以下の通りです。
まず、Terraformのライフサイクルブロックのcreate_before_destroyメタ引数をtrueに設定すると、新しいリソースが既存のリソースが削除される前に作成されます。つまり、新しいインスタンステンプレートが既存のテンプレートが削除される前に作成されるのです。これにより、この選択肢は、マネージドインスタンスグループがいつでも正常に機能することを保証します。
次に、エラーメッセージから判断すると、新しいインスタンステンプレートの作成を試みたときに既存のテンプレートがまだ使用されていたためにエラーが生じたと分かります。しかし、create_before_destroyメタ引数をtrueに設定すると、新しいテンプレートが作成され、それが新しいインスタンスに使用されるようになってから古いテンプレートが削除されます。これにより、アプリケーションの中断が最小限に抑えられ、CI/CDパイプラインが正常に完了します。このような手段でTerraformコードがデプロイされるようにすることで、システムのダウンタイムを削減し、生産性を向上させることができます。
不正解についての説明：
選択肢：マネージドインスタンスグループを削除し、インスタンステンプレートの更新後に再作成します
この選択肢が正しくない理由は以下の通りです。
マネージドインスタンスグループを削除するという手順は、アプリケーションの中断を引き起こし、パイプラインの実行数も増えます。これは問題の要件に反します。
さらに、正解ではcreate_before_destroyメタ引数を利用してインスタンステンプレートを更新することで問題を解決しています。
選択肢：新しいインスタンステンプレートを追加し、新しいインスタンステンプレートを使用するようにマネージドインスタンスグループを更新し、古いインスタンステンプレートを削除します
この選択肢が正しくない理由は以下の通りです。
新しくインスタンステンプレートを追加し、マネージドインスタンスグループを更新し、古いテンプレートを削除するという手順は、多くの手間と時間を必要とします。
一方、正解選択肢であるライフサイクルブロックのcreate_before_destroyをtrueに設定するのは、新旧のテンプレートの切り替えをスムーズに実行でき、アプリケーションの中断とパイプラインの実行数を最小限に抑える効果があります。
選択肢：Terraformステートファイルからマネージドインスタンスグループを削除し、インスタンステンプレートを更新し、マネージドインスタンスグループを再インポートします
この選択肢が正しくない理由は以下の通りです。
Terraformステートファイルからマネージドインスタンスグループを削除し、インスタンステンプレートを更新し、マネージドインスタンスグループを再インポートするという選択肢は、アプリケーションの中断とパイプラインの実行数を最小限に抑えるという要件を満たせません。
一方、インスタンステンプレートのライフサイクルブロックで、create_before_destroyメタ引数をtrueに設定することで、新しいインスタンステンプレートを作成し、それを使用するインスタンスを起動してから既存のインスタンステンプレートを削除することができ、適切な更新が可能です。
参考リンク：
https://cloud.google.com/compute/docs/instance-templates
https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances
https://www.terraform.io/docs/language/meta-arguments/lifecycle.html
<details><div>

### Q. 問題18: 未回答
あなたは、再利用可能なインフラストラクチャをコードモジュールとして開発しています。各モジュールには、テストプロジェクトでモジュールを起動する統合テストが含まれています。ソース管理にGitHubを使っています。機能ブランチを継続的にテストし、変更が受け入れられる前にすべてのコードがテストされるようにする必要があります。統合テストを自動化するソリューションを実装する必要があります。
この要件を満たすために、どうすればよいですか？
1. Cloud Buildを使用してテストを実行します。プルリクエストがマージされた後、すべてのテストをトリガーして実行します
2. Cloud Buildを使用して特定のフォルダでテストを実行します。GitHubプルリクエストごとにCloud Buildをトリガーします
3. CI/CDパイプラインにはJenkinsサーバーを使います。featureブランチのすべてのテストを定期的に実行します
4. プルリクエストのレビュアーに、コードを承認する前に統合テストを実行するよう依頼します
<details><div>
    答え：2
説明
この問題では、GitHubで管理されている再利用可能なインフラストラクチャのコードモジュールに対して、変更が受け入れられる前にテストを行い、それらのテストを自動化するソリューションを選択することが求められています。具体的には、コードモジュールの統合テストを自動化し、GitHubのプルリクエストごとにテストをトリガーする必要があります。そのために選択肢から最も合致するソリューションを選ぶ必要があります。この問題を解く上で、CI/CDパイプライン、特に統合テストの自動化とGitHubとの連携について理解していることが必要です。統合テストの自動化に関する選択肢とGitHubのプルリクエストを連携させる選択肢に焦点を当てることが重要です。
基本的な概念や原則：
Cloud Build：Google Cloudのコンテナ化されたアプリケーションのビルドとテストを行うフルマネージドサービスです。CI/CDパイプラインを設定し、自動化されたビルドとテストを実行できます。
統合テスト：ソフトウェアの異なる部分を組み合わせてテストする方法です。異なるコードモジュールが適切に連携して動作することを確認します。
GitHub：ソースコードを管理するためのプラットフォームです。ブランチの作成、プルリクエストの作成とマージ、コードのレビューなどの機能を提供します。
トリガー：あるアクションが発生したときに自動的に処理を開始するためのメカニズムです。例えば、Cloud Buildでは、特定のイベント（GitHubのプルリクエストの作成など）に応じてビルドをトリガーできます。
Jenkins：オープンソースの自動化ツールで、CI/CDパイプラインを構築するために使用されます。しかし、設定と管理が複雑であるため、非専門家には難しい可能性があります。
レビュアー：他者のコードを検討し問題を指摘する人です。手動テストは確認作業に時間がかかり、エラーを見逃す可能性があります。
マージ後のテスト：全てのテストをプルリクエストがマージされた後に実行する手法です。しかし、問題が発見された場合に修正が難しい場合があります。
正解についての説明：
（選択肢）
・Cloud Buildを使用して特定のフォルダでテストを実行します。GitHubプルリクエストごとにCloud Buildをトリガーします
この選択肢が正解の理由は以下の通りです。
Cloud BuildはGoogle Cloudが提供している継続的インテグレーションとデリバリー（CI/CD）プラットフォームで、アプリケーションのビルド、テスト、デプロイを自動化します。GitHubのリポジトリとCloud Buildは直接連携が可能であり、GitHubのプルリクエストやプッシュが行われるごとにCloud Buildをトリガーして自動テストを行うことができます。開発者はそれぞれの機能ブランチに対して修正を加えると、Cloud Buildが自動的に統合テストを行い、その結果がすぐに分かるようになります。
このようにCloud Buildを使用すれば、開発者が手動でテストを行う手間を省き、コードの品質管理が効率化し、再利用可能なインフラストラクチャを継続的にテストし、変更をより迅速に受け入れられるようにすることが可能となるため、最適な選択肢と言えます。
不正解についての説明：
選択肢：CI/CDパイプラインにはJenkinsサーバーを使います。featureブランチのすべてのテストを定期的に実行します
この選択肢が正しくない理由は以下の通りです。
Jenkinsを使用すると、サーバーの管理や設定が必要となり、それ自体が負担となります。
また、"定期的に実行"するための選択肢は、要求されている"継続的にテスト"するという要件を満たしていません。Cloud BuildのようにGitHubプルリクエストごとにテストを実行する方法が最適です。
選択肢：プルリクエストのレビュアーに、コードを承認する前に統合テストを実行するよう依頼します
この選択肢が正しくない理由は以下の通りです。
人為的なテスト実行依頼は自動化の要件を満たしません。これは、ミスや見落としを生む可能性があり、繰り返しの作業を必要とします。
一方、Cloud Buildを使用すると、テストは自動的に行われるため、効率的かつ安全性が高まります。
選択肢：Cloud Buildを使用してテストを実行します。プルリクエストがマージされた後、すべてのテストをトリガーして実行します
この選択肢が正しくない理由は以下の通りです。
すべての変更が受け入れられる前にコードがテストされるべきですが、この選択肢ではプルリクエストがマージされた後にテストが実行されるため、その要件を満たしていません。正解はプルリクエストごとにテストをトリガーすることで、この要件を満たします。
参考リンク：
https://cloud.google.com/build/docs/automating-builds/build-repos-from-github
https://cloud.google.com/build/docs/configuring-builds/run-builds-on-github
https://cloud.google.com/solutions/infrastructure-as-code
<details><div>

### Q. 問題19: 未回答
あなたの会社では、グローバルに分散した複数のGoogle Kubernetes Engine（GKE）クラスターを使用してサービスを運用しています。あなたの会社の運用チームは、メトリクス、アラート、およびダッシュボードの生成にPrometheusベースのツールを使用するワークロード監視を設定しました。このセットアップでは、すべてのクラスターにわたってグローバルにメトリクスを表示する方法は提供されていません。グローバルなPrometheusクエリをサポートし、管理オーバーヘッドを最小限に抑えるために、スケーラブルなソリューションを実装する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. データアクセスを一元化するために、Prometheus階層フェデレーションを設定します
2. データアクセスを一元化するために、Prometheusのクロスサービスフェデレーションを設定します
3. Cloud Operations for GKEでワークロードメトリクスを設定します
4. Google Cloud Managed Service for Prometheusを設定します
<details><div>
    答え：4
説明
この問題では、Prometheusベースのツールを使用してメトリクス、アラート、ダッシュボードを生成する設定が既にあり、それを使用してGoogle Kubernetes Engine（GKE）クラスターを監視しているシナリオを想定しています。ただし、全クラスターについてグローバルにメトリクスを表示する機能がなく、その要件を満たす方法が求められています。この点に注意しながら問題に取り組むことが必要でしょう。また、解を導き出す上でのキーとなるのはGoogle Cloud Managed Serviceを活用すること、Prometheusのフェデレーション設定やCloud Operations for GKEを設定する方法などが考慮肝要なポイントであることも理解しておかなければなりません。
基本的な概念や原則：
Prometheus：オープンソースのモニタリングツールで、大規模システムのメトリクスの収集とストレージに使用されます。アラートの生成やダッシュボードの作成も可能です。
Google Kubernetes Engine（GKE）：Google Cloudが提供するマネージドKubernetesサービスで、コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を効率的に行うことができます。
Google Cloud Managed Service for Prometheus：Google Cloudが提供するPrometheusに互換性のある監視サービスで、クラスターリングされた環境におけるメトリクスの一元管理を可能にします。
Prometheusのフェデレーション：Prometheusが提供するデータのスクレイピング機能で、複数のPrometheusサーバーからメトリクスを収集し、一元的に管理することができます。ただし、スケーラビリティと管理の観点からは制限があります。
Cloud Operations for GKE：Google Cloudが提供するGKE向けの監視、ログ管理、デバッグツール群です。GKEクラスターの運用を支援しますが、Prometheus互換のメトリクス収集は行いません。
正解についての説明：
（選択肢）
・Google Cloud Managed Service for Prometheusを設定します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Managed Service for Prometheusは、Prometheusを基盤としてGoogle Cloudで管理されている可視化とモニタリングのサービスです。このサービスは、複数のGKEクラスターから収集したメトリクスを一元的に管理し、分析する能力を持っています。これにより、ユーザーはグローバルな視点での監視と分析が可能になります。これは、問題文で述べられている"すべてのクラスターにわたってグローバルにメトリクスを表示する"要件を満たします。
また、Google Cloud Managed Service for Prometheusはマネージドサービスであるため、運用チームは自身でサービスの管理やメンテナンスをする必要がありません。これは問題文で指摘されている"管理のオーバーヘッドを最小限に抑える"要件を満たすとともに、運用チームがより本質的なタスクに注力できるようにする効果があります。
以上の理由から、Google Cloud Managed Service for Prometheusを設定することは、この問題を解決する適切な手段となります。
不正解についての説明：
選択肢：データアクセスを一元化するために、Prometheusのクロスサービスフェデレーションを設定します
この選択肢が正しくない理由は以下の通りです。
Prometheusのクロスサービスフェデレーションは、ある程度の管理オーバーヘッドが必要であり、この問題の要求である管理オーバーヘッドを最小限に抑えるという要件には合致しません。
また、Google Cloud Managed Service for Prometheusを使用すると、簡単にスケーラブルなソリューションが実装でき、複数のGKEクラスターにわたるメトリクスの一元的な表示が可能となります。
選択肢：Cloud Operations for GKEでワークロードメトリクスを設定します
この選択肢が正しくない理由は以下の通りです。
Cloud Operations for GKEは、GKEクラスターの運用を容易にするための集成管理ソリューションであり、必要とされるグローバルなPrometheusクエリのサポートは提供していません。
それに対して、正解のGoogle Cloud Managed Service for Prometheusは、多数のクラスターからのメトリクスを統合的に管理し、アラートやダッシュボードの作成を助けるサービスです。
選択肢：データアクセスを一元化するために、Prometheus階層フェデレーションを設定します
この選択肢が正しくない理由は以下の通りです。
Prometheus階層フェデレーションを設定すると確かにデータアクセスは一元化できますが、その管理オーバーヘッドは大きく、各GKEクラスター間でのメトリクス収集にはスケーラビリティが低いです。反対に、Google Cloud Managed Service for Prometheusを使うと、スケーラビリティと管理の効率が向上します。
参考リンク：
https://cloud.google.com/stackdriver/docs/managed-prometheus
https://cloud.google.com/kubernetes-engine/docs/how-to/monitoring
https://prometheus.io/docs/prometheus/latest/federation/
<details><div>

### Q. 問題20: 未回答
あなたの会社は、サイト信頼性エンジニアリングの原則に従っています。あなたは、ソフトウェアの変更がきっかけとなり、ユーザーに深刻な影響を与えたインシデントのポストモーテムを書いています。あなたは、今後深刻なインシデントが発生しないようにしたいと考えています。
この要件を満たすために、どうすればよいですか？
1. 新しいソフトウェアがリリースされる前に、この種のエラーを検出するテストケースが正常に実行されるようにします
2. インシデントが発生した場合、常駐運用チームが直ちにエンジニアと経営陣に連絡し、対応策を協議することを義務付ける方針を策定します
3. インシデントの責任者であるエンジニアを特定し、上級管理職にエスカレーションします
4. 変更を検討した従業員をフォローアップし、今後従うべき慣行を規定します
<details><div>
    答え：1
説明
この問題では、サイト信頼性エンジニアリング（SRE）原則に基づいた問題対応の最善のアプローチを選択することが求められています。SREの原則は、システムやサービスの信頼性を確保し、問題が再発しないように行動することに重点を置いています。問題を発見した際に、単なる症状の対処に終始するのではなく、問題の根本原因を特定し、修正することが重要です。また、SREの原則では問題の責任を個人に求めるのではなく、システム全体の改善を追求します。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：Googleが開発したシステム管理手法で、システムのスケーラビリティと信頼性を確保するためにソフトウェアエンジニアリング原則を適用します。強調点は自動化と数値目標設定です。
ポストモーテム：インシデント発生後に行われるレビュープロセスで、原因分析や問題解決のための改善提案を行います。故障や問題発生時に何が起こったか、なぜ起こったか、どのように解決したか、今後どのように予防すべきかを詳述します。
テストケース：ソフトウェアが予期した通りに動作することを確認するためのシナリオです。テストケースには、評価すべき条件、期待される結果、実行手順などが含まれる場合があります。
予防的な対策：既知のエラーや問題を未然に防ぐために行われる措置です。新規のソフトウェアリリース前などの段階で予期される問題を検出し、対策を講じることが含まれます。
正解についての説明：
（選択肢）
・新しいソフトウェアがリリースされる前に、この種のエラーを検出するテストケースが正常に実行されるようにします
この選択肢が正解の理由は以下の通りです。
まず、サイト信頼性エンジニアリング（SRE）の原則に従っている場合、深刻なインシデントに対するポストモーテムは重要なプロセスの一部です。ポストモーテムでは、何が起こったか、なぜそれが起こったか、そして同様の問題が将来再発しないようにするための具体的な対策を挙げることが求められます。そのため、テストケースの作成と実行は、再発防止のための一つの対策となります。このテストケースは、新しいソフトウェアがリリースされる前に実行され、同種のエラーの発生を未然に防ぐロールを果たします。
また、ソフトウェアの変更が問題の根源である場合、これらのテストケースはその変更が問題を引き起こす可能性がある状況を模倣し、これにより開発チームは問題を修正し、将来的な影響を避けることができます。
不正解についての説明：
選択肢：インシデントの責任者であるエンジニアを特定し、上級管理職にエスカレーションします
この選択肢が正しくない理由は以下の通りです。
サイト信頼性エンジニアリングの原則は、システム全体の改善と予防策の策定に重きを置いています。
したがって、個々のエンジニアを特定しエスカレーションすることは適切な対応ではありません。対象的に、テストケースを正常に実行することで、発生するエラーを早期に検出し対処することが求められます。
選択肢：変更を検討した従業員をフォローアップし、今後従うべき慣行を規定します
この選択肢が正しくない理由は以下の通りです。
サイト信頼性エンジニアリングの原則は、システム変更の問題を個々の人に責任付けるのではなく、システム全体の改善に焦点を当てます。指名は問題解決に寄与せず、改善のためのシステムやプロセス介入が重要です。
それに対して、正解の選択肢はシステム全体を改善するためのアプローチを表しています。
選択肢：インシデントが発生した場合、常駐運用チームが直ちにエンジニアと経営陣に連絡し、対応策を協議することを義務付ける方針を策定します
この選択肢が正しくない理由は以下の通りです。
インシデント発生後に運用チームがエンジニアや経営陣と対応策を協議する方針は、既にユーザーに影響が出てからの対応に過ぎません。
一方、新しいソフトウェアがリリースされる前にエラーを検出するテストケースの実行は、ユーザー影響を防ぐ事前の対策となります。
参考リンク：
https://cloud.google.com/sre/docs/sre-book/managing-incidents
https://cloud.google.com/sre/docs/sre-book/postmortem-culture
https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html
<details><div>

### Q. 問題21: 未回答
サポートしている本番システムで大量の障害が発生しました。すべての停止に対してアラートを受信していますが、そのアラートは不健全なシステムによるもので、1分以内に自動的に再起動されます。あなたは、サイト信頼性エンジニアリング（SRE）のプラクティスに従いつつ、スタッフへの負担を最小化するプロセスを設定したいと考えています。
この要件を満たすために、どうすればよいですか？
1. 異なるタイムゾーンのエンジニアにアラートを配信します
2. 関連するSLOを再定義し、エラーバジェットが枯渇しないようにします
3. 各アラートのインシデントレポートを作成します
4. 実用的でないアラートを排除します
<details><div>
    答え：4
説明
この問題では、システム障害とその結果のアラートに対する対応という実際のシナリオが提示されており、このシナリオに対して最小限のスタッフ負担で対応する方法を求めています。選択肢を選ぶ時には、システムの健全性と、エンジニアに余計な負担をかけないという両方の観点を念頭に置き、シナリオ上の問題を解決できるかどうかを慎重に検討することが重要になります。不必要なアラートに対する対処法が最終的な解答であることを覚えておきましょう。
基本的な概念や原則：
実用的でないアラート：システムの状態についての情報を提供せず、またはすぐに解決される問題に関するアラートです。これらは過度のノイズを生み出し、インシデント対応を遅らせる可能性があります。
サイト信頼性エンジニアリング（SRE）：システムの信頼性、スケーラビリティ、効率を確保するためのソフトウェアエンジニアリングのアプローチです。アラートの健全性と、それに対する適切な対応が焦点になります。
SLO（Service Level Objective）：サービスの特定の側面についての目標値です。例えば、可用性、レイテンシ、システム容量などがあります。
エラーバジェット：SLOに基づき、許容できるエラーの量や時間を定義したものです。この枠内でエラーが発生しても、サービスは受け入れられる範囲内とされます。
正解についての説明：
（選択肢）
・実用的でないアラートを排除します
この選択肢が正解の理由は以下の通りです。
まず、サイト信頼性エンジニアリング（SRE）のプラクティスの一環として、アラートは実際にアクションを必要とする重要な問題に対してのみ生成されるべきです。短時間で自動的に解決する問題や、アラートが発生する原因がシステムの不健全さなど、本質的に重要でない問題に対してアラートを生成すると、エンジニアの注目があまり重要でない問題に向けられてしまいます。これは"アラート疲れ"を引き起こし、真に重要なアラートが見過ごされる原因となる可能性があります。
したがって、実用的でないアラートを排除することで、エンジニアは本当にアクションを必要とする問題に集中でき、かつスタッフへの負担を最小化することが可能になります。
不正解についての説明：
選択肢：関連するSLOを再定義し、エラーバジェットが枯渇しないようにします
この選択肢が正しくない理由は以下の通りです。
関連するSLOを再定義し、エラーバジェットが枯渇しないようにするという方法は、アラートを減らすのではなく、アラートが生成される閾値を変えてしまうことになります。今回の問題設定では、本質的に健全なシステムから発生している不必要なアラートを削減することが求められています。そのため、不健全なシステムから発生する実用的でないアラートを排除する方が適切です。
選択肢：異なるタイムゾーンのエンジニアにアラートを配信します
この選択肢が正しくない理由は以下の通りです。
異なるタイムゾーンのエンジニアにアラートを配信すると彼らに負荷がかかり、SREのプラクティスである負荷最小化に反します。
また、問題は一時的なもので自動的に解決するため、アラート自体が不要です。正解のアラートの排除は、このような不要なアラートを無効化し負荷を最小化します。
選択肢：各アラートのインシデントレポートを作成します
この選択肢が正しくない理由は以下の通りです。
各アラートのインシデントレポートを作成する選択肢は、スタッフへの負担を増加させる可能性があります。これは本質的にすべての停止で報告を行うため、一分以内に自己修復する不健全なシステムについてもレポートを作成することになります。正解の選択肢である"実用的でないアラートを排除する"は、不必要な警告を削除してスタッフの負担を減らす効果的な方法です。
参考リンク：
https://cloud.google.com/sre/docs/sre-book/managing-incidents#alerts
https://cloud.google.com/sre/docs/sre-book/being-on-call#alerts
https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/
<details><div>

### Q. 問題22: 未回答
あなたはGitブランチが更新されたときにTerraformコードをデプロイするCloud Buildジョブをデプロイしています。テスト中に、あなたはジョブが失敗することに気づきました。ビルドログに次のようなエラーがあります：
Initializing the backend...
Error: Failed to get existing workspaces: querying Cloud Storage failed: googleapi: Error 403
Googleが推奨する方法に従って問題を解決する必要があります。
この要件を満たすために、どうすればよいですか？
1. Terraformの設定で指定した名前でストレージバケットを作成します
2. ローカルの状態を使用するようにTerraformのコードを変更します
3. ステートファイルバケットのCloud Buildサービスアカウントにroles/storage.objectAdminのIAM（Identity and Access Management）ロールを付与します
4. プロジェクトのCloud Buildサービスアカウントに、roles/ownerのIAM（Identity and Access Management）ロールを付与します
<details><div>
    答え：3
説明
この問題では、Terraformコードのデプロイ時に発生したCloud Buildジョブのエラーの解決方法を尋ねています。エラーメッセージを注意深く読むと、Cloud Storageへの問い合わせが403エラー、つまりアクセス権限が足りないことで失敗していることが分かります。ここで役立つのはGoogle CloudのIAMロールに関する知識と、Cloud Buildの仕組みに関する知識です。このエラーから、Cloud Buildジョブが実行されているサービスアカウントがCloud Storageバケット内のTerraformステートファイルに適切なアクセス権限を持っていない可能性があることを推測することができます。Googleが推奨する方法に従って問題を解決するために、既存のIAMロールを適切に調整することや適切なバケットを指定することが重要になります。
基本的な概念や原則：
Cloud Build：Google Cloudのサービスで、ソースコードからコンテナイメージやその他のビルドアーティファクトを高速にビルドして提供します。トリガーにより、ソースコードの変更に自動的に反応します。
Terraform：Infrastructure as Code（IaC）ツールで、バージョン管理や再利用可能なモジュールを扱うことができます。定義した状態に基づいて、リソースの作成、更新、削除を自動化します。
IAM（Identity and Access Management）ロール：Google Cloudにおいて特定のユーザーやサービスアカウントに特定の操作を許可するための概念です。ロールを持つユーザーまたはサービスは、そのロールに関連付けられた操作を実行できます。
サービスアカウント：アプリケーションやサービスがGoogle Cloud APIを呼び出すために使用する特殊なGoogleアカウントです。制限された範囲のリソースへのアクセスを付与できます。
ステートファイル：Terraformが現在のリソースの状態を捉えて保存するファイルです。リソースの作成や削除、更新における差分を確認するために使用されます。
Cloud Storage：Google Cloudが提供するスケーラブルなオブジェクトストレージです。バケットと呼ばれるコンテナ内にデータを保存できます。このバケットに対するアクセス権はIAMにより管理されます。
正解についての説明：
（選択肢）
・ステートファイルバケットのCloud Buildサービスアカウントにroles/storage.objectAdminのIAM（Identity and Access Management）ロールを付与します
この選択肢が正解の理由は以下の通りです。
エラーメッセージはCloud Storageに問題があることを示しています。具体的には、Cloud Buildがステートファイルをバケットに設定・取得するための適切な承認が得られていないことが問題となっています。Googleの推奨方法は、必要な権限を持たせるためにCloud BuildサービスアカウントにIAMロールを付与することです。ここで付与すべきロールは"roles/storage.objectAdmin"です。このロールを付与すると、Cloud Buildはオブジェクトを作成、読み取り、更新、削除するためのフルコントロール権限を与えられます。この権限があることで、Cloud BuildはTerraformを正常にデプロイし、エラーを解消できます。
不正解についての説明：
選択肢：ローカルの状態を使用するようにTerraformのコードを変更します
この選択肢が正しくない理由は以下の通りです。
ローカルの状態を使用するようにTerraformのコードを変更すると、Gitのブランチが更新されたときに毎回新規の状態が生成されるため、既存のリソースに対する一貫性を維持することが困難となります。そのため、この問題を解決する最善策はIAMロールを付与してアクセス権を与えることです。
選択肢：Terraformの設定で指定した名前でストレージバケットを作成します
この選択肢が正しくない理由は以下の通りです。
エラーメッセージは、Terraformがステートファイルを読み書きするための適切な権限を持っていないことを示しています。つまり、ストレージバケットが存在しない、という問題ではなく、Cloud Buildサービスアカウントに必要なアクセス権を付与していないことが問題です。
選択肢：プロジェクトのCloud Buildサービスアカウントに、roles/ownerのIAM（Identity and Access Management）ロールを付与します
この選択肢が正しくない理由は以下の通りです。
roles/ownerのIAMロールは、全てのリソースに対する全ての操作が可能となるという大幅な権限を持つため、セキュリティ上のリスクが高まります。
一方、正解の選択肢では、必要な操作のみが許可されるため、原則として最小限の権限を付与する事が推奨されているのです。
参考リンク：
https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build
https://cloud.google.com/storage/docs/access-control/iam-roles
https://www.terraform.io/docs/language/settings/backends/gcs.html
<details><div>

### Q. 問題23: 未回答
あなたは最近、あるサービスが現在のローリングウィンドウ期間のエラーバジェットを超過していることに気づきました。あなたの会社の製品チームは新機能を発表しようとしています。あなたは、サイト信頼性エンジニアリング（SRE）のプラクティスに従いたいと考えています。
この要件を満たすために、どうすればよいですか？
1. エラーバジェットを使い切ったことをチームに通知します。ローンチを凍結するか、ユーザーエクスペリエンスが多少悪くても許容するか、チームと交渉します
2. エラーバジェットが不足していることをチームに通知し、すべてのテストが成功したことを確認します
3. 製品に関連する他のメトリクスに目を通し、エラーバジェットが残っているSLOを見つける。エラーバジェットを再配分し、機能のローンチを許可します
4. 状況をエスカレーションし、追加エラーバジェットを要求します
<details><div>
    答え：1
説明
この問題では、エラーバジェットを超過し、新機能ローンチを予定している状況をどのように扱うべきか、という観点からサイト信頼性エンジニアリング（SRE）のプラクティスを適用する必要があります。エラーバジェットの使い切るという制約と、新機能ローンチというタスクの両方を考慮にいれつつ、最適な戦略を選択することが求められます。選択肢に含まれる各行動のSREのプラクティスとの整合性を考えて判断することが必要です。
基本的な概念や原則：
エラーバジェット：システムがある期間に許容できるエラーの上限を示す指標です。この指標はサイト信頼性エンジニアリング（SRE）のワークフローの一部で、サービスレベル目標（SLO）を超過したエラーの量を計測します。
サイト信頼性エンジニアリング（SRE）：Googleから生まれたソフトウェアエンジニアリングのアプローチで、大規模システムの信頼性と効率を向上させることを目指しています。
サービスレベル目標（SLO）：サービスが達成すべき最小限の性能水準を設定した指標です。これらの目標を元にエラーバジェットが定義され、システムの運用や開発の判断基準となります。
ローリングウィンドウ期間：特定の時間範囲内でのシステムのパフォーマンスを評価するための期間です。この期間内でのエラーバジェットの消費状況をみて、リスクの評価やリリース判断を行います。
正解についての説明：
（選択肢）
・エラーバジェットを使い切ったことをチームに通知します。ローンチを凍結するか、ユーザーエクスペリエンスが多少悪くても許容するか、チームと交渉します
この選択肢が正解の理由は以下の通りです。
まず、サイト信頼性エンジニアリング（SRE）の基本的な原則は、エラーバジェットを尊重することです。エラーバジェットを使い切ってしまった場合、新機能のローンチがサービスの信頼性を更に損なう可能性があるため、すぐにチームにその事実を通知することが重要です。
次に、エラーバジェットの超過を受けてローンチを凍結するか、ユーザーエクスペリエンスが多少悪くなることを許容するかチームとの交渉も重要となります。SREの目的は、サービスの信頼性と新機能開発の間でバランスを取ることにあるため、適応的な対応が求められます。以上から、この選択肢が適切な対応とされる理由が理解できます。
不正解についての説明：
選択肢：エラーバジェットが不足していることをチームに通知し、すべてのテストが成功したことを確認します
この選択肢が正しくない理由は以下の通りです。
すべてのテストが成功したことを確認するだけでは、サイト信頼性エンジニアリング（SRE）のプラクティスに従った対応とは言えません。テスト成功はエラー発生を完全に防ぐことを保証しないため、エラーバジェット超過に対する具体的な対策をチームで協議する必要があります。
選択肢：状況をエスカレーションし、追加エラーバジェットを要求します
この選択肢が正しくない理由は以下の通りです。
SREのプラクティスではエラーバジェットの超過が発生した場合、その状況のエスカレーションや追加エラーバジェットの要求は推奨されていません。
逆に、その情報をチームに共有し、必要な修正を行い結果的にエラーバジェットを下げる戦略が求められています。
選択肢：製品に関連する他のメトリクスに目を通し、エラーバジェットが残っているSLOを見つける。エラーバジェットを再配分し、機能のローンチを許可します
この選択肢が正しくない理由は以下の通りです。
SREのプラクティスに従うと、エラーバジェットをオーバーした時点で、すぐにチームへ通知しローンチを制御することが必要です。他のメトリクスに目を通しエラーバジェットの再配分は、問題の解決には繋がらず、ただエラーバジェットの枠を広げるだけとなります。
参考リンク：
https://cloud.google.com/sre/docs/sre-book/managing-service-level-objectives
https://cloud.google.com/sre/docs/sre-book/implementing-slos
https://cloud.google.com/blog/products/Google Cloud/sre-fundamentals-slis-slas-and-slos
<details><div>

### Q. 問題24: 未回答
あなたは、Google Kubernetes Engine（GKE）クラスター上で実行されている一連のアプリケーションを持っており、Google Cloud Operation Suite Kubernetes Engine Monitoringを使用しています。あなたは、会社で必要とされる新しいコンテナ化されたアプリケーションを本番稼動させようとしています。このアプリケーションはサードパーティによって書かれており、変更や再構成はできません。このアプリケーションは、/var/log/app_messages.logにログ情報を書き込み、あなたはこれらのログエントリをCloud Loggingに送信したいと考えています。
この要件を満たすために、どうすればよいですか？
1. GKEにFluentdデーモンセットをデプロイします。それから、アプリケーションのポッドでログファイルをテールし、Cloud Loggingに書き込むように、カスタマイズした入出力設定を作成します
2. デフォルトのGoogle Cloud Operation Suite Kubernetes Engine Monitoringエージェント設定を使用します
3. ポッド内でログファイルをテールし、標準出力にエントリを書き出すスクリプトを記述します。このスクリプトを、アプリケーションのポッドのサイドカーコンテナとして実行します。コンテナ間で共有ボリュームを構成し、スクリプトがアプリケーションコンテナ内の/var/logに読み取りアクセスできるようにします
4. Google Compute Engine（GCE）にKubernetesをインストールし、アプリケーションを再デプロイします。そして、組み込みのCloud Logging設定をカスタマイズして、アプリケーションのポッド内のログファイルをテールし、Cloud Loggingに書き込みます
<details><div>
    答え：1
説明
この問題では、変更や再構成ができないサードパーティ製のコンテナ化されたアプリケーションのログをCloud Loggingに送信する方法が求められています。問題文からアプリケーションが"/var/log/app_messages.log"へのログ書き込みを行うこと、そして現在利用しているのがGoogle Kubernetes Engine（GKE）とGoogle Cloud Operation Suite Kubernetes Engine Monitoringだということがわかります。これらの情報を元に、どのような手段やツールを使ってアプリケーションのログを無効化せずにCloud Loggingに転送することができるのかを考えることが必要となります。　
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供する、完全マネージド型のKubernetesサービスです。コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を行います。
Google Cloud Operation Suite Kubernetes Engine Monitoring：Google Cloudが提供するKubernetes向けの監視ツールです。アプリケーションのパフォーマンスやリソースの利用状況をリアルタイムに追跡・分析することができます。
Fluentd：オープンソースのデータコレクターで、各種のデータソースからログ情報を収集し、ユーザーが指定した目的地に転送します。デーモンセットとしてデプロイすることでログの収集が可能です。
Cloud Logging：Google Cloudが提供する、アプリケーションと仮想マシンのログデータを集約・表示・分析するサービスです。
サイドカーコンテナ：メインのアプリケーションコンテナと一緒に起動し、その機能を補完するためのコンテナです。ログの収集やプロキシなどの補完的なロールを果たします。
共有ボリューム：複数のコンテナ間で共有されるストレージ空間です。サイドカーコンテナがメインのアプリケーションコンテナのファイルやデータにアクセスするために利用されます。
Google Compute Engine（GCE）：Google CloudのIaaS（Infrastructure as a Service）で、仮想マシンを提供します。- より詳細な設定やカスタマイズが可能な一方で、管理負荷はGKEよりも高くなります。
正解についての説明：
（選択肢）
・GKEにFluentdデーモンセットをデプロイします。それから、アプリケーションのポッドでログファイルをテールし、Cloud Loggingに書き込むように、カスタマイズした入出力設定を作成します
この選択肢が正解の理由は以下の通りです。
まず、Fluentdはオープンソースのデータコレクターで、ログデータを統一的に取り扱うためのツールです。Google Kubernetes Engine（GKE）は、Fluentdを用いてコンテナのログをCloud Loggingに転送する設定を提供しています。しかし、標準ではコンテナの標準出力と標準エラー出力しか監視対象にしていません。
したがって、指定されたファイルにログを出力するアプリケーションのログを取得するために、Fluentdの設定をカスタマイズする必要があります。またそのために、デーモンセットを使用してKubernetesスケジューラにFluentdを全てのノードにデプロイさせ、各ノードで実行されるアプリケーションのログを取得します。これらの理由から、この選択肢が正解となります。
不正解についての説明：
選択肢：デフォルトのGoogle Cloud Operation Suite Kubernetes Engine Monitoringエージェント設定を使用します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Kubernetes Engine Monitoringのデフォルト設定は、アプリケーションの出力から直接ログを取り込むことを想定していますが、本問題ではアプリケーションが/var/log/app_messages.logにログ情報を書き込むため、ログ情報の収集にはカスタマイズした設定が必要となります。そのため、デフォルト設定の使用は問題の要件を満たしません。
選択肢：Google Compute Engine（GCE）にKubernetesをインストールし、アプリケーションを再デプロイします。そして、組み込みのCloud Logging設定をカスタマイズして、アプリケーションのポッド内のログファイルをテールし、Cloud Loggingに書き込みます
この選択肢が正しくない理由は以下の通りです。
まず、GCEにKubernetesをインストールすることで、管理オーバーヘッドが大幅に増加します。
さらに、アプリケーションの再デプロイやGCEの使用は無駄なステップであり、GKEにFluentdをデプロイするだけで要件を満たすことができます。
選択肢：ポッド内でログファイルをテールし、標準出力にエントリを書き出すスクリプトを記述します。このスクリプトを、アプリケーションのポッドのサイドカーコンテナとして実行します。コンテナ間で共有ボリュームを構成し、スクリプトがアプリケーションコンテナ内の/var/logに読み取りアクセスできるようにします
この選択肢が正しくない理由は以下の通りです。
この選択肢は新しいアプリケーションを変更または再構成することを要求してしまいますが、問題文によれば新しいアプリケーションの変更や再構成は不可能とされています。
それに対し、正解の選択肢におけるFluentdデーモンセットのデプロイはアプリケーション自体への変更が不要なため、問題の要件を満たします。
参考リンク：
https://cloud.google.com/logging/docs/agent/logging/configuration
https://cloud.google.com/logging/docs/agent
https://kubernetes.io/docs/concepts/cluster-administration/logging/
<details><div>

### Q. 問題25: 未回答
サードパーティアプリケーションが正しく動作するために、サービスアカウントキーが必要です。クラウドプロジェクトからキーをエクスポートしようとすると、以下のようなエラーが発生します：
組織ポリシーの制約iam.disableServiceAccounKeyCreationが強制されています。
Googleが推奨するセキュリティプラクティスに従いながら、サードパーティアプリケーションを動作させる必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. 組織レベルのiam.disableServiceAccountKeyCreationポリシーを削除し、キーを作成します
2. デフォルトのサービスアカウントキーを有効にし、キーをダウンロードします
3. プロジェクトにiam.disableServiceAccountKeyCreationポリシーをオフに設定するルールを追加し、キーを作成します
4. プロジェクトのフォルダーでサービスアカウントキー作成ポリシーを無効にし、デフォルトのキーをダウンロードします
<details><div>
    答え：1
説明
この問題では、Google Cloudの組織ポリシーによってサービスアカウントキーの作成が制限されている状況に対処する方法が求められています。問題文からは、これらのキーがサードパーティのアプリケーションで必要であることと、Googleが推奨するセキュリティプラクティスに従う必要があることが明らかとなります。組織ポリシーとサービスアカウントキーの関係性を理解し、それがプロジェクト運用とセキュリティ要件とどのように関連するかを考慮することが重要です。これに基づいて最適な解決策を選択することが求められています。
基本的な概念や原則：
サービスアカウントキー：Google Cloudの認証と認可を行うための資格情報の一部です。サービスアカウントキーを使用すると、アプリケーションがGoogle Cloud APIを呼び出すことができます。
組織レベルのiam.disableServiceAccountKeyCreationポリシー：組織全体でサービスアカウントキー作成の禁止を制御するGoogle Cloudのポリシーです。このポリシーが強制されている場合、組織内の全てのプロジェクトでサービスアカウントキーの作成が無効化されます。
デフォルトのサービスアカウントキー：各プロジェクトが最初に作成されたときにGoogle Cloudによって自動的に作成されるサービスアカウントキーです。
フォルダーレベルのポリシー：フォルダレベルで制御できるGoogle Cloudのポリシーです。フォルダ内のすべてのプロジェクトに影響しますが、組織レベルのポリシーには優先されません。
サードパーティアプリケーション：Google Cloud外部のアプリケーションで、通常、API呼び出しを行うためにサービスアカウントキーが必要です。
正解についての説明：
（選択肢）
・組織レベルのiam.disableServiceAccountKeyCreationポリシーを削除し、キーを作成します
この選択肢が正解の理由は以下の通りです。
Googleは一般に、サービスアカウントキーを直接作成してエクスポートする代わりに、Google CloudのIAMとAdminによって提供されるより安全な方法を使用することを推奨しています。しかし、上記の問題の具体的な状況では、サードパーティのアプリケーションがサービスアカウントキーを必要としているため、iam.disableServiceAccountKeyCreationの組織ポリシーを削除してキーを作成する必要があります。それによりサービスアカウントキーの作成が可能になりサードパーティのアプリケーションが正常に機能します。
ただし、その後、不必要なアクセスを回避するために、この組織ポリシーを再度強制することも重要です。
不正解についての説明：
選択肢：デフォルトのサービスアカウントキーを有効にし、キーをダウンロードします
この選択肢が正しくない理由は以下の通りです。
デフォルトのサービスアカウントキーを有効にしてダウンロードすると、セキュリティ上のリスクがあります。デフォルトのキーは普通分散されません。
また、既に組織ポリシーでキーの作成が禁止されているため、この方法は使用できません。
選択肢：プロジェクトのフォルダーでサービスアカウントキー作成ポリシーを無効にし、デフォルトのキーをダウンロードします
この選択肢が正しくない理由は以下の通りです。
フォルダーレベルでのポリシー設定は、組織レベルに強要されているポリシーにより上書きされます。
したがって、サービスアカウントキーの作成を許可するために、必ず組織レベルのポリシーを変更する必要があります。
選択肢：プロジェクトにiam.disableServiceAccountKeyCreationポリシーをオフに設定するルールを追加し、キーを作成します
この選択肢が正しくない理由は以下の通りです。
元のエラーメッセージによると、制約iam.disableServiceAccountKeyCreationは組織レベルで強制されているため、プロジェクトレベルでそのポリシーを無効に設定しようとしても無駄となります。組織レベルのポリシーが優先されるため、この不正解の選択肢は問題を解決することはできません。
参考リンク：
https://cloud.google.com/iam/docs/service-accounts
https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints
https://cloud.google.com/iam/docs/creating-managing-service-account-keys
<details><div>

### Q. 問題26: 未回答
あなたは、主力サービスのキャパシティプランニングを半期ごとに実施しています。あなたは、今後6ヶ月間で前月比10％のサービスユーザー増加を見込んでいます。サービスは完全にコンテナ化され、Google Cloud上で実行されます。Google Kubernetes Engine（GKE）Standardリージョナルクラスターを3つのゾーンで使用し、クラスターオートスケーラーが有効になっています。現在、デプロイされた総CPU容量の約30%を消費しており、ゾーンの障害に対する回復力が必要です。不必要なコストを回避しつつ、この増加やゾーン障害によるユーザへの悪影響を最小限に抑えたいと考えています。
予測される増加に対処するために、どのように準備すべきですか？
1. GKEにデプロイされ、クラスターオートスケーラを使用しているため、GKEクラスターは成長率に関係なく自動的にスケールします
2. 成長率10％の6カ月分を考慮し、ノード容量を積極的に60％追加し、負荷テストを行って十分な容量があることを確認します
3. 最大ノードプールサイズを確認し、水平ポッドオートスケーラーを有効にし、負荷テストを実行して予想されるリソースニーズを確認します
4. 稼働率が30％しかないため、ヘッドルームはかなりあり、この成長率で容量を追加する必要はありません
<details><div>
    答え：3
説明
この問題では、リソースの増加を予測し、その成長を効率的に管理するための戦略が求められています。問題文から読み取れる要素には、コンテナ化されたサービス、前月比10％のユーザー増加の予測、GKEの使用、クラスターオートスケーラの有効化、現行のCPU使用率、ゾーン障害に対する回復力の必要性があります。これらの要素から、サービスのスケーラビリティと耐性を維持しつつ、余分なコストを抑えるための準備方法を見つけることが求められています。したがって、適切なスケーリング戦略を選択し、予想される増加に対する準備のために負荷テストを行うなどの手段を検討する必要があります。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudの完全マネージドなKubernetesサービスです。アプリケーションをデプロイ、スケール、管理するための環境を提供します。
クラスターオートスケーラー：GKEの機能で、負荷に応じてクラスターのノード数を自動的に調整します。不要なリソース削減とパフォーマンスの最適化が可能です。
水平ポッドオートスケーラー：GKEの機能で、負荷に応じてポッドの数を自動的に調整します。アプリケーションのスケールアウトを自動化します。
負荷テスト：システムの性能を測定するためのテストです。特定の負荷下でシステムが適切に機能し、スケールするかを確認します。
リソースの消費率：サービスが使用するリソースの割合です。これが低い場合、リソースが過剰に用意されてしまっている可能性もあります。
ノードプール：GKEのクラスター内に作成されるノードのグループです。クラスター全体のリソースの管理と最適化を可能にします。
ゾーン障害：特定の地理的エリア内のリソースが利用できなくなる状態です。マルチゾーンまたはマルチリージョナルな配置を行うことで、これに対処します。
正解についての説明：
（選択肢）
・最大ノードプールサイズを確認し、水平ポッドオートスケーラーを有効にし、負荷テストを実行して予想されるリソースニーズを確認します
この選択肢が正解の理由は以下の通りです。
まず、今回のシナリオではGoogle Kubernetes Engine（GKE）を活用しており、サービスユーザーが増加することが予想されています。GKEでは、最大ノードプールサイズを指定することであらかじめインフラ用意を行うことができます。これにより、ユーザーの増加に伴うリソースの需要が増えてもGKEのクラスターが自動的にスケールアウトすることで対応できます。水平ポッドオートスケーラーを有効にすることで、ポッドレベルでのスケーリングを自動化し、リソースの使用効率を最大化できます。これら二つの設定を行うことで、ユーザー増加に伴うリソース需要を柔軟にカバーできるようになります。
また、負荷テストを実行することで、予測されるリソースニーズが現実的であるかどうかを検証できます。これにより、不必要なインフラコストの上昇を防ぐことができます。
さらに、テストによって現在のシステムの性能と耐久性を明確に理解でき、未来の需要に対してより適切に対応できます。
したがって、この選択肢はユーザー増加を想定した際の対策として適切であると言えます。
不正解についての説明：
選択肢：GKEにデプロイされ、クラスターオートスケーラを使用しているため、GKEクラスターは成長率に関係なく自動的にスケールします
この選択肢が正しくない理由は以下の通りです。
GKEクラスターは自動的にスケールしますが、それはリソース要求に応じたスケーリングです。予測される成長率には対応できず、また最大ノードプールサイズの上限やノードの利用率についての確認も必要です。負荷テストにより予想されるリソースニーズを確認することが重要です。
選択肢：稼働率が30％しかないため、ヘッドルームはかなりあり、この成長率で容量を追加する必要はありません
この選択肢が正しくない理由は以下の通りです。
運用に余裕があるからといって、サービスの成長に対応するための計画が不要とは言えません。これは、予想されるユーザー増に備えた最適な対応策を見つけることが求められる場面となります。ヘッドルームがある今こそ、予測される増加に対して適切な計画を行い、水平ポッドオートスケーラーを有効にしたり、負荷テストを実行してリソースニーズを確認するなどの行動が必要です。
選択肢：成長率10％の6カ月分を考慮し、ノード容量を積極的に60％追加し、負荷テストを行って十分な容量があることを確認します
この選択肢が正しくない理由は以下の通りです。
ノード容量を60%追加すると、必要以上のリソースを積極的に確保しすぎる可能性があり、不必要なコストがかかる可能性があります。正解の選択肢では、最大ノードプールサイズを確認し、水平ポッドオートスケーラーを有効にすることで、リソースの需要に応じてスケーリングが行われ、よりコスト効率的であると言えます。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
https://cloud.google.com/architecture/framework/resilience/redundancy-design
<details><div>

### Q. 問題27: 未回答
あなたは、アプリケーションログを7年間アーカイブすることを要求する政府機関と仕事をしています。Google Cloud Operation Suiteを構成して、保管コストを最小限に抑えながらログをエクスポートし、保管する必要があります。
この要件を満たすために、どうすればよいですか？
1. Google Cloud Operation Suiteからログを取得し、BigQueryに保存するApp Engineアプリケーションを開発します
2. Google Cloud Operation Suiteでシンクを作成し、名前を付け、アーカイブされたログを保存するためのバケットをCloud Storage上に作成し、ログのエクスポート先としてバケットを選択します
3. Cloud Storageバケットを作成し、そのバケットに直接ログを送信するアプリケーションを開発します
4. Google Cloud Operation Suiteでエクスポートを作成し、Cloud Pub/Subを構成してログを7年間永久保存します
<details><div>
    答え：
説明
この問題では、Google Cloud Operation Suiteを使用してアプリケーションログをアーカイブし、その一方で保管コストを最小限に抑える方法が求められています。問題文の要点は、アーカイブの長期間な要件とコストの抑制という二つの要素を平行して満たす必要がある点です。ログのエクスポートについての理解と、Google Cloudの各サービスが提供する機能とそのコスト構造について知識をもっていればより良い答えを出すことが可能です。また、適切なログ保管場所と、エクスポートとアーカイブの方法を選択することが重要です。
基本的な概念や原則：
Google Cloud Operation Suite：Google Cloudが提供するログ管理およびモニタリングツールのスイートです。ログデータのエクスポートや保存機能を提供します。
シンク：Google Cloud Operation Suiteの機能で、ログのエクスポート先を指定することができます。
Cloud Storage：Google Cloudが提供するオブジェクトストレージサービスです。データの長期保管に適しており、ストレージクラスによっては非常に低いコストで使用することができます。
アーカイブ：長期間保管するためのデータ管理戦略です。コストを低く抑えることができますが、アクセスする際のレイテンシが高くなることがあります。
BigQuery：Google Cloudが提供するフルマネージドなビッグデータ分析ツールです。ログ分析に使用されることがありますが、長期間のログ保管には不適な場合があります。
Cloud Pub/Sub：Google Cloudが提供するリアルタイムのメッセージングサービスです。ログのストリーミングや処理に使用されますが、永久保存には適していません。
正解についての説明：
（選択肢）
・Google Cloud Operation Suiteでシンクを作成し、名前を付け、アーカイブされたログを保存するためのバケットをCloud Storage上に作成し、ログのエクスポート先としてバケットを選択します
この選択肢が正解の理由は以下の通りです。
Google Cloud Operation Suite（以前のStackdriver）は、Google Cloud内のアプリケーションログの管理と監視を実現するツールで、シンクを作成することでログをエクスポートすることが可能です。ここでの"シンク"とは、特定のフィルタ条件に一致するログエントリーを一元的にルーティングするための設定のことを指します。
一方、Google Cloud Storageは、オブジェクトストレージサービスで、大量のデータを安全に、持続的に保管し、アクセスできることが特徴です。特に、ColdlineやArchiveストレージクラスを利用することで、長期間のデータ保管を低コストで実現できます。
したがって、シンクを作成し、そのエクスポート先をCloud Storageのバケットに設定することで、長期間に渡るアプリケーションログの保管を、コスト効率良く実現できます。
不正解についての説明：
選択肢：Cloud Storageバケットを作成し、そのバケットに直接ログを送信するアプリケーションを開発します
この選択肢が正しくない理由は以下の通りです。
Cloud Storageバケットに直接ログを送信するアプリケーションを開発すると、新たな開発及びメンテナンスのコストが発生します。
一方、Google Cloud Operation Suiteでシンクを作成しログのエクスポート先としてバケットを選択する方が効率的で、予め提供さているサービスを用いて保管コストを最小限に抑えることが可能です。
選択肢：Google Cloud Operation Suiteからログを取得し、BigQueryに保存するApp Engineアプリケーションを開発します
この選択肢が正しくない理由は以下の通りです。
BigQueryを使用すると、保存に関連するコストが増加します。
また、App Engineアプリケーションの開発も追加のコストと時間を必要とします。直接Cloud Storageにエクスポートすることで、これらの余分なコストを無くし、保管コストを最小限に抑えることが可能になります。
選択肢：Google Cloud Operation Suiteでエクスポートを作成し、Cloud Pub/Subを構成してログを7年間永久保存します
この選択肢が正しくない理由は以下の通りです。
Cloud Pub/Subはリアルタイムのメッセージングサービスであり、ログの長期保存には適していません。
また、Logs Exportsを使用する代わりにCloud Storageを使用すると、保管コストをより効率的に管理できます。これが正解となるCloud Storageを選択する方法に対して、不適切な方法となります。
参考リンク：
https://cloud.google.com/logging/docs/export
https://cloud.google.com/storage/docs
https://cloud.google.com/logging/docs/storage#storage-location
<details><div>

### Q. 問題28: 未回答
アプリケーションイメージはビルドされ、Google Container Registry（GCR）にプッシュされます。開発工数を最小限に抑えつつ、イメージが更新されたときにアプリケーションをデプロイする自動化パイプラインを構築したいと考えています。
この要件を満たすために、どうすればよいですか？
1. Cloud Buildのカスタムビルダーを使用して、Jenkinsパイプラインをトリガーします
2. Cloud Buildを使用してSpinnakerパイプラインをトリガーします
3. Cloud Pub/Subを使用してSpinnakerパイプラインをトリガーします
4. Cloud Pub/Subを使用して、Google Kubernetes Engine（GKE）で実行されているカスタムデプロイメントサービスをトリガーします
<details><div>
    答え：
説明
この問題では、アプリケーションの自動デプロイメントパイプラインの作成要件に対する理解が求められています。ここでは、アプリケーションイメージが更新されたときに、開発工数を最小限に抑えつつアプリケーションの再デプロイを行いたいという要件に注目します。その上で、選択肢を確認するときには、どの選択肢がイメージの更新をトリガーにして自動的にデプロイを行うかを理解することが重要です。
基本的な概念や原則：
Cloud Pub/Sub：Google Cloudのリアルタイムメッセージングサービスで、独立したアプリケーション間でメッセージを交換することができます。イメージの更新時に発生するイベントをハンドリングして、自動化パイプラインをトリガーするのに使われます。
Spinnaker：マルチクラウド環境での継続的デリバリーを実現するためのオープンソースツールです。Cloud Pub/Subと組み合わせて、自動化パイプラインを実行する際のトリガーとして使用できます。
Google Container Registry（GCR）: Dockerイメージをプライベートに保存、共有できるストレージサービスです。開発されたアプリケーションイメージを保存し、その更新を検知してパイプラインを触発するために使われます。
Cloud Build：Google Cloudのビルドサービスで、ソースコードからコンテナイメージまたはアプリケーションのビルドを自動化するのに使われます。本問題の場合では、このサービスが自動化パイプラインのトリガーとして使用対象から外れています。
Jenkins：オープンソースのソフトウェアで、継続的なソフトウェア開発と継続的デリバリーを実現するためのツールです。本問題の場合では、このツールを使ったパイプラインが自動化パイプラインのトリガーとして使用対象から外れています。
Google Kubernetes Engine（GKE）：Google CloudのフルマネージドKubernetesサービスです。本問題の場合では、GKEで実行されているカスタムデプロイメントサービスがパイプラインのトリガーとして使用対象から外れています。
正解についての説明：
（選択肢）
・Cloud Pub/Subを使用してSpinnakerパイプラインをトリガーします
この選択肢が正解の理由は以下の通りです。
まず、Container Registryは新しいイメージがプッシュされたときにCloud Pub/Subに通知を送ることができます。そのため、Cloud Pub/Subを使用してこの通知を待ち受けることで、アプリケーションイメージが更新された際に自動的に処理をトリガーすることが可能になります。
また、Spinnakerはパイプラインオーケストレーションツールであり、Cloud Pub/Subのメッセージをトリガーとしてパイプラインを実行することができます。つまり、Pub/Subメッセージを受け取ったら、イメージのデプロイを自動で開始する設定を行うことができます。この組み合わせにより、イメージがプッシュされるとすぐに自動デプロイが開始され、開発工数を削減しながら更新がすばやく反映される仕組みを作ることができます。
以上の理由から、Google Container Registryの新しいイメージプッシュをトリガーとしたCloud Pub/SubとSpinnakerの組み合わせは、要件を満たす適切な解答となります。
不正解についての説明：
選択肢：Cloud Buildを使用してSpinnakerパイプラインをトリガーします
この選択肢が正しくない理由は以下の通りです。
Cloud Buildはビルドプロセスを自動化するサービスであり、Spinnakerパイプラインを直接トリガーする機能はありません。
一方、Cloud Pub/Subはメッセージングサービスで、イメージがGCRにプッシュされたイベントをトリガーとしてSpinnakerパイプラインを起動することができます。
選択肢：Cloud Buildのカスタムビルダーを使用して、Jenkinsパイプラインをトリガーします
この選択肢が正しくない理由は以下の通りです。
Cloud Buildのカスタムビルダーを使用してJenkinsパイプラインをトリガーする方法では、開発工数が増加します。カスタムビルダーの設計・開発が必要となるためです。
一方、Cloud Pub/Subを使用してSpinnakerパイプラインをトリガーすれば、イメージ更新時に自動でデプロイでき、開発工数を最小限に抑えられます。
選択肢：Cloud Pub/Subを使用して、Google Kubernetes Engine（GKE）で実行されているカスタムデプロイメントサービスをトリガーします
この選択肢が正しくない理由は以下の通りです。
カスタムデプロイメントサービスを用いると、その開発と管理に追加の工数が必要となります。これは開発工数を最小限に抑えるという要件に反します。対してSpinnakerは既存のオープンソースのデプロイメントシステムであり、そのパイプラインのトリガーにCloud Pub/Subを使用できます。
参考リンク：
https://cloud.google.com/pubsub/docs/overview
https://cloud.google.com/spinnaker/docs/using-cd-pipelines
https://cloud.google.com/architecture/automating-image-deployments-to-gke-with-cloud-build-using-docker-maven-and-spring-boot
<details><div>

### Q. 問題29: 未回答
あなたは、Compute Engineで実行されるアプリケーションを管理しています。このアプリケーションは、カスタムHTTPサーバーを使用してAPIを公開し、内部TCP/UDPロードバランサーを介して他のアプリケーションからアクセスされます。ファイアウォールルールは、0.0.0.0/0からのAPIポートへのアクセスを許可します。最も少ないステップ数を使用して、APIにアクセスする各IPアドレスをログに記録するようにCloud Loggingを構成する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. VPCでPacket Mirroringを有効にします
2. ファイアウォールルールのロギングを有効にします
3. Compute EngineインスタンスにOps Agentをインストールします
4. サブネットでVPCフローログを有効にします
<details><div>
    答え：4
説明
この問題では、Compute Engineで実行されるアプリケーションに関連するロギング設定についての理解と適切な構成を目指して考える必要があります。問題にはAPIへのアクセスをする各IPアドレスをロギングを行う必要があると明示されています。そのため、IPアドレス情報を取得可能かつ簡便性を優先した機能を選択すべきです。また、ステップ数を最小限に抑えることも要求されており、この条件を満たすためには最も直接的かつ効率的な方法を選ぶ必要があります。
基本的な概念や原則：
VPCフローログ：仮想プライベートクラウド（VPC）ネットワークのIPトラフィックをキャプチャし、Cloud LoggingやBigQuery、Cloud Storageにログを記録する機能です。サブネットレベルでの活動を記録し、ネットワークの問題解析や適切なネットワーク使用のための洞察を提供します。
Packet Mirroring：指定したVPCネットワーク内の仮想マシンインスタンスのネットワークトラフィックをキャプチャし、特定の受信者インスタンスにミラーリングする機能です。これはネットワークのパフォーマンス監視やセキュリティ分析に使用されますが、個々のIPアドレスのログ記録には適していません。
Ops Agent：システムとアプリケーションのメトリクスを収集し、転送するGoogle Cloudのエージェントです。ログ記録には使用しますが、APIへのアクセスする各IPアドレスのログ記録には特化していません。
ファイアウォールルールのロギング：Google Cloudのファイアウォールルールに関連するトラフィックの詳細をCloud Loggingに記録する機能です。しかし、これによるログ記録には多くのステップが必要となります。
Subnet：VPCネットワーク内の区分です。各サブネットは特定の地域に存在し、その地域のリソースを含むことができます。
正解についての説明：
（選択肢）
・サブネットでVPCフローログを有効にします
この選択肢が正解の理由は以下の通りです。
VPCフローログは、Google Cloud VPCネットワークのインターネットプロトコルのDataflow（つまり、ネットワーク内の仮想マシン（VM）インスタンス間の通信）に関連する情報をキャプチャし、それをCloud Loggingに保存します。これにより、VPCネットワークで生成されたネットワークフローデータをCloud Loggingで詳細に見ることができ、それを元に監視や分析を行うことができます。なお、有効化はサブネットごとに行うことが可能であり、柔軟な運用が可能です。
具体的には、VPCフローログは各ネットワークフローの送信元IPアドレスと宛先IPアドレス、及びその他の詳細を提供するため、この問題で述べられているようにAPIにアクセスする各IPアドレスを連続的にログに記録することが可能です。
したがって、VPCフローログを有効にすることで、APIにアクセスする各IPアドレスを効率的にログに記録することができます。これは、最も少ないステップ数を使用しつつ、必要な監視の要件を満たすための最適な手段となります。
不正解についての説明：
選択肢：VPCでPacket Mirroringを有効にします
この選択肢が正しくない理由は以下の通りです。
VPCでPacket Mirroringを有効にすることで、ネットワークパケットのコピーや検査が可能ですが、目的のAPIアクセスに関するIPアドレスをログに記録するためには適していません。
それに対して、サブネットでVPCフローログを有効にすることで、必要なIPアドレスの情報をCloud Loggingに直接送信し、最も少ないステップで要件を満たすことができます。
選択肢：Compute EngineインスタンスにOps Agentをインストールします
この選択肢が正しくない理由は以下の通りです。
Ops Agentをインストールする手法は、インスタンス上のアプリケーションログを取得する際に用いられますが、ここではAPIにアクセスする各IPアドレスをログに記録するネットワークフローロギングが求められています。よって、VPCフローログを有効にする選択肢が適しています。
選択肢：ファイアウォールルールのロギングを有効にします
この選択肢が正しくない理由は以下の通りです。
ファイアウォールルールのロギングを有効にしたとしても、それはあくまで許可・拒否されたトラフィックを記録するものであり、APIにアクセスする各IPアドレスを記録するための機能ではありません。
一方で、VPCフローログを有効にすると、IPパケットレベルの詳細な情報が取得でき、この中には接続元のIPアドレスも含まれます。
参考リンク：
https://cloud.google.com/vpc/docs/using-flow-logs
https://cloud.google.com/compute/docs/load-balancing/internal
https://cloud.google.com/logging/docs/refs/api
<details><div>

### Q. 問題30: 未回答
あなたはGoogle Kubernetes Engine（GKE）Autopilotクラスターにデプロイされたマイクロサービスの常駐運用サイト信頼性エンジニアです。あなたの会社は、Pub/Subに注文メッセージを発行するオンラインストアを運営しており、マイクロサービスがこれらのメッセージを受信し、倉庫システムの在庫情報を更新します。ある販売イベントによって注文が増加し、在庫情報が十分に迅速に更新されていません。このため、在庫切れの商品に対して大量の注文が受け付けられています。マイクロサービスのメトリクスをチェックし、標準的なレベルと比較した結果は以下のとおりです：
- すべてのポッドの平均CPU：
通常の状態: ポッド制限の20%
現在の状態: ポッド制限の40%
- すべてのポッドの平均メモリ：
通常の状態: ポッド制限の10%
現在の状態: ポッド制限の10%
- Pub/Subサブスクリプションの最も古い未確認メッセージの平均経過時間：
通常の状態: 331ミリ秒
現在の状態: 6774ミリ秒
- Pub/Subサブスクリプションの平均未配信メッセージ数：
通常の状態: 10メッセージ
現在の状態: 12405メッセージ
- Pub/Subサブスクリプションの平均確認応答レイテンシ：
通常の状態: 302ミリ秒
現在の状態: 329ミリ秒
注文が入った時点で、倉庫システムが商品の在庫を正確に反映するようにし、顧客への影響を最小限に抑える必要があります。
この要件を満たすために、どうすればよいですか？
1. ポッドのCPUとメモリの制限を増やします
2. ポッドレプリカの数を増やします
3. 典型的なトラフィックレベルを許容する仮想キューをオンラインストアに追加します
4. サブスクリプションの承認期限を引き下げます
<details><div>
    答え：2
説明
この問題では、Pub/Subの未確認メッセージの処理遅延が発生している状況に対応するための最適な方法を求められています。問題文し、メッセージの増加により、ポッドのリソース使用率が上昇し、それによって未確認メッセージが増加しています。しかし、ポッドのCPUやメモリ使用率は全体として見てまだ余裕があります。このため、リソース使用率が標準範囲内でありながらも処理待ちのメッセージが増えていることから、並行処理能力が問題となっています。そのため、解決策としてはポッドのスケーリングが適切な選択となります。ここでは、適切な問題解決策を選ぶために、システムのメトリクスを適切に解釈し、システムの動作を理解することが重要となります。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google CloudのマネージドKubernetesサービスです。スケーリング、アプリケーションのデプロイメント、アップデートの管理が可能です。
GKE Autopilot：Google Kubernetes Engineのフルマネージドモードで、インフラストラクチャの管理をGoogleに委託します。リソースの効率的な利用と高い可用性を提供します。
マイクロサービス：小さい単位の機能を提供し、独立してデプロイとスケーリングが可能なサービスの設計パターンです。
Pub/Sub：リアルタイムのメッセージングサービスで、プロデューサーがメッセージを発行し、コンシューマーがこれを受信することを可能にします。
サブスクリプション承認期限：Pub/Subメッセージがコンシューマーによって確認されるまでの最大時間です。これを下げると、メッセージが急速に再試行され、システムに負荷をかける可能性があります。
ポッドレプリカ：Kubernetesのポッドのコピーのことで、レプリカの数を増やすことにより、システムのスケーラビリティと耐久性を向上させることができます。
ポッドのCPUとメモリ制限：Kubernetesの機能で、ポッドが使用できるCPUとメモリの量を制限することができます。しかし、メトリクスから読み取れるとおり、CPUとメモリは使用率が低いため、制限を増やす必要はありません。
正解についての説明：
（選択肢）
・ポッドレプリカの数を増やします
この選択肢が正解の理由は以下の通りです。
現状では、注文の増加によりPub/Subの未確認メッセージと未配信メッセージが著しく増えていることがわかります。この状況は、メッセージの処理能力が注文の増加に追いついていないことを示しています。
したがって、その処理能力を増やすためには、メッセージを処理して在庫情報を更新するマイクロサービスのポッドレプリカの数を増やすのが適切です。
これにより、マイクロサービスのスループット、つまり単位時間あたりに処理できるメッセージの数が増え、増加した注文量に対応できます。もしメモリやCPUの使用量が高ければ、リソースが不足している可能性がありますが、両方とも制限の一部しか使用していないため、追加のポッドをサポートするための余裕があることが示されています。
つまり、このアプローチにより、在庫情報の更新が遅れる問題が改善され、顧客が在庫切れの商品を注文するという状況を防ぐことができます。
不正解についての説明：
選択肢：サブスクリプションの承認期限を引き下げます
この選択肢が正しくない理由は以下の通りです。
サブスクリプションの承認期限を引き下げても、注文が増えた際に在庫情報を迅速に更新するという問題は解決できません。承認期限を短くすると、メッセージの処理が間に合わず、更なる問題を引き起こす可能性があります。
逆に、ポッドレプリカの数を増やすことで、システムの処理能力を高め、メッセージの処理速度を向上させることが可能です。
選択肢：典型的なトラフィックレベルを許容する仮想キューをオンラインストアに追加します
この選択肢が正しくない理由は以下の通りです。
仮想キューの追加はトラフィックの処理能力自体を向上させるわけではなく、注文処理の遅延に対する一時的な対策に過ぎません。肝心の問題である注文データの処理速度を改善するためには、マイクロサービスを担当するポッドレプリカの数を増やすことで、更なる処理能力を確保する方が適切です。
選択肢：ポッドのCPUとメモリの制限を増やします
この選択肢が正しくない理由は以下の通りです。
CPUとメモリの使用率がそれぞれ40%と10%に過ぎず、リソースの制限が問題でないことが示されています。
したがって、制限を増やしてもパフォーマンスは改善されません。
代わりに、メッセージの未配信数が大幅に増えているため、レプリカを増やして並行処理の能力を改善する方が効果的です。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview
https://cloud.google.com/pubsub/docs/subscriber
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment
<details><div>

### Q. 問題31: 未回答
あなたはSREの実践と原則を取り入れている組織の一員です。あなたは開発チームから新しいサービスの管理を引き継ぐことになり、本番準備レビュー（PRR）を実施します。PRR分析フェーズの後、あなたはサービスが現在サービスレベル目標（SLO）を満たすことができないと判断しました。そのため、本番環境でサービスがSLOを満たせるようにしたいと思います。
あなたは次に何をすべきですか？
1. 引き渡しまでに完了すべき、サービスに対する推奨される信頼性改善を特定します
2. 開発チームに、サービスの本番サポートを提供しなければならないことを通知します
3. サービスによって達成可能なSLO目標を調整し、本番稼動できるようにします
4. SLOがない状態でサービスを本番稼動させ、運用データを収集してから構築します
<details><div>
    答え：1
説明
この問題では、本番環境への準備を進める上で適切なアプローチを考える必要があります。新しいサービスがService Level Objectives（SLOs）に適合していない事実、そして問題の目的がSLOを満足するようサービスを改善することであることに焦点を当てる必要があります。選択肢を考える際、最適な選択肢はSLOを達成するための具体的な改善を探して開発に取り組むことが期待されます。その他の選択肢は短絡的な対処法またはSLOを達成するための実質的なアプローチを欠いていることに注意してください。
基本的な概念や原則：
SRE（Site Reliability Engineering）：信頼性高いシステムを維持するための技術とプラクティスを適用するエンジニアリングの観点です。
サービスレベル目標（SLO）：システムの目標となる信頼性や性能を定量的に示す指標です。SLOが満たされない場合、システム改善の必要があります。
本番準備レビュー（PRR）：新しいサービスが本番環境に適しているかを評価するプロセスです。信頼性、セキュリティ、パフォーマンスなど、さまざまな項目をチェックします。
信頼性改善：システムの信頼性を向上させるための策定や実装です。具体的な対策は、障害の事前予防や対応強化、システムの性能向上などがあります。
正解についての説明：
（選択肢）
・引き渡しまでに完了すべき、サービスに対する推奨される信頼性改善を特定します
この選択肢が正解の理由は以下の通りです。
SREの実践と原則に基づくと、あらかじめ定義されたサービスレベル目標（SLO）を満たすことができて初めて、サービスが本番環境に適していると見なされます。SLOに達成できていない場合、そのサービスは本番環境での実行にはまだ準備ができていないと判断されるべきです。
したがって、何が問題であり、それをどのように解決するかを理解するために、SLO達成に必要な改善点を特定することが最優先となります。その改善点は、パフォーマンスの最適化、障害の防止や回復策の改善など、信頼性に関連するものであるべきです。これはサービス引き渡し前に対応すべき課題として適用され、サービスがSLOを満たし、信頼性の高いものとなるように推進するべきです。
不正解についての説明：
選択肢：サービスによって達成可能なSLO目標を調整し、本番稼動できるようにします
この選択肢が正しくない理由は以下の通りです。
SLOを単に調整し、本番環境への稼働を図るのは、実際のサービス品質改善には繋がりません。それは一時的な逃げ道に過ぎません。
一方、信頼性向上への具体的なアクションを特定することで、実際の問題解決に繋がるため、この方法が優れています。
選択肢：開発チームに、サービスの本番サポートを提供しなければならないことを通知します
この選択肢が正しくない理由は以下の通りです。
PRR分析フェーズ後、SLOを満たさないと判断された場合、ベストプラクティスは直接に修正を特定し実装することです。開発チームにサポート通知を送るだけでは問題を解決せず、SLOの改善に直接つながらないため、この選択肢は不適切です。
選択肢：SLOがない状態でサービスを本番稼動させ、運用データを収集してから構築します
この選択肢が正しくない理由は以下の通りです。
SLOがない状態でサービスを本番稼動させるという方法は、サービスの品質を確保する上で難しいです。本番環境での初期データ収集後にSLOを構築すると、初期の問題を検知し対応するのが遅れてしまう可能性があります。
一方、引き渡しまでにサービスに対する信頼性の改善を特定することで、初めからSLOを設定し、サービスの信頼性を保証できます。
参考リンク：
https://cloud.google.com/architecture/framework/devops/devops-measurement-monitoring-and-management
https://cloud.google.com/blog/products/Google Cloud/sre-fundamentals-slis-slas-and-slos
https://landing.google.com/sre/sre-book/chapters/service-level-objectives/
<details><div>

### Q. 問題32: 未回答
単一のCompute Engineインスタンスで実行される本番サービスをサポートしています。クラッシュしたインスタンスを削除し、関連するイメージに基づいて新しいインスタンスを作成することで、サービスの再作成に定期的に時間を費やす必要があります。サイト信頼性エンジニアリングの原則に従いつつ、手動操作に費やす時間を短縮したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. ロードバランサーをCompute Engineインスタンスの前に追加し、ヘルスチェックを使ってシステムの状態を判断します
2. 開発チームにバグを報告し、インスタンスがクラッシュする根本的な原因を探ってもらいます
3. 単一のインスタンスでマネージドインスタンスグループを作成し、ヘルスチェックを使用してシステムステータスを判断します
4. SMSアラート付きのGoogle Cloud Operation Suite Monitoringダッシュボードを作成し、クラッシュしたインスタンスの再作成をクラッシュ後すぐに開始できるようにします
<details><div>
    答え：3
説明
この問題では、Compute Engineインスタンスがクラッシュした際の再作成に手間がかかる状況を改善する解決策が求められています。問題文から、問題解決にはまず自動化を考え、インスタンスのヘルスステータスを監視し、それに基づいて必要に応じてインスタンスを自動的に再起動または再作成するような仕組みが必要であることが分かります。そして、その施策を実現するために最も適切なGoogle Cloudの機能を選択することが求められています。
基本的な概念や原則：
マネージドインスタンスグループ（MIG）：Compute Engineインスタンスを自動的に管理・スケーリングするサービスです。インスタンスの作成、削除や自動修復などをハンドルします。
ヘルスチェック：Google Cloudの機能で、Compute Engineのインスタンスやサービスの健全性を定期的に確認します。異常が見つかった場合、自動的な対策を引き起こすことができます。
サイト信頼性エンジニアリング（SRE）：システムとアプリケーションの信頼性を確保するためのエンジニアリング手法です。サービスの信頼性を最大化しつつ、開発速度も維持します。
Google Cloud Operation Suite：Google Cloudのオペレーションツールを一元化したスイートです。モニタリング、ロギング、エラーレポーティングなどを提供します。
ロードバランサー：トラフィックを複数のインスタンスに分散するサービスです。単一のエンドポイントから複数のインスタンスに負荷を分散し、高可用性とスケーラビリティを確保します。
正解についての説明：
（選択肢）
・単一のインスタンスでマネージドインスタンスグループを作成し、ヘルスチェックを使用してシステムステータスを判断します
この選択肢が正解の理由は以下の通りです。
まず、マネージドインスタンスグループ（MIG）は、定義したインスタンステンプレートに基づいて一連の同一のインスタンスを自動的に作成し、管理します。MIGは、インスタンスのスケーリング、更新、削除等を自動化することが可能であるため、手動による操作時間を短縮することになります。
また、MIGには自動ヘルスチェック機能があり、管理されたインスタンスが正常に動作しているかを監視します。この機能により、インスタンスがダウンした場合や不正常な挙動を示す場合に、自動的に新しいインスタンスを作る動作が行われます。
従って、クラッシュしたインスタンスの手動削除および新インスタンスの作成の手間が省かれます。以上のような要素が、サイト信頼性エンジニアリングの原則を満たし、最適な選択となる要因です。
不正解についての説明：
選択肢：開発チームにバグを報告し、インスタンスがクラッシュする根本的な原因を探ってもらいます
この選択肢が正しくない理由は以下の通りです。
根本的な原因を探ることは重要ですが、問題は手動操作の時間短縮であり、バグの報告・修正は自動再作成や手動介入を直接短縮するものではありません。
一方、マネージドインスタンスグループは故障したインスタンスの自動的な修復が可能で、手動操作の時間を短縮します。
選択肢：ロードバランサーをCompute Engineインスタンスの前に追加し、ヘルスチェックを使ってシステムの状態を判断します
この選択肢が正しくない理由は以下の通りです。
ロードバランサーは複数のインスタンス間で負荷を分散するために使用するもので、単一のインスタンスの管理や復旧には適していません。またヘルスチェックは状態判断に使えますが、自動で再作成を行う機能は提供していないため、手間を減らす目的には合致しません。これに対しマネージドインスタンスグループはヘルスチェックによる自動リペア機能を備えており、問題が発生したインスタンスを自動で再作成します。
選択肢：SMSアラート付きのGoogle Cloud Operation Suite Monitoringダッシュボードを作成し、クラッシュしたインスタンスの再作成をクラッシュ後すぐに開始できるようにします
この選択肢が正しくない理由は以下の通りです。
SMSアラート付きのGoogle Cloud Operation Suite Monitoringダッシュボードを作成することも有効ですが、これにより手動操作に費やす時間は必ずしも短縮されません。
また、この選択肢は自動修復の機能がなく、正解の選択肢にあるマネージドインスタンスグループとヘルスチェックの組み合わせに比べて管理の手間が増すため正しくありません。
参考リンク：
https://cloud.google.com/compute/docs/instance-groups
https://cloud.google.com/compute/docs/instance-groups/managing-groups
https://cloud.google.com/compute/docs/health-checks
<details><div>

### Q. 問題33: 未回答
あなたはGoogle Kubernetes Engineでアプリケーションを実行しています。アプリケーションはリクエストごとに複数のサービスを呼び出しますが、レスポンスが遅すぎます。どのダウンストリームのサービスが遅延の原因になっているかを特定する必要があります。
この要件を満たすために、どうすればよいですか？
1. リクエストのパスに沿ってVPCフローログを分析します
2. OpenTelemetryやGoogle Cloud Operation Suite Traceのような分散トレースフレームワークを使用します
3. 各サービスのLivenessとReadinessプローブを調査します
4. Dataflowパイプラインを作成し、サービスメトリクスをリアルタイムで分析します
<details><div>
    答え：2
説明
この問題では、複数のサービスの呼び出し時間を追跡するための効果的な方法を探す必要があります。各サービスがどれだけの時間を要しているかを把握する必要があるので、問題は分散システムのパフォーマンスの観点から解釈されます。そして、アプリケーションがGoogle Kubernetes Engine上で実行されているという情報は、選択肢を考慮する際に、Kubernetesと互換性のある解決策を選ぶために役立ちます。
基本的な概念や原則：
分散トレースフレームワーク：マイクロサービスアーキテクチャなどで複数のサービスが関与するリクエストについて、処理の経過を追跡し遅延の原因を特定するためのツールです。
OpenTelemetry：統一された方法で分散トレーシングやメトリクスを生成、管理するフレームワークです。複数サービス間のリクエスト追跡に対応しています。
Google Cloud Operation Suite Trace：Google Cloudで提供される分散トレーシングサービスです。アプリケーションのパフォーマンス問題を診断し、問題の特定と解決を助けます。
VPCフローログ：ネットワークフローデータを記録するサービスです。ネットワークの問題診断に利用することができますが、アプリケーションのパフォーマンス問題の根本原因を特定するには不足です。
LivenessとReadinessプローブ：Kubernetesがコンテナの健全性を判断するための検査ための機能です。しかし、これらのプローブはサービスのレスポンス時間の問題を特定するツールではありません。
Dataflowパイプライン：Google Cloudでストリームとバッチデータの両方を処理するための完全マネージドサービスです。リアルタイム分析は可能ですが、パフォーマンス遅延の特定には直接的には対応していません。
正解についての説明：
（選択肢）
・OpenTelemetryやGoogle Cloud Operation Suite Traceのような分散トレースフレームワークを使用します
この選択肢が正解の理由は以下の通りです。
まず、OpenTelemetryやGoogle Cloud Operation Suite Traceなどの分散トレーシングフレームワークは、マイクロサービス間のリクエストフローを視覚化し、リクエストがどのサービスを通ってどのくらいの時間をかけて処理されたかを理解するのに役立ちます。
複数のサービス間でリクエストが行き来する分散環境では、どのサービスがパフォーマンスのボトルネックとなっているのかを把握することが難しいです。分散トレースフレームワークを使用することによって、各リクエストがシステムをどのように流れ、どのサービスがレスポンスタイムの遅延に寄与しているかを正確に特定できます。
したがって、この選択肢はレスポンスが遅いアプリケーションのパフォーマンス問題を特定し、解決するために最適です。
不正解についての説明：
選択肢：リクエストのパスに沿ってVPCフローログを分析します
この選択肢が正しくない理由は以下の通りです。
VPCフローログはネットワークフローのログ情報を提供しますが、具体的なアプリケーションの遅延原因を特定するための詳細なトレーシング情報は提供できません。
それに対して、OpenTelemetryやGoogle Cloud Operation Suite Traceなどの分散トレースフレームワークは各サービスの呼び出し詳細を追跡し,遅延原因を解析できます。
選択肢：各サービスのLivenessとReadinessプローブを調査します
この選択肢が正しくない理由は以下の通りです。
LivenessやReadinessプローブはサービスが活性化しているか、リクエストを受け付ける準備ができているかを確認するもので、各サービスの応答時間を計測する目的には適していません。
それに対して、分散トレースフレームワークを使用すると、複数のサービスにまたがるリクエストのライフサイクルを視覚化し、どのサービスが遅延の原因になっているか特定することができます。
選択肢：Dataflowパイプラインを作成し、サービスメトリクスをリアルタイムで分析します
この選択肢が正しくない理由は以下の通りです。
Dataflowはストリーミングとバッチデータの処理に使用されます。しかし、アプリケーションの遅延の問題を解決するために、レスポンス時間を詳細に観察できる分散トレーシングツールの使用が適切です。OpenTelemetryやGoogle Cloud Operation Suite Traceはそのような需要に応えることができます。
参考リンク：
https://cloud.google.com/trace/docs
https://cloud.google.com/architecture/distributed-tracing-with-opentelemetry
https://opentelemetry.io/docs/concepts/what-is-opentelemetry/
<details><div>

### Q. 問題34: 未回答
あなたは、ユーザーに深刻な影響を与えたインシデントのポストモーテムを書いています。あなたは、今後同様のインシデントを防止したいと考えています。ポストモーテムに含めるべきセクションは、次のうち2つですか？（2つ選択）
1. インシデンとの根本原因の説明
2. インシデンとの再発を防止するためのアクションアイテムのリスト
3. インシデンとの原因となった従業員のリスト
4. 過去のインシデントと比較したインシデントの重大性に対するあなたの見解
5. インシデンとの影響を受けた全サービスの設計文書のコピー
<details><div>
    答え：1,2
説明
この問題では、インシデント対応後のポストモーテムに含めるべき情報について理解することが求められます。重要なのは、インシデンとが起こった原因を理解し、改善策を立て、同様のインシデンとを防止することです。基本的に責任を割り当てることは問題の解決に寄与しない上、チーム内の雰囲気を害する可能性があるため、従業員のリストは避けるべきです。また、インシデンとの重大性は客観的な事実に基づいて判断されるべきで、個々の見解はロールを果たしません。設計文書のコピーは、それ自体が改善策につながるわけではないため、不要です。
基本的な概念や原則：
インシデンとの根本原因：システム障害やインシデンとが起こった際に、それが何によって引き起こされたのかを特定するための分析行為です。その特定された原因は、将来同様のインシデンとを防止するための対策立案の参考となります。
アクションアイテム：具体的な行動計画やタスクのことです。今後の改善やインシデンと防止のために、ポストモーテムでは具体的なアクションアイテムがリストされます。
ポストモーテム：インシデンとや障害が発生した後に、その原因を分析し、再発防止策を考えるための情報をまとめる文書です。インシデンとの状況説明、原因分析、学び、そして再発防止策が含まれます。
正解についての説明：
（選択肢）
・インシデンとの根本原因の説明
・インシデンとの再発を防止するためのアクションアイテムのリスト
この選択肢が正解の理由は以下の通りです。
まず、"インシデンとの根本原因の説明"をポストモーテムに含めることは、発生したインシデントが再発しないようにするために必須です。この部分は、問題が何であったか、それが何で起こったか、そしてそれがどのようにしてシステムやユーザに影響を与えたかを詳しく分析し、理解するための情報を提供します。インシデンとの原因を明らかにすることで、同様の問題を防止するための対策を立てることができます。
次に、"インシデンとの再発を防止するためのアクションアイテムのリスト"を含めることも重要です。これはインシデントが再発しないようにするための具体的なステップや措置を明確にします。このリストには、根本原因分析から導き出された改善策や修正を行うためのパス、そしてそれらをどのように実行するかの明確な指示が含まれているべきです。これにより、チームはインシデントが再発しないように具体的な行動をとることが可能になります。
不正解についての説明：
選択肢：インシデンとの原因となった従業員のリスト
この選択肢が正しくない理由は以下の通りです。
インシデントポストモーテムの目的は問題の原因を理解し、再発を防ぐための学びを得ることであり、個人の過失を指摘する場ではありません。そのため、インシデンとの原因となった従業員のリストは不適切です。問題の根本原因と再発防止策が重要です。
選択肢：過去のインシデントと比較したインシデントの重大性に対するあなたの見解
この選択肢が正しくない理由は以下の通りです。
過去のインシデントと比較したインシデントの重大性に対する見解は、問題解決に向けて必要なアクションアイテムの明確化や根本原因分析には直接貢献しません。ポストモーテムの目的は問題解決と再発防止であり、そのためには根本原因の説明とアクションアイテムのリストが必要です。
選択肢：インシデンとの影響を受けた全サービスの設計文書のコピー
この選択肢が正しくない理由は以下の通りです。
インシデンとの影響を受けた全サービスの設計文書のコピーをポストモーテムに含めるのは適切ではありません。これは、インシデントの原因やその対策と直接的な関係がなく、ポストモーテムの目的である再発防止に直接寄与しません。
それに対して、インシデンとの根本原因や再発防止策は具体的な問題とその対策を明確にし、インシデントの再発を防止するために必要です。
参考リンク：
https://cloud.google.com/sre/docs/sre-book/managing-incidents
https://cloud.google.com/sre/docs/sre-book/postmortem-culture
https://en.wikipedia.org/wiki/Incident_management#Incident_post-mortem
<details><div>

### Q. 問題35: 未回答
Google Kubernetes Engine（GKE）にデプロイされたアプリケーションのCI/CDパイプラインを構築しています。アプリケーションは、Kubernetesのデプロイメント、サービス、Ingressを使用してデプロイされます。アプリケーションチームは、ブルー/グリーンデプロイを使用してアプリケーションをデプロイするようあなたに依頼しました。あなたはロールバックアクションを実装する必要があります。
この要件を満たすために、どうすればよいですか？
1. kubectl rollout undoコマンドを実行します
2. Kubernetes Serviceを更新して、以前のKubernetes Deploymentを指すようにします
3. 新しいKubernetes Deploymentをゼロにスケールします
4. 新しいコンテナイメージを削除し、実行中のポッドを削除します
<details><div>
    答え：2
説明
この問題では、Google Kubernetes Engine（GKE）でのCI/CDパイプライン構築と、アプリケーションのロールバック手法について理解が求められています。特に、ブルー/グリーンデプロイによるアプリケーションのデプロイとロールバックを行う方法を明らかにすることが必要です。その際、Kubernetesのデプロイメント、サービス、Ingressのロールと機能、またブルー/グリーンデプロイの仕組みについて理解していることが重要です。
基本的な概念や原則：
Kubernetes Service：Kubernetesのリソースの1つで、一連のポッドに対するアクセスポイントを提供します。Serviceは、ポッドのライフサイクルに関係なく一貫したアクセス方法を提供します。
Kubernetes Deployment：Kubernetesのリソースの1つで、アプリケーションのインスタンスを生成、更新、スケーリングすることができます。ロールバック機能を含む一連の更新機能を提供します。
ブルー/グリーンデプロイ：2つの完全に分離した環境（"ブルー"と"グリーン"）を使用してリリースを行うデプロイメント方法です。"ブルー"は現行環境、"グリーン"は新環境を表します。新環境が準備できたら、ユーザーのトラフィックを新環境に切り替えます。
kubectl rollout undo：これはKubernetesのコマンドで、Deploymentの前のリビジョンにロールバックする操作を実行します。
Ingress：Kubernetesのリソースの1つで、クラスター外部からの入口トラフィックを管理します。HTTPとHTTPSルートをポッドにルーティングするルールを定義します。
正解についての説明：
（選択肢）
・Kubernetes Serviceを更新して、以前のKubernetes Deploymentを指すようにします
この選択肢が正解の理由は以下の通りです。
まず、ブルー/グリーンデプロイメントは、新旧の二つの環境（ブルーとグリーン）を用意し、新しいバージョンのリリース時にはトラフィックを新環境に切り替える方法を指します。ロールバックが必要な場合でも、単純にトラフィックを古い環境に切り替えれば良いため、デプロイやロールバックが容易になります。この設問では、このようなロールバックを実現するために、Kubernetesのサービスを用いて以前のDeploymentにトラフィックを切り替える方法が適しています。KubernetesのServiceは、一貫したアクセスポイントを提供して、特定のDeploymentにトラフィックをルーティングします。
したがって、Serviceを更新して以前のDeploymentを指せば、ブルー/グリーンデプロイのロールバックを容易に実装できます。これが、この選択肢が正解の理由です。
不正解についての説明：
選択肢：kubectl rollout undoコマンドを実行します
この選択肢が正しくない理由は以下の通りです。
ブルー/グリーンデプロイにおいては、本番環境をロールバックするためにkubectl rollout undoコマンドを使用するのではなく、代わりに以前のデプロイメントを指すようにKubernetes Serviceを更新することで、スムーズに以前のバージョンに戻すことができます。
これに対し、kubectl rollout undoはローリングアップデートのロールバックに使用されます。
選択肢：新しいコンテナイメージを削除し、実行中のポッドを削除します
この選択肢が正しくない理由は以下の通りです。
新しいコンテナイメージを削除し、実行中のポッドを削除することはロールバックではありません。
また、これではブルー/グリーンデプロイの利点を享受できません。
対照的に、以前のKubernetes Deploymentを指すようにKubernetes Serviceを更新することでスムーズなロールバックを可能にします。
選択肢：新しいKubernetes Deploymentをゼロにスケールします
この選択肢が正しくない理由は以下の通りです。
新しいDeploymentをゼロにスケールすると、新しいバージョンのアプリケーションへのアクセスが中断されますが、以前のバージョンへ戻すロールバックアクションは実現できません。
一方、Serviceを更新して以前のDeploymentを指すようにすると、代替手段なしに以前のバージョンへ戻すことが可能となります。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/deploying-applications
https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-multi-ssl
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rollback-to-an-earlier-deployment
<details><div>

### Q. 問題36: 未回答
Google Cloud Deployのデプロイメントパイプラインを見直しています。パイプラインの手間を減らし、エンドツーエンドのデプロイを完了するのにかかる時間を最小限に抑えたいと考えています。
この要件を満たすために、どうすればよいですか？（2つ選択）
1. スクリプトを使用して、Google Cloud Deployでデプロイパイプラインの作成を自動化します
2. 手動介入が必要な場合、必要なチームに次のステップを完了するよう通知するトリガーを作成します
3. 自動化のステップを小さなタスクに分割します
4. さらにエンジニアを追加して、手動ステップを完了させます
5. 開発環境からテスト環境への昇格承認を自動化
<details><div>
    答え：3,5
説明
この問題では、Google Cloud Deployのデプロイメントパイプラインの効率化と時間短縮を目指しています。正しい選択肢を導き出すためには、デプロイメントパイプラインの自動化についての理解が必要であり、小さなタスクに分割することや自動承認を設定することがどのように結果に影響するかを検討する必要があります。また、手動介入が必要なチームへの通知、デプロイパイプラインの自動作成、エンジニアの追加といった選択肢が本当にパイプラインの効率化と時間短縮に寄与するのかを検討することも必要です。
基本的な概念や原則：
Google Cloud Deploy：Google Cloudの自動デプロイメントサービスです。ソースコードからプロダクションまでの安全なデプロイを可能にします。
デプロイメントパイプライン：ソフトウェアのリリースを自動化する一連のステップのことです。コードのアップデートを検証し、特定の環境でリリースを行うプロセスです。
自動化：手動作業を機械やソフトウェアが自動的に行うことです。作業効率の向上や経験の再現性の確保に貢献します。
タスクの分割：大きな仕事をより小さく、管理しやすい単位に分割することです。これによりトラブルシューティングが容易になり、並行してタスクを実行することで効率が向上します。
昇格承認：開発環境からテスト環境、あるいはテスト環境からプロダクション環境への昇格（リリース）を許可するプロセスです。この承認プロセスを自動化することで、デプロイメントパイプラインの速度を向上させることが可能です。
正解についての説明：
（選択肢）
・自動化のステップを小さなタスクに分割します
・開発環境からテスト環境への昇格承認を自動化
この選択肢が正解の理由は以下の通りです。
まず、パイプラインの自動化ステップを小さなタスクに分割することが有効な理由は、エラーが発生した際にその原因を特定しやすくするためです。小さなタスクであればそれぞれが完結しているので、何が問題であったのかを見つけやすくなります。
また、各タスクが短時間で完了するため、全体のデプロイ時間を最小限に抑えることも可能になります。
また、開発環境からテスト環境への昇格承認を自動化する理由は、手動の承認プロセスによる遅延を排除するためです。自動化により、人間が関与することなく昇格プロセスが行われ、デプロイ時間の最小化に寄与します。自動化はまた、人間によるエラーも減らし、一貫性と信頼性を高める利点があります。これらの手法により、パイプラインの手間を減らし、デプロイ時間を最小限に抑えることが可能です。
不正解についての説明：
選択肢：手動介入が必要な場合、必要なチームに次のステップを完了するよう通知するトリガーを作成します
この選択肢が正しくない理由は以下の通りです。
手動介入が必要なトリガーを作成すると、パイプラインの遅延が生じる可能性があります。
一方、素早いデプロイが目指されているため、ステップを小さなタスクに分割し、承認プロセスを自動化することで、手作業を減らし効率性を上げることが可能です。
選択肢：スクリプトを使用して、Google Cloud Deployでデプロイパイプラインの作成を自動化します
この選択肢が正しくない理由は以下の通りです。
スクリプトを使用してデプロイパイプラインの作成を自動化することは、パイプライン自体のセットアップを容易にしますが、パイプラインの手間を減らしデプロイの時間を最小限にするという要件には寄与しません。
それに対して、自動化のステップを小さなタスクに分割したり、昇格承認を自動化することは、デプロイメントの効率性とスピードを改善できる策となります。
選択肢：さらにエンジニアを追加して、手動ステップを完了させます
この選択肢が正しくない理由は以下の通りです。
エンジニアを追加して手動ステップを完了させるのはパイプラインの手間を増やし、エンドツーエンドのデプロイ時間を逆に延長する可能性があります。
また、これは自動化を目指す原則に反しています。小タスクへの分割や昇格承認の自動化は、タスク管理を改善しデプロイ時間を最小限に抑える効果が期待できます。
参考リンク：
https://cloud.google.com/deploy/docs/creating-managing-pipelines
https://cloud.google.com/deploy/docs/automating-promotions
https://cloud.google.com/solutions/devops/devops-tech-automation
<details><div>

### Q. 問題37: 未回答
現在、プロダクトは3つのGoogle Cloudゾーンにデプロイされており、ユーザーはゾーン間で分割されています。あるゾーンから別のゾーンにフェールオーバーすることはできますが、影響を受けるユーザーに対して10分間のサービス中断が発生します。通常、データベース障害は四半期に1回発生しますが、これは5分以内に検出できます。あなたは、製品の新しいリアルタイムチャット機能の信頼性リスクをカタログ化しています。リスクごとに次の情報をカタログ化します。
- 分単位の平均検出時間（MTTD）
- 分単位の平均修復時間（MTTR）
- 平均故障間隔（MTBF）日単位
- ユーザーへの影響の割合
チャット機能には、ゾーン間で正常にフェールオーバーするのに2倍の時間がかかる新しいデータベースシステムが必要です。1つのゾーンで新しいデータベースに障害が発生するリスクを考慮したいと考えています。
新しいシステムによるデータベースのフェイルオーバーのリスクはどのような値になりますか？
1. MTTD：5、MTTR：20、MTBF：90、インパクト：33%
2. MTTD：5、MTTR：10、MTBF：90、インパクト：50%
3. MTTD：5、MTTR：10、MTBF：90、インパクト：33%
4. MTTD：5、MTTR：20、MTBF：90、インパクト：50%
<details><div>
    答え：1
説明
この問題では、新しいデータベースシステムのフェイルオーバーのリスクを評価するための各種パラメータの値が求められています。問題文の中で、通常のデータベースの障害の検出時間、修復時間、故障間隔、ユーザーへの影響割合について情報が提供されています。また新しいシステムでは、フェールオーバーするのに普段の2倍の時間がかかるという情報があります。これらの情報を用いて、新しいシステムによるデータベースのフェイルオーバーのリスクを評価します。プロダクトが3つのゾーンにデプロイされていることも、フェイルオーバー時のユーザーへの影響を考える際に重要です。
基本的な概念や原則：
フェールオーバー：障害発生時に、自動的または手動で別のシステムまたはデバイスにサービスを移行する仕組みです。これにより、サービスの中断時間を最小限に抑えることができます。
Google Cloudゾーン：Google Cloudのインフラストラクチャが配訳される地理的エリアのことです。同一リージョン内のゾーン間では高速なネットワーク接続が提供されています。
MTTD（Mean Time To Detect）：平均故障検出時間とも呼ばれ、故障が発生してからそれが検出されるまでの時間を指します。この値が小さいほど、問題を早く検知し対処できます。
MTTR（Mean Time To Repair）：平均修復時間とも呼ばれ、故障が検出されてからそれが解決するまでの時間を指します。この値が小さいほど、サービスのダウンタイムを最小限に抑えることができます。
MTBF（Mean Time Between Failures）：平均故障間隔とも呼ばれ、2回の故障発生までの平均時間を指します。この値が大きいほど、システムの信頼性は高いと言えます。
ユーザーへの影響：あるリスクが具現化した場合に、ユーザー体験にどれほどの影響を及ぼす可能性があるかをパーセンテージで示します。
リアルタイムチャット機能：ユーザー間で即時にメッセージをやりとりできる機能のことです。これには高度なパフォーマンスと信頼性が求められます。
正解についての説明：
（選択肢）
・MTTD：5、MTTR：20、MTBF：90、インパクト：33%
この選択肢が正解の理由は以下の通りです。
まず、平均検出時間（MTTD）はデータベースの問題がどれだけ早く検出されるかを示すため、システムが変わった場合でもこれ自体は変わらず、そのまま5分となります。
次に、平均修復時間（MTTR）は問題の解決にどれだけ時間がかかるかを示します。新しいデータベースシステムではフェイルオーバーに2倍の時間がかかるとのことなので、以前の10分から20分に増加します。平均故障間隔（MTBF）は、データベースの故障の発生間隔を意味します。これに関しては、問題の発生頻度が四半期に一度であるとされ、そのまま90日とします。
最後に、ユーザーへの影響の割合はゾーンが3つあり、それが均等に分散されているため、一つのゾーンで問題が発生した場合、全体のユーザーの33％が影響を受けます。以上より、正解はMTTD：5、MTTR：20、MTBF：90、インパクト：33%となります。
不正解についての説明：
選択肢：MTTD：5、MTTR：10、MTBF：90、インパクト：33%
この選択肢が正しくない理由は以下の通りです。
問題文において新しいデータベースシステムではフェールオーバーに2倍の時間がかかると書かれています。したがってフェールオーバーに必要な時間（MTTR: Mean Time To Recover）は元々10分だったものが、新しいシステムでは20分となります。そのためMTTRが10分となっている選択肢は不正解となります。
選択肢：MTTD：5、MTTR：10、MTBF：90、インパクト：50%
この選択肢が正しくない理由は以下の通りです。
新しいシステムではフェールオーバーに2倍の時間が必要なため、平均修復時間（MTTR）が10分ではなく20分になります。
また、ユーザーへの影響の割合が不正確です。影響を受けるのは全体の1/3（33%）であり、50%ではありません。
選択肢：MTTD：5、MTTR：20、MTBF：90、インパクト：50%
この選択肢が正しくない理由は以下の通りです。
フェイルオーバーが新しいシステムでは2倍の時間（20分）かかるとしても、ユーザーの影響は均等に分散されているゾーン間では33%であり、全体のユーザーの50%が影響を受けるわけではないため、インパクトは50%ではなく33%となるべきです。
参考リンク：
https://cloud.google.com/architecture/framework/reliability
https://cloud.google.com/solutions/disaster-recovery-cookbook
https://en.wikipedia.org/wiki/Disaster_recovery#Testing
<details><div>

### Q. 問題38: 未回答
あなたは、企業のデータサービスや製品の管理を担当するサイト信頼性エンジニアです。あなたは、予測不可能なデータ量や高コストといった運用上の課題を、自社のデータ取り込みプロセスで定期的に解決しています。あなたは最近、新しいデータ取り込み製品がGoogle Cloudで開発されることを知りました。あなたは、製品開発チームと協力して、新製品に関する運用上の意見を提供する必要があります。
この要件を満たすために、どうすればよいですか？
1. プロトタイプ製品をテスト環境に配備し、負荷テストを実施し、その結果を製品開発チームと共有します
2. 初期製品バージョンが品質保証フェーズとコンプライアンス評価に合格したら、製品をステージング環境にデプロイします。エラーログとパフォーマンス指標を製品開発チームと共有します
3. 新製品が少なくとも1社の社内顧客によって本番で使用されるようになったら、エラーログとモニタリング指標を製品開発チームと共有します
4. 製品開発チームと製品の設計を検討し、設計段階の早い段階でフィードバックを提供します
<details><div>
    答え：4
説明
この問題では、サイト信頼性エンジニアとしてのロールにおける新製品開発への最適な関与方法が求められています。新しいデータ取り込み製品がGoogle Cloud上で開発される際、運用上の観点からどのタイミングで意見を提供するべきかが問われています。フィードバックの提供が製品開発のどのフェーズで最も価値をもたらすのか、そしてコストや効率上のメリットが最大化されるのかを考慮すると、設計、プロトタイピング、テスト段階、本番環境での使用という、製品開発の各フェーズに関する理解が必要になります。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：システムやサービスの信頼性、スケーラビリティ、効率性を最大化するためのエンジニアリング原則とプラクティスです。システムの運用を担当し、開発チームと連携して製品の安定性と性能を高めます。
設計フィードバックの提供: 製品開発の初期段階でフィードバックを提供することにより、運用上の問題を早期に検出して解消することができます。これにより、製品の信頼性やパフォーマンスが向上する可能性があります。
プロトタイピングと負荷テスト：製品のプロトタイプを作成し、負荷テストを行うことで、製品のパフォーマンスを事前に評価することができます。ただし、これは製品開発が進行した段階で行うべきことであり、初期段階で行うべきではありません。
ステージング環境でのデプロイ：実際の利用環境に近い条件で製品をテストするために使用されます。ただし、ステージング環境でのデプロイは、製品が一定の完成度に達した後に行うべきであり、設計段階の運用フィードバックとは異なります。
本番環境でのログと指標の共有：製品が本番環境で使用されている場合、エラーログやモニタリング指標を共有することで、製品のパフォーマンスや利用状況を把握することができます。ただし、これは製品が実際に利用されている状況を反映しており、早期のフィードバックではありません。
正解についての説明：
（選択肢）
・製品開発チームと製品の設計を検討し、設計段階の早い段階でフィードバックを提供します
この選択肢が正解の理由は以下の通りです。
まず、運用上の課題を解決するためには、製品開発チームと密接に連携することが重要です。これにより、設計段階での調整によって、後から運用上の問題が起こる可能性を低減することが可能です。製品開発が進行する前にフィードバックを提供することで、早い段階でシステムの設計を改善し、運用の難しさを軽減することができます。
また、Google Cloudは様々なデータ量やコストを効率的に管理するための豊富な機能を提供しています。しかし、これらの機能を最大限に活用するために、設計段階で適切なアーキテクチャを選択することが不可欠です。そのため、設計段階でのフィードバックは、製品の運用上の効率を大いに向上させることができます。
最後に、サイト信頼性エンジニアとしては、製品開発チームとのコミュニケーションとフィードバックの提供を通じて、予測不可能なデータ量や高コストといった運用上の課題を解決するために必要な機能や仕様を製品に組み込むことが重要です。
不正解についての説明：
選択肢：プロトタイプ製品をテスト環境に配備し、負荷テストを実施し、その結果を製品開発チームと共有します
この選択肢が正しくない理由は以下の通りです。
プロトタイプ製品のテストと負荷テスト実施は重要ですが、これによって運用上の課題が起こる可能性を早期にキャッチすることは難しいです。運用上の課題は設計段階でのフィードバックにより防ぐべきであり、それが正解選択肢となっています。
選択肢：初期製品バージョンが品質保証フェーズとコンプライアンス評価に合格したら、製品をステージング環境にデプロイします。エラーログとパフォーマンス指標を製品開発チームと共有します
この選択肢が正しくない理由は以下の通りです。
問題の要件は運用上の意見を提供することであるため、製品が品質保証フェーズとコンプライアンス評価に合格した後に初めてフィードバックを与えるのでは遅すぎます。
正解の選択肢は、設計段階の早い段階でフィードバックを提供することを提案しており、これが問題の要件をより正確に満たしています。
選択肢：新製品が少なくとも1社の社内顧客によって本番で使用されるようになったら、エラーログとモニタリング指標を製品開発チームと共有します
この選択肢が正しくない理由は以下の通りです。
新製品が本番環境で使用されるまでフィードバックを待つと、問題が発生した際の修正コストが高まる可能性があります。設計初期段階でのフィードバックは運用上の課題を防ぐ上で重要です。
参考リンク：
https://cloud.google.com/architecture/devops/devops-measurement-culture
https://cloud.google.com/solutions/devops/devops-process-designing-testable-deployments
https://cloud.google.com/solutions/devops/devops-process-adapting-change-organization
<details><div>

### Q. 問題39: 未回答
Google Cloudでコンテナ化を始めています。コンテナイメージとHelmチャート用に完全に管理されたストレージソリューションが必要です。Google Kubernetes Engine（GKE）、Cloud Run、VPC Service Controls、IAM（Identity and Access Management）など、既存のGoogle Cloudサービスにネイティブに統合できるストレージソリューションを特定する必要があります。
どうするべきですか？
1. Artifact Registryをコンテナイメージ用のOCIベースのArtifact Registryとして設定します
2. オープンソースのArtifact Registryサーバを構成し、制限付きロールベースアクセス制御（RBAC）構成でGKEで実行します
3. Dockerを使って、あなたの組織が所有するバケットを指すCloud Storageドライバを設定します
4. Helmチャートとコンテナイメージの両方に対して、OCIベースのArtifact RegistryとしてArtifact Registryを構成します
<details><div>
    答え：4
説明
この問題では、Google Cloudでのコンテナ化を始める際の、管理されたストレージソリューションの選択が問われています。このストレージソリューションは、コンテナイメージとHelmチャートの両方を扱うことができ、Google Kubernetes Engine（GKE）、Cloud Run、VPC Service Controls、IAM（Identity and Access Management）などの既存のGoogle Cloudサービスに統合できる必要があります。問題の文脈やキーワードから、選択肢が示す特定のサービスや構成の適合性や利点を評価しなければなりません。この場合、それらはArtifact Registry、Cloud Storageドライバ、制限付きロールベースアクセス制御（RBAC）を含むオープンソースのArtifact Registryサーバなどです。
基本的な概念や原則：
Artifact Registry：Google Cloudの統合アーティファクト管理サービスです。Dockerイメージや他の種類のアーティファクトを安全に保存し、共有することができます。
Helmチャート：Kubernetesアプリケーションのパッケージ管理ツールであるHelmのパッケージ（アプリケーション）のことです。これにより、Kubernetesアプリケーションのデプロイとバージョン管理が容易になります。
Google Kubernetes Engine（GKE）：Google Cloudが提供する管理型Kubernetesサービスです。Kubernetes環境のセットアップや運用が容易になります。
Cloud Run：Google Cloudが提供する完全管理型のサーバレスプラットフォームです。コンテナイメージをすばやくデプロイし、自動スケーリングを実現します。
VPC Service Controls：Google Cloudのサービスで、ネットワークセキュリティパーコメータを設定してGoogle Cloudリソースへのデータアクセスを制御します。
IAM（Identity and Access Management）：Google Cloudのセキュリティ管理サービスです。特定のユーザーやサービスアカウントに対するリソースへのアクセス権を管理します。
OCI（Open Container Initiative）：コンテナの仕様と技術の標準化を推進するオープンソースプロジェクトです。
正解についての説明：
（選択肢）
・Helmチャートとコンテナイメージの両方に対して、OCIベースのArtifact RegistryとしてArtifact Registryを構成します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloudは独自のマネージド型Artifact Registryサービスを提供しており、これはコンテナイメージとHelmチャートを取り扱うのに対応しています。Artifact Registryは演算に用いるコンテナイメージとその他のアーティファクトを公開、管理、操作するためのフルマネージド型プライベートパッケージリポジトリサービスです。
次に、Artifact RegistryはOCI（Open Container Initiative）基準に準拠したレジストリであるため、すべてのモダンなコンテナランタイムと、Helm（Kubernetesのパッケージマネージャー）と互換性があります。
また、Google Kubernetes Engine（GKE）、Cloud RunなどGoogle Cloudの既存サービスと統合されており、管理が容易で便利です。
さらに、VPC Service ControlsやIAMなどのGoogle Cloud特有のセキュリティツールと統合が可能であるため、セキュリティの維持及び管理を簡単に行うことができます。これらの要素から、Artifact Registryを設定すれば、要求されたニーズや要件を全て満たすことができます。
不正解についての説明：
選択肢：Dockerを使って、あなたの組織が所有するバケットを指すCloud Storageドライバを設定します
この選択肢が正しくない理由は以下の通りです。
Dockerを使ったCloud Storageドライバ設定は管理が難しく、用途にも限りがあります。
また、ネイティブの統合が求められていることからも、Google CloudのネイティブサービスであるArtifact Registryを用いる方が適切です。そのため、Artifact Registryの使用が望ましい選択となります。
選択肢：オープンソースのArtifact Registryサーバを構成し、制限付きロールベースアクセス制御（RBAC）構成でGKEで実行します
この選択肢が正しくない理由は以下の通りです。
オープンソースのArtifact Registryサーバの設定と運用は手間とコストがかかり、完全に管理されたストレージソリューションとは言えません。
また、Google Cloudの他のサービスとのネイティブな統合も困難です。
一方、OCIベースのArtifact Registryは、これらの要件を満たすための適切な選択です。
選択肢：Artifact Registryをコンテナイメージ用のOCIベースのArtifact Registryとして設定します
この選択肢が正しくない理由は以下の通りです。
問題の要求はコンテナイメージとHelmチャートの両方に対する完全に管理されたストレージソリューションであり、この選択肢はコンテナイメージのみをカバーしています。
したがって、この選択肢は要求を完全に満たしていません。
参考リンク：
https://cloud.google.com/artifact-registry/docs
https://cloud.google.com/kubernetes-engine/docs
https://cloud.google.com/container-registry/docs
<details><div>

### Q. 問題40: 未回答
あなたの会社では、Google Kubernetes Engine（GKE）上にデプロイされるアプリケーションを開発しています。各チームは異なるアプリケーションを管理しています。あなたは、コストを最小限に抑えながら、各チームの開発環境と本番環境を作成する必要があります。異なるチームが他のチームの環境にアクセスできないようにする必要があります。
この要件を満たすために、どうすればよいですか？
1. 開発用と本番用のGKEクラスターを別々のプロジェクトに作成します。それぞれのクラスターで、チームごとにKubernetesのネームスペースを作成し、Identity Aware Proxyを設定して、各チームが自分のネームスペースにのみアクセスできるようにします
2. チームごとにGoogle Cloudプロジェクトを1つ作成します。それぞれのプロジェクトに、Development用のKubernetesネームスペースとProduction用のネームスペースを持つクラスターを作成します。チームにそれぞれのクラスターへのIAMアクセス権を付与します
3. 別々のプロジェクトにDevelopmentとProductionのGKEクラスターを作成します。それぞれのクラスターで、チームごとにKubernetesのネームスペースを作成し、Kubernetesのロールベースのアクセス制御（RBAC）を設定して、各チームが自分のネームスペースにのみアクセスできるようにします
4. チームごとにGoogle Cloudプロジェクトを1つ作成します。それぞれのプロジェクトに、開発用と本番用のクラスターを作成します。チームにそれぞれのクラスターへのIAMアクセス権を付与します
<details><div>
    答え：3
説明
この問題では、複数の開発チームが独立にアプリケーションの開発とデプロイを行うGKE環境をコスト効率よく、かつ各チームが互いの環境にアクセスできないように構築する方法を尋ねています。問題文からすると、各チームごとに独立した名前空間を持つ形にすべきですが、各チームが他の名前空間にアクセスできないようにするため、アクセス制御が重要になります。また、コストを最小限に抑えるためには、クラスターの配置と管理ができるように、適切なGoogle Cloudプロジェクトの構造を検討しなければなりません。これらの要素を考慮に入れて、適切なアクションを選択します。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloudが提供するマネージドKubernetesサービスです。コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を容易にします。
プロジェクト：Google Cloud内でリソースを一元管理する単位です。コストトラッキングやアクセス制御などを個別に設定できます。
Kubernetesのネームスペース：Kubernetes内のリソースを論理的に分割するための単位です。独立した開発環境や本番環境を作成する際に使用します。
ロールベースのアクセス制御（RBAC）：ユーザーのロールに基づき、アクセス権を付与・管理する方式です。KubernetesではネームスペースごとにRBACを設定できます。
IAM（Identity and Access Management）：Google Cloudのアクセス制御サービスです。Google Cloudのリソースへのアクセス権を管理します。
Identity-Aware Proxy：Google Cloudのサービスで、アプリケーションへのアクセスを認証・認可して提供します。VPNなどを必要とせずに、安全にアプリケーションへアクセスできます。
正解についての説明：
（選択肢）
・別々のプロジェクトにDevelopmentとProductionのGKEクラスターを作成します。それぞれのクラスターで、チームごとにKubernetesのネームスペースを作成し、Kubernetesのロールベースのアクセス制御（RBAC）を設定して、各チームが自分のネームスペースにのみアクセスできるようにします
この選択肢が正解の理由は以下の通りです。
まず、別々のプロジェクトに開発（Development）と本番（Production）のGKEクラスターを作成することにより、各環境が互いに影響し合うことなく独立して動作可能になります。これにより、開発環境でのエラーや問題が本番環境に影響を及ぼすリスクを軽減できます。
次に、各クラスター内でチームごとにKubernetesのネームスペースを作成することにより、各チームは自身の作業環境を管理でき、他のチームの作業に影響を及ぼさずに済みます。ネームスペースはKubernetesのリソースを論理的に分離するための方法であり、1つのクラスター内で複数のワークロードを効率的に管理できるようになります。
最後に、Kubernetesのロールベースのアクセス制御（RBAC）を設定することで、各チームが自分のネームスペースにのみアクセスできるようにし、他のチームの環境へのアクセスを制限することが可能になります。これにより、セキュリティを確保しつつ、チームごとの作業の独立性を保つことができます。以上より、この選択肢は要件を満たす最適な解答です。
不正解についての説明：
選択肢：チームごとにGoogle Cloudプロジェクトを1つ作成します。それぞれのプロジェクトに、開発用と本番用のクラスターを作成します。チームにそれぞれのクラスターへのIAMアクセス権を付与します
この選択肢が正しくない理由は以下の通りです。
チームごとにGoogle Cloudプロジェクトを作成すると、管理が複雑になるだけではなく、各プロジェクトでクラスターを作成すると、コストも大幅に増加します。正解の選択肢である一つのプロジェクト内でネームスペースとRBACを使用する方が、コストを最小限に抑えつつセキュリティを保つためのより効率的な方法です。
選択肢：チームごとにGoogle Cloudプロジェクトを1つ作成します。それぞれのプロジェクトに、Development用のKubernetesネームスペースとProduction用のネームスペースを持つクラスターを作成します。チームにそれぞれのクラスターへのIAMアクセス権を付与します
この選択肢が正しくない理由は以下の通りです。
チームごとに別のプロジェクトとクラスターを作成すると、クラスターの管理とメンテナンスの負担が増え、結果的にコストも増大します。
また、1チームが1プロジェクトとクラスターを持つ状況は、リソースを効率的に共有することが難しく、コスト管理の最適化には適していません。
選択肢：開発用と本番用のGKEクラスターを別々のプロジェクトに作成します。それぞれのクラスターで、チームごとにKubernetesのネームスペースを作成し、Identity Aware Proxyを設定して、各チームが自分のネームスペースにのみアクセスできるようにします
この選択肢が正しくない理由は以下の通りです。
Identity-Aware Proxy（IAP）は、インターネット上からのアクセスを制限するサービスであり、本問の要件である"チーム間のアクセス制限"には直接対応していません。
一方、正解の選択肢に挙げられているKubernetesのロールベースのアクセス制御（RBAC）は、Kubernetesのリソースへのアクセスを細かく制御できるため、要件を満たす最適な解答となります。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture
https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
https://kubernetes.io/docs/reference/access-authn-authz/rbac/
<details><div>

### Q. 問題41: 未回答
コンテナ化したアプリケーションの新バージョンのテストが完了し、Google Kubernetes Engine（GKE）上で本番環境にデプロイする準備が整いました。プレプロダクション環境では新バージョンの負荷テストを十分に行うことができなかったため、デプロイ後にアプリケーションのパフォーマンスに問題がないことを確認する必要があります。デプロイは自動化する必要があります。
この要件を満たすために、どうすればよいですか？
1. kubectlを使用してアプリケーションをデプロイし、Config Connectorを使用してバージョン間のトラフィックを徐々に増加させます。Cloud Monitoringを使ってパフォーマンスの問題を探します
2. カナリアデプロイメントを使用して、継続的デリバリーパイプラインを通じてアプリケーションをデプロイします。Cloud Monitoringを使ってパフォーマンスの問題を探し、メトリクスによってサポートされるようにトラフィックを増加させます
3. ブルー/グリーンデプロイメントを使用して、継続的デリバリーパイプラインを通じてアプリケーションをデプロイします。トラフィックを新しいバージョンのアプリケーションに移行し、Cloud Monitoringを使ってパフォーマンスの問題を調べます
4. kubectlを使用してアプリケーションをデプロイし、spec.updateStrategy.typeフィールドをRollingUpdateに設定します。Cloud Monitoringを使用してパフォーマンスの問題を調べ、問題があればkubectl rollbackコマンドを実行します
<details><div>
    答え：2
説明
この問題では、新しいバージョンのアプリケーションを本番環境にデプロイするときにパフォーマンス問題をどのようにチェックすべきかが問われています。その際、デプロイは自動化すべきとの要件があります。要求事項は、デプロイ後のパフォーマンス問題のチェックとデプロイの自動化です。したがって、Google Kubernetes Engine（GKE）上でのデプロイ手法と、パフォーマンス問題を追跡し、必要に応じて対応するためのツールまたは手法について慎重に考える必要があります。それぞれの選択肢がこのソリューションの需要にどのように合致するかを理解することで、最適な選択を行うことができます。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google CloudのマネージドKubernetesサービスです。コンテナ化されたアプリケーションを簡単にデプロイ、スケール、更新するための機能を提供します。
カナリアデプロイメント：新バージョンのアプリケーションを一部のユーザーにだけ提供してパフォーマンスをテストする方法です。問題が確認された場合には迅速に差し戻すことができます。
Cloud Monitoring：Google Cloudの監視サービスです。アプリケーションとインフラストラクチャのパフォーマンスを監視し、問題を迅速に特定して対応することができます。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）を組み合わせた開発プロセスです。コードの変更を自動的にビルド、テスト、デプロイすることで、高速かつ安定したソフトウェアリリースを実現します。
ブルー/グリーンデプロイメント：新旧のアプリケーションバージョンを同時に稼働し、トラフィックを切り替える方法です。問題が発生した場合には、すぐに古いバージョンに戻すことができますが、リソースを二重に必要とします。
kubectl：Kubernetesクラスターを制御するためのコマンドラインツールです。アプリケーションのデプロイやスケーリングなどの操作を行うことができます。
Config Connector：Google CloudのリソースをKubernetesのオブジェクトとして管理するための機能です。これにより、KubernetesのAPIとツールを使用してCloudリソースを制御できます。
正解についての説明：
（選択肢）
・カナリアデプロイメントを使用して、継続的デリバリーパイプラインを通じてアプリケーションをデプロイします。Cloud Monitoringを使ってパフォーマンスの問題を探し、メトリクスによってサポートされるようにトラフィックを増加させます
この選択肢が正解の理由は以下の通りです。
まず、カナリアデプロイメントでは新バージョンのアプリケーションを一部のユーザーにのみ提供し、その動作を観察します。これにより、本番環境でのパフォーマンスに問題がないことを確認することができます。万が一パフォーマンスに問題が発生した場合でも、影響を受けるユーザーの数を最小限に抑えることが可能です。
また、継続的デリバリーパイプラインを使用すると、新バージョンのアプリケーションのデプロイを自動化することができます。これにより、デプロイの効率性と信頼性が向上します。
さらに、Google Cloud Monitoringを使用すると、アプリケーションのパフォーマンスを監視し、問題を検出することができます。これにより、予想外のパフォーマンスの変動に対応することが可能となります。
以上の理由からこの選択肢が適切な解答となります。
不正解についての説明：
選択肢：ブルー/グリーンデプロイメントを使用して、継続的デリバリーパイプラインを通じてアプリケーションをデプロイします。トラフィックを新しいバージョンのアプリケーションに移行し、Cloud Monitoringを使ってパフォーマンスの問題を調べます
この選択肢が正しくない理由は以下の通りです。
ブルー/グリーンデプロイメントは新旧のシステムを全面的に切り替える方法で、これは新バージョンのアプリケーションに何らかの問題が発生した場合、即座に全システムに影響を及ぼします。
一方、カナリアデプロイメントはリスクを最小化する戦略で、新バージョンへの移行を段階的に行い、問題発生時の影響を限定することができます。
選択肢：kubectlを使用してアプリケーションをデプロイし、Config Connectorを使用してバージョン間のトラフィックを徐々に増加させます。Cloud Monitoringを使ってパフォーマンスの問題を探します
この選択肢が正しくない理由は以下の通りです。
kubectlはアプリケーションのデプロイに使用することはできますが、Config Connectorを使用してバージョン間のトラフィックを徐々に増加させるというのは適切ではありません。Config Connectorはリソースの管理に使用するものでトラフィック管理には向いていません。
また、デプロイ自体を自動化してパフォーマンス監視をするためには、カナリアデプロイメントが効果的です。
選択肢：kubectlを使用してアプリケーションをデプロイし、spec.updateStrategy.typeフィールドをRollingUpdateに設定します。Cloud Monitoringを使用してパフォーマンスの問題を調べ、問題があればkubectl rollbackコマンドを実行します
この選択肢が正しくない理由は以下の通りです。
問題の要件ではデプロイ後のパフォーマンス問題を適切に管理する必要があると述べていますが、'kubectl'と'RollingUpdate'戦略を使用したデプロイメントでは全ユーザーに対して新バージョンを即座に展開するためパフォーマンスに問題があるとすぐに影響を及ぼします。カナリアデプロイメントならば一部のユーザーで新バージョンをテストすることが可能です。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment-strategies#canary
https://cloud.google.com/stackdriver/docs/solutions/gke
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment
<details><div>

### Q. 問題42: 未回答
あなたの会社では、https://example-cloudjp.a.run.app URLでアクセス可能なパブリックCloud RunホスティングサービスをトリガーするためにHTTPSリクエストを使用しています。サービスが顧客に公開される前に、開発者がサービスの最新リビジョンをテストできるようにする必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. gcloud run services update-traffic booking-engine --to-revisions LATEST=1コマンドを実行します。テストにはhttps://example-cloudjp.a.run.app URLを使用します
2. booking-engineサービスをテストする開発者にroles/run.invokerロールを付与します。テストにはhttps://example-cloudjp.private.run.app URLを使用します
3. gcloud run deploy booking-engine --no-traffic --tag devコマンドを実行します。テストにはhttps://dev-example-cloudjp.a.run.app URLを使用します
4. curl --H "Authorization:Bearer $（gcloud auth print-identity-token）"認証トークンを渡します。非公開でテストするために、https://example-cloudjp.a.run.app URLを使用します
<details><div>
    答え：3
説明
この問題では、開発者がサービスの最新リビジョンをテストできるように設定する方法について問われています。テストに用いるURLや権限管理が鍵となる要素であり、公開される前のテスト環境をどのようにセットアップするかが重要です。正解選択肢と不正解選択肢から、サービスのリビジョンの管理、認証トークンの利用、URLの構成、開発者に付与するロールなどに注意を払いながら、Google Cloudのコマンドや権限設定の適切な使用方法を考えることが求められます。
基本的な概念や原則：
Cloud Run：コンテナ化されたアプリケーションをフルマネージドで実行するサービスです。サーバーレス環境での実行を可能にし、自動スケーリング、およびHTTPS経由での接続などが提供されます。
gcloudコマンドラインツール：Google Cloudのリソースとアプリケーションを管理するためのツールです。コマンドラインから操作可能で、開発、デプロイ、実行の作業を効率化します。
リビジョン：Cloud Runでのサービス更新のそれぞれが新しいリビジョンを生成します。それぞれのリビジョンは永続的で不変であり、サービスの特定のバージョンを指します。
サービストラフィック：Cloud Runではトラフィック分割が可能で、特定のリビジョンにトラフィックを振り分けることができます。これにより、パフォーマンスのテストや新機能の段階的なロールアウトが可能になります。
認証トークン：安全な認証を行うために使用されます。認証トークンを使用することで、ユーザーやサービスが自身を認証する情報を保持することができます。
roles/run.invokerロール：Cloud Runサービスに対する特定のアクセス権限を提供するIAMロールです。このロールを付与することで、ユーザーやサービスがCloud Runサービスを呼び出すことが許可されます。
URLの制御：Cloud Runサービスに対しては、ダイレクトURLでアクセス可能です。またURLは、特定のリビジョンやサービスタグにより生成することも可能です。これにより、テスト環境と本番環境を分離することができます。
正解についての説明：
（選択肢）
・gcloud run deploy booking-engine --no-traffic --tag devコマンドを実行します。テストにはhttps://dev-example-cloudjp.a.run.app URLを使用します
この選択肢が正解の理由は以下の通りです。
まず、コマンド`gcloud run deploy booking-engine --no-traffic --tag dev`は、新しいCloud Runサービスのリビジョンをデプロイしますが、このリビジョンにはトラフィックがルーティングされません。これにより、新リビジョンは公開されることなく、開発者だけがアクセスとテストが可能となります。
また、このコマンドの`--tag dev`オプションは、開発者がテスト時に新しいリビジョンにアクセスできるようにします。このタグはURLの一部として追加されます。つまり、URL`https://dev-example-cloudjp.a.run.app`では、タグ`dev`がついたリビジョンにアクセスすることになります。これにより、開発者はこのURLを使用して新しいリビジョンのテストを行うことができます。
したがって、この選択肢は要件を満たす適切な解決策を提供します。
不正解についての説明：
選択肢：gcloud run services update-traffic booking-engine --to-revisions LATEST=1コマンドを実行します。テストにはhttps://example-cloudjp.a.run.app URLを使用します
この選択肢が正しくない理由は以下の通りです。
不正解の選択肢が提供するリビジョン更新コマンドはすでに公開し、顧客にアクセス可能なURLに適用されます。だから、開発者のテスト中に顧客の影響を引き起こす可能性があります。
一方、正解の選択肢は新たにタグ付けされたURLを提供することで、開発者が最新リビジョンをテストし、顧客への影響を避けることができます。
選択肢：curl --H "Authorization:Bearer $（gcloud auth print-identity-token）"認証トークンを渡します。非公開でテストするために、https://example-cloudjp.a.run.app URLを使用します
この選択肢が正しくない理由は以下の通りです。
curlと認証トークンを使用した方法では、最新リビジョンを非公開でテストすることはできず、また全てのユーザーが最新リビジョンにアクセスできてしまいます。正しい選択肢では、専用のテストURLを使って非公開でテストが可能になります。
選択肢：booking-engineサービスをテストする開発者にroles/run.invokerロールを付与します。テストにはhttps://example-cloudjp.private.run.app URLを使用します
この選択肢が正しくない理由は以下の通りです。
この選択肢では、roles/run.invokerロールを開発者に付与しようとしていますが、これだけで新しいリビジョンのテストを可能にするわけではありません。実際に新しいリビジョンをデプロイし、トラフィックを送らないよう設定すること（正解の選択肢）が必要です。
参考リンク：
https://cloud.google.com/run/docs/deploying#deploying-tags
https://cloud.google.com/run/docs/managing/traffic
https://cloud.google.com/iam/docs/understanding-roles#predefined_roles
<details><div>

### Q. 問題43: 未回答
アプリケーションはGoogle Cloud上で動作します。アプリケーションのリリースをGoogle CloudにデプロイするためにJenkinsを導入する必要があります。リリースプロセスを合理化し、運用の手間を減らし、ユーザーデータを安全に保ちたいと考えています。
この要件を満たすために、どうすればよいですか？
1. オンプレミスのKubernetesにJenkinsを実装します
2. ローカルワークステーションにJenkinsを導入します
3. Compute Engine仮想マシンにJenkinsを実装します
4. Google Cloud FunctionsにJenkinsを実装します
<details><div>
    答え：3
説明
この問題では、アプリケーションのリリースプロセスを合理化し、運用の手間を減らし、ユーザーデータを安全に保つという要件を満たすためのJenkinsの導入場所を特定する必要があります。Google Cloudがアプリケーションの実行環境であり、Jenkinsを使ってリリースをデプロイするため、Jenkinsをどこに導入するのが最適かを選択肢から選ぶことが求められます。運用の手間を減らすという目的を達成するためには、導入場所がアプリケーションの環境に近いことが重要となります。
基本的な概念や原則：
Compute Engine：Google Cloudのインフラストラクチャーとしてサービス（IaaS）サービスの1つで、仮想マシン（VM）をホストするのに適しています。
Jenkins：オープンソースのCI/CDツールであり、アプリケーションのビルド、テスト、デプロイを自動化するのに使用されます。プラグインの豊富さと柔軟性が特徴です。
Google Cloud Functions：Google Cloudのサーバーレス実行環境で、個々の関数をイベント駆動で実行します。Jenkinsのような複雑なワークロードには向いていません。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）を組み合わせた開発プロセスです。コードの変更を自動的にビルド、テスト、デプロイすることで、高速かつ安定したソフトウェアリリースを支援します。
正解についての説明：
（選択肢）
・Compute Engine仮想マシンにJenkinsを実装します
この選択肢が正解の理由は以下の通りです。
まず、Google Compute Engineはスケーラブルで、高性能の仮想マシンを提供します。これにより、アプリケーションのリリースを効率良く管轄するために必要な柔軟性とパワーをJenkinsに提供します。
また、 Jenkins自体はContinuous Integration/Continuous Deployment（CI/CD）ツールであり、リリースプロセスを合理化し運用の手間を減らす上で効果的です。
さらに、Google Cloudのセキュリティ特性を活用することで、ユーザーデータの保護も実現します。例えば、Compute Engineはデフォルトでデータの暗号化を行っています。これにより、ユーザーデータの安全性を維持しながらJenkinsを活用できます。
不正解についての説明：
選択肢：ローカルワークステーションにJenkinsを導入します
この選択肢が正しくない理由は以下の通りです。
ローカルワークステーションにJenkinsを導入するとアプリケーションのリリース過程にローカル環境が介在し、運用の手間が増えてしまいます。
また、ユーザーデータのセキュリティ維持も難しくなります。
一方、Compute Engine上にJenkinsを実装すれば、クラウド上で統一された環境でリリースプロセスを管理でき、効率的に運用することが可能です。
選択肢：オンプレミスのKubernetesにJenkinsを実装します
この選択肢が正しくない理由は以下の通りです。
問題文ではアプリケーションのリリースをGoogle CloudにデプロイするためにJenkinsを導入したいとされていますが、オンプレミスのKubernetesにJenkinsを実装すると、JenkinsがGoogle Cloudとは別の環境で運用されることになります。これはGoogle Cloudへのリリースプロセスを合理化し、運用の手間を減らす目的とは合致しないため、この選択肢は不正解です。
それに対して、Compute Engine仮想マシンにJenkinsを実装すると、Jenkinsの運用環境もGoogle Cloud内になります。これによりGoogle Cloudへのデプロイが容易になり、運用の手間も減らせるため、正解選択肢となります。
選択肢：Google Cloud FunctionsにJenkinsを実装します
この選択肢が正しくない理由は以下の通りです。
Google Cloud FunctionsはFaaS（Function as a Service）であり、短時間のイベント駆動型のタスクに適しています。
一方、Jenkinsは長時間稼働し続けるCI/CDツールです。
従って、Cloud Functions上にJenkinsを実装することは適切ではありません。
それに対して、Compute EngineのVMは長時間の稼働や大規模なタスクに適しており、Jenkinsの実装に適してます。
参考リンク：
https://cloud.google.com/solutions/automating-builds-and-deployments-to-compute-engine-with-jenkins
https://cloud.google.com/architecture/using-jenkins-for-distributed-builds-on-compute-engine
https://www.jenkins.io/doc/book/installing/kubernetes/
<details><div>

### Q. 問題44: 未回答
あなたは、多数の依存システムを持つインフラサービスの常駐運用をしています。あなたは、サービスがリクエストのほとんどに対応できず、数十万人のユーザを持つ依存システムすべてが影響を受けていることを示すアラートを受け取りました。サイト信頼性エンジニアリング（SRE）のインシデント管理プロトコルの一環として、あなたはインシデントコマンダー（IC）を宣言し、チームから経験豊富な2人をオペレーションリード（OL）とコミュニケーションリード（CL）として迎え入れました。
あなたは次に何をすべきですか？
1. ユーザーへの影響を軽減する方法を探し、本番環境に導入します
2. ポストモーテムを開始し、インシデンと情報を追加し、ドラフトを社内に回覧し、社内のステークホルダーに意見を求めます
3. インシデント対応者とリードが相互に連絡できるコミュニケーションチャネルを確立します
4. 影響を受けたサービスオーナーに連絡し、インシデントの状況をアップデートします
<details><div>
    答え：3
説明
この問題では、サイト信頼性エンジニアリング（SRE）のインシデント管理プロトコルにおけるインシデントコマンダー（IC）のロールに注目します。インシデント発生時にICが最初に行うべきアクションについて問われています。インシデントの解決策を提案したり、顧客への連絡を考える前に、あなたはまず全体の調整役やコミュニケーションを担うロールを果たすべく行動を起こすべきです。選択肢を見る際は、まずインシデント対応を円滑に進めるための初期のステップに焦点を当てる必要があります。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：ソフトウェアエンジニアリングの原則を適用してシステムの信頼性、拡張性、効率性を向上させるプラクティスです。
インシデント管理：サービス中断や品質低下に対する対策プロセスです。インシデントの原因や影響を特定・対策し、再発防止を図ります。
インシデントコマンダー（IC）：インシデント対応の全体的な責任と調整を担当するロールです。
オペレーションリード（OL）：具体的な技術的問題の解決を担当するロールです。
コミュニケーションリード（CL）：インシデントに関する情報の共有やコミュニケーションを担当するロールです。
コミュニケーションチャネル：情報や意見交換を行うための経路です。インシデントの対応を迅速かつ効果的に行うため、適切なコミュニケーションチャネルを確立することが重要です。
ポストモーテム：インシデント後にその原因や影響、改善策を検討・文書化するプロセスです。しかし、インシデント発生中は原因特定・対策の確立を優先すべきです。
正解についての説明：
（選択肢）
・インシデント対応者とリードが相互に連絡できるコミュニケーションチャネルを確立します
この選択肢が正解の理由は以下の通りです。
まず、インシデント管理の初段階として、適切なコミュニケーションの確立は至極重要です。これは、インシデントの解決に向けた効率的な進行を保証し、インシデント関連の情報が確実に全関係者に伝達されることを保証します。特に、このシナリオではインフラサービスに複数の依存システムが存在し、大勢のユーザが影響を受けているため、対応策の調整と実行が迅速かつ適切に行われることが求められます。
また、SREのインシデント管理プロトコルでは、これらのコミュニケーションリード（CL）とオペレーションリード（OL）がまとまった対応を指導するため、その間のスムーズなコミュニケーションが不可欠です。
したがって、次に行うべきことは、インシデント対応者とリードが相互に連絡できるコミュニケーションチャネルを確立することです。
不正解についての説明：
選択肢：ユーザーへの影響を軽減する方法を探し、本番環境に導入します
この選択肢が正しくない理由は以下の通りです。
インシデントが発生した際の最初のステップは、迅速に強固なコミュニケーションチャネルを確立し、全員が何が起こっているのかを理解し、問題を解決するためのロールを明確化することです。それに比べて、直接ユーザーへの影響を軽減する方法を探し、本番環境に導入するという選択肢は、まだ情報共有やロールの明確化など、初期対応が不十分な状況で行うことはリスクが大きいと言えます。
選択肢：影響を受けたサービスオーナーに連絡し、インシデントの状況をアップデートします
この選択肢が正しくない理由は以下の通りです。
影響を受けたサービスオーナーに連絡をするのも重要ですが、それは後回しにすべきです。まず優先すべきはインシデント対応者間のコミュニケーションチャネルを確立し、共有された情報を利用して問題解決に専念することです。適切な報告はひとまず二の次となります。
選択肢：ポストモーテムを開始し、インシデンと情報を追加し、ドラフトを社内に回覧し、社内のステークホルダーに意見を求めます
この選択肢が正しくない理由は以下の通りです。
まだインシデント発生中であり、まずはインシデントの解決に注力すべきです。ポストモーテムはインシデント解決後に行うべきなので、この段階で開始するのは適切ではありません。
一方、即時向けの適切な対応は、インシデント対応者とリードが連絡を取り合うためのコミュニケーションチャネルの確立です。これにより、効果的な協力と素早い問題解決が可能になります。
参考リンク：
https://cloud.google.com/incident-response
https://cloud.google.com/blog/products/operations/a-framework-for-sre-as-code
https://sre.google/books/
<details><div>

### Q. 問題45: 未回答
あなたはGoogle CloudのCI/CDパイプラインのデプロイとテスト戦略を開発しています。以下のようなデプロイとテストの戦略を選択したいと考えています：
- リリースのデプロイの複雑さを軽減し、デプロイのロールバックの期間を最小限に抑えます。
- 影響を受けるユーザー数を徐々に増やしながら、実際の本番トラフィックをテストします。
あなたはこのために、どうすればよいですか？
1. ローリングアップデートデプロイとシャドーテスト
2. デプロイメントとカナリアテストの再作成
3. ローリングアップデートデプロイとA/Bテスト
4. ブルー/グリーンデプロイとカナリアテスト
<details><div>
    答え：4
説明
この問題では、CI/CDパイプラインのデプロイとテスト戦略の自動化の文脈が設定されています。リリースのデプロイの複雑さを軽減し、ロールバックの期間を最小限に抑えるという要件と、影響を受けるユーザー数を徐々に増やしながら実際の本番トラフィックをテストするという要素を満たす解決策を選ぶべきです。選択肢を見る際には、これらの要件が実現可能な戦略を選ぶことを確認する必要があります。
基本的な概念や原則：
ブルー/グリーンデプロイ：同時に二つの環境（ブルーとグリーン）を持ち、一方を活用中（本番）である間にもう一方で新しいバージョンを準備するデプロイ戦略です。デプロイの複雑さを軽減し、ロールバックの期間を最小限に抑えることができます。
カナリアテスト：新しいバージョンのリリースを一部のユーザーに対してだけ行い、その影響を観察するテスト戦略です。影響を受けるユーザー数を徐々に増やしながら実際の本番トラフィックをテストすることができます。
ローリングアップデート：新バージョンのリリースを段階的に行う方法で、一度に全てのインスタンスを更新することはありません。しかしロールバックが必要な場合、時間がかかる可能性があります。
A/Bテスト：同時に複数のバージョンをランダムなユーザーグループに配布し、ユーザー行動に基づいて性能を比較するテスト手法です。一時的な影響を検証するために利用されることがあります。
シャドーテスト：実際の本番環境と同じ負荷をシステムにかけることで、そのパフォーマンスをテストする手法です。しかし、実際のユーザーに影響を及ぼすリスクはありません。
正解についての説明：
（選択肢）
・ブルー/グリーンデプロイとカナリアテスト
この選択肢が正解の理由は以下の通りです。
まず、ブルーグリーンデプロイメントは、新旧の環境（ブルーバージョンとグリーンバージョンと呼びます）を同時に稼働させる戦略で、新しいバージョンの準備が整ってからトラフィックを一度に切り替える方法です。これにより、デプロイの複雑さを軽減し、必要に応じて瞬時にロールバックすることが可能になります。これがデプロイのロールバックの期間を最小限に抑えるための戦略に一致します。
次に、カナリアテストは、新しいバージョンのシステムをまず一部のユーザーに公開し、問題がなければ全体に展開する方法です。もし問題が見つかった場合、影響を受けるユーザーの数は最小限にとどまります。
また、実際の本番環境のトラフィックをテストすることが可能です。これが徐々に影響を受けるユーザー数を増やしながら、実際の本番トラフィックをテストするための戦略に一致します。
したがって、上記の要求を満たすためには"ブルー/グリーンデプロイとカナリアテスト"が適切な選択となります。
不正解についての説明：
選択肢：デプロイメントとカナリアテストの再作成
この選択肢が正しくない理由は以下の通りです。
"デプロイメントとカナリアテストの再作成"は特定の方法を示していないため、具体的な戦略とは言えません。
それに対して、ブルー/グリーンデプロイは二つの環境間で切り替えることでデプロイの複雑さを減らし、ロールバックを容易にします。カナリアテストはユーザーに対する影響を徐々に増やし、本番環境でのテストを可能にします。
選択肢：ローリングアップデートデプロイとA/Bテスト
この選択肢が正しくない理由は以下の通りです。
ローリングアップデートデプロイは徐々に新規リリースを展開しますが、アップデート中に問題が生じた場合のロールバックが難しいです。
また、A/Bテストはトラフィックの一部にしか新規リリースが適用されないため、全体の影響を完全にテストすることができません。
これに対し、ブルー/グリーンデプロイはすぐに全トラフィックを新規リリースに切り替えてロールバックも容易、カナリアテストは徐々に影響を受けるユーザー数を増やすため、設問の要件にはこちらが適しています。
選択肢：ローリングアップデートデプロイとシャドーテスト
この選択肢が正しくない理由は以下の通りです。
ローリングアップデートデプロイは新バージョンを徐々にデプロイしますが、一部のユーザーに何も影響を与えずにデプロイをロールバックする能力が制限され、問題があればデプロイ自体が複雑になる可能性があります。
また、シャドーテストでは本番トラフィックを複製して使用しますが、影響を受けるユーザー数を徐々に増やすテスト戦略には適していません。
参考リンク：
https://cloud.google.com/architecture/blue-green-deployments
https://cloud.google.com/run/docs/rolling-out-updates
https://cloud.google.com/blog/products/devops-sre/testing-on-production-at-cloud-scale-with-Google Cloud-canarying
<details><div>

### Q. 問題46: 未回答
あなたは、ユーザー向けのウェブアプリケーションをサポートしています。過去6か月間のアプリケーションのエラーバジェットを分析したところ、特定の時間枠において、アプリケーションがエラーバジェットの5% 以上を消費したことがないことに気づきました。ビジネスステークホルダーとサービスレベル目標（SLO）のレビューを行い、SLOが適切に設定されていることを確認します。あなたは、アプリケーションのSLOが、観察された信頼性をより忠実に反映するようにしたいと考えています。速度、信頼性、およびビジネスニーズのバランスを取りながら、その目標を達成することが必要です。
この要件を満たすために、どうすればよいですか？（2つ選択）
1. アプリケーションに追加のサービスレベル指標（SLI）を導入し、測定します
2. 計画的なダウンタイムを発表し、エラーバジェットをより多く消費し、ユーザーがより厳しいSLOに依存しないようにします
3. アプリケーションのリリースをより頻繁に、または潜在的にリスクを伴うものにします
4. アプリケーションのすべてのゾーンに、より多くのサービング容量を追加します
5. SLOを、アプリケーションで観測された信頼性と一致させます
<details><div>
    答え：1,3
説明
この問題では、ウェブアプリケーションのエラーバジェットおよびサービスレベル目標（SLO）に関連する管理と最適化が求められています。エラーバジェットの消費が少ないことから、アプリケーションの信頼性が高い状態が示唆されます。これを調整するためには、リリースの頻度を増やす、もしくはリスクを伴うリリースを行う、あるいは新たなサービスレベル指標（SLI）を導入することが有効です。これらは全てアプリケーションの信頼性を調整するための措置であり、適切なバランスをとりながら行う必要があります。
基本的な概念や原則：
サービスレベル目標（SLO）：サービスの提供品質を定量的に評価し、管理するための目標値です。信頼性、パフォーマンスなどを表した指標と期待値からなります。
エラーバジェット：SLOと実際の性能とのギャップをエラーとして量的に表現したものです。エラーバジェットの消費状況をモニタリングすることで、システムの信頼性を管理します。
リリース戦略：リリースの頻度や規模を決定するための戦略です。頻繁なリリースは潜在的なリスクを伴いますが、問題が発生した場合の影響を局所化しやすいです。
サービスレベル指標（SLI）：SLOを測定するための具体的な指標です。SLIを増やすことで、より詳細な信頼性の評価が可能になります。
ゾーン：Cloudプラットフォーム内でリソースを配置する地理的な領域です。特定のゾーンにリソースを追加することで、そのゾーンでのサービスの耐障害性やパフォーマンスが向上します。
ダウンタイム：システムが正常に動作せず、利用できない状態のことです。計画的なダウンタイムを設けることで、メンテナンス等を行うが、一方でユーザーからは不便と感じられる場合があります。
正解についての説明：
（選択肢）
・アプリケーションのリリースをより頻繁に、または潜在的にリスクを伴うものにします
・アプリケーションに追加のサービスレベル指標（SLI）を導入し、測定します
この選択肢が正解の理由は以下の通りです。
過去6か月間でアプリケーションのエラーバジェットの消費が5%未満であったという情報から、アプリケーションの信頼性が非常に高いことが示されています。高い信頼性を保ちつつも、アプリケーションの改善や進化を求めてより頻繁に、または潜在的なリスクを伴うリリースを行うことで、エラーバジェットの利用可能性をよりフルに活用することが可能となります。これによりアプリケーションのSLOが観察された信頼性をより忠実に反映することが可能となります。
また、追加のサービスレベル指標（SLI）を導入して測定することにより、アプリケーションのパフォーマンスや信頼性のさまざまな側面をより詳細に把握し、それらの側面を改善するための対策を考えることが可能となります。これによりアプリケーションのSLOが観察された信頼性をより忠実に反映し、同時にビジネスニーズともバランスを取ることが可能となります。
不正解についての説明：
選択肢：アプリケーションのすべてのゾーンに、より多くのサービング容量を追加します
この選択肢が正しくない理由は以下の通りです。
サービングの容量を増やすことは、通常は可用性の向上に寄与しますが、現状のアプリケーションのエラーバジェットは問題ないため、不必要なオーバープロビジョニングとなりコスト増加のリスクを生じます。より頻繁なリリースやSLIの導入が役立つのは、それがSLO達成状況をより正確に反映し、改善への取り組みを導くからです。
選択肢：SLOを、アプリケーションで観測された信頼性と一致させます
この選択肢が正しくない理由は以下の通りです。
SLOを観測された信頼性と一致させることは、ビジネスニーズのバランスを取るためには適切ではありません。SLOは、ビジネス要件と利用者の期待を反映したものであるべきです。アプリケーションの現在のパフォーマンスだけに基づいてSLOを設定すると、予期せぬトラブル時に必要な余裕を得られない可能性があります。
選択肢：計画的なダウンタイムを発表し、エラーバジェットをより多く消費し、ユーザーがより厳しいSLOに依存しないようにします
この選択肢が正しくない理由は以下の通りです。
計画的なダウンタイムを増やすと、ユーザー体験が悪化し、信頼性が低下します。エラーバジェットの消費を増やすことが目標の達成に繋がるわけではなく、むしろ信頼性向上とは逆の効果があります。本質的には、信頼性の高い運用維持と改善が重要です。
参考リンク：
https://cloud.google.com/architecture/framework/reliability/operational-excellence
https://cloud.google.com/architecture/framework/devops/devops-measurement-using-slos
https://sre.google/workbook/implementing-slos/
<details><div>

### Q. 問題47: 未回答
Google CloudにCompute Engineインスタンスをデプロイしました。インスタンスの監視メトリクスとログが、会社の運用チームとサイバーセキュリティチームによって、Cloud LoggingとCloud Monitoringに表示されるようにする必要があります。最小権限の原則に従いながら、IAM（Identity and Access Management）を使用して、Compute Engineサービスアカウントに必要なロールを付与する必要があります。
この要件を満たすために、どうすればよいですか？
1. Compute Engineサービスアカウントにlogging.editorロールとmonitoring.metricWriterロールを付与します
2. Compute Engineサービスアカウントにlogging.adminロールとmonitoring.editorロールを付与します
3. Compute Engineサービスアカウントにlogging.logWriterとmonitoring.editorのロールを付与します
4. Compute Engineサービスアカウントにlogging.logWriterロールとmonitoring.metricWriterロールを付与します
<details><div>
    答え：4
説明
この問題では、Google CloudのIAM（Identity and Access Management）を使用して、Compute Engineのインスタンスの監視メトリクスとログをCloud LoggingとCloud Monitoringに表示する設定について問われています。重要なポイントは、"最小権限の原則"に従い、適切なロールをCompute Engineサービスアカウントに付与することです。また、選択肢からもCompute Engineのサービスアカウントに対するロールの候補が提示されています。これらの情報を元に正しいロールを選択し、最小権限の原則に合致したロール設定を行うことが重要です。
基本的な概念や原則：
IAM（Identity and Access Management）：Google Cloudの認証および権限管理システムです。特定のユーザやサービスアカウントに対して、特定のリソースに対するアクセス権を付与することができます。
最小権限の原則：セキュリティのベストプラクティスで、ユーザーまたはアカウントには、必要なタスクを遂行するために最も低いレベルの権限のみを付与するべきだという原則です。これにより、不正アクセスや権限の乱用を最小限に抑えることができます。
Compute Engineサービスアカウント：Compute EngineインスタンスがGoogle Cloud APIを使用するためのアカウントです。
logging.logWriterロール：Cloud Logging APIにログエントリを書き込む権限を付与します。これにより、特定のインスタンスからのログを収集することができます。
monitoring.metricWriterロール：Cloud Monitoring APIにメトリクスデータを書き込む権限を付与します。これにより、特定のインスタンスの稼働状況を監視することができます。
logging.admin、monitoring.editorロール：これらはそれぞれLoggingとMonitoringに対する広範な権限を持つロールで、ログの管理や監視設定の編集などを行うことができます。しかし、これらはあまりにも広範な権限を持つため、最小権限の原則には適合しません。
正解についての説明：
（選択肢）
・Compute Engineサービスアカウントにlogging.logWriterロールとmonitoring.metricWriterロールを付与します
この選択肢が正解の理由は以下の通りです。
まず、問題の要件は、Compute EngineのメトリクスとログがCloud LoggingとCloud Monitoringに表示されることとIAMを使った適切なロールの付与です。そこで必要となるロールは二つあり、一つ目はlogsへの書き込みを許可するlogging.logWriterロール、二つ目はmetricsへの書き込みを許可するmonitoring.metricWriterロールです。これらのロールをCompute Engineのサービスアカウントに付与することで、Compute Engineインスタンスが生成したメトリクスとログがそれぞれCloud MonitoringとCloud Loggingに書き込むことができます。
また、最小権限の原則に従い特定のタスクに必要な権限のみを付与することで、セキュリティリスクを最小限に抑えます。
不正解についての説明：
選択肢：Compute Engineサービスアカウントにlogging.adminロールとmonitoring.editorロールを付与します
この選択肢が正しくない理由は以下の通りです。
logging.adminロールとmonitoring.editorロールは、それぞれのサービスに関する全てのリソースを作成、設定、管理、削除できる権限を持っています。運用チームとサイバーセキュリティチームが必要としているのはメトリクスとログの書き込みのみで、これ以上のアクセスは最小権限の原則に反します。
一方、logging.logWriterとmonitoring.metricWriterは書き込み権限のみを付与するため、要件を満たします。
選択肢：Compute Engineサービスアカウントにlogging.editorロールとmonitoring.metricWriterロールを付与します
この選択肢が正しくない理由は以下の通りです。
logging.editorロールはログの閲覧と編集が可能な権限を付与しますが、原則最小権限に基づいているときは不適切です。Compute Engineサービスアカウントはログを書き込むだけなので、logging.logWriterのロールが適切です。
選択肢：Compute Engineサービスアカウントにlogging.logWriterとmonitoring.editorのロールを付与します
この選択肢が正しくない理由は以下の通りです。
monitoring.editorロールは読み取り、書き込み、管理操作が可能であり最小権限の原則に反します。必要なロールは書き込みだけなので、monitoring.metricWriterロールが適切です。
参考リンク：
https://cloud.google.com/iam/docs/roles-predefined#monitoring.roles
https://cloud.google.com/logging/docs/access-control
https://cloud.google.com/monitoring/access-control
<details><div>

### Q. 問題48: 未回答
Google Cloud上で動作するアプリケーションをサポートし、Google Cloud Operation Suite Monitoringで最も重要なアラートについてチームへのSMS通知を構成したいと考えています。設定したいアラートポリシーはすでに特定されています。
あなたはこの要件を満たすために、どうすればよいですか？
1. 各アラートポリシーのWebhook通知オプションを選択し、サードパーティの統合ツールを使用するように設定します。チームメンバーがSMS/電話番号を外部ツールに追加していることを確認します
2. Google Cloud Operation Suite MonitoringとSMSゲートウェイ間のサードパーティ統合をダウンロードして設定します。チームメンバーが外部ツールにSMS/電話番号を追加していることを確認します
3. チームメンバーがGoogle Cloud Operation Suite ProfileでSMS/電話番号を設定していることを確認します。各警告ポリシーのSMS通知オプションを選択し、リストから適切なSMS/電話番号を選択します
4. アラートポリシーごとにSlack通知を設定します。Slackメッセージ受信時にSMSメッセージを送信するために、Slack-to-SMSインテグレーションを設定します。チームメンバーがSMS/電話番号を外部統合に追加していることを確認します
<details><div>
    答え：3
説明
この問題では、Google Cloud Operation Suite Monitoringを用いて、特定のアラートをチームにSMS通知するという要件を満たす適切な手順を選ぶことが求められています。そのため、Google Cloud Operation Suite Monitoringの機能を理解し、どのようにしてSMSの通知が可能であるかについて考えることが重要です。設問から、SMS/電話番号を通知の一部として設定すること、チームメンバーがその番号をプロファイルに設定していること、それらを適切なアラートポリシーとマッチさせて設定することが求められていることが理解できます。以上により、問題を解くためには、Google Cloud Operation Suite Monitoringの通知設定機能をどのように活用するかを理解しておくことが重要です。
基本的な概念や原則：
Google Cloud Operation Suite：Google Cloudの運用支援ツール群です。Google Cloud Monitoring, Google Cloud Logging, Google Cloud Trace, Google Cloud Debuggerなどが含まれており、システムの監視、ログ分析、パフォーマンス分析などの機能を提供します。
アラートポリシー：Google Cloud Monitoringでアラートを生成するために設定するポリシーです。特定の条件が満たされたときにアラートを発生させ、ユーザーに通知します。
SMS通知：アラートが発生した際に、SMSメッセージを使用して通知する機能です。Google Cloud Operation Suiteでは、プロフィール設定でSMS/電話番号の追加が可能です。
サードパーティ統合：Google Cloudと他の外部システムやツールの連携です。一部の機能は、外部のサービスとの統合により拡張することが可能ですが、直接SMS通知を設定することでモニタリングの手間を減らします。
Webhook通知：アラートが発生した際に、Webhookを使用して通知する機能です。特定のURLに対してHTTP POSTリクエストを送信し、外部システムへの通知を行います。
Slack通知：Slackというコミュニケーションツールに対して通知を行う機能です。ただし、SMS通知が直接サポートされている場合、Slack通知を介してSMSを送信する必要はありません。
正解についての説明：
（選択肢）
・チームメンバーがGoogle Cloud Operation Suite ProfileでSMS/電話番号を設定していることを確認します。各警告ポリシーのSMS通知オプションを選択し、リストから適切なSMS/電話番号を選択します
この選択肢が正解の理由は以下の通りです。
まず、Google Cloud Operation Suite Monitoringは、Google Cloud上で動作するアプリケーションの監視と警告を行うためのツールです。このアプリケーションには、ユーザが各種通知方法を設定できる機能が存在します。
次に、SMS通知を送受信するためには、受信者の電話番号が必要となります。Google Cloud Operation Suite Profileは、ユーザの通知方法を設定する場所です。ですから、設定したい通知方法がSMSである場合、チームメンバーが各自のプロフィールでSMS/電話番号を設定しているか確認する必要があります。
最後に、具体的な警告ポリシーについて、SMS通知オプションを選択し、リストから適切な電話番号を選択することで、特定のアラートに対してSMS通知を送る設定が完成します。以上の理由からこの選択肢が最適な解答となります。
不正解についての説明：
選択肢：Google Cloud Operation Suite MonitoringとSMSゲートウェイ間のサードパーティ統合をダウンロードして設定します。チームメンバーが外部ツールにSMS/電話番号を追加していることを確認します
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Monitoringは直接SMS通知をサポートしており、サードパーティの統合をダウンロードして設定する必要はありません。このような追加的な工程は複雑さを増やし、管理コストを上げる結果となります。
一方、正解の選択肢はGoogle Cloud Operation Suiteの機能を直接利用し、シンプルな手順でSMS通知を設定します。
選択肢：各アラートポリシーのWebhook通知オプションを選択し、サードパーティの統合ツールを使用するように設定します。チームメンバーがSMS/電話番号を外部ツールに追加していることを確認します
この選択肢が正しくない理由は以下の通りです。
Webhook通知オプションとサードパーティの統合ツールを使用すると、設定が複雑かつ手間がかかります。
また、外部ツールを通じてSMSの通知を行うために、チームメンバーが自身の電話番号を外部ツールに追加することはセキュリティ上のリスクを伴います。必要な通知を直接、Google Cloud Operation Suiteから送信する方がセキュリティが高く、効率的です。
選択肢：アラートポリシーごとにSlack通知を設定します。Slackメッセージ受信時にSMSメッセージを送信するために、Slack-to-SMSインテグレーションを設定します。チームメンバーがSMS/電話番号を外部統合に追加していることを確認します
この選択肢が正しくない理由は以下の通りです。
Slackを介してSMS通知を送る設定は、不必要なステップと外部依存性を増やし、適切でない解決策です。Google Cloud Operation Suiteは、直接SMS通知をサポートしているので、これを利用することが最も単純で効率的です。
参考リンク：
https://cloud.google.com/monitoring/alerts/using-alerting-ui
https://cloud.google.com/monitoring/support/notification-options
https://www.twilio.com/docs/usage/tutorials/how-to-set-up-sms-alerts-for-new-email-messages
<details><div>

### Q. 問題49: 未回答
あなたのチームは、外部向けアプリケーションでインシデントが発生した後にポストモーテムを書いています。あなたのチームは、インシデントがポストモーテムを必要とするかどうかを示すトリガーを含むように、ポストモーテムポリシーを改善したいと考えています。
サイト信頼性エンジニアリング（SRE）のプラクティスに基づき、どのようなトリガーをポストモーテムポリシーに定義すべきですか？（2つ選択）
1. インシデントによりデータが失われたかどうかを基準にします
2. CDパイプラインが問題を検出し、問題のあるリリースをロールバックする必要があったかどうかを基準にします
3. 社外ステークホルダーからのポストモーテムの依頼の有無を基準にします
4. 社内ステークホルダーからのポストモーテムの依頼の有無を基準にします
5. 監視システムによる、アプリケーションのインスタンスの1つの故障の検知の有無を基準にします
<details><div>
    答え：1,2
説明
この問題では、サイト信頼性エンジニアリング（SRE）のプラクティスに基づいてインシデント後のポストモーテムのトリガーをどのように設定するべきかを尋ねています。ここで重要なのは、答えが質問の文脈を反映していることです。つまり、SREプラクティスに基づいてシステム上の重大な問題またはデータ損失をトリガーとして設定することが求められます。ただし、単純な故障やステークホルダーからのリクエストに反応するのではなく、真にシステム全体の健全性に影響を及ぼす問題に焦点を当てるべきだということを念頭に置いてください。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：システムの信頼性とスケーラビリティを向上させるための実践と哲学です。インシデント管理とポストモーテムの作成はこのプラクティスの一部です。
ポストモーテム：インシデントやシステム障害が発生した後に行われるレビュープロセスで、原因の特定と再発防止のための措置の決定を行います。
データロス：システムやインフラストラクチャの障害によりデータが部分的または完全に失われることです。このような重大インシデントでは、ポストモーテムが必要であると考えられます。
ロールバック：システムを以前の状態に戻すことです。本番環境で問題が発生した場合、ロールバックは修正措置の一つとなります。
CI/CDパイプライン：継続的インテグレーション（CI）と継続的デリバリー（CD）を組み合わせたソフトウェア開発プロセスです。問題のあるリリースを検出し、ロールバックする能力は重要な品質保証メカニズムです。
正解についての説明：
（選択肢）
・インシデントによりデータが失われたかどうかを基準にします
・CDパイプラインが問題を検出し、問題のあるリリースをロールバックする必要があったかどうかを基準にします
この選択肢が正解の理由は以下の通りです。
サイト信頼性エンジニアリング（SRE）のプラクティスはシステムの信頼性、負荷能力、パフォーマンスに焦点を当てており、インシデントが発生した場合は原因分析と改善策を立案するためにポストモーテム分析を強く推奨します。このため、データが失われた場合やCDパイプラインが問題を検出しロールバックが必要だった場合は現行のシステムあるいは手続きに問題があった可能性があり、それらについて改善策を探求するためにポストモーテムを行うというトリガーは適切です。データを失った際には、そのデータが何であるか、なぜ失われたのか、どうやって避けるべきだったのか、さらにはそのデータを回復するために何をすべきだったのかを理解することが重要です。同様に、問題のあるリリースをロールバックしなければならなかった場合、何が問題だったのか、それがいかにして検出されたのか、そして将来同じ問題が発生するのを防ぐにはどうすべきだったのかを理解する必要があります。
不正解についての説明：
選択肢：社外ステークホルダーからのポストモーテムの依頼の有無を基準にします
この選択肢が正しくない理由は以下の通りです。
社外ステークホルダーからのポストモーテムの依頼は重要なフィードバックではありますが、インシデントの深層的な原因調査のトリガーとするには不適切です。真の問題点やシステムの不具合が外部ステークホルダーからの依頼なしでは見落とされてしまう可能性があります。
一方、データの損失やリリースのロールバックのような明確な問題は、再発防止策を見つけ出すための詳細な調査を必要とします。
選択肢：社内ステークホルダーからのポストモーテムの依頼の有無を基準にします
この選択肢が正しくない理由は以下の通りです。
社内ステークホルダーからのポストモーテムの依頼の有無は主観的な評価になり、インシデントの重大性や影響範囲とは必ずしも連動しないため、トリガーとして適切ではありません。正解の選択肢は具体的な影響の有無やシステムの挙動に基づいています。
選択肢：監視システムによる、アプリケーションのインスタンスの1つの故障の検知の有無を基準にします
この選択肢が正しくない理由は以下の通りです。
監視システムによる、アプリケーションのインスタンスの1つの故障の検知は、システム全体に対する重大な問題を必ずしも反映していない可能性があります。中には自動回復するような故障もあり得ます。
これに対して、データの失敗や問題のあるリリースのロールバックは全体に影響を及ぼす重大なインシデントで、ポストモーテム分析の対象となり得ます。
参考リンク：
https://cloud.google.com/incident-response
https://cloud.google.com/sre/docs/sre-book/managing-incidents
https://sre.google/books/postmortem-culture/
<details><div>

### Q. 問題50: 未回答
あなたは、Google Kubernetes Engine（GKE）上にデプロイされた人気のモバイルゲームアプリケーションを、複数のGoogle Cloudリージョンでサポートしています。各リージョンには複数のKubernetesクラスターがあります。あなたは、特定のリージョンのユーザーが誰もアプリケーションに接続できないというレポートを受け取りました。あなたは、サイト信頼性エンジニアリングのプラクティスに従いながら、インシデントを解決したいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. 高メモリと高CPUのマシンタイプのインスタンスで構成される追加のノードプールをクラスターに追加します
2. Google Cloud Operation Suite Monitoringを使用して、影響を受けるリージョンのCPUまたはメモリ使用量の急増をチェックします
3. 影響を受けるリージョンから、問題を報告しない他のリージョンにユーザートラフィックを迂回させます
4. Cloud Loggingを使用して、影響を受けるリージョン内のクラスターをフィルタリングし、ログのエラーメッセージを検査します
<details><div>
    答え：3
説明
この問題では、具体的なインシデント対応の選択肢から最善のアプローチを選び出す能力が問われています。まず、Google Kubernetes Engine（GKE）を使った複数リージョンのデプロイ環境を想定して、ユーザーが接続できないという問題が発生しているという状況を把握します。問題が特定のリージョンに限定されているため、他のリージョンへのユーザートラフィックのリダイレクト、リージョンのリソース使用量の監視、ノードプールの追加、ログのエラーメッセージの検査などの手段から最善の対応手段を選びます。ここで重要なのは、影響範囲を最小化し、サービスの可用性を確保しながらインシデントを解決することです。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google Cloud上で提供されるKubernetesクラスターの管理サービスです。容易なデプロイ、更新、スケーリングが可能で、高い安全性と信頼性を提供します。
リージョン：Google Cloudのデータセンターが存在する地理的なエリアを指します。複数のリージョンにデプロイすることで、サービスの可用性と耐障害性を高めることが可能です。
ユーザートラフィックの迂回：一部のリージョンが問題を起こした場合、そのリージョンから他のリージョンへユーザーの通信をリダイレクト（迂回）する手法です。これにより、サービスの中断をユーザーに感じさせずに問題を解決することが可能です。
Google Cloud Operation Suite Monitoring：Google Cloud上のリソースやアプリケーションのパフォーマンス監視を行うサービスです。
ノードプール：GKEクラスター内で同じ構成のノード群を指します。ノードプールを追加することでクラスターの計算能力を拡張することが可能です。
Cloud Logging：Google Cloudのリソースやアプリケーションからのログデータを集積、保管、分析するサービスです。エラーの解析やトラブルシューティングのためにログデータを確認することが可能です。
正解についての説明：
（選択肢）
・影響を受けるリージョンから、問題を報告しない他のリージョンにユーザートラフィックを迂回させます
この選択肢が正解の理由は以下の通りです。
特定のリージョンのユーザーが誰もアプリケーションに接続できないというレポートを受け取った場合、そのリージョンに問題がある可能性が高いです。結果として、直ちにユーザー体験に影響を与えるこの問題を解決する最善の選択肢は、影響を受けるリージョンから別のリージョンにユーザートラフィックを迂回することです。これにより、問題の解決が図られている間でも、ユーザーはサービスを引き続き利用することが可能となります。
また、このアプローチはサイト信頼性エンジニアリング（SRE）のプラクティスに従っています。SREの主要な目標の一つは、システムの可用性を維持しつつ、問題の影響を最小限に抑えることです。ユーザートラフィックの迂回は、その目標を達成するための効果的な手段です。時間をかけて問題の根本原因を特定し、修正するための時間を確保しつつ、同時にユーザーへのサービスを継続することができます。この問題解決の戦略は、GKE環境全体の効率と信頼性を向上させることに寄与します。
不正解についての説明：
選択肢：Google Cloud Operation Suite Monitoringを使用して、影響を受けるリージョンのCPUまたはメモリ使用量の急増をチェックします
この選択肢が正しくない理由は以下の通りです。
Google Cloud Operation Suite Monitoringを使用しても、ユーザーがアプリケーションに接続できない原因の特定や解決には至りません。対して正解の選択肢では、問題を報告しない他のリージョンにトラフィックを迂回することで、即座にユーザーの問題を緩和することができます。
選択肢：高メモリと高CPUのマシンタイプのインスタンスで構成される追加のノードプールをクラスターに追加します
この選択肢が正しくない理由は以下の通りです。
問題が接続にあるのであれば、高メモリや高CPUのインスタンスを追加することは解決策とはならず、不必要なコスト増加を招くだけです。無事に動作している他のリージョンにトラフィックを迂回させる方が合理的かつ効率的な解決策となります。
選択肢：Cloud Loggingを使用して、影響を受けるリージョン内のクラスターをフィルタリングし、ログのエラーメッセージを検査します
この選択肢が正しくない理由は以下の通りです。
Cloud Loggingでエラーを調査するの自体は問題ないですが、ユーザーがサービスに接続できない状態という緊急性を考えると、最優先すべきは問題を回避しサービスを回復させることです。そこで正解はユーザートラフィックを別の正常なリージョンへリダイレクトするべきです。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/how-to/multi-cluster-ingress
https://cloud.google.com/load-balancing/docs
https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/

## 4
### Q. 問題1: 未回答
サポートするプロダクションシステムで大量の障害が発生しています。夜中であっても、すべての機能停止のアラートを受け取るような状態です。アラートは、1分以内に自動的に再起動される不健全なシステムによるものです。あなたは、サイト信頼性エンジニアリングのプラクティスに従いながら、スタッフの疲弊を防ぐプロセスを設定したいと考えています。
この要件を満たすために、どうすればよいですか？
1. 各アラートのインシデントレポートを作成します
2. 異なるタイムゾーンにいるエンジニアにアラートを配信します
3. 対処不可能なアラートを排除します
4. エラーバジェットが枯渇しないように、関連するサービスレベル目標を再定義します
<details><div>
    答え：3
説明
この問題では、大量の障害が発生しているプロダクションシステムを効果的に管理し、同時にスタッフの疲弊を防ぐための方法を問いています。サイト信頼性エンジニアリングのプラクティスに従うという要点を念頭に置きつつ、各選択肢がシステムの問題を解決し、かつチームの疲弊を防ぐかどうかを考える必要があります。また、問題文の中で述べられている"1分以内に自動的に再起動される不健全なシステムによるもの"という点に注目することも重要です。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：システムの信頼性、スケーラビリティ、および効率を確保するプラクティスです。エンジニアリングの原則を適用して、ITインフラストラクチャの問題を解決します。
アラート設定：システムのどういった振る舞い、または状態変化を監視し、問題が発生した際に通知を行うかを定義した設定です。適切なアラート設定により、機能停止等の問題を早めに検知し、早期の問題解決に繋げます。
対処不能なアラート：システムの問題やエラーの通知のことで、ただし、エンジニアの対処不要な自動的に解消する問題などがこれに該当します。これらのアラートは、エンジニアの作業効率を下げる原因となるため、適切に設定・フィルタリングすることが求められます。
インシデントレポート：システム障害やエラーが発生した際に作成される報告書のことです。注目すべき問題や障害の詳細な情報を提供し、原因分析や回避策検討の取り組みに利用されます。
エラーバジェット：サービスレベル目標（SLO）を満たすために許されるエラーの最大量のことです。エラーバジェットの枯渇はサービス品質の低下を意味し、これを防ぐための対策が必要です。
正解についての説明：
（選択肢）
・対処不可能なアラートを排除します
この選択肢が正解の理由は以下の通りです。
サイト信頼性エンジニアリング（SRE）の主要な目標の一つは、システムの信頼性を保ちつつ、不要な作業や障害対応を削減することです。アラートに対する反応がシステムを改善しない場合、そのアラートはノイズとなり、スタッフの生産性と満足度を低下させます。このケースでは、アラートは1分以内に自動的に修復されるシステムの問題を指摘していることから、対処不可能なアラートと言えます。このようなアラートを排除することで、エンジニアの作業負荷を軽減し、重要な問題に対処するためのリソースを確保できます。応答可能で有意義なアラートだけを維持することで、SREプラクティスを適切に遵守し、スタッフの疲弊を防ぐプロセスを設定できます。
不正解についての説明：
選択肢：各アラートのインシデントレポートを作成します
この選択肢が正しくない理由は以下の通りです。
各アラートのインシデントレポートを作成する方法は、スタッフの疲弊を防ぐという要件に合致せず、むしろ作業負荷が増える可能性があります。反対に、対処不可能なアラートを排除することで、不要なアラートに対する対応を省くことができ、スタッフの疲弊を防ぐことが可能となります。
選択肢：異なるタイムゾーンにいるエンジニアにアラートを配信します
この選択肢が正しくない理由は以下の通りです。
異なるタイムゾーンのエンジニアにアラートを配信すると、確かに時間帯による負荷を分散することは可能です。しかし、そもそもの問題が1分以内に自動的に解決するアラートに対して人手を割いているということは無視しています。対照的に正解の選択は対処不要なアラートを排除し、その結果としてスタッフの疲弊を軽減するため適切です。
選択肢：エラーバジェットが枯渇しないように、関連するサービスレベル目標を再定義します
この選択肢が正しくない理由は以下の通りです。
エラーバジェットの枯渇を防ぐためにサービスレベル目標を再定義する方法は、基本的には許容する障害率を増やすということであり、それはスタッフの疲弊や頻繁なアラートの問題に対する直接的な解決策にはなりません。
それに対して、対処不可能なアラートの排除は、不必要なアラートを減らしスタッフの疲弊を防ぐ具体的な対策となります。
参考リンク：
https://cloud.google.com/blog/products/management-tools/best-practices-for-setting-slos-and-slas
https://cloud.google.com/monitoring/alerts
https://sre.google/sre-book/being-on-call/
<details><div>

### Q. 問題2: 未回答
あなたは、開発、品質保証（QA）、本番の3つの異なる環境を持つシステムを設計しています。各環境はTerraformでデプロイされ、アプリケーションチームがアプリケーションをデプロイできるようにGoogle Kubernetes Engine（GKE）クラスターが作成されます。各GKEクラスターにインフラレベルのリソースをデプロイするために、Anthos Config Managementが使用され、テンプレート化されます。すべてのユーザー（インフラストラクチャオペレーターやアプリケーションオーナーなど）はGitOpsを使用します。
Infrastructure as Code（IaC）とアプリケーションコードの両方のソース管理リポジトリをどのように構成すべきですか？
1. 
- クラウドインフラ（Terraform）リポジトリは共有されます。異なるディレクトリは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは分離されています。異なるブランチは異なる環境になるようにします
- アプリケーション（アプリのソースコード）リポジトリは分離されています。異なるブランチは異なる機能になるようにします
2. 
- クラウドインフラ（Terraform）リポジトリは共有されます。異なるブランチは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは共有されているいます。オーバーレイディレクトリが異なれば環境も異なるようにします
- アプリケーション（アプリのソースコード）リポジトリは共有されています。異なるディレクトリは異なる機能になるようにします
3. 
- クラウドインフラ（Terraform）リポジトリは共有されます。異なるブランチは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは共有されているいます。オーバーレイディレクトリが異なれば環境も異なるようにします
- アプリケーション（アプリのソースコード）リポジトリは分離されています。異なるブランチは異なる機能になるようにします
4. 
- クラウドインフラ（Terraform）リポジトリは共有されます。異なるディレクトリは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは共有されているいます。オーバーレイディレクトリが異なれば環境も異なるようにします
- アプリケーション（アプリのソースコード）リポジトリは分離されています。異なるブランチは異なる機能になるようにします
<details><div>
    答え：4
説明
この問題では、開発、品質保証、および本番の環境でTerraformとAnthos Config Managementを利用したGitOpsフローをうまく実装するための最適なリポジトリ構成を理解することが求められています。ここでは、インフラストラクチャとアプリケーションコードの管理方法、およびそれらの異なる環境間での適用方法に注意する必要があります。適切なリポジトリ戦略を選択するためには、GitOpsの原則、TerraformやAnthos Config Managementの使い方について深く理解する必要があります。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：Google CloudのフルマネージドKubernetesサービスです。コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を行うことができます。
Terraform：Infrastructure as Code（IaC）を提供するオープンソースツールです。複数のクラウドプロバイダに対応し、再利用可能なインフラストラクチャコードを作成します。
Anthos Config Management：Kubernetesの設定を一元管理するGoogle Cloudのツールです。一貫性のある設定をすぐに適用し、安全に維持することができます。
GitOps：開発と運用のプロセスを統一し、自動化するアプローチです。Gitリポジトリーを唯一のソースとして、継続的なデプロイメントと修復を実現します。
Kustomize：Kubernetesリソースの設定管理を簡素化するツールです。リソースマニフェストのカスタム化和リユースを可能にします。
正解についての説明：
（選択肢）
・- クラウドインフラ（Terraform）リポジトリは共有されます。異なるディレクトリは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは共有されているいます。オーバーレイディレクトリが異なれば環境も異なるようにします
- アプリケーション（アプリのソースコード）リポジトリは分離されています。異なるブランチは異なる機能になるようにします
この選択肢が正解の理由は以下の通りです。
まず、TerraformリポジトリとAnthos Config Managementリポジトリを共有することで、各環境のインフラを再現可能かつ統一的に管理することができます。これにより、設定の不一致やミスを防止し、環境間でのインフラ設定の差異を最小化できるためです。Terraformの各ディレクトリが異なる環境を示し、Anthos Config Managementにおいてはオーバーレイディレクトリを使用して環境を区別することで、各環境設定を適切に分離し、一方で全体的な一貫性を維持します。
次に、アプリケーションリポジトリはインフラと別々にすることで、アプリケーションの開発と運用をインフラの変更から独立して行うことができます。
加えて、異なるブランチを異なる機能に使うことで、機能単位の開発とデプロイを可能にします。これにより、アプリケーションの開発チームは機能のバージョン管理とリリース管理をより効率的に行い、独立性と柔軟性を維持できます。
不正解についての説明：
選択肢：- クラウドインフラ（Terraform）リポジトリは共有されます。異なるディレクトリは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは分離されています。異なるブランチは異なる環境になるようにします
- アプリケーション（アプリのソースコード）リポジトリは分離されています。異なるブランチは異なる機能になるようにします
この選択肢が正しくない理由は以下の通りです。
GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリを分離してしまうと、各環境間でのコンフィグの共有や管理が難しくなります。
一方で、共有リポジトリを使用すれば、オーバーレイディレクトリを通じて環境ごとの違いを管理しやすくなり、全ての環境で一貫性を保つことが可能です。
選択肢：- クラウドインフラ（Terraform）リポジトリは共有されます。異なるブランチは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは共有されているいます。オーバーレイディレクトリが異なれば環境も異なるようにします
- アプリケーション（アプリのソースコード）リポジトリは共有されています。異なるディレクトリは異なる機能になるようにします
この選択肢が正しくない理由は以下の通りです。
第一に、Terraformリポジトリでは、ブランチ単位で環境を分けるのではなく、ディレクトリ単位で分けることで可読性と管理性が向上します。ブランチを使うと環境ごとの変更が追跡しづらいためです。
第二に、アプリケーションリポジトリは独立しているべきです。共有されると、特定のアプリケーションの変更が他の全てのアプリケーションに影響を及ぼす可能性があるためです。ディレクトリで機能を分けるのではなく、ブランチで分けることで各機能の変更を追跡しやすくなります。
選択肢：- クラウドインフラ（Terraform）リポジトリは共有されます。異なるブランチは異なる環境になるようにします
- GKE Infrastructure（Anthos Config Management Kustomize manifests）リポジトリは共有されているいます。オーバーレイディレクトリが異なれば環境も異なるようにします
- アプリケーション（アプリのソースコード）リポジトリは分離されています。異なるブランチは異なる機能になるようにします
この選択肢が正しくない理由は以下の通りです。
ディレクトリで環境を分けるのではなく、ブランチで環境を分ける方法を提案していますが、これは予期しないエラーや競合を引き起こす可能性があります。ブランチ間でマージが発生すると、他の環境に影響を及ぼす可能性があるためです。様々な環境に対して互いに影響を与えずに異なる設定を適用するためには、環境ごとに別のディレクトリを使用すべきです。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/anthos-config-management
https://cloud.google.com/architecture/managing-infrastructure-as-code
https://www.terraform.io/docs/providers/google/guides/getting_started.html
<details><div>

### Q. 問題3: 未回答
あるeコマースアプリケーションをGoogle Cloudに移行しました。次の繁忙期に向けてアプリケーションを準備したいと思います。
繁忙期に備えて、まず何をすべきですか？
1. 昨年の2倍のコンピュートパワーを事前にプロビジョニングしてきます
2. 本番クラスターでAutoScalingを有効にします
3. アプリケーションに負荷をかけ、スケーリングのためのパフォーマンスをプロファイリングします
4. ディザスタリカバリ（DR）環境が増大した場合のためのに、拡大に関するランブックを作成します
<details><div>
    答え：3
説明
この問題では、eコマースアプリケーションを繁忙期に向けて準備する際の最初のアクションについて問われています。特定の期間に大量のアクセスが集中すると予想されるアプリケーションに対し、その負荷を適切に処理できるように準備するための最適な方法を見つけることが求められています。各選択肢がアプリケーションのパフォーマンスとスケーラビリティをどのように改善するかを考慮し、繁忙期に適切に対応できる能力を備えたアクションを選ぶことが重要です。
基本的な概念や原則：
ロードテスト：システムのパフォーマンスや耐久性を評価するために、一定の負荷をかけるテストです。スケーリング戦略の設計や設定の調整に役立ちます。
パフォーマンスプロファイリング：アプリケーションの振る舞いを詳細に分析し、ボトルネックや問題箇所を特定するプロセスです。パフォーマンスの最適化や問題解決に寄与します。
AutoScaling：リソースの使用率に応じてインスタンス数を自動的に増減させる機能です。負荷の変動に柔軟に対応し、コスト効率とパフォーマンスを最適化します。
事前プロビジョニング：必要なリソースを事前に確保することです。スケーリングの遅延や資源不足を回避しますが、リソースが過剰になるとコストが増大します。
ディザスタリカバリ（DR）：災害や障害が発生した際にシステムを復旧するための戦略や手順です。事業継続性を保つために必要な要素です。
正解についての説明：
（選択肢）
・アプリケーションに負荷をかけ、スケーリングのためのパフォーマンスをプロファイリングします
この選択肢が正解の理由は以下の通りです。
繁忙期に向けて、まずアプリケーションのパフォーマンスを見極めることが最も重要です。そのため、負荷テストを行い、アプリケーションの性能やスケーラビリティをプロファイリングする必要があります。これによってアプリケーションが大量のリクエストに対してどのように反応するか、システムがどの程度スケールするかを評価し、それに基づいた最適な対策を考えることができます。
特に負荷テストは、実際のトラフィック状況をシミュレートして、予期せぬトラブルを事前に発見し、修正することが可能にします。
また、それを元にするとスケールアップまたはスケールダウンの戦略を立てることで、ピーク時のパフォーマンスを確保しながら、同時にコストを効率的に抑えることもできます。そのため、アプリケーションの負荷テストとスケーリングのためのパフォーマンスプロファイリングが、繁忙期に向けた準備として最初に行うべきこととなります。
不正解についての説明：
選択肢：本番クラスターでAutoScalingを有効にします
この選択肢が正しくない理由は以下の通りです。
AutoScalingを有効にするだけでは、アプリケーションのスケーリングのためのパフォーマンスが適切に設定・調整されているか確認できません。まず、アプリケーションに負荷をかけてパフォーマンスプロファイリングを行い、理想的な設定を見つけるべきです。その後でAutoScalingを適用すべきです。
選択肢：昨年の2倍のコンピュートパワーを事前にプロビジョニングしてきます
この選択肢が正しくない理由は以下の通りです。
昨年の2倍コンピュートパワーを事前にプロビジョニングすると、実際の需要に合わずリソースが無駄になる可能性があります。
それに対して、負荷をかけてパフォーマンスのプロファイリングを行うことで、実際に必要となるリソースを正確に割り出すことができます。
選択肢：ディザスタリカバリ（DR）環境が増大した場合のためのに、拡大に関するランブックを作成します
この選択肢が正しくない理由は以下の通りです。
ディザスタリカバリ（DR）環境についてのランブック作成は、緊急時の対応計画を細部まで記載したマニュアル作成のことですが、それは繁忙期の直接的な対策とは必ずしも一致せず、優先して行うべきではありません。
対照的に、負荷をかけてパフォーマンスをプロファイリングすることは、繁忙期に向けたスケーリングの効果能力と必要性を検証するために重要です。
参考リンク：
https://cloud.google.com/architecture/framework/resiliency/performing-a-load-test
https://cloud.google.com/compute/docs/autoscaler
https://cloud.google.com/solutions/disaster-recovery-cookbook
<details><div>

### Q. 問題4: 未回答
将来の分析のために、Cloud LoggingからBigQueryにログエントリをエクスポートするために、Cloud Loggingシンクを作成しています。あなたの組織には、開発プロジェクトを含むDevという名前のGoogle Cloudフォルダと、本番プロジェクトを含むProdという名前のフォルダがあります。開発プロジェクトのログエントリはdev_datasetにエクスポートする必要があり、本番プロジェクトのログエントリはprod_datasetにエクスポートする必要があります。作成されるログシンクの数を最小限に抑え、ログシンクが将来のプロジェクトにも適用されるようにする必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. 各プロジェクトにログシンクを作成します
2. 組織レベルで2つの集約ログシンクを作成し、プロジェクトIDでフィルタリングします
3. DevフォルダとProdフォルダに集約されたログシンクを作成します
4. 組織レベルで単一の集約ログシンクを作成します
<details><div>
    答え：3
説明
この問題では、Cloud LoggingからBigQueryにログをエクスポートする方法が問われています。開発プロジェクトと本番プロジェクトが分けられた環境が存在し、それぞれのログエントリを別々のデータセットにエクスポートする必要があります。ただし、今後プロジェクトが追加された際も同じルールが適用されるよう、作成するログシンクの数を最小限に抑えた構成が求められています。この点に注目し、適切な構成を選択することが求められています。具体的には、ログシンクの作成元となる階層構造と、ログエントリのフィルタリングに必要な条件を考慮に入れることが重要です。
基本的な概念や原則：
Cloud Logging：Google Cloudのログ管理サービスです。アプリケーションやサービスの動作を監視し、トラブルシューティングと詳細な分析のための詳細なログデータを提供します。
BigQuery：Google Cloudのサーバーレス、高度にスケーラブルでコスト効率の高いデータウェアハウスです。大量のデータをすばやく分析することができます。
ログシンク：Cloud Loggingにおけるエクスポートの仕組みです。ログエントリを指定した宛先（例えばBigQuery）にエクスポートします。
Google Cloudフォルダ：リソースを階層的に整理するための仕組みです。様々なプロジェクトをフォルダごとにまとめることができます。
集約ログシンク：Google Cloudフォルダや組織レベルで作成するログシンクのことです。これにより、複数のプロジェクトにわたる一貫したログ管理とエクスポートが可能となります。
フィルタリング：ログエントリを特定の基準に基づいて抽出する仕組みです。プロジェクトIDなどのフィールドでログエントリをフィルタリングすることができます。
正解についての説明：
（選択肢）
・DevフォルダとProdフォルダに集約されたログシンクを作成します
この選択肢が正解の理由は以下の通りです。
まず、"作成されるログシンクの数を最小限に抑え、ログシンクが将来のプロジェクトにも適用されるようにする必要があります"という要件があるため、フォルダレベルでのシンク作成が効果的です。各フォルダ（DevとProd）に属するプロジェクト全体のログをそれぞれの目的のデータセット（dev_datasetとprod_dataset）にエクスポートするため、フォルダレベルで１つのシンクを作成するだけで、新しいプロジェクトが追加された場合でも自動的にそれらのログシンクへのエクスポートが適用されます。
また、フォルダごとに適切なデータセットにエクスポートすることで、開発環境と本番環境のログ分析を区別しやすくなるというメリットもあります。これは、開発と本番で発生する問題やパターンが異なる可能性があるため、それぞれを別々に分析することでより具体的な洞察を得ることが可能となります。
したがって、この設定は、ログデータの取り扱いを効率化し、より具体的な分析と洞察を可能にします。
不正解についての説明：
選択肢：組織レベルで単一の集約ログシンクを作成します
この選択肢が正しくない理由は以下の通りです。
開発と本番のログエントリを別々のデータセットにエクスポートする必要がありますが、組織レベルで単一の集約ログシンクを作成すると、すべてのログが同じデータセットに送られてしまいます。
したがって、この選択肢は問題の要件を満たしません。
選択肢：各プロジェクトにログシンクを作成します
この選択肢が正しくない理由は以下の通りです。
各プロジェクトにログシンクを作成する方法は、ログシンクの数を最小限に抑えることができず、また、新規プロジェクトが追加される度に新たにログシンクを作成する必要が出てしまいます。
一方、フォルダレベルでログシンクを作成すると、そのフォルダ内の全てのプロジェクトに対して一度に適用することが可能であり、要件を満たします。
選択肢：組織レベルで2つの集約ログシンクを作成し、プロジェクトIDでフィルタリングします
この選択肢が正しくない理由は以下の通りです。
組織レベルでの集約ログシンクは適用範囲が広く、DevフォルダとProdフォルダの外部のプロジェクトまで含まれる可能性があります。
それに対して、フォルダレベルのログシンクは指定したフォルダの範囲でしか適用されないため、要件を満たすことが可能です。
参考リンク：
https://cloud.google.com/logging/docs/export/aggregated_sinks
https://cloud.google.com/logging/docs/export/configure_export_v2
https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json
<details><div>

### Q. 問題5: 未回答
あなたは、あるサービスで発生中のインシデントのオペレーションリーダーです。サービスは通常、約70%のキャパシティで稼働しています。あるノードがすべてのリクエストに対して5xxエラーを返していることに気づきました。また、顧客からのサポートケースも顕著に増加しています。ロードバランサープールから問題のノードを削除し、ノードを分離して調査する必要があります。インシデントを管理し、ユーザーへの影響を減らすために、Googleが推奨するプラクティスに従いたいと考えています。
この要件を満たすために、どうすればよいですか？
1. 1.自分の意図をインシデントチームに伝えます
2.新しいノードをプールに追加し、新しいノードが健全であると報告するのを待ちます
3.新しいノードでトラフィックが提供されるようになったら、アンヘルシーなノードからトラフィックを排出し、古いノードをサービスから削除します
2. 1.アンヘルシーなノードからトラフィックを排出し、そのノードをサービスから外します
2.トラフィックを監視して、エラーが解決され、プール内の他のノードがトラフィックを適切に処理していることを確認します
3.新しい負荷を処理するために、必要に応じてプールを拡張します
4.自分の行動をインシデントチームに伝えます
3. 1.自分の意図をインシデントチームに伝えます
2.負荷分析を実行し、残りのノードが削除されたノードからオフロードされたトラフィックの増加に対応できるかどうかを判断し、適切にスケーリングします
3.新しいノードが健全であることを報告したら、不健全なノードからトラフィックを排出し、不健全なノードをサービスから削除します
4. 1.アンヘルシーなノードからトラフィックを排出し、古いノードをサービスから削除します
2.新しいノードをプールに追加し、新しいノードが健全であると報告するのを待ち、新しいノードにトラフィックを提供します
3.プールが健全で、トラフィックを適切に処理していることを確認するために、トラフィックを監視します
4.自分の行動をインシデントチームに伝えます
<details><div>
    答え：3
説明
この問題では、サービスインシデント発生時の適切な対応策を問われています。サービスのキャパシティ、エラーメッセージ、顧客からのサポートケースの増加等の状況を踏まえ、最良の解決策を選ぶことが求められています。Googleが推奨するプラクティスに従うことが要求されているので、問題のノードの削除、新たなノードの追加、スケーリング調整など、問題対応の手順に注目して選択する事が重要です。また、効果的なインシデント管理のために、全体のコミュニケーションや意図の明示も要素として考慮に入れるべきです。
基本的な概念や原則：
インシデント管理：サービスやシステムに問題が発生した際に、その問題を解決するためのプロセスやプラクティスです。問題の特定、影響分析、対策の実施、通知、レビューなどが含まれます。
ロードバランサー：ネットワークトラフィックを複数のサーバー間で適切に分散することでシステムの性能、耐障害性、信頼性を向上させる装置やソフトウェアのことです。
5xxエラー：サーバー側に問題があることを示すHTTPステータスコードの一つです。これらのエラーは、サーバーがリクエストを処理できないことを示しています。
負荷分析：システムに対するリクエストやトラフィックなどの負荷を分析し、パフォーマンスや耐障害性を評価または改善するためのプロセスです。
スケーリング：システムのリソースを増減することでパフォーマンスを調整する手法です。需要に応じてリソースを柔軟に増減することで、システムの効率とコスト効率を最適化することができます。
健全性チェック：システムやサービスが正常に稼働しているかを確認するためのプロセスまたは手段です。異常がある場合に早期に検出し、対処することを可能にします。
正解についての説明：
（選択肢）
・1.自分の意図をインシデントチームに伝えます
2.負荷分析を実行し、残りのノードが削除されたノードからオフロードされたトラフィックの増加に対応できるかどうかを判断し、適切にスケーリングします
3.新しいノードが健全であることを報告したら、不健全なノードからトラフィックを排出し、不健全なノードをサービスから削除します
この選択肢が正解の理由は以下の通りです。
まず、自分の意図をインシデントチームに伝えることで、全体の計画を理解した上で適切なアクションを取ることができ、誤解や混乱を防ぐことができます。これは、チームが一体となって問題を解決するための重要なステップです。
次に、負荷分析を実行し、残りのノードがオフロードされたトラフィックの増加に対応できるかどうかを判断することは、サービスの可用性を保つために不可欠です。サービスが通常70%のキャパシティで動作しているとしても、特定のノードを削除すると、その他のノードに対する負荷が増加します。この負荷増加が残りのノードの容量を超えると、サービス全体がダウンする可能性があるので、事前に分析・評価を行うことが重要です。
最後に、新しいノードが健康であることを確認した上で、アンヘルシーなノードを取り外すことで、サービスの中断を最小限に抑えつつ問題の分析と修正を進めることが可能になります。これにより、ユーザーへの影響を最小限に抑えられます。
以上から、この解答が正しいと言えます。
不正解についての説明：
選択肢：1.自分の意図をインシデントチームに伝えます
2.新しいノードをプールに追加し、新しいノードが健全であると報告するのを待ちます
3.新しいノードでトラフィックが提供されるようになったら、アンヘルシーなノードからトラフィックを排出し、古いノードをサービスから削除します
この選択肢が正しくない理由は以下の通りです。
新しいノードを追加する前に、残ったノードが追加のトラフィックを処理できるかどうかを評価することが重要です。新しいノードが健全かを確認せずに追加すると、問題が悪化する可能性があります。正解の選択肢では、適切にスケーリングするために負荷分析を実行することを含んでいるため、より適切な対応策となります。
選択肢：1.アンヘルシーなノードからトラフィックを排出し、そのノードをサービスから外します
2.トラフィックを監視して、エラーが解決され、プール内の他のノードがトラフィックを適切に処理していることを確認します
3.新しい負荷を処理するために、必要に応じてプールを拡張します
4.自分の行動をインシデントチームに伝えます
この選択肢が正しくない理由は以下の通りです。
まず、最初に不健全なノードをサービスから削除すると、残りのノードが追加のトラフィックを処理できるかどうかを確認せずに、サービスへの影響が予測できません。
また、行動をインシデントチームに伝えるのは最後であるため、彼らが問題を理解して対応する機会が減り、より効果的なインシデント管理が難しくなります。
選択肢：1.アンヘルシーなノードからトラフィックを排出し、古いノードをサービスから削除します
2.新しいノードをプールに追加し、新しいノードが健全であると報告するのを待ち、新しいノードにトラフィックを提供します
3.プールが健全で、トラフィックを適切に処理していることを確認するために、トラフィックを監視します
4.自分の行動をインシデントチームに伝えます
この選択肢が正しくない理由は以下の通りです。
まず、インシデントの管理はチームに対する意図の明確な伝達から始めるべきですが、この選択肢ではノードの削除が最初に行われてしまっています。
次に、新しいノードの追加は装置の健全性を確認してから行い、そして最後にチームに行動を伝えるという手順は逆であるため、効率的なインシデント管理方法とは言えません。
参考リンク：
https://cloud.google.com/load-balancing/docs
https://cloud.google.com/compute/docs/instance-groups/removing-an-instance
https://cloud.google.com/architecture/framework/incident-response
<details><div>

### Q. 問題6: 未回答
あなたは、Google Kubernetes Engine（GKE）で実行され、ブルー/グリーンデプロイメント手法を使用するアプリケーションを管理しています。Kubernetesマニフェストの抜粋を以下に示します：
```
--
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-green
  labels:
    app: cloudjp-app
    version: green
<other fields snipped>
--
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-blue
  labels:
    app: cloudjp-app
    version: blue
<other fields snipped>
--
apiVersion: v1
kind: Service
metadata:
  name: app-svc
spec:
  selector:
    app: cloudjp-app
    version: green
<other fields snipped>
--
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
spec:
  defaultBackend:
    service:
      name: app-svc
<other fields snipped>
```
デプロイメントapp-greenは、アプリケーションの新しいバージョンを使用するように更新されました。デプロイ後のモニタリング中に、ユーザーリクエストの大半が失敗していることに気づきました。テスト環境では、この動作は観察されませんでした。ユーザへのインシデントの影響を軽減し、開発者が問題をトラブルシューティングできるようにする必要があります。
この要件を満たすために、どうすればよいですか？
1. 新しいバージョンのアプリケーションを使用するように、Deployment app-blueを更新します
2. Deployment app-greenを更新し、以前のバージョンのアプリケーションを使用します
3. サービスapp-svcのセレクタをapp: cloudjp-app、version: blueに変更します
4. サービスapp-svcのセレクタをapp: cloudjp-appに変更します
<details><div>
    答え：3
説明
この問題では、Kubernetesのブルー/グリーンデプロイメント戦略を用いたアプリケーション運用について、新バージョンデプロイ後のユーザーリクエスト大半失敗という問題を迅速に軽減しつつ開発者がトラブルシューティングできる状況を整える適切な対応を問われています。答えを見つけるには、ブルー/グリーンデプロイメントの戦略とKubernetesのServiceとIngressの振る舞い・設定の操作に関する理解が求められます。
基本的な概念や原則：
Google Kubernetes Engine（GKE）：管理された環境でKubernetesアプリケーションをデプロイ、マネージ、スケーリングするためのGoogle Cloudサービスです。
ブルー/グリーンデプロイメント：あるバージョンのアプリケーション（"ブルー"）が稼働している間に、新しいバージョンのアプリケーション（"グリーン"）を同時にデプロイし、全てが準備できたらトラフィックを新しいバージョンに切り替えるデプロイメント戦略です。
Kubernetesマニフェスト：Kubernetesオブジェクトの設定を記述するためのYAMLまたはJSON形式のファイルです。
デプロイメント：アプリケーションのリリースや更新を管理するKubernetesオブジェクトです。ステートレスなアプリケーションのローリングアップデートとロールバックをサポートします。
サービス：Kubernetesの内部または外部のネットワークトラフィックをポッドにルーティングする抽象化されたAPIオブジェクトです。
セレクタ：Kubernetesオブジェクトのキーバリュー組を指定して、一連の他のオブジェクトを識別する方法です。サービスはセレクタを使用してルーティング先のポッドを識別します。
正解についての説明：
（選択肢）
・サービスapp-svcのセレクタをapp: cloudjp-app、version: blueに変更します
この選択肢が正解の理由は以下の通りです。
この問題では、ブルー/グリーンデプロイメント戦略が使用されており、新しいバージョン（green）へのデプロイが行われた後に多くのユーザーリクエストが失敗しています。現在の設定では、サービスapp-svcが新しいバージョン（green）を指しているため、ユーザーリクエストは新しいバージョンに送られています。この失敗を取り消すためには、サービスのセレクタを古いバージョン（blue）に変更すれば、旧バージョンのアプリをプロイスにルーティングします。これにより、ユーザーへの影響を最小限に抑えつつ、開発者が新バージョンでの問題をトラブルシューティングできるようになります。
したがって、サービスapp-svcのセレクタをapp: cloudjp-app、version: blueに変更すべきです。
不正解についての説明：
選択肢：新しいバージョンのアプリケーションを使用するように、Deployment app-blueを更新します
この選択肢が正しくない理由は以下の通りです。
問題のシナリオでは、新しいバージョンに問題があると認識されています。そのため、app-blueも同様の新しいバージョンに更新すると、同じ問題が発生し、システム全体の停止を招く恐れがあります。
一方、正解の選択肢では、バージョンを古いものに戻すことで、システム全体の影響を軽減しています。
選択肢：Deployment app-greenを更新し、以前のバージョンのアプリケーションを使用します
この選択肢が正しくない理由は以下の通りです。
Deployment app-greenを以前のバージョンにロールバックすると、新しいバージョンのアプリケーションをトラブルシューティングする機会が失われます。
一方で、サービスのセレクタを変更すると、ユーザーの影響を最小限に抑えながら、開発者は引き続き新しいバージョンのアプリケーションをテストすることができます。
選択肢：サービスapp-svcのセレクタをapp: cloudjp-appに変更します
この選択肢が正しくない理由は以下の通りです。
まず、サービスのセレクタをapp: cloudjp-appに変更すると、サービスは"blue"と"green"両方のデプロイメントにトラフィックをルーティングするようになります。これは問題のある"green"デプロイメントからトラフィックを完全に移行させる目的を達成できません。
一方、サービスのセレクタをapp: cloudjp-app、version: blueに変更すれば、"blue"デプロイメントだけにトラフィックをルーティングし、ユーザーの問題を即座に軽減できます。
参考リンク：
https://cloud.google.com/kubernetes-engine/docs/concepts/service
https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps
https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors
<details><div>

### Q. 問題7: 未回答
トラフィックの多い複数地域のウェブアプリケーションのサービスレベル目標（SLO）を定義する必要があります。顧客はアプリケーションが常に利用可能で、レスポンスタイムが速いことを期待しています。現在、顧客はアプリケーションのパフォーマンスと可用性に満足しています。現在の測定に基づくと、28日間の待ち時間の90パーセンタイルは120ms、95パーセンタイルは275msです。
どのようなレイテンシSLOを公表するようチームに推奨しますか？
1. 90パーセンタイル：150ms、95パーセンタイル：300ms
2. 90パーセンタイル：100ms、95パーセンタイル：250ms
3. 90パーセンタイル：120ms、95パーセンタイル：275ms
4. 90パーセンタイル：250ms、95パーセンタイル：400ms
<details><div>
    答え：1
説明
この問題では、現在のパフォーマンスレベルと顧客の期待を両立するSLO（Service Level Objective）を公表することでサービスの品質を維持することが求められています。実測値として90パーセンタイルで120ms、95パーセンタイルで275msとのデータが提示されているため、これが事実上のパフォーマンス水準を示しています。ただし、公表するSLOはこの実績データだけでなく、将来のパフォーマンス変動の余地も考慮して設定しなければなりません。
基本的な概念や原則：
サービスレベル目標（SLO）：サービスの性能や可用性の目標を定量的に設定したものです。これによって、サービスの品質を保証し、ユーザーの期待に対してサービスが準拠することを定義します。
レイテンシ：システムがリクエストに応答するまでの時間を指します。低いレイテンシは、システムが応答速度が速いことを示します。
パーセンタイル：データ分布の特定の位置を示す統計的指標です。90パーセンタイルは、すべての観測値の90%がその値以下であることを意味します。
SLOの設定：現在のパフォーマンスデータを基に設定しなければならず、達成可能であることが重要です。また、SLOは顧客の期待を超えるよう設定することが望ましいです。期待を超えたSLOを設定することにより、システムのパフォーマンスが一時的に低下したとしても、顧客の満足度は維持されます。
正解についての説明：
（選択肢）
・90パーセンタイル：150ms、95パーセンタイル：300ms
この選択肢が正解の理由は以下の通りです。
まず、SLO（サービスレベル目標）の設定はサービスの品質を維持し確保するために重要ですが、理想的な目標だけではなく、現状の運用実態に基づいた実現可能な目標を設定することが求められます。この問題においては、現在の90パーセンタイルでの待ち時間が120ms、95パーセンタイルでの待ち時間が275msで、顧客はこのパフォーマンスと可用性に満足しています。
そのため、オペレーションの余裕を見込みつつも、現状のパフォーマンスを維持することを優先するべきです。その結果、90パーセンタイルでの待ち時間を150ms、95パーセンタイルでの待ち時間を300msと設定することは適切です。これらは現在の実績と大きく差がなく、顧客の満足を保ちつつ、約束できるレベルの余地を確保しています。
不正解についての説明：
選択肢：90パーセンタイル：100ms、95パーセンタイル：250ms
この選択肢が正しくない理由は以下の通りです。
不正解の選択肢が推奨できないのは、既存の測定値よりも厳しい目標を設定し、それは現在の性能を反映していません。
一方、正解の選択肢は現状の測定値を余裕をもって上回る値を設定しており、顧客の期待を裏切ることなく、同時に予期せぬ問題が発生した場合のギャップを確保しています。
選択肢：90パーセンタイル：120ms、95パーセンタイル：275ms
この選択肢が正しくない理由は以下の通りです。
現在の測定値と同等のSLOを設定すると、どんな小さなパフォーマンス変動でもSLOを満たさない状況を引き起こします。適切な運用を保つためには、余裕をもった目標を設定することが重要で、正解の選択肢はその考え方を反映しています。
選択肢：90パーセンタイル：250ms、95パーセンタイル：400ms
この選択肢が正しくない理由は以下の通りです。
SLOは顧客の期待を反映し、かつ達成可能であるべきで、その基準は現在のパフォーマンスを考慮に入れます。測定結果から90パーセンタイルの待ち時間は120ms、95パーセンタイルは275msであり、これを超える250msと400msの設定では、遅延が増加し、顧客の満足度を低下させる恐れがあります。
それに対して、正解の選択では、適切なバッファを設けつつも顧客の期待を満たす値になっています。
参考リンク：
https://cloud.google.com/architecture/framework/reliability/slos-measuring-managing
https://cloud.google.com/monitoring/alerts/concepts-indepth
https://sre.google/workbook/implementing-slos/
<details><div>

### Q. 問題8: 未回答
あなたの会社のセキュリティチームは、_Requiredバケットにあるデータアクセス監査ログに読み取り専用でアクセスする必要があります。あなたは、最小権限の原則とGoogleが推奨するプラクティスに従って、セキュリティチームに必要な権限を与えたいと考えています。
あなたはこの要件を満たすために、どうすればよいですか？
1. セキュリティチームの各メンバーにroles/logging.viewerロールを割り当てます
2. セキュリティチームメンバー全員のいるグループに、roles/logging.viewerロールを割り当てます
3. セキュリティチームの各メンバーにroles/logging.privateLogViewerロールを割り当てます
4. roles/logging.privateLogViewerロールを、セキュリティチームメンバ全員のいるグループに割り当てます
<details><div>
    答え：4
説明
この問題では、セキュリティチームが監査ログに読み取り専用でアクセスできるようにするための最適な方法を判断する必要があります。問題文からは、最小権限の原則とGoogleが推奨するプラクティスに従うことが要求されています。また、与えるべき適切な権限とその割り当て方に注目が必要です。詳細に留意することで、個々の選択肢が要件を満たすかどうかを判断することができます。
基本的な概念や原則：
最小権限の原則：ユーザー、プログラム、プロセスが必要最低限の権限だけを持つべきというセキュリティ原則です。これにより、誤操作や悪意のある攻撃からシステムを守ります。
roles/logging.privateLogViewer：このロールは、データアクセス監査ログを含むプライベートログエントリの読み取りを許可します。これにより、システムのアクセスを監視し、異常を検出することができます。
グループでの権限の割り当て：Google Cloudでは、管理作業を効率的に行うために、同じロールを持つ複数のユーザーを一つのグループにまとめて、そのグループに一度に権限を付与することが推奨されます。
roles/logging.viewer：このロールは、すべてのログエントリの読み取りを許可しますが、データアクセス監査ログは含まれません。よって、必要最低限のアクセス権限を遵守するためには、roles/logging.privateLogViewerの使用が適切です。
正解についての説明：
（選択肢）
・roles/logging.privateLogViewerロールを、セキュリティチームメンバ全員のいるグループに割り当てます
この選択肢が正解の理由は以下の通りです。
まず、"roles/logging.privateLogViewer"は、プライベートログエントリ（アクセス可能なものはプロジェクトのアクティビティや監査ログなど）の読み取りアクセスを適切に提供します。これは"最小限の権限の原則"に基づいて適切なロールです。この原則は、ユーザーに必要最小限の権限だけを付与し、それ以上の権限は付与しないというセキュリティのベストプラクティスです。
また、ロールを特定の人ではなく、セキュリティチーム全体のグループに割り当てるというのは、Google Cloudの推奨するベストプラクティスに沿った行動です。これにより、新しいメンバーがチームに加わるたびに権限を手動で付与したり、メンバーが離脱した時に手動で権限を剥奪するといった管理作業を削減できます。これは、効率的な管理とセキュリティ上のリスクを減らすための良い手法です。
不正解についての説明：
選択肢：セキュリティチームの各メンバーにroles/logging.viewerロールを割り当てます
この選択肢が正しくない理由は以下の通りです。
roles/logging.viewerのロールはプロジェクトレベルで読み取りアクセスを許可するので、全てのログを見ることが可能となり最小権限の原則に反します。
一方、roles/logging.privateLogViewerはデータアクセス監査ログの読み取りアクセスを許可する限定的なロールであり、要件を満たすことができます。
選択肢：セキュリティチームメンバー全員のいるグループに、roles/logging.viewerロールを割り当てます
この選択肢が正しくない理由は以下の通りです。
roles/logging.viewerロールではプライベートログを表示する権限がありません。
一方、roles/logging.privateLogViewerロールはプライベートログの表示権限を含んでおり、セキュリティチームが必要とするデータアクセス監査ログの読み取りに対応しています。
選択肢：セキュリティチームの各メンバーにroles/logging.privateLogViewerロールを割り当てます
この選択肢が正しくない理由は以下の通りです。
セキュリティチームの各メンバーに直接ロールを割り当てるのではなく、セキュリティチームメンバー全員が所属するグループにロールを割り当てる方が効率的で管理が容易です。個々のメンバーに直接ロールを付与すると、メンバーの追加や削除が発生した際に手動でロールの付与や剥奪を行う必要があり、管理が煩雑になるからです。
参考リンク：
https://cloud.google.com/logging/docs/access-control
https://cloud.google.com/iam/docs/understanding-roles#logging-roles
https://cloud.google.com/logging/docs/audit#data-access-logs
<details><div>

### Q. 問題9: 未回答
あなたの会社では、本番システムにバグや停止、遅れが発生しています。開発者は新機能の開発やバグ修正のために本番環境を使用しています。設定やテストが本番環境で行われ、ユーザーに障害が発生します。テスターは本番環境を負荷テストに使用し、本番システムの速度をしばしば低下させます。本番環境でのバグや停止を減らし、テスターが新機能をテストできるように環境を再設計する必要があります。
この要件を満たすために、どうすればよいですか？
1. 開発者が変更できないように本番環境を保護し、年に1回の管理されたアップデートを設定します
2. 本番環境で自動テストスクリプトを作成し、障害が発生したらすぐに検出します
3. サーバーの容量を小さくして開発環境を作り、開発者とテスターだけにアクセス権を与えます
4. コードを書くための開発環境と、設定とテスト、負荷テストのためのテスト環境を作ります
<details><div>
    答え：4
説明
この問題では、会社のソフトウェア開発とテスト環境における問題を解決するための最良のアプローチを選択することを求められています。特に、開発者とテスターが本番環境で作業するという現行のプロセスが、アプリケーションへのバグや停止を引き起こす問題にどのように対処するかが問われています。つまり、新機能の開発やバグ修正、設定とテスト、負荷テストを行うための適切な環境が必要で、それが本番環境とは別の環境であるべきということを理解することが重要です。
基本的な概念や原則：
開発環境：開発者が新機能の開発やバグ修正を進めるための環境です。プロダクション（本番）環境の影響を受けずに作業を進めることができます。
テスト環境：新機能や修正が期待通りに動作することを確認するための環境です。開発環境とは別に設定され、負荷テストや絵合わせテストを実施する場所です。
プロダクション環境：実際にユーザーが使用する本番のシステム環境です。バグや停止が発生するとユーザー体験に影響を及ぼします。
負荷テスト：システムが高負荷状態でも適切に動作するかを確認するためのテストです。テスト環境で行われ、本番環境に影響を及ぼさないようにします。
自動テストスクリプト：手動のテストプロセスを自動化するためのスクリプトです。効率的なテストを実現しますが、本番システムへの影響を考慮する必要があります。
正解についての説明：
（選択肢）
・コードを書くための開発環境と、設定とテスト、負荷テストのためのテスト環境を作ります
この選択肢が正解の理由は以下の通りです。
本番環境での開発やテストを行うと、バグや停止、パフォーマンスの低下といった問題がユーザーに影響を与える可能性があります。これは、ユーザー体験を悪化させ、ビジネスにおける信頼性を損なう可能性があります。本番環境とは別に開発環境とテスト環境を設定することで、開発者が新機能の開発やバグ修正を行い、テスターが設定、テスト、負荷テストを行うことができます。これにより、新機能のテストやバグ修正がユーザーに影響を与えることなく、本番環境に影響を及ぼすことなく進行できます。
また、本番環境のパフォーマンスを向上させ、システムの停止や遅延を減らすことができます。
一方で、開発やテストが完成したら、これらの変更を本番環境へ安全に移行することが可能になります。これは、本番環境でのバグや停止を減らし、テスターが新機能をテストできるようにするための最善の解決策です。
不正解についての説明：
選択肢：本番環境で自動テストスクリプトを作成し、障害が発生したらすぐに検出します
この選択肢が正しくない理由は以下の通りです。
自動テストスクリプトを本番環境で作成し、障害を早期検出する方法は、既存問題の検出は可能ですが、バグの発生や本番環境の停止を防ぐ方法とはなりません。
一方、開発環境とテスト環境を分ける選択肢は、本番環境への影響を減らすという要件を満たします。
選択肢：サーバーの容量を小さくして開発環境を作り、開発者とテスターだけにアクセス権を与えます
この選択肢が正しくない理由は以下の通りです。
単に開発環境のサーバー容量を小さくし、開発者とテスターだけがアクセスできるようにするだけでは、バグや遅延などの問題が本番環境で発生する問題を解決できません。必要なのは開発環境だけでなく、テスト環境も分けることで、バグや停止を減らし、安全に新機能のテストが行えることです。
選択肢：開発者が変更できないように本番環境を保護し、年に1回の管理されたアップデートを設定します
この選択肢が正しくない理由は以下の通りです。
開発者が変更できないように本番環境を保護し、年に1回のアップデートを設定する方法では、新機能の開発やバグ修正の速度や効率が大幅に下がる可能性があります。
また、負荷テストのために別の環境が必要となります。
対照的に、開発用とテスト用の2つの環境を設けることで、本番環境でのバグや停止を減らし、テスターが新機能をテストできるようになります。
参考リンク：
https://cloud.google.com/solutions/dev-test-environments
https://cloud.google.com/architecture/devops/devops-tech-test-automation
https://cloud.google.com/architecture/continuous-integration-workloads-on-Google Cloud-with-spinnaker
<details><div>

### Q. 問題10: 未回答
あなたの会社では、GitOps手法に従ってデプロイされたGoogle Kubernetes Engine（GKE）でアプリケーションを実行しています。アプリケーション開発者は、アプリケーションをサポートするためにクラウドリソースを頻繁に作成します。あなたは、Googleが推奨するプラクティスに確実に従いながら、開発者がインフラストラクチャをコードとして管理できるようにしたいと考えています。コンフィギュレーションのドリフトを避けるために、インフラストラクチャをコードとして定期的に調整する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. Terraform builderでCloud Buildを構成し、terraform planとterraform applyコマンドを実行します
2. Terraform planとterraform applyコマンドを実行するためのTerraform dockerイメージでポッドリソースを作成します
3. Google Kubernetes Engine（GKE）にConfig Connectorをインストールして設定します
4. terraform planコマンドとterraform applyコマンドを実行するためのTerraform dockerイメージでJobリソースを作成します
<details><div>
    答え：3
説明
この問題では、Google Kubernetes Engine（GKE）でアプリケーションを実行している状況下で、開発者がインフラストラクチャをコードとして管理するための最善の手法を選ぶことが求められています。ここで重要な点は、Googleが推奨するプラクティスを守りつつ、コンフィギュレーションのドリフトを避けることです。選択肢には、GKEに対する設定手段やTerraformを使用したインフラストラクチャのコード化を行う手法等があるので、これらの選択肢を用いて、Googleの推奨プラクティスに沿いつつ、開発者が容易にインフラストラクチャ管理ができる解答を選ぶべきです。
基本的な概念や原則：
GitOps：インフラストラクチャ管理のためのオペレーションプラクティスです。Gitを単一のソースとして使用し、デプロイメントやアップデートを自動化することで、迅速で信頼性の高いシステム変更を可能にします。
Google Kubernetes Engine（GKE）：Google CloudのマネージドKubernetesサービスです。アプリケーションをコンテナ化し、スケーリングと自動更新などの操作を行うことができます。
Config Connector：Google Cloudのサービスで、Google CloudリソースをKubernetesオブジェクトとして管理します。これにより、Kubernetesを使用してGoogle Cloudリソースとインフラを管理することができます。
インフラストラクチャ管理：インフラリソース（サーバ、ストレージ、ネットワークなど）をプロビジョニング、設定、運用するプロセスです。
Cloud Build：Google Cloudのサービスで、ソースコードからコンテナイメージやアプリケーションをビルドします。
Terraform：インフラストラクチャをコード化し、クラウドサービス、オンプレミスのリソースなどを安全かつ効率的にプロビジョニングするためのオープンソースのツールです。
コンフィギュレーションドリフト：運用環境の設定が、基準となる設定から時間とともに変わっていく現象です。これにより予期しない問題やエラーが発生する可能性があります。
正解についての説明：
（選択肢）
・Google Kubernetes Engine（GKE）にConfig Connectorをインストールして設定します
この選択肢が正解の理由は以下の通りです。
Google Kubernetes Engine（GKE）のConfig Connectorは、KubernetesリソースのようにGoogle Cloudリソースを管理するためのツールであり、これを用いることで開発者はインフラストラクチャをコードとして管理できます。GitOpsの手法は、ソースコードリポジトリを単一のソースとし、その変更が自動的に本番環境へデプロイされる方法論であり、Config Connectorをインストールすることで、そのアプローチをリソースとコンフィグの管理にも適用することが可能となります。
さらに、Config Connectorを使用すると、各リソースの望ましい状態がコードで定義されているため、定期的に適用することでコンフィギュレーションドリフトを防ぐことが可能となります。
このように、Config ConnectorはGitOps手法と相性が良く、Googleの推奨するプラクティスに従う結果となります。
不正解についての説明：
選択肢：Terraform builderでCloud Buildを構成し、terraform planとterraform applyコマンドを実行します
この選択肢が正しくない理由は以下の通りです。
Terraformを使用すると、インフラストラクチャの定義をコードとして管理できますが、GitOpsの原則に基づく自動同期機能は持っていません。そのため、手動での適用が必要となり、設定のドリフトの可能性が高まります。
一方、Config Connectorを使用すれば、GKE上でインフラストラクチャを直接管理でき、GitOpsの手法に自然に合致します。
選択肢：Terraform planとterraform applyコマンドを実行するためのTerraform dockerイメージでポッドリソースを作成します
この選択肢が正しくない理由は以下の通りです。
Terraformを用いたポッドリソースの作成はGitOpsメソッドに沿っていません。GitOpsはリポジトリを単一情報源とし、変更はプルリクエストによるレビューを通じて承認されます。明示的なTerraformコマンドに頼るアプローチは、この原則に反します。GKEにConfig Connectorを設定することは、Kubernetesリソースとしてクラウドリソースを直接管理するように設計されているため、GitOpsのスタイルに適合します。
選択肢：terraform planコマンドとterraform applyコマンドを実行するためのTerraform dockerイメージでJobリソースを作成します
この選択肢が正しくない理由は以下の通りです。
Terraform dockerイメージでJobリソースを作る方法は調整のために手動でコマンドを実行する必要があり、コンフィギュレーションドリフトの自動対策にはなりません。
一方、Config Connectorは自動化に適しており、Googleが推奨しているプラクティスも満たせます。
参考リンク：
https://cloud.google.com/config-connector/docs/overview
https://cloud.google.com/kubernetes-engine/docs/tutorials/config-connector
https://cloud.google.com/anthos-config-management/docs/tutorials/gitops-cloud-build
<details><div>

### Q. 問題11: 未回答
あなたの会社は、サイト信頼性エンジニアリングの原則に従っています。あなたは、ユーザーに深刻な影響を与えたソフトウェア変更に起因するインシデントのポストモーテムを書いています。あなたは、今後このような深刻なインシデントが発生しないようにしたいと考えています。
この要件を満たすために、どうすればよいですか？
1. 新しいソフトウェアがリリースされる前に、この種のエラーを検出するテストケースが正常に実行されるようにします
2. インシデントの責任者であるエンジニアを特定し、上級管理職にエスカレーションします
3. 変更を検討した従業員をフォローアップし、今後従うべき慣行を規定します
4. インシデントが発生した場合、常駐運用チームが直ちにエンジニアと経営陣に連絡し、対応策を協議することを義務付ける方針を策定します
<details><div>
    答え：1
説明
この問題では、サイト信頼性エンジニアリング（SRE）原則とソフトウェア変更が引き起こした深刻なインシデントに対するアプローチを理解することが求められています。"サイト信頼性エンジニアリング"は故障を防止し、システムの高可用性を維持する重要な手法です。したがって、正解は個々のエンジニアの責任を問うものではなく、エラーを防ぐシステムまたはプロセスを強化するための長期的な解決策を推奨するものであるべきです。
基本的な概念や原則：
サイト信頼性エンジニアリング（SRE）：サービスやシステムの信頼性、スケーラビリティ、およびパフォーマンスを向上させることを目指すソフトウェアエンジニアリングの一形態です。
ポストモーテム：インシデント後のレビュープロセスで、インシデント発生の原因と結果、対応した解決策、そしてそれをどのように予防または改善するかについて詳しく分析、記録します。
ソフトウェアテスト：新しいソフトウェアや変更が期待通りに機能すること、またユーザーやシステムに悪影響を与えないことを確認するプロセスです。
テストケース：特定の入力が与えられたときにソフトウェアが期待通りに機能することを確認するステップの集合です。
正解についての説明：
（選択肢）
・新しいソフトウェアがリリースされる前に、この種のエラーを検出するテストケースが正常に実行されるようにします
この選択肢が正解の理由は以下の通りです。
インシデントが発生した後のポストモーテムでは、問題の原因を特定し、そのインシデントが再発しないように何を改善すべきかを明確にすることが重要です。
そして、その改善策としてテストケースの適用が挙げられます。テストケースを導入することで、新たにリリースするソフトウェアに同様のエラーが発生する可能性を未然にチェックし、防ぐことができます。特に、新しいソフトウェアがリリースされる前にエラーを検出するというのは、問題が実際のプロダクトに影響を与える前に、開発段階で修正を行うことができるため、ユーザーに深刻な影響を及ぼすことを避けられ、また修正コストも大幅に削減されます。そのため、この選択肢が最も適切な対応となります。
不正解についての説明：
選択肢：インシデントの責任者であるエンジニアを特定し、上級管理職にエスカレーションします
この選択肢が正しくない理由は以下の通りです。
SREの原則は個々の人間のエラーを責めるものではなく、システムの弱点を明らかにし、専門家によってそれを理解し改善することを重視します。
したがって、エンジニアを特定し上級管理職にエスカレーションすることは効果的ではありません。
一方、エラーを検出できるテストケースを事前に実行することはシステムの改善につながります。
選択肢：変更を検討した従業員をフォローアップし、今後従うべき慣行を規定します
この選択肢が正しくない理由は以下の通りです。
変更を検討した従業員をフォローアップし、今後従うべき慣行を規定する方法は個々の行動改善に焦点を当て過ぎており、システム的な対策にはなりません。
それに対して、エラーを検出するテストケースを実行することで、具体的なシステム改善に繋がります。
選択肢：インシデントが発生した場合、常駐運用チームが直ちにエンジニアと経営陣に連絡し、対応策を協議することを義務付ける方針を策定します
この選択肢が正しくない理由は以下の通りです。
不正解の選択肢はインシデントの発生後の対応策に焦点を当てていますが、問題の要件は深刻なインシデント発生防止に関するものです。正解は新しいソフトウェアリリース前にエラーを検出し、事前に問題を防ぐ対策を取ることで要件を満たします。
参考リンク：
https://cloud.google.com/sre/workbook/implementing
https://cloud.google.com/sre/workbook/postmortem-culture
https://cloud.google.com/devops
<details><div>

### Q. 問題12: 未回答
あなたはCloud Run上で動作するアプリケーションを構築しています。アプリケーションはAPIキーを使用してサードパーティAPIにアクセスする必要があります。Googleが推奨するプラクティスに従って、アプリケーション内でAPIキーを安全に保存し使用する方法を決定する必要があります。
あなたはこの要件を満たすために、どうすればよいですか？
1. Secret ManagerでAPIキーをシークレットキーとして保存します。シークレットキーを /sys/api_keyディレクトリの下にマウントし、Cloud Runアプリケーションでキーを復号化します
2. APIキーをSecret Managerにシークレットとして保存します。シークレットをCloud Runアプリケーションの環境変数として参照します
3. APIキーをキーとしてCloud Key Management Service（Cloud KMS）に保存します。Cloud Runアプリケーションの環境変数としてキーを参照します
4. Cloud Key Management Service（Cloud KMS）を使用してAPIキーを暗号化し、そのキーを環境変数としてCloud Runに渡します。暗号化解除し、Cloud Runでキーを使用します
<details><div>
    答え：2
説明
この問題では、APIキーを安全に保存してアプリケーションから利用する最善の方法をGoogleの推奨プラクティスに基づいて選択することが求められています。Cloud Run上で動作するアプリケーションの文脈で設問が出されていることから、Cloud Runと親和性の高いGoogle Cloudのサービスを利用する選択肢が有効である可能性が高いです。また、選択肢の中にはSecret ManagerやCloud Key Management Service等の様々なサービスが出てきますが、各サービスが提供する機能とそのロールを理解することが重要です。
基本的な概念や原則：
Cloud Run：Googleのフルマネージドなサーバーレス環境の一つで、コンテナイメージをベースにアプリケーションを実行します。
APIキー：特定のAPIを呼び出すために使用される識別子です。APIキーは、所有者のアクションを追跡し、APIの使用状況を制限するために使用されます。
Secret Manager：Google Cloudのサービスで、センシティブなデータ（パスワード、APIキー、証明書など）を安全に管理し、アクセスするためのツールです。
環境変数：アプリケーションの実行環境で利用可能な値で、設定情報やシステムプロパティを保持します。安全な管理が求められる情報を含む場合もあります。
Cloud Key Management Service（Cloud KMS）：Google Cloudの暗号キー管理サービスで、暗号化キーの作成、使用、管理、インポート、回転、削除を安全に行うことが可能です。
暗号化・復号化：データの安全性を確保するための手段で、暗号化はデータを読み取れない形式に変換し、復号化はその逆のプロセスです。
正解についての説明：
（選択肢）
・APIキーをSecret Managerにシークレットとして保存します。シークレットをCloud Runアプリケーションの環境変数として参照します
この選択肢が正解の理由は以下の通りです。
まず、Google CloudのSecret Managerは、シークレット（APIキーなどの機密情報）を安全に管理し、アクセスするためのサービスです。APIキーをSecret Managerに保存することで、APIキーのセキュリティが強化され、認証やアクセス制御などの安全な管理が可能になります。
また、Cloud Runは、環境変数を使用してアプリケーションに構成情報を提供します。シークレットをCloud Runアプリケーションの環境変数として参照すると、アプリケーションはAPIキーを自身のコード内に含めることなく、必要に応じてシークレットをCloud Runの動作環境から取得することが可能になります。これにより、機密情報の取り扱いが安全になり、リスクが低減されます。
これらの理由から、APIキーをSecret Managerに保存し、それをCloud Runアプリケーションの環境変数として参照することは、Googleが推奨するプラクティスに従った、安全で効率的な方法と言えます。
不正解についての説明：
選択肢：Secret ManagerでAPIキーをシークレットキーとして保存します。シークレットキーを /sys/api_keyディレクトリの下にマウントし、Cloud Runアプリケーションでキーを復号化します
この選択肢が正しくない理由は以下の通りです。
Cloud Runは現在、シークレットを特定のディレクトリに直接マウントする機能をサポートしていません。これは、正解の選択肢のように、シークレットを環境変数として参照する方法とは異なります。
また、この選択肢では、不必要に複雑さが増すため、Googleの推奨するシンプルで安全なアプローチから外れてしまいます。
選択肢：APIキーをキーとしてCloud Key Management Service（Cloud KMS）に保存します。Cloud Runアプリケーションの環境変数としてキーを参照します
この選択肢が正しくない理由は以下の通りです。
Cloud KMSは暗号化キーの生成と管理のためのサービスであり、APIキーの保存用途には適していません。
一方、Secret Managerは秘密や認証情報の安全な保管・管理を提供するサービスであり、APIキーの保管に適しています。
選択肢：Cloud Key Management Service（Cloud KMS）を使用してAPIキーを暗号化し、そのキーを環境変数としてCloud Runに渡します。暗号化解除し、Cloud Runでキーを使用します
この選択肢が正しくない理由は以下の通りです。
Cloud KMSは暗号キーの管理に使用しますが、APIキーの世話や環境変数としての利用を容易にするわけではありません。
対照的に、Secret Managerは秘密情報を安全に保存し管理するためのサービスで、それらを環境変数として参照することが可能です。
参考リンク：
https://cloud.google.com/secret-manager/docs/overview
https://cloud.google.com/run/docs/configuring/secrets
https://cloud.google.com/kms/docs/encrypt-decrypt-apis
<details><div>

### Q. 問題13: 未回答
あなたは、Compute Engine上にデプロイされたアプリケーションをサポートしています。アプリケーションは、Cloud SQLインスタンスに接続してデータを保存および取得します。アプリケーションの更新後、ユーザーはデータベースのタイムアウトメッセージを表示するエラーを報告しました。同時アクティブユーザー数は安定しています。あなたは、データベースのタイムアウトの最も可能性の高い原因を見つける必要があります。
この要件を満たすために、どうすればよいですか？
1. Compute Engineインスタンスのシリアルポートのログを確認します
2. Cloud Security Scannerを使用して、Cloud SQLが分散型サービス拒否（DDoS）攻撃を受けているかどうかを確認します
3. Google Cloud Operation Suite Profilerを使用して、アプリケーション全体のリソース使用率を可視化します
4. Cloud SQLインスタンスへの接続数が増加しているかどうかを判断します
<details><div>
    答え：3
説明
この問題では、アプリケーションの更新後にデータベースのタイムアウトエラーが発生した場合の対応が求められています。同時アクティブユーザー数は安定しているため、ユーザー増加による負荷が原因ではないことが示唆されています。ここで注意すべきは、エラーがアプリケーション更新後に発生し、Cloud SQLインスタンスにつながっていることです。この情報を基に、アプリケーションとCloud SQLインスタンスの間のインタラクション、特にリソースの使用に注目し、考えられる原因を特定すべきです。適切なツールや方法を使用して原因を解明し、問題を解決します。
基本的な概念や原則：
Cloud SQL：Google Cloudの完全マネージド型リレーショナルデータベースサービスで、MySQL, PostgreSQL, SQL Serverをサポートしています。
Google Cloud Operation Suite Profiler：アプリケーションのパフォーマンスのボトルネックを可視化し、解析するためのGoogle Cloudのツールです。CPU使用率、メモリ使用率、ネットワークIOなどのリソース使用率を可視化します。
Compute Engineインスタンスのシリアルポートログ：インスタンスの起動時のログやインスタンス内部からの出力を記録します。トラブルシューティングにおける重要な情報源です。
接続数：データベースのパフォーマンスを評価するための重要な指標です。接続数が多いと、データベースのタイムアウトが発生する可能性があります。
Cloud Security Scanner：Google Cloudのセキュリティスキャンツールです。アプリケーションの脆弱性を感知し、アラートを提供します。ただし、DDoS攻撃の確認には使えません。
正解についての説明：
（選択肢）
・Google Cloud Operation Suite Profilerを使用して、アプリケーション全体のリソース使用率を可視化します
この選択肢が正解の理由は以下の通りです。
まず、Cloud Operation Suite Profilerを使用するとアプリケーション全体のリソース使用状況を可視化できます。これにより、アプリケーションが使用する各リソースのパフォーマンスについて深く理解することができます。例えば、どのサービスが過度に負荷をかけ、それがデータベースのタイムアウトにどのように影響しているかを理解するのに役立ちます。
また、更新後にエラーが発生しているとのことなので、アプリケーションの更新がリソース使用にどのような影響をもたらしたかを明らかにするのに役立ちます。同時アクティブユーザー数が安定しているとの情報からユーザー側に原因がないことが推測されるため、アプリケーション側で何が起こっているのかを理解する必要があります。
このような解析は、問題の特定と解決に必要な情報を提供します。
したがって、アプリケーション全体のリソース使用率を可視化することは、データベースタイムアウトの最も可能性の高い原因を特定するのに最適な手段です。
不正解についての説明：
選択肢：Compute Engineインスタンスのシリアルポートのログを確認します
この選択肢が正しくない理由は以下の通りです。
Compute Engineインスタンスのシリアルポートのログを確認すると、主にインスタンスの起動やシャットダウンなどのOSレベルのイベント情報を参照できますが、この問題ではユーザーからのデータベースのタイムアウトのエラーレポートが問題なので、シリアルポートのログではなくアプリケーション全体のリソース使用率を詳しく可視化するCloud Operation Suite Profilerの方が適切です。
選択肢：Cloud SQLインスタンスへの接続数が増加しているかどうかを判断します
この選択肢が正しくない理由は以下の通りです。
アプリケーションの更新後、ユーザー数は変わっていないため、単にCloud SQLへの接続数が増えたかどうかだけを調べるだけでは、タイムアウトの本当の原因を突き止めることができません。
一方、Cloud Operation Suite Profilerを使用すると、アプリケーション全体のリソース使用状況を確認でき、タイムアウトの原因をより的確に特定できます。
選択肢：Cloud Security Scannerを使用して、Cloud SQLが分散型サービス拒否（DDoS）攻撃を受けているかどうかを確認します
この選択肢が正しくない理由は以下の通りです。
Cloud Security Scannerはセキュリティ脆弱性を特定するためのサービスで、リソースの使用率やパフォーマンスに関連する問題を調査したり、DDoS攻撃が発生しているかどうかを判断するためのツールではありません。そのため、ユーザーがデータベースのタイムアウトメッセージを報告している問題を解決するのには適していません。
参考リンク：
https://cloud.google.com/sql/docs/mysql
https://cloud.google.com/compute/docs/instances/viewing-serial-port-output
https://cloud.google.com/profiler
<details><div>

### Q. 問題14: 未回答
アプリケーションをCloud Runにデプロイしています。アプリケーションの起動にはパスワードが必要です。あなたの組織は、すべてのパスワードが24時間ごとにローテーションされることを要求しており、アプリケーションは最新のパスワードを持っている必要があります。ダウンタイムなしでアプリケーションをデプロイする必要があります。
この要件を満たすために、どうすればよいですか？
1. Secret Managerにパスワードを保存し、アプリケーション内にシークレットをボリュームとしてマウントします
2. Secret Managerにパスワードを保存し、環境変数を使ってアプリケーションにシークレットを送信します
3. Cloud Buildを使用して、ビルド時にパスワードをアプリケーションコンテナに追加します。Artifact Registryが公開アクセスから保護されていることを確認します
4. パスワードをコードに直接保存します。Cloud Buildを使用して、パスワードが変更されるたびにアプリケーションを再構築してデプロイします
<details><div>
    答え：1
説明
この問題では、アプリケーションのセキュリティとデプロイに対する要件を満たすためにどのような手法を選ぶべきかを尋ねています。基本的には、パスワードのローテーションの要求、新しいパスワードをアプリケーションが認識する必要性、およびダウンタイムなしにデプロイする必要性といった諸要件を満たすために、適切なサービスと機能を選ぶことが求められます。選択肢を見て明らかなように、パスワードの保管場所やパスワードの更新方法が重要な焦点となります。ここでの鍵となるのは、パスワードの管理とアプリケーションへの安全な組み込み方法をどのように達成するかです。
基本的な概念や原則：
Cloud Run：Google Cloudの完全に管理されたサービスで、スケーラブルなコンテナアプリケーションを実行するためのものです。
Secret Manager：Google Cloudの秘密管理サービスです。アプリケーションの秘密を安全に管理し、アクセスするメカニズムを提供します。
シークレットのボリュームマウント：Container系のサービスでよく用いられる、秘密情報をアプリケーションに安全に供給する手法です。シークレットがコンテナ内の指定されたパスに作成され、アプリケーションから参照できます。
ダウンタイムなしデプロイメント：サービスの継続性を保つためのデプロイメント戦略です。新旧のバージョンが同時に存在し、新しいバージョンへのトラフィックが旧バージョンから徐々に移行します。
環境変数：プログラムの実行環境を設定するための変数です。秘密情報を保存する場合もありますが、組織のセキュリティポリシーや業界のベストプラクティスによっては推奨されないこともあります。
Cloud Build：Google Cloudのサービスで、ソースコードからコンテナイメージやアプリケーションをビルド、テスト、デプロイするためのものです。",
パスワードの直接保存：セキュリティ上のリスクを伴うため、通常は推奨されません。パスワードは秘密管理サービスのような安全な場所に保存し、必要なときにのみ取り出すべきです。
正解についての説明：
（選択肢）
・Secret Managerにパスワードを保存し、アプリケーション内にシークレットをボリュームとしてマウントします
この選択肢が正解の理由は以下の通りです。
Secret Managerは、Google Cloudサービスの中でセキュアな環境で証明書、APIキー、パスワードなどの機密情報を管理するサービスです。Secret Managerによりパスワードを管理することで、パスワードのローテーション、つまり定期的なパスワードの更新が簡単に行えます。これは組織の要求である24時間毎のパスワード更新要求と一致します。
さらに、Secret Managerから提供されるシークレットをアプリケーションにボリュームとしてマウントすることで、アプリケーションは常に最新のパスワードを取得できます。これは組織が必要とする最新のパスワードを保持する要件を満たします。
また、この方法を用いて少しでも古いパスワードを使用してしまうリスクを低減できます。
最後に、この方法はアプリケーションのダウンタイムを発生させずにパスワードの更新を実現します。これによりアプリケーションの利用者に対するサービスの中断を防ぎ、サービスの品質を保つことが出来ます。
不正解についての説明：
選択肢：Secret Managerにパスワードを保存し、環境変数を使ってアプリケーションにシークレットを送信します
この選択肢が正しくない理由は以下の通りです。
環境変数を使用した場合、シークレットを更新する際にアプリケーションの再デプロイが必要となるため、要求されている24時間毎のパスワードローテーションとダウンタイムなしの要件を満たすことができません。ボリュームとしてマウントすることで、シークレットが更新されたときにアプリケーションが新しいシークレットを自動的に読み込むことが可能になります。
選択肢：Cloud Buildを使用して、ビルド時にパスワードをアプリケーションコンテナに追加します。Artifact Registryが公開アクセスから保護されていることを確認します
この選択肢が正しくない理由は以下の通りです。
この方法では、Cloud Buildを使用したものの、この方法ではパスワードが24時間ごとにローテーションされた際に、アプリケーションが最新のパスワードを反映するためにコンテナを再ビルドし、再デプロイする必要が出てしまいます。
したがって、要求されたダウンタイムなしでのデプロイは達成できません。
選択肢：パスワードをコードに直接保存します。Cloud Buildを使用して、パスワードが変更されるたびにアプリケーションを再構築してデプロイします
この選択肢が正しくない理由は以下の通りです。
パスワードをコードに直接保存すると、セキュリティ上のリスクが高まります。
また、パスワードが変更されるたびにアプリケーションを再構築してデプロイすると、ダウンタイムが生じやすくなる可能性があります。このため、ダウンタイム無しでのアプリケーションのデプロイという要件を満たせません。
一方、正解の選択肢ではシークレット情報の管理が容易で、ダウンタイム無しでパスワードのローテーションを実現できます。
参考リンク：
https://cloud.google.com/secret-manager/docs/mounting-secrets-as-volumes
https://cloud.google.com/run/docs/configuring/secrets
https://cloud.google.com/secret-manager/docs/automating-rotation

# 2


# 3


# 4
