## 1
### Q.  問題1: 回答
組織には、オンプレミスで実行されるコンテナー化された Web アプリケーションがあります。Google Cloud への移行計画の一環として、次の受け入れ基準を満たすデプロイ戦略とプラットフォームを選択する必要があります:
1. プラットフォームは、Android デバイスからのトラフィックを Android 固有のマイクロサービスに転送できる必要があります。
2. プラットフォームは、任意の割合ベースのトラフィック分割
を許可する必要があります 3.デプロイ戦略では、マイクロサービスの複数のバージョンを継続的にテストできる必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
指定された受け入れ基準が与えられた場合、要件に最も適したオプションは次のとおりです。
D. Anthos Service Mesh を使用して Canary リリースを Google Kubernetes Engine にデプロイします。トラフィック分割を使用して、仮想サービスで構成されたユーザ エージェント ヘッダーに基づいて、ユーザ トラフィックの 10% を新しいバージョンに転送します。
理論的根拠は次のとおりです。
1. Android 固有のマイクロサービス: Anthos Service Mesh(ASM)では、ユーザー エージェントなどの HTTP ヘッダーなど、さまざまな条件に基づいてきめ細かなルーティングを行うことができます。Anthos Service Mesh で仮想サービスを構成することで、Android デバイスからのトラフィックを Android 固有のマイクロサービスに転送できます。
2. パーセンテージベースのトラフィック分割: Google Kubernetes Engine(GKE)と Anthos Service Mesh ではトラフィックを分割できるため、Canary リリースはユーザー エージェント ヘッダーの構成に基づいて特定の割合(この場合は 10%)のトラフィックを受信できます。
3. 複数のバージョンの継続的なテスト: Anthos Service Mesh を使用した GKE のカナリア リリースでは、特定の条件に基づいてトラフィックのサブセットをカナリア リリースに誘導することで、さまざまなバージョンのテストが可能になります。このアプローチにより、完全な展開の前に新しいバージョンの継続的なテストと検証が容易になります。
このアプローチは、GKE の Kubernetes 機能と Anthos Service Mesh を活用してきめ細かなトラフィック ルーティングとカナリア デプロイ戦略を実現することで、指定されたすべての受け入れ基準を満たしています。
</div></details>

### Q.  問題2: 回答
グローバルな組織で働いており、限られたエンジニアリング リソースで 99% の可用性目標でサービスを実行しています。
現在の暦月では、サービスの可用性が 99.5% であることがわかりました。サービスが定義された可用性の目標を満たし、今後の新機能のリリースなど、ビジネスの変化に対応できることを確認する必要があります。
また、運用コストを最小限に抑えながら、技術的負債を削減する必要もあります。Google が推奨する方法に従う場合。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
このシナリオでは、サービスの可用性の維持、技術的負債の削減、運用コストの最小化、Google が推奨するプラクティスの遵守に関する懸念事項が概説されています。これらの目的を考えると、提供されるオプションの中から最も適切な選択は次のとおりです。
C. サービス レベルの可用性のエラー バジェットを定義し、残りのエラー バジェットを最小限に抑えます。
以下にその説明を示します。
A. サービスにコンピューティング リソースを追加することで、サービスに N+1 の冗長性を追加します。
フォールトトレランスには冗長性が重要ですが、単にコンピューティングリソースを追加するだけでは、最も効率的なソリューションではない可能性があります。技術的負債に直接対処したり、最適なリソース使用率を確保したりすることなく、運用コストが増加する可能性があります。
B. 反復的なタスクを自動化することで、労力を特定、測定、排除します。
反復的なタスクの自動化は、手作業 (toil) を減らし、運用を合理化し、リソース割り当てを最適化するための重要なプラクティスです。このプラクティスは、技術的負債と運用コストの削減と一致していますが、可用性の目標や今後のビジネスの変化に対応する必要性に直接対処するものではありません。
C. サービス レベルの可用性のエラー バジェットを定義し、残りのエラー バジェットを最小限に抑えます。
Google のサイト信頼性エンジニアリング(SRE)の原則では、多くの場合、エラー バジェットの概念が提唱されています。エラーバジェットは、サービスの中断の許容しきい値を設定し、チームがサービスの信頼性を確保しながら、イノベーション(新機能のリリース)のバランスをとることを可能にします。エラーバジェットを定義して管理することで、チームはサービスを許容可能な可用性制限内に保ちながら、機能開発に取り組むことができます。これは、可用性の目標を達成し、ビジネスの変化に対応するという目標と一致しています。
D. 対応可能なエンジニアを機能バックログに割り当て、サービスが可用性目標内にとどまるようにします。
機能バックログにエンジニアを割り当てることはビジネスの成長にとって重要ですが、サービスの可用性を維持することとのバランスを取ることが不可欠です。サービスの定義された可用性目標を考慮せずに機能開発に優先順位を付けると、潜在的な信頼性の問題が発生する可能性があります。
要約すると、エラー バジェットを定義することは、Google の SRE のベスト プラクティスに沿ったものであり、チームはイノベーションと信頼性のバランスを取り、技術的負債を削減し、今後の機能リリースやビジネスの変更に対応しながら、定義された可用性目標内にサービスを維持することができます。
</div></details>

### Q.  問題3: 回答
アプリケーション イメージは Cloud Build を使用してビルドされ、Google Container Registry(GCR)にプッシュされます。ソース管理でタグ付けされたリリース バージョンに基づいて、デプロイするアプリケーションの特定のバージョンを指定できるようにする必要があります。画像をプッシュするときはどうすればよいですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
適切なオプション – C. Cloud Build を使用して、リリース バージョン タグをアプリケーション イメージに含める
オプション C の Cloud Build を使用してリリース バージョン タグをアプリケーション イメージに含めると、ビルド プロセス中にリリース バージョン タグをアプリケーション イメージに追加するプロセスを自動化できます。これにより、アプリケーションのさまざまなバージョンとコードベースとの関連付けを簡単に追跡できます。さらに、Cloud Build では、イメージのビルド、テスト、デプロイなど、ビルドとデプロイのプロセスのさまざまなステップを自動化できるため、デプロイの管理と追跡が容易になります。
オプション B と C はどちらも、ソース管理でタグ付けされたリリース バージョンに基づいて特定のバージョンのアプリケーションを確実にデプロイするための有効な方法ですが、オプション C は、アプリケーションのデプロイを管理するためのより堅牢で柔軟なソリューションです。
オプション B: イメージ名内のパラメータとしてソース管理タグを指定する場合は、イメージを GCR にプッシュするときにソース管理タグを手動で含める必要があります。これにより、人為的エラーが発生しやすくなり、さまざまなバージョンとコードベースとの対応を追跡するために、より多くの手作業が必要になる可能性があります。
https://cloud.google.com/build/docs/build-push-docker-image#build_an_image_using_a_build_config_file
</div></details>

### Q.  問題4: 回答
カスタム Debian イメージを使用して仮想マシン (VM) でアプリケーションを実行しています。イメージには Stackdriver Logging エージェントがインストールされています。VM のスコープは cloud-platform です。アプリケーションはsyslogを介して情報を記録しています。Google Cloud Platform Console で Stackdriver Logging を使用してログを可視化します。syslogがログビューアの[All logs]ドロップダウンリストに表示されていないことに気付きました。
最初にすべきことは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
右のオプション – D. VM に SSH 接続し、VM で次のコマンドを実行します。grep fluentd
VM には VM 上のクラウド プラットフォーム スコープがあり、既定のコンピューティング エンジン アカウントを使用している場合はフル アクセスを意味します。したがって、Option-Dは、fluentdデーモンが実行されているかどうかを調べるための正しいチェックです。
ロギングエージェントのインストールまたは使用に問題がある場合は、次の点を確認してください。 - エージェント サービスが VM インスタンスで実行されていることを確認します
参照 - https://cloud.google.com/logging/docs/agent/logging/troubleshooting#checklist
さらに、インスタンスの作成時に、Google Cloud API を呼び出すときにインスタンスが使用するサービス アカウントを指定できます。インスタンスはアクセス スコープで自動的に構成され、そのようなアクセス スコープの 1 つが monitoring.write です
詳細 - https://cloud.google.com/compute/docs/access/service
</div></details>

### Q.  問題5: 回答
Google Kubernetes Engine(GKE)で実行されるアプリケーションがあります。アプリケーションは、Deployments と Services を使用して GKE にデプロイされる複数のマイクロサービスで構成されています。マイクロサービスの 1 つで、Pod が 5 時間以上実行された後、Pod が 403 エラーを返すという問題が発生しています。開発チームは解決策に取り組んでいますが、問題は 1 か月間解決されません。マイクロサービスが修正されるまで、運用を継続する必要があります。Google が推奨する方法に従い、使用する手順を最小限にする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
恒久的な修正を待っている間に Google Kubernetes Engine(GKE)で 5 時間以上実行した後に 403 エラーの問題が発生し、マイクロサービスの運用を継続するための最も適切で Google が推奨する方法は次のとおりです。
B. マイクロサービスのデプロイに HTTP liveness probe を追加します。
説明：
liveness probe は、Pod が正常で動作しているかどうかを判断する Kubernetes のヘルスチェックメカニズムです。リクエストを送信してマイクロサービスの健全性を定期的にチェックし、マイクロサービスが適切に応答しなかった場合(この場合は403エラーが返された場合)、KubernetesはPodを自動的に再起動できます。
マイクロサービスのデプロイ構成に HTTP liveness probe を追加することで、Kubernetes はマイクロサービスの正常性を継続的に監視します。マイクロサービスが403エラー(問題を示す)を返し始めると、KubernetesはPodを再起動し、恒久的な修正が実装されるまでサービスが利用可能な状態を維持します。
このアプローチは Google が推奨するプラクティスに準拠しており、Kubernetes が liveness probe の定義された条件に基づいて Pod の再起動を自動的に処理するため、最小限の介入で済みます。
他のオプション(A、C、D)は、症状に対処したり、問題に関するアラートを発したりする可能性がありますが、Liveness Probeのように効率的にPodの再起動を自動的に処理することで、問題を直接解決したり、操作の継続を保証したりすることはできません。
</div></details>

### Q.  問題6: 回答
Compute Engine で実行されているアプリケーション サーバーのプールがある。最小限の構成しか必要とせず、開発者がトラブルシューティングのためにアプリケーション ログに簡単にアクセスできる安全なソリューションを提供する必要があります。このソリューションを GCP に実装するにはどうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
適切なオプション – A. Stackdriver ロギング エージェントをアプリケーション サーバーにデプロイします。開発者に IAM Logs Viewer のロールを付与して、Stackdriver にアクセスしてログを表示する
Stackdriver にプッシュされるカスタムログを有効にするには、ログエージェントが必要です。開発者に必要なのはログ閲覧者の権限のみで、この場合これで十分であり、プライベートログビューアは、ログ内のプライベートデータを表示するための昇格された権限を持つログ閲覧者の権限のスーパーセットです。この場合、これは必要ありません。
roles/logging.viewer (ログ ビューアー) を使用すると、アクセスの透明性ログとデータ アクセス監査ログを除く、ログのすべての機能への読み取り専用アクセスが許可されます。
privateLogViewerは、必要のないデータアクセスログへの追加アクセスを提供します
インサイト:https://cloud.google.com/logging/docs/view/logs-explorer-interface
https://cloud.google.com/logging/docs/agent/logging
</div></details>

### Q.  問題7: 不正解
Cloud Run アプリケーションは、非構造化ログをテキスト文字列として Cloud Logging に書き込みます。非構造化ログを JSON ベースの構造化ログに変換する。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
正しいオプション – D. ログテキストペイロードを JSON ペイロードに変換するようにログエージェントを設定します。
このコンテキストでは、テキスト文字列として記述された非構造化ログがあり、それらを JSON ベースの構造化ログに変換する場合は、通常、ログエージェントまたはパーサーを使用してログエントリを変換します。ログエージェントは、ログの構造を認識し、テキストペイロードをJSONペイロードに変換するように構成されています。
ログエージェントは、非構造化ログを解析し、構造化された形式に変換する役割を担います。このプロセスでは、テキストペイロードから関連情報を抽出し、それをJSON構造に整理する方法を指定します。このアプローチでは、非構造化ログから構造化ログへの変換をサポートするログエージェントまたはパーサーがあることを前提としていることに注意してください。
https://cloud.google.com/logging/docs/structured-logging
https://cloud.google.com/logging/docs/agent/logging/configuration#process-payload
</div></details>

### Q.  問題8: 回答
Google Cloud にデプロイされたアプリケーションのフロントエンド層を構成しています。フロントエンド層は nginx でホストされ、Envoy ベースの外部 HTTP(S) ロードバランサーを前面に持つマネージド インスタンス グループを使用してデプロイされます。このアプリケーションは、europe-west2 リージョン内に完全にデプロイされ、英国に拠点を置くユーザーのみにサービスを提供します。最もコスト効率の高いネットワーク層と負荷分散構成を選択する必要があります。何を使うべきですか?
1. 
2.  
3. 
4. 
<details><div>
    答え：
説明
リージョン ロード バランサーで Standard レベルを使用するオプション D は、説明されているシナリオに最も適した選択肢です。これが正しいオプションである理由と、他のオプションがあまり適していない理由は次のとおりです。
D. リージョン ロード バランサーを備えた Standard レベル:
説明：
スタンダードレベル:Standard レベルは、Premium レベルと比較してコスト効率の高いネットワークを提供します。基本的なネットワーク機能を低コストで提供するため、地域展開のアプリケーションに適しており、高度なグローバル ネットワーク機能の必要性は低くなります。
リージョン・ロード・バランサ:このタイプのロードバランサーは特定のリージョン内で動作し、europe-west2 リージョン内でのみアプリケーションのデプロイと完全に連携します。リージョナルロードバランサーは、グローバルロードバランサーに関連する高いコストを発生させることなく、特定のリージョン内でトラフィックを分散するための費用対効果と効率性に優れています。
他のオプションが適さない理由:
A. グローバル ロード バランサーを備えた Premium レベル:
Premium レベルには、グローバル規模のネットワーク用に設計された追加機能が含まれています。ただし、アプリケーションが英国内のユーザーのみにサービスを提供し、完全に europe-west2 リージョン内にデプロイされるこのシナリオでは、Premium レベルのグローバル規模の機能は不要であり、必要以上にコストがかかります。
B. リージョン ロード バランサーを備えた Premium レベル:
このオプションにはリージョンのロード バランサー (費用対効果が高い) が含まれますが、ネットワークには引き続き Premium レベルを利用します。前述したように、Premium レベルには、英国などの特定の地域にサービスを提供するアプリケーションには必要のない高度なグローバル ネットワーク機能が含まれているため、Standard レベルよりもコスト効率が低くなります。
C. グローバル ロード バランサーを備えた Standard レベル:
Standard レベルは、よりローカライズされたデプロイ用に設計されているため、グローバル ロード バランサーで Standard レベルを使用することは矛盾しているように思えるかもしれません。さらに、グローバル ロード バランサーはより高価であり、英国のユーザーのみにサービスを提供し、europe-west2 リージョン内で完全にホストされているアプリケーションには必要ないグローバル規模の機能を提供する場合があります。
したがって、このシナリオでは、コストを最適化しながら、アプリケーションのデプロイとユーザー ベースの特定の要件にうまく適合するため、最もコスト効率が高く適切なオプションは、リージョン ロード バランサーを備えた D: Standard レベルです。
</div></details>

### Q.  問題9: 回答
アプリケーション イメージがビルドされ、Google Container Registry(GCR)にプッシュされます。開発作業を最小限に抑えながら、イメージの更新時にアプリケーションをデプロイする自動パイプラインを構築する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
開発作業を最小限に抑えながら、イメージの更新時にアプリケーションのデプロイを自動化するための理想的なアプローチは次のとおりです。
B. Cloud Pub/Sub を使用して Spinnaker パイプラインをトリガーします。
この選択の背後にある理由は次のとおりです。
1. Cloud Pub/Sub: Cloud Pub/Sub は、システムのコンポーネント間の非同期通信を可能にするメッセージング サービスです。これは、イベントのトリガーメカニズムとして機能します。この場合、イベント(Google Container Registry 内の更新されたイメージなど)をリッスンするために使用できます。
2. Spinnaker Pipeline:Spinnakerは、マルチクラウド展開をサポートする強力な継続的デリバリープラットフォームです。堅牢なデプロイ戦略を提供し、Cloud Pub/Sub などのさまざまなトリガー メカニズムと簡単に統合できます。Cloud Pub/Sub がイメージ更新イベントを検出すると、Spinnaker パイプラインをトリガーしてデプロイ プロセスを自動化できます。
このアプローチでは、カスタム実装を必要とせずに既存の Google Cloud サービス(Cloud Pub/Sub と Spinnaker)を活用することで、開発作業を最小限に抑えることができます。これにより、デプロイメント・プロセスが合理化され、更新されたイメージがコンテナ・レジストリで使用可能になるたびに、デプロイメント・パイプラインが自動的にトリガーされ、継続的なデプロイ・サイクルが維持されます。
オプション A と C はどちらも、それぞれ Cloud Build と Jenkins の使用について言及しており、Cloud Pub/Sub と Spinnaker を利用する場合と比較して、より多くのカスタマイズと統合の作業が必要になり、開発の労力が増加する可能性があります。
オプション D では、Cloud Pub/Sub を使用して、GKE で実行されるカスタム デプロイ サービスをトリガーすることを提案しています。これは実現可能かもしれませんが、継続的デプロイ シナリオ用に特別に設計され、より包括的なデプロイ機能を提供する Spinnaker を使用する場合と比較して、開発とメンテナンスのオーバーヘッドが多くなる可能性があります。
インサイト - https://cloud.google.com/architecture/continuous-delivery-toolchain-spinnaker-cloud#triggering_a_spinnaker_pipeline_when_a_docker_image_is_pushed_to_container_registry
</div></details>

### Q.  問題10: 回答
Compute Engine にデプロイされたアプリケーションをサポートしている。アプリケーションは Cloud SQL インスタンスに接続して、データの保存と取得を行います。アプリケーションの更新後、ユーザーはデータベースタイムアウトメッセージを表示するエラーを報告します。同時アクティブユーザー数は安定しています。データベースのタイムアウトの最も可能性の高い原因を見つける必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
右オプション - B. Stackdriver Profiler を使用して、アプリケーション全体のリソース使用率を可視化します。
同時アクティブ・ユーザー数が安定しているときにデータベースがタイムアウトする原因として最も可能性が高いのは、パフォーマンスの問題です。Stackdriver Profiler を使用すると、アプリケーションのパフォーマンスの問題を特定して診断できます。プロファイラーは、CPU やメモリの使用率など、アプリケーション全体のリソース使用率を視覚化し、高負荷の原因となっている可能性のあるアプリケーションの部分を特定するのに役立ちます。これは、アプリケーションがリソースをどのように使用しているかを理解し、タイムアウトの原因となっている可能性のあるコード内のボトルネックを特定するのに役立ちます。
他の可能な解決策は、特定の状況では役に立ちますが、このシナリオではそれほど重要ではありません。
オプションAは、問題に関連しないため、関係ありません。
オプション C 特定のシナリオでは役立ちますが、この場合、問題の原因である可能性は低くなります。
Option-Dは、アプリケーションの脆弱性を検出できるセキュリティツールですが、データベースのタイムアウトとは関係ありません。
グッドリード - https://cloud.google.com/profiler/docs/about-profiler
</div></details>

### Q.  問題11: 回答
チームは Google Kubernetes Engine(GKE)でマイクロサービスを実行しています。エラーバジェットの消費を検出して、顧客を保護し、リリースポリシーを定義する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
オプション D は、Google Kubernetes Engine(GKE)でマイクロサービスを運用する際のエラー バジェットの管理、顧客の保護、リリース ポリシーの定義に適しています。
それでは、他のオプションが適切ではない理由を評価してみましょう。
A. メトリックから SLI を作成します。サービスが成功しない場合は、アラートポリシーを有効にします。
メトリックからサービスレベル指標(SLI)を作成することは、サービスパフォーマンスを監視するための優れたプラクティスですが、エラーバジェットの管理に不可欠なサービスレベル目標(SLO)の作成やSLOに基づくアラートポリシーの設定は明示的に必要ありません。
B. Anthos Service Mesh の指標を使用して、マイクロサービスの健全性を測定します。
Anthos Service Mesh はマイクロサービスの指標とモニタリング機能を提供しますが、エラー バジェットの管理とリリース ポリシーの定義に不可欠な SLO の確立や SLO に基づくアラート ポリシーの構成には本質的に対応していません。
C. SLO を作成します。select_slo_burn_rate でアラート ポリシーを作成します。
このオプションは、SLO を作成し、特にselect_slo_burn_rateにアラートポリシーを設定する必要があるため、有効なアプローチでもあります。ただし、オプション D では、SLO を作成するだけでなく、サービスの稼働時間チェックも構成することで、より包括的な監視設定が可能になります。
オプション D は、SLO の作成だけでなく、サービスの稼働時間チェックの構成にも重点が置かれているため、際立っています。この包括的なアプローチは、サービスに合格しなかった場合にアラートポリシーを有効にすることと相まって、エラーバジェットの効果的な監視と管理、顧客保護の確保、およびサービスの正常性に基づく適切なリリースポリシーの定義を可能にします。
インサイト - https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo
</div></details>

### Q.  問題12: 不正解
Google Kubernetes Engine(GKE)にデプロイされたアプリケーションの CI / CD パイプラインを構築しています。アプリケーションは、Kubernetes Deployment、Service、Ingress を使用してデプロイされます。アプリケーション チームは、ブルー/グリーン デプロイ手法を使用してアプリケーションをデプロイするように依頼しました。ロールバックアクションを実装する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
Google Kubernetes Engine(GKE)の Blue/Green デプロイ手法でロールバック アクションを実装するための正しいオプションは次のとおりです。
A. kubectl rollout undo コマンドを実行します。
説明：
kubectl rollout undo は、デプロイを以前の既知の状態に戻すことでロールバックを管理するために Kubernetes で特別に設計されています。
ブルー/グリーン デプロイ戦略では、kubectl rollout undo によってデプロイが以前の安定バージョンに効率的に元に戻され、制御および自動化されたロールバック プロセスが可能になります。
このコマンドは、手動による介入を必要とせずに以前の展開バージョンに切り替えるために必要な手順を処理することで、ロールバック手順を簡素化し、新しい展開によって発生した問題やエラーが発生した場合にスムーズなロールバックを保証します。
それでは、間違ったオプションとその正当化について説明しましょう。
B. 新しいコンテナイメージを削除し、実行中のPodを削除します。
新しいコンテナイメージを手動で削除してPodを実行することは、Kubernetesで推奨されるロールバック戦略ではありません。この手動のアプローチでは、中断やデータ損失が発生する可能性があり、制御されたロールバック プロセスは保証されません。
C. 以前のKubernetesデプロイメントを指すようにKubernetes Serviceを更新します。
Kubernetes Service を更新しても、以前のバージョンへのデプロイのロールバックが直接処理されない場合があります。KubernetesのServiceは、主にトラフィックを抽象化してPodにルーティングする役割を担います。サービス構成を変更するだけでは、デプロイは以前のバージョンに戻りません。
D. 新しい Kubernetes デプロイを 0 にスケーリングします。
新しいDeploymentをゼロにスケールダウンすると、実行中のPodが効果的に停止し、制御されたロールバックではなく、サービスのダウンタイムが発生します。これは、運用環境でロールバック戦略を実装するのに適したアプローチではない可能性があります。
インサイト - https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands
</div></details>

### Q.  問題13: 未回答
内部アプリケーションの新しいリリースは、ユーザーの悲劇的な状況が最小限に抑えられる週末のメンテナンス期間中にデプロイします。ウィンドウが終了すると、新機能の 1 つが運用環境で期待どおりに動作していないことがわかります。長時間の停止の後、新しいリリースをロールバックし、修正をデプロイします。
リリース プロセスを変更して、復旧までの平均時間を短縮し、将来の長時間の停止を回避できるようにする必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
パーフェクトオプション– B、E
B. CD サーバー経由で新しいコードをリリースするときに Blue/Green デプロイ戦略を採用する
E. CI サーバーを構成します。一連の単体テストをコードに追加し、CI サーバーでコミット時に実行し、変更を確認します
オプション – A、良くない、レビューするピアが増えても、何も自動化されません。
オプション – B、その継続的デリバリー (CD) と正しい。
オプション– C、いいえ、リンティングはコード形式用です。
オプション-D:いいえ、統合テストが必要ですが、自動的に改善されます
オプション – E、その継続的インテグレーション (CI) と、ここでは正しいです。
</div></details>

### Q.  問題14: 未回答
あなたは組織のDevOpsプロジェクトをリードしています。DevOps チームは、サービス インフラストラクチャの管理とインシデントのオンコールを担当します。ソフトウェア開発チームは、コードの記述、送信、およびレビューを担当します。どちらのチームも SLO を公開していません。DevOpsチームとソフトウェア開発チームの間で、サービスの新しい共同所有モデルを設計したいと考えています。新しい共同所有モデルでは、各チームにどのような責任を割り当てる必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
正しいオプション–C。
DevOpsチームとソフトウェア開発チーム間のサービスの共同所有モデルでは、オプションCとDがより適しています。
DevOps チームはインフラストラクチャを管理し、両方のチームはコード レビュー、オンコール業務、SLO の定義の責任を共有します。ソフトウェア開発チームは、コード変更を提出することで貢献します。
このアプローチにより、コラボレーション、説明責任、およびサービスの信頼性とパフォーマンスの目標に対するコミットメントの共有が促進されます。
https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started
</div></details>

### Q.  問題15: 回答
トラフィックの多い Web アプリケーションをサポートしており、ホーム ページがタイムリーに読み込まれるようにする必要があります。最初の手順として、許容可能なページ読み込み時間を 100 ミリ秒に設定してホームページ要求の待機時間を表すサービス レベル インジケーター (SLI) を実装することにしました。この SLI を計算するために Google が推奨する方法は何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
正しいオプション – C. 100 ミリ秒未満で読み込まれるホーム ページ要求の数をカウントし、ホーム ページ要求の合計数で割ります
SRE の原則書では、SLI を 2 つの数値の比率 (良いイベントの数をイベントの総数で割った値) として扱うことが推奨されています。例: 成功した HTTP 要求の数 / HTTP 要求の合計 (成功率)
インサイト - https://sre.google/workbook/implementing-slos/
</div></details>

### Q.  問題16: 回答
最近、アプリケーションを Google Kubernetes Engine(GKE)にデプロイし、新しいバージョンのアプリケーションをリリースする必要があります。新しいバージョンに問題がある場合に備えて、以前のバージョンのアプリケーションに即座にロールバックする機能が必要です。どのデプロイ モデルを使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
新しいバージョンで問題が発生した場合に以前のバージョンのアプリケーションに即座にロールバックする機能の要件に最も適したデプロイ モデルは D です。 ブルー/グリーン デプロイを実行し、デプロイの完了後に新しいアプリケーションをテストします。
他のオプションではインスタントロールバック機能が提供されない理由は次のとおりです。
A. ローリング デプロイを実行し、デプロイの完了後に新しいアプリケーションをテストします。
ローリング デプロイでは、アプリケーションのインスタンスが徐々に更新され、プロセス中の継続的な可用性が確保されます。ただし、この方法では、本質的にインスタント ロールバック機能は提供されません。デプロイ後に新しいアプリケーションをテストできますが、問題が発生した場合、ローリングデプロイでは以前のバージョンにすぐにロールバックできない場合があります。
B. A/B テストを実行し、デプロイの完了後にアプリケーションを定期的にテストします。
A/B テストでは、アプリケーションの 2 つのバージョンを比較して、パフォーマンスやユーザーの反応を判断します。これは、即時ロールバック機能ではなく、バージョンの比較に重点を置いています。A/B テストでは、新しいバージョンで問題が発生した場合に、以前のバージョンへの即時復帰がサポートされない場合があります。
C. カナリア デプロイを実行し、新しいバージョンがデプロイされた後、新しいアプリケーションを定期的にテストします。
カナリアデプロイでは、評価のために新しいバージョンがユーザーまたはトラフィックのサブセットにリリースされます。インクリメンタルテストは可能ですが、以前のバージョンへの即時ロールバックを直接サポートしていない場合があります。定期的なテストによって問題が検出される場合もありますが、カナリア デプロイでは本質的に即時のロールバック メカニズムは提供されません。
したがって、提供されているオプションの中で、ブルー/グリーンデプロイ(オプションD)は、デプロイ後に徹底的なテストが可能であり、新しいバージョン(緑)で問題が発生した場合に以前のバージョン(青)に即座に切り替えることができるため、最適な選択肢です。このデプロイ モデルは、本質的に迅速なロールバックをサポートしているため、新しいアプリケーション バージョンで問題が発生した場合の中断が最小限に抑えられます。
インサイト - https://cloud.google.com/architecture/application-deployment-and-testing-strategies
</div></details>

### Q.  問題17: 回答
サポートする実動システムで多数の停止が発生しました。すべての停止に関するアラートを受信しますが、アラートは、1分以内に自動的に再起動される異常なシステムが原因です。サイト信頼性エンジニアリング (SRE) のプラクティスに従いながら、スタッフの燃え尽き症候群を防ぐプロセスを設定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明されているシナリオでは、すべてのオプションが状況の改善に寄与する可能性がありますが、サイト信頼性エンジニアリング (SRE) のプラクティスに従ってスタッフの燃え尽き症候群を防ぐための最も適切なアクションは、異常なシステムを自動的に再起動することによる絶え間ないアラートを考えると、実際には次のようになります。
A. アクションに適さないアラートを排除します。
説明：
実用的でないアラートや、すぐに注意を払う必要のないアラートを削除することで、スタッフのアラート疲れを大幅に軽減できます。必要なアクションにつながらないアラートや誤検知であるアラートを除外することで、チームはアクション不可能なアラートに圧倒されるのではなく、重要な問題への対処に集中できます。
ただし、この特定のシナリオでは他のオプションが適していない理由に注意することが重要です。
B. 関連する SLO を再定義して、エラーバジェットを使い果たさないようにします。
サービス レベル目標 (SLO) を再定義すると、エラー バジェットの管理に役立ちますが、異常なシステムを 1 分以内に自動的に再起動する場合、これらは停止に対する正当なシステム応答であるため、アラートの絶え間ないフローに直接対処できない可能性があります。SLO を再定義しても、このコンテキストでのアラートの生成を完全に防ぐことはできません。
C. 異なるタイムゾーンのエンジニアにアラートを配布します。
異なるタイム ゾーンにアラートを分散すると、ワークロードの分散に役立ちますが、頻繁な停止のために多数のアラートが生成される問題は解決されない可能性があります。アラート疲れを複数のチームに分散させるだけです。
D. アラートごとにインシデント レポートを作成します。
インシデント レポートの作成は、インシデント後の分析とパターンの理解に役立ちますが、絶え間ないアラートの洪水によって引き起こされるスタッフの燃え尽き症候群を直接軽減することはできない場合があります。さらに、アラートごとにレポートを作成すると、主要な問題を解決せずに管理上の負担がさらに増加する可能性があります。
すべての選択肢が状況管理にプラスに働く可能性がありますが、この文脈では、アクション不可能なアラートを排除することで、アラート疲れを即座に軽減し、チームが重要なタスクに集中できるようにし、運用における不要なノイズや気晴らしを最小限に抑えるというSREのプラクティスに沿ったものになります。
</div></details>

### Q.  問題18: 回答
会社では、運用システムでバグ、停止、および速度低下が発生しています。開発者は、新機能の開発とバグ修正のために運用環境を使用します。構成と実験は運用環境で行われるため、ユーザーは停止します。テスト担当者は負荷テストに運用環境を使用しますが、多くの場合、運用システムの速度が低下します。運用環境でのバグや停止の数を減らし、テスト担当者が新機能のロード テストを行えるように、環境を再設計する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
運用環境のバグ、停止、中断を減らしながら、効率的なテストと開発を可能にする最善の方法は次のとおりです。
D. コードを記述するための開発環境と、構成、実験、およびロード テスト用のテスト環境を作成します。
このオプションが最も適している理由は次のとおりです。
1. 環境の分離: 開発環境とテスト環境を分けることで、コアとなる運用環境と、変更やテストを行う環境が分離されます。これにより、本番環境での実験、構成、負荷テストによって引き起こされる中断のリスクが最小限に抑えられます。
2.開発環境:この環境は、本番システムに影響を与えることなくコードを記述およびテストするための開発者に特化しています。運用環境を模倣できますが、主に開発目的で使用されるため、容量が小さくなります。
3. テスト環境: この環境は、構成、実験、および負荷テストを実施する目的を果たします。テスト担当者は、稼働中の運用システムに影響を与えることなく、ここで負荷テストを実行できます。これは、変更を運用環境にデプロイする前に、潜在的な問題とボトルネックを特定するのに役立ちます。
4. 制御された更新および変更管理: このアプローチにより、より制御され構造化された変更管理プロセスが可能になります。開発者は、開発環境でコードを記述し、テスト環境で徹底的にテストし、徹底的に検証した後、変更を運用環境にデプロイできます。
このアプローチにより、より安全で制御された開発およびテストプロセスが促進され、本番環境でのバグ、停止、パフォーマンスの問題の可能性が低減されると同時に、個別の専用環境での効果的な開発およびテストのプラクティスが可能になります。
</div></details>

### Q.  問題19: 回答
Google Kubernetes Engine(GKE)クラスタで一連のアプリケーションを実行しており、Stackdriver Kubernetes Engine Monitoring を使用している。会社で必要な新しいコンテナ化されたアプリケーションを本番環境に導入します。このアプリケーションはサードパーティによって作成されており、変更または再構成することはできません。アプリケーションはログ情報を /var/log/app_messages.log に書き込み、これらのログ エントリを Stackdriver Logging に送信します。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
適切なオプション – B. Fluentd デーモンセットを GKE にデプロイします。次に、カスタマイズされた入力と出力の構成を作成して、アプリケーションのポッド内のログファイルを追跡し、Stackdriver Logging に書き込みます
Stackdriver Kubernetes Engine Monitoring は、GKE 上で動作する Kubernetes クラスタのログの収集と分析を標準で提供しますが、デフォルト設定には特定のログファイルを追跡する機能は含まれていません。/var/log/app_messages.log に書き込まれたログエントリを収集するには、Fluentd デーモンセットを GKE クラスタにデプロイします。Fluentd は、特定のログファイル(この場合は /var/log/app_messages.log)を追跡し、ログエントリを Stackdriver Logging に送信するように設定できるログ コレクターおよびフォワーダーです。
Fluentd デーモンセットをデプロイすることで、カスタマイズされた入出力設定を作成し、この設定を使用してアプリケーションのポッド内のログファイルを追跡し、Stackdriver Logging に書き込むことで、特定のアプリケーションからログを収集し、ログが Stackdriver に送られ、後で分析できるようにすることができます。
オプションDは理想的な選択ではありません - ログファイルを追跡し、エントリを標準出力に書き込むために、アプリケーションのポッドでデプロイする必要があるカスタムスクリプトを作成して維持する必要があります。つまり、ポッドの新規デプロイ、更新、スケーリングのたびにこのスクリプトのメンテナンスを行う必要があり、また、スクリプトはすべてのエッジケース、エラー、および権限の問題を自分で処理する必要があります。
さらに、このオプションでは Stackdriver Logging サービスを使用しないため、スクリプトは標準出力に書き込むため、Stackdriver Logging などの集中型ログ サービスへの書き込みほど信頼性と安全性が高くない可能性があります。
インサイト - https://cloud.google.com/architecture/customizing-stackdriver-logs-fluentd
</div></details>

### Q.  問題20: 回答
セキュリティのシフトレフトに向けた企業の取り組みの一環として、InfoSec チームはすべてのチームに対し、すべての Google Kubernetes Engine(GKE)クラスタにガードレールを実装して、信頼できる承認済みのイメージのみのデプロイを許可するよう求めています。セキュリティのシフトレフトという情報セキュリティチームの目標を達成する方法を決定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
特に Google Kubernetes Engine(GKE)クラスタに関するセキュリティをシフトレフトし、信頼できる承認済みのイメージのみがデプロイされるようにするという InfoSec チームの目標を達成するには、次のような最適なアプローチが適しています。
B. バイナリ承認を使用して、CI/CD パイプライン中にイメージを証明します。
説明：
Google Kubernetes Engine(GKE)の Binary Authorization は、セキュリティ ポリシーを適用し、承認されたコンテナ イメージのみがクラスタにデプロイされるようにするための堅牢なソリューションです。このプロセスでは、GKE クラスタにデプロイする前に、CI / CD パイプライン中に構成証明を作成して適用します。Binary Authorization を統合することで、特定の証明や署名などの設定されたポリシーに基づいてイメージに "must-pass" 要件を適用し、信頼できるイメージのみがデプロイされるようにすることができます。
このコンテキストで他のオプションがあまり適していない理由:
A. アーティファクト・レジストリでコンテナ分析を有効にし、コンテナ・イメージの共通脆弱性識別子(CVE)を確認します。
Container Analysis は、コンテナー イメージ内の CVE を特定するのに役立ちますが、未承認のイメージのデプロイを直接制限するものではありません。デプロイ前に承認を強制するよりも、既知の脆弱性のイメージをスキャンすることに重点を置いています。
C. Identity and Access Management(IAM)ポリシーを構成して、GKE クラスタに最小権限モデルを作成します。
IAMポリシーは、リソースへのアクセスを制御するために不可欠ですが、信頼できるイメージのみがデプロイされるように特別に設計されているわけではありません。アクセス許可は制御されますが、承認されていないイメージのデプロイを本質的に防ぐことはできません。
D. GKEにFalcoまたはTwistlockをデプロイして、実行中のPodの脆弱性を監視します。
FalcoまたはTwistlockは、脆弱性を監視し、Podのランタイム保護を提供するランタイムセキュリティツールです。ランタイムセキュリティには価値がありますが、そもそも信頼できないイメージのデプロイを防ぐためのポリシーは適用されません。
結論として、CI / CD パイプラインでバイナリ認証を活用することで、ポリシーを適用し、承認されたイメージのみが GKE クラスタにデプロイされるようにすることで、イメージ デプロイのコンテキストでセキュリティをシフトレフトするという InfoSec チームの目標を効果的に達成できます。
インサイト - https://cloud.google.com/binary-authorization/docs/overview
</div></details>

### Q.  問題21: 回答
Cloud Run と Cloud Functions でクライアント アプリケーションを構築して実行している。クライアントでは、クライアントがログをログ・サービスにインポートできるように、すべてのログを 1 年間使用可能にする必要があります。必要なコード変更は最小限に抑える必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
Cloud Run と Cloud Functions のすべてのログを 1 年間利用できるようにし、クライアントがコードの変更を最小限に抑えながらログをログサービスにインポートできるようにするための正しいオプションは次のとおりです。
D. ログ バケットとログ シンクを作成します。ログバケットの保持期間を 365 日に設定します。バケットにログを送信するようにログシンクを設定します。ログを取得するためのバケットへのアクセス権をクライアントに付与します。
説明：
ログの保持: ログ専用のストレージ バケットを作成し、その保持期間を 365 日に設定して、必要な期間ログが保存されるようにします。
ロギングシンクの構成: Cloud Run と Cloud Functions の両方からこのログバケットにログを送信するようにロギングシンクを構成し、アプリケーション内のコードを変更することなく、ログを一元的に保存できるようにします。
クライアントアクセス: ログバケットへのアクセス権をクライアントに付与し、必要に応じてログサービスにインポートするログを取得できるようにします。
それでは、間違ったオプションについて説明しましょう。
A. Cloud Run のすべてのイメージと Cloud Functions のすべての関数を更新して、Cloud Logging とクライアントのロギング サービスの両方にログを送信します。
このオプションでは、Cloud Logging とクライアントのログ記録サービスの両方にログを直接送信するようにアプリケーションを変更することを提案します。ただし、アプリケーションの大幅なコード変更が必要になる場合があり、コードの変更を最小限に抑えるという要件と矛盾します。
B. Pub/Sub トピック、サブスクリプション、ログ シンクを作成します。すべてのログをトピックに送信するようにログシンクを構成します。ログを取得するためのトピックへのアクセス権をクライアントに付与します。
ログの処理とストリーミングに Pub/Sub を使用しても、本質的には、必要な期間のログの長期保存や保持は提供されません。これは、ログを 1 年間保持するという要件に完全には対応していない可能性があり、ストレージと保持のための追加の手順がないとクライアントのニーズに合わない可能性があります。
C. ストレージ バケットと適切な VPC ファイアウォール ルールを作成します。Cloud Run のすべてのイメージと Cloud Functions のすべての関数を更新して、ストレージ バケット内のファイルにログを送信します。
ストレージバケット内のファイルにログを送信しても、ログの一元管理が保証されない場合があり、ログをファイルとして管理すると、取得やクライアントのログサービスへの統合が困難になる可能性があります。このアプローチでは、アプリケーションのコードを大幅に変更する必要がある場合もあります。
</div></details>

### Q.  問題22: 回答
再利用可能なインフラストラクチャをコードモジュールとして開発しています。各モジュールには、テスト プロジェクトでモジュールを起動する統合テストが含まれています。ソース管理に GitHub を使用しています。機能ブランチを継続的にテストし、変更が受け入れられる前にすべてのコードがテストされていることを確認する必要があります。統合テストを自動化するソリューションを実装する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明されているシナリオでは、変更を受け入れる前に統合テストを自動化するための最も適切なオプションは次のとおりです。
オプション D: Cloud Build を使用して、特定のフォルダでテストを実行します。GitHub のプルリクエストごとに Cloud Build をトリガーします。
説明：
Cloud Build の統合:Cloud Build を利用すると、インフラストラクチャ コード モジュールに特定のテスト手順を定義できます。
ターゲットを絞ったテスト:特定のフォルダーでテストを実行すると、変更または影響を受けるコードに関連するテストのみが実行され、リソース使用量とテスト速度が最適化されます。
自動トリガー:GitHub のプルリクエストごとに Cloud Build をトリガーすることで、提案された変更時にテストが自動的に開始され、マージ前の継続的なテストと統合が保証されます。
他のオプションには自動化やテストの要素がいくつかあるかもしれませんが、同じレベルの効率や自動化は提供されない場合があります。
オプションA(Jenkinsを使用):これにより、テスト プロセスを自動化できますが、定期的なテストでは、プル要求ごとにテストをトリガーするほど迅速に問題を検出できない場合があります。
オプション B (レビュー担当者にテストの実行を依頼する):この方法は手動による介入に依存しており、コードが受け入れられる前に一貫性のある自動テストが保証されない可能性があります。
オプション C(プルリクエストのマージ後に Cloud Build を使用):プル要求がマージされた後にテストを実行すると、潜在的な問題の検出が遅れる可能性があり、問題のあるコードがメイン ブランチに入るのを防ぐことができない可能性があります。
したがって、オプション D は、機能ブランチを継続的にテストし、すべてのコード変更が受け入れられる前に徹底的にテストされるようにするという目標に最も効果的に適合します。
フォームの上部
インサイト - https://cloud.google.com/build/docs/automating-builds/create-manage-triggers
</div></details>

### Q.  問題23: 回答
複数ステップの Cloud Build パイプラインを使用して、アプリケーションをビルドし、Google Kubernetes Engine(GKE)にデプロイします。Webhook へのビルド情報の HTTP POST を実行することで、サードパーティの監視プラットフォームと統合する必要があります。開発作業を最小限にしたい。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
適切なオプション – D. Cloud Build への Cloud Pub/Sub プッシュ サブスクリプションを作成する cloud-builds PubSub トピックを Webhook に HTTP POST する
オプション – A、手順に http 要求を作成するための構造属性がなく、開発作業を最小限に抑える必要があることを覚えているため、正しくありません。
オプション– B、Aと同じ問題で正しくありません。
オプション – C、開発作業を最小限に抑える必要があるため、正しくありません
オプション – D: プッシュ サブスクリプションからメッセージを受信し、Webhook を使用して、Pub/Sub がプッシュ エンドポイントに送信する POST リクエストを処理するのが適切です。App Engine でのこれらの POST リクエストの処理の詳細については、Pub/Sub メッセージの作成と応答をご覧ください。
詳細 - https://cloud.google.com/pubsub/docs/push
https://cloud.google.com/build/docs/subscribe-build-notifications
</div></details>

### Q.  問題24: 回答
本番環境で Google Kubernetes Engine(GKE)で実行されている Node.js アプリケーションをサポートしている。アプリケーションは、依存するアプリケーションに対していくつかの HTTP 要求を行います。どの依存アプリケーションがパフォーマンスの問題を引き起こす可能性があるかを予測します。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
正しいオプション – B. Stackdriver Trace を使用してすべてのアプリケーションをインストルメント化し、サービス間 HTTP リクエストを確認する
主なキーワードは「依存アプリに複数のリクエストを行う」です。したがって、トレースが必要です。
Cloud Trace(Stackdriver は cloud Trace に変更) - 本番環境におけるパフォーマンスのボトルネックを見つけます。
Google Cloud の分散トレース システムである Cloud Trace は、アプリケーションがユーザーや他のアプリケーションからの受信リクエストを処理するのにかかる時間と、リクエストを処理するときに実行される RPC 呼び出しなどの操作を完了するのにかかる時間を把握するのに役立ちます。
インサイト - https://cloud.google.com/trace/docs/overview
Cloud Profiler - CPU とヒープの継続的なプロファイリングにより、パフォーマンスの向上とコストの削減を実現します。
追加:クラウドプロファイラーは、多かれ少なかれ、どのメソッドがどれだけのハードウェアリソースを使用しているかを知ることです。一方、Cloud Trace は、レイテンシ レポートとアプリケーションのパフォーマンスの問題を検出します。
</div></details>

### Q.  問題25: 回答
組織では、変更諮問委員会 (CAB) を使用して、既存のサービスに対するすべての変更を承認します。このプロセスを修正して、ソフトウェア配信のパフォーマンスへの悪影響を排除する必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
オプション C と E は、変更承認プロセスを修正し、ソフトウェア配信のパフォーマンスへの悪影響を最小限に抑えるための適切なアプローチです。
C. 個々の変更に対するピア レビュー ベースのプロセスに移行し、コード チェックイン時に適用され、自動テストによってサポートされます。ピアレビュープロセスに移行すると、統合前にピアによる変更のレビューが行われるため、コードの品質が保証されます。コードのチェックイン時にこのプロセスを自動テストと組み合わせることで、開発サイクルの早い段階で問題を特定して修正することができます。
E. チームの開発プラットフォームで、開発者が変更の影響に関するフィードバックを迅速に得られるようにします。変更に関する迅速なフィードバックを提供する開発プラットフォームにより、開発者は問題を迅速に検出して対処できるため、中央の承認委員会への依存が減り、展開プロセスが迅速化されます。
その他のオプションについて:
A. CAB をシニア マネージャーに置き換えて、開発から展開まで継続的に監視します。これは監視を提供する可能性がありますが、意思決定にボトルネックを再導入する可能性があり、依然として管理者の承認に大きく依存している可能性があるため、ソフトウェア配信のパフォーマンスが大幅に向上しない可能性があります。
イ.開発者は独自の変更をマージできますが、問題が見つかった場合は、チームのデプロイ プラットフォームが変更をロールバックできるようにします。開発者が変更をマージできるようにすることは、プロセスを迅速化する上で有益ですが、堅牢なロールバックメカニズムがないと、特にプラットフォームに問題のある変更を迅速に元に戻す機能がない場合、エラーのリスクが高まる可能性があります。
D. バッチ変更は、より大規模で頻度の低いソフトウェア リリースに変更されます。変更を大規模なリリースにバッチ処理すると、デプロイの頻度が減る可能性がありますが、リリースが大規模でリスクが高くなり、開発サイクルの早い段階で問題を検出して修正することが困難になる可能性があります。
要約すると、オプションCとEは、コード品質、フィードバックループの高速化、効率的な問題解決を促進し、中央承認委員会への依存を減らしながら、ソフトウェア配信パフォーマンスの最適化をより助長します。
インサイト - https://cloud.google.com/architecture/devops/devops-process-streamlining-change-approval
</div></details>

### Q.  問題26: 回答
あなたの会社は、規制の厳しい分野で事業を展開しています。セキュリティ チームでは、信頼できるコンテナ イメージのみを Google Kubernetes Engine(GKE)にデプロイできるようにする必要があります。管理オーバーヘッドを最小限に抑えながら、セキュリティ チームの要件を満たすソリューションを実装する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
管理オーバーヘッドを最小限に抑えながら、信頼できるコンテナ イメージのみを Google Kubernetes Engine(GKE)にデプロイするというセキュリティ チームの要件を満たすには、次のような最適なオプションを使用します。
A. GKE クラスタでバイナリ認証を構成して、デプロイ時のセキュリティ ポリシーを適用します。
説明：
Binary Authorization は、GKE クラスタへのイメージのデプロイにポリシーを適用するための堅牢なソリューションを提供します。これにより、デジタル署名と構成証明に基づいてデプロイできるコンテナー イメージを指定するポリシーを設定できます。Binary Authorization を構成することで、事前定義されたポリシーに照らして検証された、承認済みで信頼できるコンテナ イメージのみを GKE クラスタにデプロイできるようになります。この方法では、展開中のイメージ信頼ポリシーの適用を自動化することで、管理オーバーヘッドを最小限に抑えます。
このコンテキストで他のオプションが適していない理由:
B. roles/artifactregistry.writer ロールを Cloud Build サービス アカウントに付与します。アーティファクト・レジストリの書込み権限を持つ従業員がいないことを確認します。
アーティファクト レジストリへの書き込みアクセス権を Cloud Build サービス アカウントに付与すると、イメージのプッシュが容易になる場合がありますが、デプロイ時にイメージの信頼ポリシーが直接適用されるわけではありません。特定のセキュリティ ポリシーを適用するよりも、アクセス許可の管理に重点を置いています。
C. Cloud Run を使用してカスタム バリデーターを作成し、デプロイします。Eventarc トリガーを有効にして、新しい画像がアップロードされたときに検証を実行します。
カスタムバリデーターを作成し、検証に Eventarc トリガーを使用することは可能ですが、このアプローチでは、カスタム検証ロジックを作成および維持するための追加の開発作業が必要です。また、複雑さと管理オーバーヘッドが増す可能性もあります。
D. デプロイ時のセキュリティポリシーを適用するために GKE クラスタで実行するように Kritis を設定します。
KritisはBinary Authorizationと統合されており、ポリシーエンジンを使用してコンテナイメージを検証することで、デプロイ時のセキュリティポリシーを適用できます。ただし、Binary Authorization は GKE とより直接的に関連しており、このコンテキストではより合理化されたソリューションとなり、管理オーバーヘッドが削減されます。
要約すると、GKE クラスタで Binary Authorization を構成すると、デプロイ時のセキュリティ ポリシーが効果的に適用され、信頼できるコンテナ イメージのみがデプロイされると同時に、ポリシーの適用を自動化することで管理オーバーヘッドが最小限に抑えられます。
インサイト - https://cloud.google.com/binary-authorization/docs/overview
</div></details>

### Q.  問題27: 回答
組織は、Google Cloud プロジェクトの Cloud Operations でダッシュボードを生成するために使用されるシステムログを収集したいと考えています。現在および将来のすべての Compute Engine インスタンスがシステムログを収集するように設定し、Ops Agent が最新の状態に保たれるようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
現在および将来のすべての Compute Engine インスタンスを構成してシステムログを収集し、Cloud Operations でダッシュボードを生成するために Ops Agent を最新の状態に保つには、次の適切なアクションを実行します。
C. gcloud CLI を使用してエージェント ポリシーを作成します。
Ops エージェントのエージェント ポリシーを使用することは、Google Cloud Platform のインスタンス間でエージェントを構成および管理するための推奨される方法です。エージェント ポリシーを使用すると、複数の Compute Engine インスタンスにわたる Ops Agent の構成、設定、動作を定義できます。これにより、一元管理が可能になり、エージェントの構成の均一性が確保されます。
この方法により、現在のすべてのインスタンスがシステムログを収集するように設定でき、将来のインスタンスにも設定が自動的に適用されます。
オプション A では、gcloud CLI を使用して Cloud Asset Inventory にリストされている各 VM に Ops Agent をインストールしますが、各インスタンスに手動でインストールする必要があり、将来のインスタンスをカバーしない可能性があるため、多数のインスタンスを管理する際の効率が低下します。
オプション B の [エージェントの状態] が [未検出] の VM を選択し、 [エージェントのインストール] を選択するのは手動のアプローチであり、すべてのインスタンス (特に将来のインスタンス) をカバーできない可能性があります。
オプション D(起動スクリプトを使用して Compute Engine イメージに Ops Agent をインストールする)は、現在のインスタンスでは機能する可能性がありますが、イメージ自体が更新されない限り新しいインスタンスはカバーされないため、将来のインスタンスの管理の効率が低下します。
したがって、オプション C は、現在と将来の Compute Engine インスタンスの両方で Ops Agent の一元管理と構成を可能にするため、最適です。
インサイト - https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/managing-agent-policies
</div></details>

### Q.  問題28: 回答
現在、プロダクトは 3 つの Google Cloud Platform(GCP)ゾーンにデプロイされており、ユーザーはゾーン間で分割されています。あるゾーンから別のゾーンにフェールオーバーできますが、影響を受けるユーザーには 10 分間のサービス中断が発生します。通常、データベース障害は四半期に 1 回発生し、5 分以内に検出できます。製品の新しいリアルタイム チャット機能の信頼性リスクをカタログ化しています。各リスクについて、次の情報をカタログ化します。 平均検出時間 (MTTD) (分)* 平均修復時間 (MTTR) (分)* 平均故障間隔 (MTBF
) (日数
)* ユーザー影響率
チャット機能には、ゾーン間で正常にフェールオーバーするのに 2 倍の時間がかかる新しいデータベース システムが必要です。
新しいデータベースが 1 つのゾーンで失敗するリスクを考慮する必要があります。新しいシステムでのデータベース・フェイルオーバーのリスクの値はどうなるでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
正しいオプション – B. MTTD: 5 MTTR: 20 MTBF: 90 影響: 33%
故障は 3 つのゾーンのうちの 1 つのゾーン (1/3) であるため、影響は 33% になります。
MTTR = 20 は、「チャット機能には、ゾーン間のフェールオーバーを正常に行うのに 2 倍の時間がかかる新しいデータベース システムが必要である」ため、2x10 分です。
https://www.atlassian.com/incident-management/kpis/common-metrics https://linkedin.github.io/school-of-sre/
</div></details>

### Q.  問題29: 回答
CPU 使用率の Stackdriver グラフをワークスペース プロジェクト内のダッシュボードで作成しました。グラフをサイト信頼性エンジニアリング (SRE) チームとのみ共有します。最小特権の原則に従っていることを確認する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
右オプション – C. [Share chart by URL] をクリックし、SRE チームに URL を提供します。SRE チームに、ワークスペース プロジェクトの監視ビューアー IAM ロールを割り当てます。
「ダッシュボード閲覧者」というロールはなく、正しい名前は監視ダッシュボード構成閲覧者です(権限はダッシュボード構成への読み取り専用アクセスです)。したがって、オプションB、Dは無効です。
さらに、有効なロールは、Monitoring Viewer と Monitoring Dashboard Configuration Viewer です。メトリックス エクスプローラーを使用して URL でグラフを共有することしかできません。
roles/monitoring.dashboardViewer: ダッシュボード構成への読み取り専用アクセス権を付与します。roles/monitoring.viewer: Google Cloud Console と API のモニタリングへの読み取り専用アクセス権を付与します。
モニタリング ダッシュボード構成ビューア ロールのみの場合、ユーザーは [モニタリング] ページに何も表示できません。
オプションAとCのうち、Aはダッシュボード全体ではなくチャートのみを共有したいので、それほど関連性はありません。したがって、Cは正しい選択肢のままです。
インサイト - https://cloud.google.com/iam/docs/understanding-roles#monitoring.viewer
</div></details>

### Q.  問題30: 回答
CTO は、内部で使用するすべてのインシデントに事後分析ポリシーを実装するように依頼しました。ポリシーが会社で成功していることを確認するために、優れた事後分析とは何かを定義する必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
事後分析ポリシーを成功に導くという目的に基づいて、次の 2 つの適切な選択肢があります。
オプションC:
インシデントの重大度を含めると、システムまたは組織への影響を理解するのに役立ちます。
内部システムコンポーネントに名前を付けずに防止戦略と根本原因を詳細に説明することで、機密情報を保護しながら包括的な分析を行うことができます。
オプション E:
事後分析の作成にすべてのインシデント参加者を関与させることで、多様な視点とインシデントのより包括的な理解が保証されます。
事後分析をできるだけ広く共有することで、透明性が高まり、組織全体の学習が促進されます。
他のオプションが最良の選択ではない理由の説明:
オプションA:
責任者やチームを特定すると、責任指向の文化が生まれ、オープンな議論が妨げられ、分析が妨げられる可能性があります。
責任を転嫁するのではなく、システム的な原因に焦点を当てる方が良いでしょう。
オプションB:
インシデントがどのように悪化したかを推測しても、将来のインシデントの防止に大きく貢献しない可能性があります。
重症度と予防戦略に焦点を当てることは、学習と改善にとってより有益です。
オプションD:
インシデントの解決だけに集中すると、根本原因が見落とされ、今後同様のインシデントを防ぐための取り組みが妨げられる可能性があります。
顧客情報を省略することはプライバシーにとって重要ですが、将来の発生を理解して防止することほど適切ではない可能性があります。
要約すると、優れた事後分析は、深刻度の評価、予防戦略、すべての利害関係者の関与、および広範な共有に焦点を当てて、包括的な分析を確保し、継続的な改善の文化を育む必要があります。オプション C と E は、これらの目的により効果的に整合します。
フォームの上部
インサイト - https://sre.google/workbook/postmortem-culture/
</div></details>

### Q.  問題31: 回答
Google Kubernetes Engine(GKE)クラスタで実行される携帯電話ゲームのバックエンドをサポートします。アプリケーションは、ユーザーからの HTTP 要求を処理しています。
ネットワーク コストを削減するソリューションを実装する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
適切なオプション – D. Google Cloud HTTP ロードバランサをイングレスとして構成する
オプション - A: いいえ、意味がありません
オプション - B: Premium レベルが使用されているかどうかはわかりません。
オプション - C: これはネットワーク コストには役立ちません。
オプション– D:これは正しいです。
ロードバランサーに関連するコストは、ロードバランサーコンポーネントを含むプロジェクトに請求されます。これらの利点により、コンテナネイティブの負荷分散は、Ingressによる負荷分散に推奨されるソリューションです。NEG を GKE Ingress で使用すると、Ingress Controller によって L7 ロードバランサのあらゆる側面の作成が容易になります。これには、仮想 IP アドレスの作成、転送ルール、ヘルスチェック、ファイアウォールルールなどが含まれます。
直観：
https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke
https://cloud.google.com/vpc/network-pricing
</div></details>

### Q.  問題32: 回答
企業は、Pub/Sub、App Engine スタンダード環境、Go で記述されたアプリケーションを使用して、IoT データを大規模に処理します。ピーク負荷時にパフォーマンスが一貫して低下することに気付きました。ワークステーションでこの問題を再現できませんでした。運用環境のアプリケーションを継続的に監視して、コード内の低速パスを特定する必要があります。パフォーマンスへの影響と管理オーバーヘッドを最小限に抑える必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
説明されているシナリオと、運用環境のアプリケーションを継続的に監視して、パフォーマンスへの影響と管理オーバーヘッドを最小限に抑えながら、コード内の低速パスを特定するという目的を考えると、最適なオプションは次のとおりです。
D. Cloud Profiler を構成し、アプリケーションで cloud.google.com/go/profiler ライブラリを初期化します。
説明：
クラウドプロファイラ:Google Cloud Profiler は、Google Cloud Platform で実行されているアプリケーションのパフォーマンスを分析および最適化するために特別に設計されています。アプリケーションのパフォーマンスに大きな影響を与えることなく、プロファイリング情報を継続的に収集できます。
cloud.google.com/go/profiler ライブラリによる初期化:アプリケーションコード内でプロファイラーライブラリを初期化することで、シームレスな統合と自動データ収集が可能になり、管理オーバーヘッドが削減されます。
継続的な監視:Cloud Profiler では、継続的なパフォーマンス モニタリングが可能で、読み込みのピーク時でもコード内の低速パスを特定できます。
他のオプションがあまり効果的でない理由を簡単に分析しましょう。
オプション A(CPU 使用率に Cloud Monitoring を使用):CPU 使用率の監視は役に立ちますが、コード内の低速パスを具体的に特定できない場合があります。システム全体のメトリックを提供しますが、アプリケーション コードのパフォーマンスのボトルネックを特定できない場合があります。
オプション B(Compute Engine への継続的プロファイリング ツールのインストール):これはうまくいく可能性がありますが、Cloud Profiler などの専用サービスを使用する場合と比較して、管理オーバーヘッドが大きくなり、パフォーマンスへの影響が大きくなる可能性があります。また、Compute Engine は App Engine インスタンスとそれほど密接に結びついていない可能性があります。
オプション C (アプリケーション・インスタンスに対して go ツール pprof を定期的に実行する):この方法では、手動による介入と定期的な分析が行われるため、継続的な監視や、ピーク時の負荷時のパフォーマンスの問題を、専門のプロファイリング サービスほど効果的に特定できない場合があります。
したがって、オプション D(Cloud Profiler を使用し、アプリケーションでプロファイラー ライブラリを初期化する)は、パフォーマンスへの影響と管理オーバーヘッドを最小限に抑えながら、本番環境でアプリケーションを継続的に監視するための最も効率的かつ効果的な選択肢です。
インサイト - https://cloud.google.com/profiler/docs/profiling-go#app-engine
</div></details>

### Q.  問題33: 回答
Spinnaker を使用してアプリケーションをデプロイし、パイプラインにカナリア デプロイ ステージを作成しておきます。アプリケーションには、起動時にオブジェクトを読み込むメモリ内キャッシュがあります。カナリアバージョンと本番バージョンの比較を自動化したい。カナリア分析をどのように構成する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
適切なオプション – A. カナリアを現在の製品バージョンの新しいデプロイと比較する
カナリア デプロイと現在の運用デプロイを比較したくなるかもしれません。代わりに、Canary を、同時にデプロイされた同等のベースラインと常に比較します。ベースラインは、現在本番環境で実行されているものと同じバージョンと構成を使用しますが、それ以外はカナリアと同じです。
同時展開
同じサイズのデプロイ
同じ種類とトラフィック量
この方法では、バージョンと構成のみを制御し、キャッシュのウォームアップ時間やヒープ・サイズなど、分析に影響を与える可能性のある要因を減らします。
インサイト - https://spinnaker.io/guides/user/canary/best-practices/#compare-canary-against-baseline-not-against-production
</div></details>

### Q.  問題34: 回答
サービスのすべてのユーザーに数時間にわたって影響を与える大規模なサービス停止が発生しました。数時間のインシデント管理の後、サービスは通常に戻り、ユーザー アクセスが復元されました。インシデントの概要を、サイト信頼性エンジニアリングの推奨プラクティスに従って、関連する関係者に提供する必要があります。最初に何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
正しいオプション – B. 関係者に配布する事後分析を作成する
オプション - A: 正しくありません
オプション-B:正解
オプション - C: 正しくない、インシデント状態ドキュメントはインシデント参加者との協議に使用されます。
選択肢 - D: いいえ、それは非難の余地がないようになければなりません
インサイト - https://sre.google/sre-book/postmortem-culture/
</div></details>

### Q.  問題35: 回答
会社では、Google Kubernetes Engine(GKE)を使用してサービスを実行しています。開発環境の GKE ダスターは、詳細ロギングを有効にしてアプリケーションを実行します。デベロッパーは kubectl logs コマンドを使用してログを表示し、Cloud Logging は使用しません。アプリケーションには、統一されたログ構造が定義されていません。アプリケーションのログ記録に関連する費用を最小限に抑えながら、GKE の運用ログを収集する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
詳細ロギングが有効になっている開発環境で GKE 操作ログを収集しながら、アプリケーション ロギングに関連するコストを最小限に抑えることを目的としたシナリオでは、最も適切なオプションは次のとおりです。
D. 重大度 >= DEBUG resource.type = "k8s_container" 除外フィルターを、開発環境に関連付けられているプロジェクトの_Defaultログ シンクに追加します。
説明：
除外フィルター:除外フィルタを適用すると、Cloud Logging に送信するログを制御できます。リソースタイプ「k8s_container」の重大度レベルが DEBUG 以下のログを除外するフィルタを追加することで、詳細なアプリケーションログが Cloud Logging に取り込まれるのを防ぐことができます。
コストの最小化:詳細なアプリケーション ログ(特に DEBUG などの重大度が低いログ)を除外すると、Cloud Logging に保存されるログの量が減り、関連費用が最小限に抑えられます。
GKE 操作ログの収集:除外フィルタは、特に Kubernetes コンテナ(「k8s_container」)からのログを対象とし、GKE からの操作ログのみが収集されるようにします。
オプション A、B、C では、GKE オペレーション ログを収集しながら、アプリケーションのロギングに関連するコストを最小限に抑える必要性には特に対応していません。
オプション A(gcloud container clusters update --logging=SYSTEM):このコマンドはクラスタのログ記録システムを調整しますが、Cloud Logging に送信するログは指定しません。
オプション B(gcloud container clusters update --logging=WORKLOAD):このコマンドはワークロードのログ構成を変更し、GKE 操作ログにも影響を与える可能性があります。
オプション C(gcloud logging sinks update _Default --disabled):このコマンドは、既定のログ シンクを無効にしますが、特定のログまたはログの重大度レベルを除外するための詳細なアプローチは提供しません。
したがって、オプション D(_Default ロギング シンクに除外フィルタを追加する)は、GKE 操作ログを確実に収集しながら、アプリケーション ロギングに関連するコストを最小限に抑えるという目標を達成するための最適な選択肢です。
インサイト - https://cloud.google.com/logging/docs/view/query-library
</div></details>

### Q.  問題36: 回答
App Engine で実行されているアプリケーションをサポートしている。このアプリケーションはグローバルに使用され、さまざまなデバイスタイプからアクセスされます。接続数を知りたい。App Engine の Stackdriver Monitoring を使用している。どの指標を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
Stackdriver Monitoring for App Engine でアプリケーションへの接続数をトラッキングするには、次の指標を使用します。
A. flex/connections/current (フレックス/接続/カレント)
この指標は、App Engine のフレキシブル環境インスタンスの現在の接続数を測定します。これは、インスタンスへのアクティブな接続の数を監視するように特別に設計されており、アプリケーションが処理しているワークロードとトラフィックに関する分析情報をいつでも提供します。
オプション B() とオプション C () は、TCP SSL プロキシの負荷分散に関連付けられた指標を参照し、App Engine インスタンス自体への接続を直接表していない場合があります。tcp_ssl_proxy/new_connectionstcp_ssl_proxy/open_connections
オプション D() はオプション A と似ていますが、App Engine のフレキシブル環境インスタンスの特定のタイプまたはバージョンを対象としている可能性があります。flex/instance/connections/current
したがって、Stackdriver Monitoring で App Engine で実行されているアプリケーションへの接続数をモニタリングするのに最も適した指標は です。flex/connections/current
</div></details>

### Q.  問題37: 回答
組織のために Cloud Run でマイクロサービスを構築してデプロイしている。サービスは、内部的に多くのアプリケーションによって使用されます。新しいリリースをデプロイし、ステージング環境と運用環境で新しいバージョンを広範囲にテストする必要があります。ユーザーと開発者への影響を最小限に抑える必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
このシナリオでは、マイクロサービスの新しいリリースを Cloud Run にデプロイし、ユーザーとデベロッパーへの影響を最小限に抑えながら、ステージング環境と本番環境の両方で広範囲にテストすることを目的としています。
最善のアプローチは、
A. 新しいバージョンのサービスをステージング環境にデプロイします。トラフィックを分割し、トラフィックの 1% を最新バージョンに通過させます。最新バージョンをテストします。テストに合格した場合は、最新バージョンをステージング環境と運用環境に段階的にロールアウトします。
その理由は次のとおりです。
A. 新しいバージョンのサービスをステージング環境にデプロイします。トラフィックを分割し、トラフィックの 1% を最新バージョンに通過させます。最新バージョンをテストします。テストに合格した場合は、最新バージョンをステージング環境と運用環境に段階的にロールアウトします。
このアプローチでは、新しいバージョンをステージング環境にデプロイし、最初はテスト目的でトラフィックのごく一部(1%)のみを新しいバージョンにルーティングします。これにより、ユーザーや開発者への影響が最小限に抑えられ、徹底的なテストが可能になります。テストに合格すると、新しいバージョンをステージング環境と本番環境の両方に制御された方法で段階的にロールアウトできるため、スムーズな移行が保証されます。
他のオプションが適していない理由を評価してみましょう。
B. 新しいバージョンのサービスをステージング環境にデプロイします。トラフィックを分割し、トラフィックの 50% を最新バージョンに通過させます。最新バージョンをテストします。テストに合格した場合は、すべてのトラフィックを最新バージョンに送信します。本番環境についても繰り返します。
トラフィックの 50% を新しいバージョンに転送すると、問題が発生した場合にユーザーに大きな影響を与える可能性があります。これにより、新しいバージョンで問題が発生した場合、ユーザーエクスペリエンスと開発アクティビティが中断される可能性があります。
C. トラフィックを処理せずに、新しいバージョンのサービスを new-release タグを使用してステージング環境にデプロイします。新しいリリース バージョンをテストします。テストに合格した場合は、このタグ付けされたバージョンを徐々にロールアウトします。本番環境についても繰り返します。
この方法では、新しいリリース バージョンのテストが含まれますが、制御されたテストのための初期トラフィック分割がありません。テスト段階では、ユーザーへの影響を効果的に最小限に抑えることができず、問題が発見された場合に中断が発生する可能性があります。
D. ステージング環境として使用する緑色のタグを持つ新しい環境をデプロイします。新しいバージョンのサービスをグリーン環境にデプロイし、新しいバージョンをテストします。テストに合格したら、すべてのトラフィックをグリーン環境に送信し、既存のステージング環境を削除します。本番環境についても繰り返します。
環境を削除して再作成すると、ダウンタイムや中断が発生する可能性があります。段階的で制御された移行は提供されないため、新しいバージョンへの移行中に問題が発生した場合に混乱が発生する可能性があります。
要約すると、オプション A は、ステージング環境と運用環境の両方で新しいバージョンをデプロイおよびテストするための制御されたアプローチを提供すると同時に、最初にテスト目的でトラフィックのごく一部をルーティングすることで、ユーザーと開発者への影響を最小限に抑えます。
</div></details>

### Q.  問題38: 回答
組織は、サイト信頼性エンジニアリング (SRE) の文化と原則を実装したいと考えています。最近、サポートしているサービスで限定的な停止が発生しました。別のチームのマネージャーから、修復を実行できるように、何が起こったのかを正式に説明するように求められます。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
適切なオプション – B. 根本原因、解決策、学んだ教訓、およびアクションアイテムの優先順位付けされたリストを含む事後分析を作成します。エンジニアリング組織のドキュメントポータルで共有
C と D は、Blameless 事後分析の SRE ポリシーに反するため、間違っています。
これらのアクションはマネージャーのみと共有できないため、Aは間違っています。
インサイト - https://sre.google/sre-book/postmortem-culture/#:~:text=Blameless%20postmortems%20are%20a%20tenet,for%20bad%20or%20inappropriate%20behavior
</div></details>

### Q.  問題39: 回答
会社には、本番環境、テスト、開発用のフォルダを含む Google Cloud リソース階層があります。サイバーセキュリティ チームは、セキュリティ問題の特定と解決を迅速化するために、会社の Google Cloud セキュリティ体制を確認する必要があります。Google Cloud サービスによって生成されたすべてのプロジェクトからのログは、本番フォルダ内のみに一元化して、アラートとほぼリアルタイムの分析を可能にする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
Google Cloud サービスによって生成されたログを本番フォルダ内のすべてのプロジェクトから一元管理して、アラートとほぼリアルタイムの分析を行うには、適切なアクションは次のとおりです。
C. Pub/Sub トピックを宛先として使用する本番フォルダーに関連付けられた集約ログ シンクを作成します。
説明：
集約されたログシンクを使用すると、Google Cloud の組織またはフォルダ内の複数のプロジェクトからログを収集し、特定の宛先にルーティングできます。
運用フォルダーに関連付けられた集約ログ シンクを作成することで、そのフォルダー内のさまざまなプロジェクトのログを統合できます。
Pub/Sub トピックを宛先として使用すると、ログの管理と処理に柔軟性を持たせることができます。その後、Pub/Sub トピックをサブスクライブし、Cloud Functions や Dataflow などのサービスを使用することで、アラートやほぼリアルタイムの分析のためにこれらのログを処理できます。
オプション A では、Workflows API ですべてのログを Cloud Logging にルーティングできますが、本番環境フォルダ内のプロジェクトからのみログを送信するために必要な特異性は提供されません。これにより、すべてのプロジェクトからログがルーティングされます。
オプション B の Cloud Monitoring ワークスペースを一元的に作成し、関連プロジェクトをアタッチするオプションでは、本番フォルダ内でログを一元管理するという要件に特に対応しておらず、必要な分離が行われない可能性があります。
オプション D は、Cloud Logging バケットをデスティネーションとして使用する本番フォルダに関連付けられた集約ログシンクを作成するというもので、Pub/Sub トピックを使用する場合ほど汎用性が高くありません。ログを Cloud Logging バケットに保存することは可能ですが、Pub/Sub を使用すると、より柔軟で効率的なログ処理と分析が可能になります。
したがって、オプション C は、Google Cloud サービスのログを本番フォルダ内に一元化して、アラートとほぼリアルタイムの分析を容易にするのに最も適した選択肢です。
インサイト - https://cloudplatform.googleblog.com/2015/06/Real-Time-Log-Streaming-and-Analysis-with-Google-Cloud-Platform-Logentries.html
</div></details>

### Q.  問題40: 回答
主力サービスの容量計画を半年ごとに実施しています。今後 6 か月間のサービス利用者の増加率は前月比で 10% になると予想しています。サービスは完全にコンテナ化され、Google Cloud Platform(GCP)上で実行され、クラスタ オートスケーラーが有効になっている 3 つのゾーンで Google Kubernetes Engine(GKE)Standard リージョン クラスタを使用します。現在、デプロイされた合計 CPU 容量の約 30% を消費しており、ゾーンの障害に対する回復性が必要です。この増加の結果として、またはゾーン障害の結果としてユーザーが受ける悪影響を最小限に抑えながら、不要なコストを回避する必要があります。予測される成長に対処するために、どのように準備する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
説明されているシナリオを考えると、悪影響と費用対効果を最小限に抑えながら、予測される成長に対処するための最善のアプローチは次のとおりです。
A. ノード プールの最大サイズを確認し、ポッドの水平オートスケーラーを有効にしてから、ロード テストを実行して、予想されるリソースのニーズを確認します。
この選択の背後にある理由は次のとおりです。
1. 最大ノードプールサイズを確認する: GKE クラスタで許可されている最大ノードプールサイズを確認します。これにより、クラスターをスケールアップして需要の増加に対応できます。
2. Horizontal Pod Autoscaler (HPA) を有効にする: これにより、観測された CPU 使用率またはその他の選択したメトリックに基づいて、デプロイ内のポッドの数を自動的にスケーリングできます。これは、過剰にプロビジョニングすることなくリソースを効率的に利用するのに役立ちます。
3. ロード テストを実行する: ロード テストを実施することで、予想される成長をシミュレートし、現在のインフラストラクチャと構成が予想される負荷を処理できるかどうかを検証できます。この手順は、リソースのニーズを正確に見積もるのに役立ちます。
Cluster Autoscaler は Kubernetes Engine クラスタのサイズを自動的に管理しますが、リソース要件を検証し、クラスタが水平方向 (HPA を使用) と垂直方向 (Cluster Autoscaler を使用) の両方で効率的にスケーリングできることを確認することが重要です。
ノード容量を 60% 増やすと (オプション D で説明)、過剰なプロビジョニングと不要なコストが発生する可能性があります。さらに、オプション C では、現在の 30% の使用率で十分であると想定されていますが、成長率が予想外に増加した場合や、追加の回復力が必要な場合は、当てはまらない可能性があります。
したがって、オプションAは、現在のセットアップを最適化し、適切なツールを使用してスケーラビリティを確保し、リソースの大幅な拡張にコミットする前に予想される成長を処理するインフラストラクチャの能力を検証することで、よりバランスの取れたアプローチを提供します。
インサイト - https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
</div></details>

### Q.  問題41: 回答
オンプレミスと Google Cloud Platform にデプロイされた大規模な Google Kubernetes Engine(GKE)クラスタで実行される e コマース アプリケーションをサポートしています。アプリケーションは、コンテナーで実行されるマイクロサービスで構成されます。CPU とメモリを最も多く使用しているコンテナーを特定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
Google Kubernetes Engine(GKE)クラスタで CPU とメモリを最も多く使用しているコンテナを特定するには、次のような最適なアプローチがあります。
A. Stackdriver Kubernetes Engine Monitoring を使用する
このオプションが推奨される理由は次のとおりです。
Stackdriver Kubernetes Engine Monitoring(現在は Google Cloud のオペレーション スイートとして知られています)は、GKE クラスタ向けに特別に調整された包括的なモニタリング機能を提供します。
これにより、CPU やメモリの消費量などのリソース使用率メトリックを、クラスター レベルとポッド/コンテナー レベルの両方で視覚化して分析できます。
カスタムダッシュボードを設定して、さまざまなコンテナのリソース使用状況を監視し、CPUとメモリを最も多く消費しているコンテナを簡単に特定できます。
Stackdriver は履歴データを提供し、時間の経過に伴う傾向を分析し、過去のパフォーマンスに基づいて情報に基づいた意思決定を行うことができます。
Grafana で Prometheus を使用したり、分析のためにログを BigQuery にエクスポートしたりするなど、他のオプションも可能ですが、Stackdriver Kubernetes Engine Monitoring は、GKE クラスタをモニタリングするためのよりネイティブで統合されたアプローチを提供し、リソース モニタリング専用のツールを提供するため、このシナリオにより適しています。
インサイト - https://cloud.google.com/anthos/clusters/docs/on-prem/1.5/concepts/logging-and-monitoring
</div></details>

### Q.  問題42: 回答
一連の Google Kubernetes Engine(GKE)クラスタへの本番環境のデプロイを管理しています。信頼できる CI/CD パイプラインによって正常にビルドされたイメージのみが運用環境にデプロイされるようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明
ここで説明するシナリオでは、CI / CD パイプラインの信頼できる検証済みのコンテナ イメージのみが Google Kubernetes Engine(GKE)クラスタの本番環境にデプロイされるようにします。
このレベルの画像検証と適用を実現するための最も適切な選択肢は次のとおりです。
D. Kubernetes Engine クラスタをバイナリ認証でセットアップします。
この選択の背後にある理由は次のとおりです。
Binary Authorization は Google Kubernetes Engine が提供する機能で、GKE クラスタへのコンテナ イメージのデプロイを制御するポリシーを定義して適用できます。Container Registry やその他のサービスと統合して、承認されたコンテナー イメージのみがデプロイされるようにします。
GKE クラスタで Binary Authorization を設定することで、信頼できる CI / CD パイプラインによって署名されたイメージのみを本番環境にデプロイすることを義務付けるポリシーを確立できます。これにより、未承認のイメージや未検証のイメージが本番環境で使用されるのを効果的に防ぎます。
オプション A と B(クラウド セキュリティ スキャナーと脆弱性分析)は、主にコンテナーまたはクラスター内の脆弱性や脅威のスキャンなどのセキュリティの側面に焦点を当てていますが、信頼できるパイプラインからの特定のイメージのデプロイを直接強制するものではありません。
オプション C(Kubernetes Engine クラスタをプライベート クラスタとして設定)は、GKE クラスタのコントロール プレーンへの外部アクセスを制限するセキュリティ対策ですが、CI / CD パイプラインの信頼できるイメージのみを本番環境にデプロイできるようにするという特定の要件には対応していません。
したがって、オプション D のバイナリ承認は、信頼できる CI / CD パイプラインによって構築された検証済みで信頼できるコンテナ イメージのみが本番環境の GKE クラスタにデプロイされるようにする必要性に最も密接に関連しています。
インサイト - https://cloud.google.com/binary-authorization
</div></details>



## 2
### Q.  問題1: 回答
Stackdriver Workspaces を使用して、本番環境で Google Cloud Platform(GCP)プロジェクトを監視するための戦略を策定しています。要件の 1 つは、開発プロジェクトやステージング プロジェクトからの誤ったアラートなしに、運用環境の問題を迅速に特定して対応できるようにすることです。関連するチーム メンバーに Stackdriver Workspaces へのアクセス権を付与する際には、最小権限の原則に従う必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明適切なオプション – D. 新しい GCP モニタリング プロジェクトを作成し、その中に Stackdriver ワークスペースを作成します。運用プロジェクトをこのワークスペースにアタッチします。関連するチームメンバーに Stackdriver ワークスペースへの読み取りアクセス権を付与する
複数のプロジェクトのメトリックを管理する場合は、そのメトリック スコープのスコープ プロジェクトとなるプロジェクトを作成することをお勧めします。
インサイト - https://cloud.google.com/monitoring/settings/multiple-projects
</div></details>

### Q.  問題2: 回答
Cloud Monitoring のカスタム ダッシュボードをパートナー チームと共有したい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Cloud Monitoring カスタム ダッシュボードをパートナー チームと共有するための適切なアクションは次のとおりです。
D. ダッシュボードの JSON 定義をダウンロードし、パートナー チームに JSON ファイルを送信します。
説明：
ダッシュボードの JSON 定義をダウンロードすると、ダッシュボードの完全な構成、レイアウト、および設定をファイルとしてエクスポートできます。
この JSON ファイルをパートナー チームと共有すると、パートナー チームはダッシュボードを独自の Cloud Monitoring 環境に直接インポートできます。
この方法により、パートナーチームは、手動で再作成することなく、レイアウト、ウィジェット、構成など、ダッシュボード全体にアクセスできます。
他のオプションでは、カスタムダッシュボードを共有するための包括的な方法が提供されない場合があります。
A. パートナー チームにダッシュボード URL を提供する: これによりアクセス権は付与されますが、パートナー チームが独自のコピーを作成したり、環境にインポートしたりすることはできません。
B. 指標を BigQuery にエクスポートして Looker Studio を使用する: このアプローチでは、別のプラットフォームで新しいダッシュボードを作成するため、元の Cloud Monitoring ダッシュボードと同じカスタマイズや構成が保持されない可能性があります。
C. Monitoring Query Language (MQL) クエリのコピー: クエリのみを共有する場合、同じ視覚的表現を再現するために重要なダッシュボード構成、レイアウト、またはウィジェット全体は含まれません。
</div></details>

### Q.  問題3: 回答
グローバル HTTP/S Cloud Load Balancer(CLB)の背後で Google Kubernetes Engine(GKE)上で動作するマルチリージョン ウェブサービスをサポートしている。従来の理由により、ユーザー要求は最初にサードパーティのコンテンツ配信ネットワーク (CDN) を経由し、次にトラフィックが CLB にルーティングされます。可用性サービスレベルインジケータ (SLI) を CLB レベルですでに実装しておきます。ただし、ロードバランサーの設定ミス、CDNの障害、またはその他のグローバルなネットワークの大惨事が発生した場合に備えて、カバレッジを増やす必要があります。この新しいSLIはどこで測定すべきでしょうか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明ロードバランサーの設定ミス、CDNの障害、その他のグローバルなネットワーク障害が発生した場合にカバレッジを拡大するために、新しいサービスレベル指標(SLI)を測定する理想的な場所は次のとおりです。
C. アプリケーションサーバーからエクスポートされたメトリック。アプリケーション・サーバーからエクスポートされたメトリックは、サーバー・レベルでのアプリケーションのパフォーマンスと動作に関する洞察を提供できます。リクエストの成功率、レイテンシー、エラー率などの指標を監視することで、アプリケーションのパフォーマンスを可視化し、ロードバランサーの設定ミス、CDNの障害、アプリケーションのバックエンドに影響を与えるグローバルネットワークの問題に関連する問題を検出できます。
E.シミュレートされたユーザー要求を定期的に送信する合成クライアント。合成クライアントを使用すると、さまざまな場所からのユーザー要求をシミュレートし、さまざまなリージョン間でのアプリケーションのアクセシビリティとパフォーマンスについてより広い視点を提供できます。これは、CDN の障害、ロードバランサーの設定ミス、またはアプリケーションサーバーに到達する前にユーザー要求に影響を与える可能性のあるグローバルネットワークの問題に関連する問題を検出するのに役立ちます。
他のオプション (A、B、D) は貴重な情報を提供する可能性がありますが、サービスの可用性に影響を与える可能性のあるグローバル アクセシビリティ、ロード バランサーの構成ミス、CDN 障害、またはグローバル ネットワークの問題に影響を与える潜在的な問題のより広い範囲を直接カバーしていない可能性があります。
インサイト - https://cloud.google.com/architecture/adopting-slos#choosing_a_measurement_method
</div></details>

### Q.  問題4: 回答
現在、組織の Google Cloud プロジェクトの Cloud Monitoring 指標を表示する方法を計画しています。組織には 3 つのフォルダーと 6 つのプロジェクトがあります。
Cloud Monitoring ダッシュボードを構成して、1 つのフォルダ内のプロジェクトの指標のみを表示する場合。ダッシュボードに他のフォルダー内のプロジェクトのメトリックが表示されないようにする必要があります。Google が推奨する方法に従う場合。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明特定のプロジェクトの Cloud Monitoring 指標を 1 つのフォルダに表示し、他のフォルダの指標がダッシュボードに含まれないようにするには、次の方法をお勧めします。
B. フォルダーごとに新しいスコープ プロジェクトを作成します。
説明：
1. プロジェクトのスコープ設定: フォルダごとに個別のスコープ プロジェクトを作成することで、Cloud Monitoring ダッシュボードに表示されるモニタリング指標をより詳細に制御し、分離できます。各スコープ プロジェクトは、対応するフォルダー内のプロジェクトのメトリックに特に焦点を当てます。
2. メトリックの分離: このアプローチでは、ダッシュボードに表示される監視メトリックの範囲を定義および制限して、各フォルダー内のプロジェクトのみを含めることができます。他のフォルダーのメトリックが混在したり、不適切に表示されたりすることはありません。
この方法は、プロジェクトフォルダに基づいて指標を明確に分離し、プロジェクトの特定のグループごとに明確さと焦点を絞った監視を確保することで、ベストプラクティスに従います。
他のオプションでは、同じレベルの粒度や分離は提供されません。
A. 1 つの新しいスコープ プロジェクトを作成しても、各フォルダーのメトリックに必要な分離が得られず、他のフォルダーのメトリックが含まれる可能性があります。
C. スコープ プロジェクトとして 1 つのプロジェクト (app-one-prod) を使用すると、特定のフォルダー内のプロジェクトのみを含むようにメトリックを効果的に制限できない場合があります。
D. 複数のプロジェクト (app-one-dev、app-one-staging、app-one-prod) を各フォルダーのスコープ プロジェクトとして使用すると、各フォルダーのメトリックのスコープが明確で独立していない可能性があり、フォルダー間でメトリックが混乱したり混在したりする可能性があります。
インサイト - https://cloud.google.com/monitoring/settings/multiple-projects
</div></details>

### Q.  問題5: 回答
1 つの Compute Engine インスタンスで実行される本番稼働サービスをサポートする。クラッシュしたインスタンスを削除し、関連するイメージに基づいて新しいインスタンスを作成することで、サービスの再作成に定期的に時間を費やす必要があります。サイト信頼性エンジニアリングの原則に従いながら、手動操作の実行に費やす時間を削減したい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明サイト信頼性エンジニアリング (SRE) の原則に従いながら、手動操作に費やす時間を短縮するための最も適切なアクションは次のとおりです。
B. 1 つのインスタンスでマネージド インスタンス グループを作成し、ヘルス チェックを使用してシステムの状態を判断します。
その理由は次のとおりです。
1. マネージド インスタンス グループ(MIG): マネージド インスタンス グループを作成すると、1 つのエンティティとして管理される Compute Engine インスタンスのグループを定義できます。最初はグループ内に 1 つのインスタンスがある場合でも、将来必要になった場合に複数のインスタンスに簡単にスケーリングできる柔軟性を提供します。
2. ヘルス・チェック: マネージド・インスタンス・グループ内でヘルス・チェックを使用すると、インスタンスのヘルスを自動的に監視できます。ヘルスチェックでは、インスタンスのステータスが定期的に評価され、インスタンスが正常に動作しているか、交換が必要かを判断します。
オプション A、C、D は役に立ちますが、クラッシュするインスタンスの繰り返し発生する問題に直接対処したり、手動による介入を減らしたりすることはできません。
· オプションAでは、開発チームにバグを報告して、クラッシュしたインスタンスの根本原因を見つけます。根本原因に対処することは長期的な安定性に不可欠ですが、インスタンスの手動再作成を減らすための即時の解決策を提供するものではありません。
· オプション C では、ロードバランサーを追加し、ヘルスチェックを使用します。これは冗長性と信頼性のための優れたプラクティスですが、1 つのインスタンスでは過度に複雑になる可能性があり、再作成プロセスに直接対処しない可能性があります。
· オプション D では、インスタンスがクラッシュしたときにすぐに通知される SMS アラートを含む Stackdriver Monitoring ダッシュボードを作成することを提案しています。監視とアラートは問題を検出するために重要ですが、このオプションは、アラートの受信後にインスタンスを再作成するために手動による介入に依存しています。
ヘルスチェック(オプションB)を使用してマネージドインスタンスグループを設定することで、インスタンスがクラッシュしたり異常になったりしたことをシステムが自動的に検出し、その後、関連するイメージに基づいてインスタンスを再作成できます。このアプローチは、復旧プロセスを自動化し、手動操作に費やす時間を短縮することで、システムの信頼性を高めることで、SREの原則に沿ったものです。
</div></details>

### Q.  問題6: 不正解
Cloud Run で実行されるアプリケーションがあります。ライブ運用トラフィックを使用してアプリケーションの新しいバージョンをテストし、品質保証チームに手動テストを実行させたい。新しいバージョンのテスト中に問題の潜在的な影響を制限し、必要に応じて以前のバージョンのアプリケーションにロールバックできる必要があります。新しいバージョンをどのように展開する必要がありますか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明新しいバージョンのアプリケーションを Cloud Run にデプロイして、本番環境のライブ トラフィックでテストし、必要に応じてロールバックできるようにするには、次の 2 つの手順をお勧めします。
A. アプリケーションを新しい Cloud Run サービスとしてデプロイします。
新しいバージョンを別のサービスとしてデプロイすると、本番トラフィックから分離されます。このアプローチにより、新しいバージョンの潜在的な問題が本番環境に影響を与えるのを防ぐことができます。
E. 新しいアプリケーションバージョンを導入し、トラフィックを新しいバージョンに分割します。
トラフィック分割を利用すると、ライブプロダクショントラフィックの一部を新しいバージョンに徐々に移行できます。これにより、問題が発生した場合の影響を制限しながら、実際のユーザートラフィックで新しいバージョンをテストできます。問題が発生した場合は、トラフィックの分割を簡単に調整したり、すべてのトラフィックを以前のバージョンにルーティングしたりできます。
他のオプションが適していない理由の説明:
オプション B(タグを使用して新しい Cloud Run リビジョンをデプロイし、--no-traffic オプションを使用する):--no-traffic オプションを使用すると、新しいリビジョンはライブプロダクショントラフィックを受信しなくなります。ライブトラフィックでのテストは許可されません。
オプション C(タグなしで新しい Cloud Run リビジョンをデプロイし、--no-traffic オプションを使用する):オプション B と同様に、--no-traffic を使用すると、新しいリビジョンがライブトラフィックを受信できなくなり、テストの有効性が制限されます。
オプション D (新しいアプリケーション・バージョンをデプロイし、--no-traffic オプションを使用します。本番環境のトラフィックをリビジョンの URL にルーティングする): --no-traffic を使用すると、新しいリビジョンへのライブトラフィックが防止されるため、本番環境のトラフィックをリビジョンの URL にルーティングすると、この設定と矛盾します。
そのため、新しいバージョンを別の Cloud Run サービスとしてデプロイし、トラフィックを新しいバージョンに分割することで、必要に応じてロールバックする機能を維持しながら、本番環境のライブ トラフィックでテストするための効果的なアプローチが提供されます。
フォームの上部
https://cloud.google.com/run/docs/deploying
</div></details>

### Q.  問題7: 回答
Compute Engine インスタンスのフリートを Google Cloud にデプロイしました。インスタンスのモニタリング指標とログが、会社の運用チームとサイバーセキュリティ チームによって Cloud Logging と Cloud Monitoring に表示されるようにする必要があります。最小権限の原則に従いながら、Identity and Access Management(IAM)を使用して、Compute Engine サービス アカウントに必要なロールを付与する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明最小権限の原則に従いながら、Compute Engine インスタンスのモニタリング指標とログを Cloud Logging と Cloud Monitoring で確実に表示するには、次の選択肢が最も適切です。
A. logging.logWriter ロールと monitoring.metricWriter ロールを Compute Engine サービス アカウントに付与します。
説明：
· logging.logWriter に追加します。このロールは、Cloud Logging のログへの書き込みアクセス権を提供します。これにより、サービス アカウントはログを書き込むことができ、Compute Engine インスタンスによって生成されたログを可視化できます。
· monitoring.metricWriter:このロールにより、サービス アカウントは Cloud Monitoring に指標を書き込むことができ、モニタリング目的で指標を公開するためのアクセス権を付与できます。
これらのロールは、運用チームとサイバーセキュリティ チームがログとメトリックを表示および管理するために必要なアクセス許可を提供し、特定のタスクに必要な権限を超えて過剰なアクセス許可を付与するため、最小特権の原則と一致します。
オプション B、C、D では、より広範なアクセス許可が付与されますが、必要なアクセス レベルを超える可能性があり、不要な特権を提供することでセキュリティ リスクが生じる可能性があります。
· オプション B (logging.admin および monitoring.editor ロール):これらのロールは、それぞれ管理機能と編集機能を付与しますが、これらは指定されたタスクに必要以上に広範囲に及ぶ可能性があります。
· オプション C (logging.editor および monitoring.metricWriter ロール):logging.editor ロールは、必要以上の権限を与えるため、ログ構成に不要な変更が加えられる可能性があります。
· オプション D (logging.logWriter および monitoring.editor ロール):monitoring.editor は、監視構成を編集するためのアクセス権を付与しますが、メトリックを表示するためだけに必要以上のアクセス許可を提供します。
したがって、logging.logWriter ロールと monitoring.metricWriter ロールを使用して特定のターゲットを絞ったアクセス許可を付与するオプション A は、最小特権の原則に従いながら、監視メトリックとログの可視性を確保するための最も適切な選択肢です。
インサイト - https://cloud.google.com/logging/docs/access-control
</div></details>

### Q.  問題8: 回答
今後の分析のために Cloud Logging から BigQuery にログエントリをエクスポートするための Cloud Logging シンクを作成しています。組織には、開発プロジェクトを含む Dev という名前の Google Cloud フォルダと、本番環境プロジェクトを含む Prod という名前のフォルダがあります。開発プロジェクトのログエントリは dev_dataset にエクスポートし、運用プロジェクトのログエントリは prod_dataset にエクスポートする必要があります。作成されるログ シンクの数を最小限にとどめ、ログ シンクが将来のプロジェクトに適用されるようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明このシナリオの正しいオプションは次のとおりです。
D. Dev フォルダーと Prod フォルダーに集約されたログ シンクを作成します。
説明：
Dev フォルダー レベルと Prod フォルダー レベルで集計されたログ シンクを設定することで、これらのフォルダー内に作成された新しいプロジェクトが、フォルダー レベルで設定されたログ構成を自動的に継承するようになります。
このアプローチは、指定された Dev フォルダーと Prod フォルダー内のすべての既存および将来のプロジェクトを効率的にカバーし、開発プロジェクトのログ エントリが dev_dataset に移動し、運用プロジェクトのログ エントリが prod_dataset に移動することを保証します。
これにより、必要なログ シンクの数が最小限に抑えられると同時に、指定されたフォルダー内の将来のプロジェクトに対してスケーラブルなソリューションが提供されます。
他のオプションが最良の選択ではない理由:
A. 組織レベルで 1 つの集約ログ シンクを作成します。
このオプションは、開発プロジェクトと運用プロジェクトのログエントリを別々のデータセット (dev_dataset と prod_dataset) に送信するという特定の要件には対応していません。これは、プロジェクト環境に基づいてログを分離する必要性と一致していません。
B. 各プロジェクトにログ シンクを作成します。
このオプションでは、個々のプロジェクトに基づいてログを分離できますが、作成されるログ シンクの数を最小限に抑えるという目標と矛盾します。これは効率的な方法ではなく、将来のプロジェクトの一貫性を保証するものではありません。
C. 組織レベルで 2 つの集約ログ シンクを作成し、プロジェクト ID でフィルター処理します。
組織レベルでのプロジェクト ID によるフィルター処理は、特に一貫した命名規則がない場合、将来のプロジェクトにとってスケーラブルまたは信頼性の高いアプローチではない可能性があります。また、ログをdev_datasetとprod_datasetに適切に分離することも保証されない場合があります。
</div></details>

### Q.  問題9: 不正解
Google Cloud Platform(GCP)で実行されるトラフィックの多いウェブアプリケーションをサポートしています。アプリケーションの信頼性は、エンジニアリング上の変更を行わずに、ユーザーの観点から測定する必要があります。
あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Cloud Platform(GCP)で実行されているトラフィックの多いウェブアプリケーションに技術的な変更を加えることなく、ユーザーの視点からアプリケーションの信頼性を測定するには、次のアプローチが最も適しています。
A. 現在のアプリケーション メトリックを確認し、必要に応じて新しいメトリックを追加します
既存のアプリケーションメトリックを評価および確認することで、アプリケーションのパフォーマンスに関する洞察を得ることができます。必要に応じて、ユーザーエクスペリエンスを反映した新しいメトリックを追加することで、アプリケーションを直接変更することなく理解を深めることができます。
C. Web プロキシ ログのみを分析し、各要求の応答時間をキャプチャします。
Web プロキシ ログを分析して各要求の応答時間をキャプチャすると、アプリケーション コードを変更することなく、ユーザー エクスペリエンスとアプリケーションのパフォーマンスに関する貴重な分析情報を提供できます。
説明：
オプション B (ユーザー操作のための追加情報をキャプチャするようにコードを変更する) では、アプリケーション コードに変更を加える必要がありますが、これは、アプリケーションにエンジニアリング上の変更を加えないという要件と矛盾します。
オプション D (Create new synthetic clients to simulate a user journey using the application) では、実際のユーザーエクスペリエンスや行動を正確に反映していない可能性のある合成ユーザージャーニーを生成します。
オプション E (現在および過去の要求ログを使用して、顧客とアプリケーションとの対話を追跡する) は、ユーザーの操作に関する分析情報を提供する場合があります。ただし、ユーザージャーニーを完全にカバーしておらず、リアルタイムの分析が不足している可能性があります。
したがって、エンジニアリング上の変更を行わずにユーザーの観点からアプリケーションの信頼性を測定するのに最適なオプションは、A (現在のアプリケーション メトリックを確認し、必要に応じて新しいメトリックを追加する) と C (Web プロキシ ログのみを分析し、各要求の応答時間をキャプチャする) です。これらのメソッドは、アプリケーションのコードを直接変更することなく、アプリケーションのパフォーマンスとユーザーエクスペリエンスに関する貴重な洞察を提供します。
</div></details>

### Q.  問題10: 回答
まもなく公開されるサービスの Cloud Monitoring SLO を作成する必要があります。サービスへの要求が、暦月あたり少なくとも 90% の時間で 300 ミリ秒未満で処理されることを確認する必要があります。使用するメトリックと評価方法を特定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明サービスへの要求が暦月あたり少なくとも 90% の時間で 300 ミリ秒未満で処理されるようにすることを目指すシナリオでは、メトリックと評価方法に最も適した選択肢は次のとおりです。
A. 要求ベースの評価方法の待機時間メトリックを選択します。
説明：
· レイテンシーメトリック:ここで重要な要件は、要求が処理されるまでにかかる時間を監視および測定することであり、これは待機時間のメトリックと一致します。
· 要求ベースの評価方法:要求の少なくとも 90% が指定された時間枠 (300 ミリ秒) 内に処理されるようにするには、要求ベースの評価方法を利用するのが適切です。この方法では、個々の要求の待機時間を評価し、これらの要求の高い割合が定義されたしきい値を満たすようにします。
要求ベースの評価方法に待機時間メトリックを使用すると、各要求のパフォーマンスを追跡し、サービスが応答時間に対して指定されたパフォーマンス目標を満たしていることを確認できます。
他のオプション(B、C、D)は、要件とあまり一致しません。
· オプション B (ウィンドウベースの評価方法のレイテンシ メトリックを選択):これには、個々の要求ではなく、特定の時間枠内の待機時間メトリックの評価が含まれるため、要求の 90% が 300 ミリ秒以内に処理されたかどうかを正確に測定できない可能性があります。
· オプション C (要求ベースの評価方法の可用性メトリックを選択):可用性メトリックは、通常、要求の処理にかかった時間ではなく、サービスにアクセスできるかどうかを測定します。
· オプション D (ウィンドウベースの評価方法の可用性メトリックを選択):オプション C と同様に、これは特定の時間枠内の可用性を含み、待機時間の要件に直接対処するものではありません。
したがって、オプション A(リクエストベースの評価方法のレイテンシ指標の選択)は、Cloud Monitoring SLO の指定された期間内にリクエストが確実に処理されるようにするための最も適切な選択肢です。
インサイト - https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#compliance-period
https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#slo-types
</div></details>

### Q.  問題11: 回答
Terraform を使用して、CI/CD パイプライン内のコードとしてのインフラストラクチャを管理しています。Google Cloud プロジェクトにはインフラストラクチャ スタック全体のコピーが複数存在し、既存のインフラストラクチャに変更が加えられるたびに新しいコピーが作成されていることに気付きました。クラウド支出を最適化するには、インフラストラクチャスタックのインスタンスが一度に1つだけ存在するようにする必要があります。Google が推奨する方法に従う場合。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明提供されているオプションの中で、Terraform の状態を管理し、インフラストラクチャ スタックのインスタンスが一度に 1 つだけ存在するようにするための Google 推奨プラクティスに最も適したアプローチは次のとおりです。
B. パイプラインが Terraform gcs バックエンドを使用して Cloud Storage から terraform.tfstate ファイルを保存および取得していることを確認します。
説明：
A. 新しいパイプラインを作成して、不要になった古いインフラストラクチャスタックを削除します。
古いインフラストラクチャ スタックを削除すると、リソースの管理に役立つ可能性がありますが、変更が行われるたびに複数のコピーが作成されるという問題には特に対処していません。このアプローチでは、インフラストラクチャの追跡と管理が困難になる可能性があります。
B. パイプラインが Terraform gcs バックエンドを使用して Cloud Storage から terraform.tfstate ファイルを保存および取得していることを確認します。
Google Cloud Storage(GCS)などのリモートバックエンドをTerraformで利用することで、CI/CDパイプラインからアクセス可能な単一の状態ファイルを維持できます。これにより、状態情報への同時アクセスが可能になり、インフラストラクチャ スタックのインスタンスが一度に 1 つだけ存在するようになります。
C. パイプラインがソース管理から terraform.tfstate ファイルを格納および取得していることを確認します。
Terraform状態ファイルをソース管理に格納することは、状態ロックを処理せず、複数のユーザーまたはプロセスが同じ状態を同時に管理しようとすると不整合や競合が発生する可能性があるため、お薦めしない場合があります。
D. パイプラインを更新して、最新の構成を適用する前に既存のインフラストラクチャを削除します。
このアプローチでは、新しい構成を適用する前に既存のインフラストラクチャを削除することを提案するため、ダウンタイムとデータ損失につながる可能性があります。インフラストラクチャの変更を管理するための安全で効率的な方法ではない可能性があります。
Google Cloud Storage(GCS)のようなリモート バックエンドと Terraform を使用するオプション B は、より優れた状態管理、同時アクセス、ロック メカニズムを提供し、インフラストラクチャの変更全体で一貫性のある単一状態管理を確保し、インフラストラクチャ スタックの単一インスタンスを常に維持することでクラウド支出を最適化するため、Google が推奨しています。
</div></details>

### Q.  問題12: 回答
あなたは、会社のデータサービスと製品の管理を担当するサイト信頼性エンジニアです。予測不可能なデータ量や高コストなどの運用上の課題を、会社のデータ インジェスト プロセスで定期的に解決します。最近、新しいデータ インジェスト プロダクトが Google Cloud で開発されることを知りました。製品開発チームと協力して、新製品に関する運用上のインプットを提供する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明データ サービスと製品の管理を担当するサイト信頼性エンジニアとしての役割を考えると、新しいデータ インジェスト製品に関する運用上のインプットを提供する最も効果的なアプローチは次のとおりです。
D. 製品開発チームと一緒に製品の設計をレビューし、設計段階の早い段階でフィードバックを提供します。
説明：
· 早期フィードバック:開発フェーズの早い段階で製品設計を見直すことで、プロセスの後半で潜在的な運用上の問題を防ぐことができるインプットや提案を提供することができます。このプロアクティブなアプローチにより、運用上の考慮事項が最初から考慮され、後の段階でコストのかかる調整ややり直しを回避できる可能性があります。
· 共同アプローチ:設計フェーズで製品開発チームと緊密に連携することで、データ量の課題、コストへの影響、潜在的なパフォーマンスのボトルネック、データ インジェストのスケーラビリティと信頼性に関するベスト プラクティスに関する分析情報を共有できます。
オプション A、B、C では、開発または展開の特定の段階の後に情報を共有したり、テストを実施したりします。
· オプション A (テスト環境でのロード テスト):負荷テストは重要ですが、プロトタイプが既にデプロイされている後に実施すると、問題が特定される可能性がありますが、重要な変更を簡単に行うには開発サイクルの後半になる可能性があります。
· オプション B (QA とコンプライアンスに合格した後のログとメトリックの共有):この段階でのログとメトリックの共有は不可欠ですが、製品はすでに高度な開発フェーズにあるため、影響のある変更の範囲が制限される可能性があります。
· オプション C (内部顧客による運用環境での使用後のログとメトリックの共有):製品が使用されるまで待つと、ライブ環境で運用上の課題が発見され、中断と対処にコストがかかる可能性があります。
したがって、オプションDは、貴重な洞察と運用上のインプットを早い段階で提供し、最適なパフォーマンス、スケーラビリティ、および費用対効果に必要な考慮事項を製品開発プロセスに統合できるため、最も効果的な選択肢です。
インサイト - https://sre.google/sre-book/evolving-sre-engagement-model/
</div></details>

### Q.  問題13: 回答
コンテナ化されたアプリケーションの新しいバージョンがテスト済みで、Google Kubernetes Engine(GKE)の本番環境にデプロイする準備が整いました。実稼働前の環境で新しいバージョンを完全にロード テストすることができず、デプロイ後にアプリケーションにパフォーマンスの問題がないことを確認する必要があります。デプロイは自動化する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明コンテナ化されたアプリケーションの新しいバージョンが、本番環境前環境での完全な負荷テストなしで Google Kubernetes Engine(GKE)の本番環境にデプロイする準備ができており、デプロイ後にパフォーマンスの問題がないことを確認するために自動デプロイが必要な場合、最適な選択肢は次のとおりです。
A. カナリア デプロイを使用して、継続的デリバリー パイプラインを介してアプリケーションをデプロイします。Cloud Monitoring を使用してパフォーマンスの問題を探し、指標でサポートされているトラフィックを増やします。
説明：
カナリアデプロイメント:
カナリア デプロイでは、既存のバージョンの大部分を維持しながら、ユーザーまたはトラフィックのサブセットに新しいバージョンを段階的にロールアウトできます。
これにより、公開が制限されたライブプロダクション環境での新しいバージョンのパフォーマンスの監視が容易になり、リスクが最小限に抑えられます。
トラフィックの増加とクラウド監視:
観測されたパフォーマンス メトリックに基づいて新しいバージョンへのトラフィックを徐々に増やすことで、実際の負荷条件下での動作を制御的に評価できます。
Cloud Monitoring は、このプロセス中のさまざまな指標を可視化し、パフォーマンスの問題や不規則性の特定に役立ちます。
カナリア デプロイは、運用前の環境で完全なロード テストを実行できない場合に適しており、リスクを軽減して運用環境でアプリケーションのパフォーマンスを検証するための露出を制御できます。
ブルー/グリーン デプロイ (オプション B) も効果的ですが、カナリア デプロイでは、テストされていないバージョンを運用環境にデプロイするときに重要な、潜在的なパフォーマンスの問題を早期に特定するための制御された露出が提供されます。
オプション C と D には、カナリア デプロイが提供する制御された限定的な公開特性が欠けており、テストされていないバージョンを運用環境にデプロイするときにリスクを最小限に抑え、パフォーマンスの問題を早期に検出するために不可欠です。
</div></details>

### Q.  問題14: 回答
Google Cloud での CI / CD パイプラインのデプロイとテストの戦略を開発しています。次のことができる必要があります:
• リリース展開の複雑さを軽減し、展開ロールバックの期間を最小限に抑えます。
• 実際の本番トラフィックをテストし、影響を受けるユーザーの数を徐々に増やします。
要件を満たすデプロイとテストの戦略を選択する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明デプロイの複雑さを軽減し、ロールバック期間を最小化し、影響を受けるユーザーの数を徐々に増やして実際の運用トラフィックをテストするという要件を考えると、最適なデプロイとテストの戦略は次のとおりです。
B. ブルー/グリーン デプロイとカナリア テスト。
説明：
A. デプロイとカナリア テストを再作成します。
デプロイを再作成すると、環境の再作成が含まれるため、ロールバック中の複雑さとダウンタイムが増加する可能性があります。カナリア テストでは、新しいバージョンを運用トラフィックに徐々に公開するには不十分な場合があります。
C. ローリング更新プログラムのデプロイと A/B テスト:
ローリングアップデートはシーケンシャルであり、ロールバック期間が長くなる可能性があります。A/B テストでは 2 つのバージョンが比較されますが、トラフィックの露出を徐々に増やすことはできません。
D. ローリング更新プログラムの展開とシャドウ テスト:
通常、ローリング アップデートのデプロイでは効率的な更新が可能ですが、実際の運用テストでは段階的なトラフィックの露出に対応できない場合があります。シャドウ テストでは、ユーザー トラフィックに影響を与えることなく、既存のバージョンと並行して新しいコードを実行しますが、必要に応じてトラフィックの露出を徐々に増やすことはできません。
B. ブルー/グリーンデプロイとカナリアテスト:
ブルー/グリーンデプロイでは、2 つの同一の環境 (現在の環境は青、新しいバージョンは緑) を維持し、トラフィックを切り替えることで迅速なロールバックを可能にします。カナリア テストでは、新しいバージョンがユーザーのサブセットに徐々に公開されます。この戦略は、デプロイの複雑さを軽減し、ロールバック期間を最小限に抑え、影響を受けるユーザーに段階的にさらされる実際の運用トラフィックをテストすることとよく一致します。
したがって、指定された要件を満たすための最も適切な戦略は、カナリア テストと組み合わせたブルー/グリーン デプロイを使用することです。
</div></details>

### Q.  問題15: 回答
アプリケーション成果物は、CI/CD パイプラインを介して構築およびデプロイされています。CI/CD パイプラインでアプリケーション シークレットに安全にアクセスする必要があります。また、セキュリティ侵害が発生した場合に備えて、シークレットをより簡単にローテーションする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明CI/CD パイプラインでアプリケーション シークレットを管理しながら、セキュリティ侵害が発生した場合のローテーションを容易にするための最も安全で推奨されるアプローチは、オプションです
C: Cloud KMS の鍵で暗号化されたシークレットを Cloud Storage に保存し、CI / CD パイプラインに IAM(Identity and Access Management)経由で Cloud KMS へのアクセスを提供します。
オプション C が推奨される理由は次のとおりです。
1. セキュア ストレージ: Cloud KMS の鍵で暗号化して Cloud Storage にシークレットを保存すると、シークレットが安全な方法で保存されます。
2. 鍵管理: Cloud KMS は安全な鍵管理を提供し、鍵へのアクセスを制御し、鍵のローテーションを含む鍵のライフサイクルを管理できるようにします。これにより、キーのローテーションが容易になり、その結果、必要に応じて保存されているシークレットの暗号化をローテーションできます。
3. 最小権限アクセス: CI / CD パイプラインに IAM 経由で Cloud KMS へのアクセスを提供することで、最小権限の原則を適用し、シークレットへのアクセスに必要な権限のみをパイプラインに付与できます。これにより、シークレットへの不正アクセスのリスクを軽減できます。
オプション A、B、および D には、重大なセキュリティ上の懸念があります。
· オプションA:ビルド時に開発者にシークレットの入力を求め、シークレットを保存しないように指示すると、シークレットが誤って公開される可能性があり、シークレットを管理するための安全で一元化されたアプローチが提供されません。
· オプションB:シークレットを Git 上の別の構成ファイルに格納し、選択した開発者にそのファイルへのアクセスを提供すると、Git リポジトリへの不正アクセスがある場合にシークレットが公開される可能性があるため、セキュリティ上のリスクが生じます。
· オプションD:ソースコードリポジトリは一般に、シークレットなどの機密情報を格納するように設計されていないため、シークレットを暗号化してソースコードリポジトリに保存することは安全ではありません。復号化キーを別のリポジトリに保存すると、不正アクセスが発生した場合にリスクが生じる可能性もあります。
要約すると、オプション C は、Cloud KMS によって管理される鍵を使用して Cloud Storage の暗号化機能を活用し、IAM を介してこれらのシークレットへのアクセスを制御することで、CI / CD パイプラインでシークレットを管理するためのより堅牢で安全な方法を提供します。
インサイト - https://cloud.google.com/security-key-management
</div></details>

### Q.  問題16: 回答
キャッシュされたメモリに製品情報を格納するアプリケーションをサポートします。キャッシュ ミスが発生するたびに、エントリが Stackdriver Logging に記録されます。キャッシュミスが時間の経過とともに発生する頻度を視覚化する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明時間の経過に伴うキャッシュ ミスの頻度を視覚化するには、次の方法が最も適切なソリューションです。
オプション C: Stackdriver Logging でログベースの指標を作成し、Stackdriver Monitoring でその指標のダッシュボードを作成します。
このオプションが推奨される方法である理由は次のとおりです。
1. ログベースの指標の作成: Stackdriver Logging でログベースの指標を作成すると、ログから特定の情報(この場合はキャッシュミス)を抽出し、モニタリング可能な指標に変換できます。
2. Stackdriver Monitoring でのダッシュボードの作成: ログベースの指標を作成したら、それを使用して Stackdriver Monitoring でダッシュボードを生成できます。このダッシュボードには、時間の経過に伴うキャッシュミスの頻度が表示され、監視目的で明確に視覚化されます。
オプション A、B、D には、いくつかの制限または非効率性があります。
· オプションA:Google データポータルで Stackdriver Logging をソースとしてリンクし、キャッシュミスのログをフィルタリングすると、可視化できる可能性がありますが、Stackdriver Monitoring のような専用のモニタリング ソリューションと比較して、リアルタイムで便利なモニタリング機能を提供しない可能性があります。
· オプションB:Stackdriver Profiler の設定は、主にコード内のパフォーマンスの問題を特定することに重点が置かれており、時間の経過に伴うキャッシュ ミスの視覚化と監視は特に目的としていません。
· オプションD:BigQuery を Stackdriver Logging のシンクとして設定し、スケジュールされたクエリを作成してキャッシュ ミス ログをフィルタリングすると、データの詳細な分析に役立つ可能性がありますが、Stackdriver Monitoring ダッシュボードと比較して、リアルタイムの可視化とモニタリングの機能が提供されない可能性があります。
要約すると、Stackdriver Logging でキャッシュミスのログベースの指標を作成し、Stackdriver Monitoring を利用してその指標に基づいてダッシュボードを構築する(オプション C)と、キャッシュの失敗をリアルタイムで可視化して経時的にモニタリングする効率的な方法が提供されます。
インサイト - https://cloud.google.com/logging/docs/logs-based-metrics#counter-metric
</div></details>

### Q.  問題17: 回答
ユーザーが Web アプリケーションの新機能を気に入っているかどうかを確認するために実験を実行しています。この機能をカナリアリリースとしてデプロイした直後に、ユーザーに送信される 500 エラーの数が急増し、モニタリングレポートにレイテンシーの増加が示されます。ユーザーへの悪影響を迅速に最小限に抑えたい。最初に何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明正しいオプション - A. 実験的な Canary リリースをロールバックします。
説明: ユーザーに送信される 500 エラーの数が急増し、新しい機能のデプロイ後に待機時間が増加した場合は、ユーザーへの悪影響を最小限に抑えるために直ちにアクションを実行することが重要です。最初のステップは、実験的なカナリアリリースをロールバックすることです。これにより、新しい機能が本番環境から削除され、システムが以前の状態に戻るため、エラーの数が減り、待機時間が短縮されます。
カナリアリリースをロールバックした後、レイテンシー、トラフィック、エラー、および飽和のモニタリングを開始して (オプション B)、ロールバックの影響を判断し、システムが安定していることを確認できます。
また、インシデントの事後分析ドキュメント (オプション C) のデータを記録して、インシデントから学習し、将来のリリースを改善することもできます。
最後に、カナリアリリースをロールバックした後、500エラーの原因とレイテンシー増加の根本原因(オプションD)を追跡して、何が問題だったのかを理解し、今後同様の問題が再発するのを防ぐことができます。
</div></details>

### Q.  問題18: 回答
最近、e コマース アプリケーションを Google Cloud に移行しました。次に、次のトラフィックのピーク シーズンに向けてアプリケーションを準備する必要があります。Google が推奨する方法に従う場合。繁忙期に備えるために、まず何をすべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google が推奨するプラクティスに従いながら、来たるトラフィックのピーク シーズンに向けて Google Cloud で e コマース アプリケーションを準備するには、まず次の手順を行います。
C. アプリケーションのロード テストを行い、スケーリングのパフォーマンスをプロファイリングします。
説明：
A. アプリケーションを Cloud Run に移行して自動スケーリングを利用すると、スケーラビリティの面では有益ですが、負荷がかかった状態でのアプリケーションのパフォーマンスを把握していないと、リソースの最適な使用や最適なスケーリング設定を実現できない可能性があります。
B. アプリケーションの基盤となるインフラストラクチャの Terraform 構成を作成して、追加のリージョンにすばやくデプロイすると、準備に役立つ場合があります。ただし、負荷が増加した場合のアプリケーションのパフォーマンス プロファイルを知らなければ、主要な手順にならない場合があります。
C. 負荷テスト アプリケーションの動作、パフォーマンスのボトルネック、およびトラフィックの多い環境でのスケーラビリティ特性を理解するには重要です。負荷テストを通じてパフォーマンスをプロファイリングすることで、アプリケーションを効果的にスケーリングできる場所と方法に関する分析情報が得られます。
D. 負荷テストを行わずに追加のコンピューティング能力を事前にプロビジョニングすると、リソースの過剰プロビジョニングまたは過少プロビジョニングが発生し、非効率的な使用率とコストの増加につながる可能性があります。
したがって、繁忙期に備えるための推奨される最初の手順は、アプリケーションに対して徹底的な負荷テストを実施することです。このプロセスは、高負荷時のアプリケーションの動作を理解するのに役立ち、トラフィックのピークシーズンに備えて、スケーリングとリソース割り当てをより適切に決定できます。
</div></details>

### Q.  問題19: 回答
チームは、Google Kubernetes Engine(GKE)にデプロイするための新しいアプリケーションを設計しています。さまざまなアプリケーションレベルのメトリックを一元的に収集して集計するには、監視を設定する必要があります。Google Cloud Platform サービスを使用しながら、監視の設定に必要な作業量を最小限に抑えたい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明ワークロードを最小限に抑えながら、Google Kubernetes Engine(GKE)内の一元的な場所でさまざまなアプリケーションレベルの指標を収集、集計するためのモニタリングを設定するのに最適なアプローチは次のとおりです。
C. OpenTelemetry クライアント ライブラリをアプリケーションにインストールし、指標のエクスポート先として Stackdriver を構成してから、Stackdriver でアプリケーションの指標を確認します。
OpenTelemetry は、メトリック、ログ、トレースなどのオブザーバビリティ データを収集するための標準化された方法を提供する API、ライブラリ、エージェント、インストルメンテーションのセットです。これは、オブザーバビリティの目的でアプリケーションをインストルメント化するための、一貫性のあるベンダーに依存しない方法を提供します。
OpenTelemetry クライアント ライブラリをアプリケーション内にインストールし、Stackdriver を指標のエクスポート先として構成することで、アプリケーションの指標を Stackdriver に簡単にストリーミングできます。このアプローチでは、OpenTelemetry がメトリックを収集およびエクスポートするための標準化された方法を提供するため、監視目的でアプリケーションをインストルメント化するために必要な労力が最小限に抑えられます。また、Stackdriver とシームレスに統合されているため、Stackdriver Monitoring インターフェース内でアプリケーションの指標を監視、分析できます。
オプション A では、さまざまな指標を Stackdriver Monitoring API に直接公開しますが、これはより複雑になる可能性があり、カスタム指標の処理と統合に追加の作業が必要になる場合があります。
オプション B の Cloud Pub/Sub では、指標をトピックにプッシュして集計することで不必要な複雑さが生じる可能性があり、Stackdriver での指標の収集と監視に最も効率的な方法ではない可能性があります。
オプション D は、指標をログ メッセージとして出力して Stackdriver Logging に渡すことですが、主にログを処理するため、指標の収集と監視に最も適した方法ではない可能性があり、ログから指標への変換は簡単でも効率的でもありません。
その他のインサイト - https://cloud.google.com/monitoring/custom-metrics
</div></details>

### Q.  問題20: 回答
貴社は、サイト信頼性エンジニアリングの原則に従っています。ソフトウェアの変更によって引き起こされ、ユーザーに深刻な影響を与えたインシデントの事後分析を作成しています。今後、重大なインシデントの発生を未然に防ぎたい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明サイト信頼性エンジニアリング (SRE) のコンテキストでは、ソフトウェアの変更によって引き起こされたインシデントの事後分析を作成し、将来の重大なインシデントを防ぐことを目的とするには、プロアクティブなソリューションと体系的な改善に焦点を当てる必要があります。その上でですね：
B. この種類のエラーをキャッチするテスト ケースが、新しいソフトウェア リリースの前に正常に実行されることを確認します
説明：
このオプションは、テストプロセスの改善の重要性を強調することで、SRE の原則と密接に連携しています。包括的なテストケースがインシデントに関連する特定のエラータイプをカバーしていることを確認すると、将来のソフトウェアリリースで同様の問題を特定して防止するのに役立ちます。
テスト手順の強化などの予防策により、同様のインシデントが発生する可能性を大幅に減らすことができます。
他の選択肢には一定のメリットがあるかもしれませんが、将来の同様のインシデントの防止に直接対処できない場合や、SRE の原則の中心である体系的な改善ではなく、責任の所在や即時の対応に重点が置かれている可能性があります。
したがって、SREの原則に基づき、将来の重大インシデントの未然防止を目指す場合、選択肢Bが最も適切な対応となります。
重要なのは「ソフトウェアの変更によるトリグ」であるため、テストを徹底的に行う必要があります。
</div></details>

### Q.  問題21: 回答
Google Kubernetes Engine(GKE)で実行される本番環境アプリケーションの問題を調査しています。問題の原因は最近更新されたコンテナー イメージであると判断しましたが、コードの正確な変更は特定されませんでした。デプロイは現在、最新のタグを指しています。意図したとおりに機能するバージョンのコンテナーを実行するには、クラスターを更新する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明混乱を招いたことをお詫びしますが、提供されたシナリオのコンテキストで、オプションBが実際に最も適切な選択肢である理由と、他のオプションがそれほど効果的ではない理由を明確にしましょう。
B. 以前に動作していたコンテナーの sha256 ダイジェストを指すようにデプロイを変更します。
説明：
· バージョンを識別する精度:sha256 ダイジェストを使用すると、以前は正しく機能していたコンテナー イメージの正確なバージョンを非常に正確に参照できます。このアプローチにより、曖昧さやタグへの依存なしに、以前に知られていた安定バージョンを具体的にデプロイできます。
· 特定の画像への直接参照:sha256 ダイジェストは、タグやリポジトリ内のその他の変更に関係なく、特定のイメージを一意に識別し、正確な既知の動作バージョンがデプロイされるようにします。
では、他のオプションに制限がある理由を説明しましょう。
オプションA(新しい「安定版」タグの作成):「安定版」タグの作成は簡単に思えるかもしれませんが、以前に機能していたバージョンを正確に指し示さない場合、あいまいさのリスクが生じます。sha256 ダイジェストを使用する場合と同じレベルの特異性は提供されない可能性があります。
オプション C (以前の Git タグから新しいコンテナーをビルドする):このオプションは一般的に信頼性がありますが、デプロイが以前に動作していたコンテナーのバージョンを正確に参照していることを確認するために必要な精度が得られない可能性があります。また、コンテナの再構築も必要になりますが、sha256ダイジェストを直接参照できる場合は不要かもしれません。
オプション D (以前のコンテナー イメージに最新のタグを適用する):最新のタグを以前のコンテナー イメージに適用すると、最近の更新によって問題が発生した場合、問題が悪化する可能性があります。未検証のバージョンや問題のあるバージョンを再度デプロイするリスクがあります。
したがって、オプション B(sha256 ダイジェストを使用)はより技術的になる可能性がありますが、以前に確認された特定の安定バージョンを特定してデプロイする際の精度と信頼性が提供されるため、GKE デプロイで作業中のコンテナ イメージに戻すのに最も適した選択肢となります。
インサイト - https://cloud.google.com/kubernetes-engine/docs/concepts/about-container-images
</div></details>

### Q.  問題22: 回答
あなたの会社は、ホリデーショッピングシーズン中にオンライン小売業者の大規模なマーケティングイベントを計画しています。Web アプリケーションが短期間に大量のトラフィックを受信することが予想されます。イベント中に発生する可能性のある障害に備えて、アプリケーションを準備する必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明予想されるトラフィックの急増時に発生する可能性のある障害に対処するためにアプリケーションを準備するには、次の 2 つの効果的なアクションを考慮する必要があります。
B. 関連するシステム指標が Cloud Monitoring で取得されていることを確認し、関心のあるレベルでアラートを作成します。
関連するシステム指標を Cloud Monitoring でキャプチャするように設定することで、アプリケーションのパフォーマンスをリアルタイムでモニタリングできます。高い CPU 使用率、メモリ使用率、要求の待機時間、エラー率などの重要なメトリックに対してアラートを作成すると、トラフィック負荷が高いときに潜在的な問題を事前に特定し、即座に対応するのに役立ちます。
C. 増加した容量要件を確認し、必要なクォータ管理を計画します。
容量要件の増加を事前に評価して計画することで、予想されるトラフィックの急増を処理するために必要なリソースをアプリケーションに確保できます。これには、CPU、メモリ、ネットワークなどのさまざまなリソースのクォータを管理し、それに応じてアプリケーション インフラストラクチャをスケーリングして負荷の増加を処理することが含まれます。
他のオプションでは、トラフィックの急増時に発生する可能性のある障害への準備に直接対処できない理由の説明:
A. アプリケーションで Anthos Service Mesh を構成して、トポロジ マップ上の問題を特定します。
Anthos Service Mesh はマイクロサービス ベースのアプリケーションのモニタリングと管理に役立ちますが、トポロジ マップに重点を置くだけでは、トラフィックの急激な増加時に発生する可能性のある障害をすぐに処理する必要性に直接対処できない場合があります。
D. サービスの待機時間を監視して、平均パーセンタイル待機時間を確認します。
待機時間の監視は重要ですが、トラフィックの多いイベント中に発生する可能性のある障害に備えるという差し迫った必要性に直接対処できない場合があります。待機時間の傾向を理解することは重要ですが、このようなイベントでは、即時の障害に対して特定のアラートを作成することがより重要になる場合があります。
E. Cloud Monitoring で、アプリケーションで発生するすべての一般的な障害に関するアラートを作成します。
一般的な障害に対してアラートを設定することは有益ですが、トラフィックの急激な急増時に発生する可能性のあるすべての障害シナリオをカバーできるとは限りません。このようなイベントでは、パフォーマンス、リソース使用率、エラー率などに関連する重要なメトリックに対する特定のアラートがより重要になります。
したがって、オプション B と C は、重要なメトリックの監視と容量増加の計画の重要な側面に対処し、トラフィックの多い期間に発生する可能性のある障害を処理するように特別に調整されているため、推奨されます。
</div></details>

### Q.  問題23: 回答
マイクロサービス アーキテクチャでトラフィックの多い Web アプリケーションをサポートします。アプリケーションのホームページには、現在の天気、株価、ニュースの見出しなどのコンテンツを含む複数のウィジェットが表示されます。メイン サービング スレッドは、ウィジェットごとに専用のマイクロサービスを呼び出し、ユーザーのホームページをレイアウトします。マイクロサービスで障害が発生することがあります。その場合、サービングスレッドは、欠落しているコンテンツを含むホームページを提供します。アプリケーションのユーザーは、この低下モードが頻繁に発生することに不満を抱きますが、コンテンツをまったく提供しないのではなく、何らかのコンテンツを提供することを望んでいます。サービスレベル目標(SLO)を設定して、ユーザーエクスペリエンスが過度に低下しないようにする必要があります。
これを測定するには、どのサービスレベル指標(SLI)を使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明説明されているコンテキストでは、ユーザーエクスペリエンスを測定し、サービスレベル目標(SLO)を設定するのに最も適したサービスレベル指標(SLI)は次のとおりです。
ある。品質SLI:全回答数に対する劣化していない回答の比率。
このSLIは、提供された応答の合計数に対する、低下していない応答(つまり、すべてのウィジェットのコンテンツが正常に取得されて表示された応答)の比率を測定することにより、ユーザーエクスペリエンスに焦点を当てています。これは、ユーザーがコンテンツをまったく提供しないよりも、何らかのコンテンツを提供することを好むため、ユーザーエクスペリエンスが過度に低下しないようにするという目標に直接一致しています。このSLIは、ユーザーエクスペリエンスの全体的な品質を反映し、利用可能なコンテンツをユーザーに提供しながら、マイクロサービスの障害によって一部のコンテンツが欠落する可能性がある場合を考慮します。
インサイト - https://cloud.google.com/architecture/adopting-slos
</div></details>

### Q.  問題24: 回答
会社では、グローバルに分散された複数の Google Kubernetes Engine(GKE)クラスタを使用してサービスを実行しています。運用チームは、メトリック、アラート、およびダッシュボードの生成に Prometheus ベースのツールを使用するワークロード監視を設定しました。このセットアップでは、すべてのクラスターでメトリックをグローバルに表示する方法は提供されません。グローバルな Prometheus クエリをサポートし、管理オーバーヘッドを最小限に抑えるために、スケーラブルなソリューションを実装する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明管理オーバーヘッドを最小限に抑えながら、複数の Google Kubernetes Engine(GKE)クラスタ間で Prometheus 指標をグローバルにクエリするためのスケーラブルなソリューションを実装するのに最適なオプションは次のとおりです。
D. Google Cloud Managed Service for Prometheus を構成します。
説明：
Google Cloud は、複数のクラスタやその他の Google Cloud サービスにわたる指標の管理、クエリ、可視化に特化した Managed Service for Prometheus を提供しています。このサービスは、スケーラブルなソリューションを提供し、Prometheus の実行に必要なインフラストラクチャを処理することで管理オーバーヘッドを削減します。
他のオプションが最良の選択ではない理由:
A. 一元化されたデータ アクセスのために Prometheus クロスサービス フェデレーションを構成します。
フェデレーションではサービス間のクエリが可能ですが、構成の管理が複雑になり、Prometheus サーバーのセットアップを慎重に行う必要があります。これにより、管理オーバーヘッドが増加し、クラスター間でメトリックをグローバルにクエリするためのスケーラブルなソリューションが提供されない可能性があります。
B. Cloud Operations for GKE 内でワークロード指標を構成します。
Cloud Operations for GKE はモニタリング機能を提供しますが、複数の GKE クラスタ間で Prometheus 指標をグローバルにクエリする必要性に直接対応できない場合があります。高度なクエリと分析のための柔軟性や Prometheus メトリクスへの直接アクセスが提供されない場合があります。
C. 集中型データアクセスのために Prometheus 階層フェデレーションを構成します。
サービス間フェデレーションと同様に、Prometheus の階層フェデレーションには複雑な構成とメンテナンスが含まれる場合があります。これは、複数の GKE クラスタ間で Prometheus 指標のグローバルクエリを実現するための最も効率的なソリューションではない可能性があります。
全体として、Google Cloud Managed Service for Prometheus を利用するオプション D は、このようなシナリオを処理するように特別に設計されており、管理オーバーヘッドを最小限に抑えながら、分散した GKE クラスタ全体で Prometheus 指標をクエリするためのスケーラブルで管理されたアプローチを提供します。
インサイト - https://cloud.google.com/stackdriver/docs/managed-prometheus
</div></details>

### Q.  問題25: 不正解
Google Cloud でコンテナ化されたアプリケーションの CI / CD パイプラインを構築する必要があります。開発チームは、トランクベースの開発に中央の Git リポジトリを使用します。品質を向上させるために、アプリケーションの新しいバージョンに対してパイプライン内のすべてのテストを実行する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Cloud でコンテナ化されたアプリケーションの CI / CD パイプラインを構築して、新しいバージョンのテストを確実に行うための最適なアプローチは次のとおりです。
B.
Git フックをインストールして、コードを中央リポジトリにプッシュする前に単体テストを実行するように開発者に要求します。すべてのテストが成功したら、コンテナーをビルドします。
Cloud Build をトリガーして、アプリケーション コンテナをテスト環境にデプロイし、統合テストと受け入れテストを実行します。
すべてのテストが成功したら、コードに運用準備完了のタグを付けます。Cloud Build をトリガーして、アプリケーション コンテナをビルドし、本番環境にデプロイします。
説明：
このオプションには、単体テスト、統合テスト、運用環境へのデプロイ前の受け入れテストなど、包括的なテスト手順が含まれます。
これにより、開発者はコードを中央リポジトリにプッシュする前に単体テストを実行し、コード品質基準を維持できます。
統合テストと受け入れテストは、コードが運用環境対応としてタグ付けされ、運用環境にデプロイされる前に、テスト環境で実行されます。
他のオプションが最良の選択ではない理由:
ある。このオプションには、運用環境にデプロイする前にコードに運用準備完了として明示的にタグを付ける手順がないため、バージョン管理とリリース管理に影響を与える可能性があります。
C. このオプションには単体テストが含まれますが、コードに運用準備完了のタグを付けたり、運用環境へのデプロイをテスト環境から分離したりすることについては特に言及していません。
D. このオプションでは、アプリケーションを運用環境にデプロイし、バージョンの準備状況と運用デプロイ前のテストの個別のステージを明確に示さずにスモーク テストを実行するため、未検証のデプロイで問題が発生する可能性があります。
</div></details>

### Q.  問題26: 回答
ビジネスクリティカルなワークロードを、固定された Compute Engine インスタンスのセットで数か月間実行する必要がある。ワークロードは安定しており、正確な量のリソースが割り当てられています。パフォーマンスに影響を与えることなく、このワークロードのコストを削減する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明パフォーマンスに影響を与えずにコストを削減することが主な目標である、固定された Compute Engine インスタンスのセットで安定したワークロードを実行するシナリオでは、最適なオプションは次のとおりです。
A. 確約利用割引の購入。
確約利用割引を購入すると、1 年または 3 年の期間、特定の使用量を約束する代わりに、Compute Engine インスタンスの費用を大幅に削減できます。このオプションは、既存のインスタンスのパフォーマンスやセットアップに影響を与えることなくコストを削減できるため、リソース要件がよく知られ、固定されている安定したワークロードに最適です。
オプション B、C、および D は、説明されている状況に最適ではない可能性があります。
B. マネージド インスタンス グループへの移行は、インスタンスの管理、スケーリング、および自動処理の向上に役立つ可能性がありますが、特定のスケーリングまたはインスタンスの適切なサイズ設定戦略が実装されていない限り、必ずしもコスト削減につながるとは限りません。
C. インスタンスをプリエンプティブル VM に変換するとコストを削減できますが、これらのインスタンスは Google Cloud によっていつでも終了できるため、一貫したアップタイムを必要とするビジネスクリティカルなワークロードには適していない可能性があります。
D. アンマネージドインスタンスグループを作成すると、インスタンスをグループ化するという点でメリットがあるかもしれませんが、本質的にコスト削減や最適化は提供されません。
したがって、リソース要件が固定され、パフォーマンスに影響を与えない安定したワークロードのコスト削減を求める場合は、確約利用割引を利用するのが最も適切な選択です。
</div></details>

### Q.  問題27: 回答
チームは、開発、ステージング、本番環境の 3 つの Google Kubernetes Engine(GKE)環境にアプリケーションをデプロイします。GitHub リポジトリを信頼できる情報源として使用します。3 つの環境が一貫していることを確認する必要があります。Google が推奨する方法に従って、これらの環境内のすべての GKE クラスタにネットワーク ポリシーとロギング DaemonSet を適用してインストールする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明ネットワーク ポリシーを適用し、GitHub リポジトリをソースとするロギング DaemonSet をデプロイすることで、GKE 環境(開発、ステージング、本番環境)全体の一貫性を確保するという目標を考えると、Google が推奨するプラクティスに沿った最適なアプローチは次のとおりです。
D. Cloud Build を使用して、ネットワーク ポリシーと DaemonSet をレンダリングしてデプロイします。ポリシーコントローラを設定して、3 つの環境の設定を適用します。
説明：
A. ネットワーク ポリシーに Google Cloud Deploy を使用し、ドリフト アラートに DaemonSet、Cloud Monitoring を使用します。
Google Cloud Deploy は、ネットワーク ポリシーと DaemonSet デプロイのすべての側面をカバーしているわけではありません。ドリフトを監視しても、本質的に不一致は修正されません。
B. DaemonSet の Google Cloud Deploy、ネットワーク ポリシーの Policy Controller、ドリフトの修正のための Cloud Monitoring、Cloud Functions。
Policy Controller はポリシーを適用できますが、すべての構成をカバーできるとは限りません。Cloud Monitoring と Cloud Functions を使用すると、セットアップが複雑になります。
C. Cloud Build と Config Sync を使用して設定を同期する。
Config Sync は設定を同期する場合がありますが、ネットワーク ポリシーや DaemonSet の展開を処理するように特別に設計されていない場合があります。
D. Cloud Build を使用してネットワーク ポリシーと DaemonSet をレンダリングしてデプロイする。設定を適用するためのポリシーコントローラの設定。
Cloud Build は、GitHub リポジトリからリソースをデプロイするのに適しています。Policy Controller は、ネットワーク ポリシーと DaemonSet デプロイの管理に関する Google が推奨するプラクティスに沿って、環境全体で構成を適用することで一貫性を確保します。
オプション D は、Cloud Build を使用して GitHub リポジトリからリソースをデプロイし、Policy Controller を実装して、複数の GKE 環境間でネットワーク ポリシーと DaemonSet の一貫した構成を適用および維持するため、最も適しています。
インサイト - https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview
</div></details>

### Q.  問題28: 回答
最近、サービスの 1 つが現在のローリング ウィンドウ期間のエラー バジェットを超えていることに気付きました。会社の製品チームが新機能をリリースしようとしています。サイト信頼性エンジニアリング (SRE) のプラクティスに従う必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明このシナリオでの正しいオプションは、サイト信頼性エンジニアリング (SRE) のプラクティスに沿って、オプション B: エラー予算を使い切ったことをチームに通知することです。ローンチの凍結についてチームと交渉するか、ユーザーエクスペリエンスが少し悪化することを許容します。
オプションBが正しい理由:
通知と連絡:エラー予算の超過についてチームに通知することは、チーム間の透明性とコラボレーションにとって重要です。
起動フリーズまたはユーザーエクスペリエンスの許容に関するネゴシエーション:ローンチの凍結を交渉したり、ユーザーエクスペリエンスがわずかに低下したりすることは、SRE の原則と一致しており、より多くのインシデントを引き起こすリスクを冒して、機能のリリースよりもシステムの安定性と信頼性を優先します。
それでは、間違ったオプションに対処し、それらが最も適切な選択ではない理由を説明しましょう。
オプション A: エラーバジェットの不足をチームに通知し、すべてのテストが成功することを確認して、ローンチによってエラーバジェットのリスクがさらに高まらないようにします。
このオプションは、エラーバジェットがすでに超過しているという事実に対処することなく、テストの成功を保証することのみに焦点を当てています。システムの安定性と信頼性を優先していないため、既存の問題を考慮せずに打ち上げを続行することで、さらなるインシデントのリスクを冒す可能性があります。
オプション C: 状況をエスカレーションし、追加のエラー バジェットを要求します。
エスカレーションが必要な場合もありますが、即時の対応として追加のエラーバジェットを要求すると、既存のリソースを効率的に管理することの重要性が見落とされる可能性があります。これは、エラーバジェットの超過という差し迫った問題には対処しておらず、差し迫った機能リリースのタイムライン内では実現できない可能性があります。
オプション D: 製品に関連する他の指標を調べ、エラーバジェットが残っている SLO を見つけます。エラーバジェットを再割り当てし、機能の起動を許可します。
他の指標からエラーバジェットを再配分することは、手っ取り早い解決策のように思えるかもしれませんが、他の重要なサービスやコンポーネントの信頼性を損なう可能性があります。これは、システムの不安定性の波及効果につながる可能性があり、システムの信頼性を維持するための理想的なアプローチではありません。
要約すると、オプションBは、サイト信頼性エンジニアリングの原則に沿って、新機能の即時リリースよりもシステムの安定性を優先するために、コミュニケーション、透明性、およびネゴシエーションを重視するため、最適な選択肢です。
インサイト - https://sre.google/workbook/table-of-contents/
</div></details>

### Q.  問題29: 回答
Cloud Run で実行されるアプリケーションを構築しています。アプリケーションは、API キーを使用してサードパーティの API にアクセスする必要があります。Google が推奨する方法に従って、アプリケーションで API キーを保存して使用する安全な方法を決定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Cloud Run アプリケーションで API キーを保存して使用するための最も安全で Google が推奨するアプローチは次のとおりです。
A. Secret Manager の API キーをシークレットとして保存します。シークレットを Cloud Run アプリケーションの環境変数として参照します。
説明：
1. シークレット マネージャー: シークレット マネージャーは、API キー、パスワード、証明書などの機密データを保存するための完全に管理された安全なサービスです。API キーを Secret Manager に保存すると、機密情報が保存時に暗号化され、安全に管理されます。
2. 環境変数のリファレンス: Secret Manager に保存されている API キーを Cloud Run アプリケーションの環境変数として参照すると、アプリケーションはコードベースにハードコーディングすることなく、キーに安全にアクセスできます。また、キーがアプリケーション コードから分離されたままになり、セキュリティと管理性が向上します。
このアプローチは、API キーなどの機密情報を安全かつ一元的に管理するための Google 推奨のプラクティスに沿ったものです。
他のオプションには欠点があるか、ベストプラクティスに従っていません。
B. API 鍵を秘密鍵として Secret Manager に保存し、Cloud Run アプリケーションにマウントすると、コンテナ ファイルシステムで鍵が公開され、セキュリティが損なわれる可能性があります。
C. Secret Manager はシークレット管理専用に設計されているため、API キーを Cloud KMS にキーとして保存し、それを環境変数として Cloud Run で参照することは、API キーを KMS に直接保存するためのベスト プラクティスではない可能性があります。
D. Cloud KMS で API キーを暗号化し、環境変数として Cloud Run に渡すと、複雑さが増し、Secret Manager に直接保存するよりも簡単で安全ではない可能性があります。さらに、アプリケーションで復号化を処理すると、オーバーヘッドと潜在的なセキュリティリスクが加わる可能性があります。
</div></details>

### Q.  問題30: 回答
現在、仮想マシン(VM)の使用状況ログは Stackdriver に保存されています。リアルタイムで更新され、四半期ごとに集計された情報を含む、共有しやすい対話型の VM 使用率ダッシュボードを提供する必要があります。Google Cloud Platform ソリューションを使用したい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Cloud Platform ソリューションを使用して、リアルタイムで更新され、四半期ごとに集計される、共有しやすいインタラクティブな VM 使用率ダッシュボードを作成する必要がある場合、最適なアプローチは次のとおりです。
A. 1. はい。VM 使用率ログを Stackdriver から BigQuery にエクスポートします。2. データポータルでダッシュボードを作成します。3. ダッシュボードを関係者と共有します。
説明：
VM 使用率ログを Stackdriver から BigQuery にエクスポートする: Stackdriver では、ログを BigQuery にエクスポートできるため、大量のデータを構造化して保存、分析できます。これにより、分析のために VM 使用率ログに簡単にアクセスできるようになります。
データポータルでダッシュボードを作成する: データポータルは、BigQuery と統合された強力な可視化ツールで、インタラクティブなダッシュボードを作成できます。BigQuery に保存されているデータに基づいて、カスタムのレポートや可視化を作成できます。
ダッシュボードを関係者と共有する: データポータルを使用すると、ダッシュボードを簡単に共有できます。特定の関係者にアクセス権を付与するか、リンクを介してダッシュボードをパブリックに共有して、情報にリアルタイムでアクセスできるようにすることができます。
このアプローチでは、ログの収集に Stackdriver、データの保存と分析に BigQuery、インタラクティブなダッシュボードの作成に Data Studio などの Google Cloud Platform サービスを利用します。リアルタイムの更新、簡単な共有、四半期ごとの情報集約の要件を満たしています。
したがって、このシナリオにはオプション A が最も適切なソリューションです。
</div></details>

### Q.  問題31: 回答
SRE のプラクティスと原則に従う組織の一員である。開発チームから新しいサービスの管理を引き継ぎ、運用準備レビュー (PRR) を実施します。PRR 分析フェーズの後、サービスが現在サービス レベル目標 (SLO) を満たせないと判断します。サービスが本番環境で SLO を満たせるようにする必要があります。
次に何をすべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明運用準備レビュー (PRR) 後にサービスが現在サービス レベル目標 (SLO) を達成できないシナリオでは、最も適切なアクションは次のとおりです。
C. 引き渡し前に完了すべきサービスの信頼性向上の推奨項目を特定する。
その理由は次のとおりです。
SLO 目標の調整 (オプション A) は、手っ取り早い解決策のように思えるかもしれませんが、理想的な解決策ではありません。SLO は、利害関係者が合意した実際のサービスパフォーマンス基準を反映する必要があります。これらを下方修正すると、サービス品質が低下し、ビジネスやユーザーの期待に沿わない可能性があります。
運用サポート (オプション B) の提供について開発チームに通知することは、最終的には必要になる可能性がありますが、サービスが SLO を満たせない理由の根本原因には対処できません。サポートの責任をすぐに割り当てるのではなく、サービスの信頼性の向上に重点を置くことが重要です。
SLO なしでサービスを運用環境に導入し、後で構築することはお勧めできません (オプション D)。SLO は、最初から望ましいパフォーマンスと信頼性の期待値を確立するために重要です。運用データを収集した後にそれらを構築すると、構造化されていないアプローチになり、重要なサービスパフォーマンス指標の確立が遅れる可能性があります。
推奨される信頼性の向上 (オプション C) を特定することは、最も賢明なアクションです。これには、SLO の達成を妨げるサービス内の弱点とボトルネックを理解することに重点を置いた取り組みが含まれます。この分析に基づいて改善を実装することで、定義されたSLOを満たしながら、サービスが本番環境で確実に実行でき、サイトの信頼性エンジニアリング(SRE)の原則に沿って、システムの信頼性とパフォーマンスをプロアクティブに向上させることができます。
かつ
SREの本によると、Simple PRRモデルでPRRを実施する次のフェーズは、SREチームにサービスを引き継ぐ前に、PRRで改善すべき項目を選択することです。
インサイト - https://sre.google/sre-book/evolving-sre-engagement-model/#improvements-and-refactoring-xqsrUdcyO
</div></details>

### Q.  問題32: 不正解
Compute Engine でアプリケーションを実行し、Stackdriver でログを収集している。個人を特定できる情報(PII)が特定のログエントリフィールドに漏洩していることがわかりました。すべての PII エントリは、userinfo というテキストで始まります。これらのログエントリを安全な場所にキャプチャして後で確認し、Stackdriver Logging に漏洩しないようにします。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明「userinfo」というテキストで始まる個人を特定できる情報(PII)を含むログエントリを安全な場所にキャプチャして後で確認し、Stackdriver Logging への漏洩を防ぐには、次の方法が最も適切な解決策です。
C. userinfo に一致する高度なログフィルタを作成し、Cloud Storage をシンクとして Stackdriver Console でログのエクスポートを設定してから、userinfo をフィルタとしてログの除外を設定します。
説明：
1. ユーザー情報に一致する高度なログフィルターの作成: この手順では、ユーザー情報文字列を含むログエントリを具体的にターゲットにしてフィルタリングし、関連するログを確実にキャプチャできます。
2. Cloud Storage をシンクとして使用して Stackdriver Console でログのエクスポートを構成する: この手順では、フィルタリングされたログを Cloud Storage にエクスポートし、Stackdriver Logging に含められないようにしながら、保存と確認のための安全な場所を提供できます。
3. userinfo をフィルタとしてログ除外を構成する: この追加手順により、userinfo を含むログが Stackdriver Logging に保存されなくなるため、機密情報の漏洩を防ぐことでセキュリティが強化されます。
このアプローチにより、特定のログエントリをキャプチャして安全な場所に保存し、Stackdriver Logging に保持されないようにすることで、PII 漏洩の問題に効果的に対処できます。
インサイト - https://cloud.google.com/logging/docs/exclusions
https://cloud.google.com/logging/docs/routing/overview
</div></details>

### Q.  問題33: 回答
あなたは、Google Kubernetes Engine(GKE)Autopilot クラスタにデプロイされたマイクロサービスのオンコール サイト信頼性エンジニアです。会社では、注文メッセージを Pub/Sub に公開するオンライン ストアを運営しており、マイクロサービスはこれらのメッセージを受信して、倉庫システムの在庫情報を更新します。販売イベントにより注文が増加し、在庫情報が十分に迅速に更新されていません。これにより、在庫切れの製品の注文が大量に受け付けられます。マイクロサービスのメトリックを確認し、一般的なレベルと比較します。
倉庫システムが注文時に製品在庫を正確に反映し、顧客への影響を最小限に抑える必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明説明されている状況には、在庫情報が十分に迅速に更新されず、在庫切れの製品に対して多数の注文が受け入れられるという問題が含まれます。注文時に倉庫システムに製品在庫が正確に反映され、顧客への影響を最小限に抑えるには、次のアクションが最も適切なアクションになります。
C. Pod レプリカの数を増やします。
説明：
A. サブスクリプションの確認期限を短くします。
受信確認の期限を短くすると、メッセージの処理時間を短縮できる可能性がありますが、マイクロサービスがメッセージをタイムリーに処理できないことがボトルネックにある場合は、受信確認の期限を短くするだけでは問題に十分に対処できない可能性があります。
B. 一般的なトラフィック レベルを許可する仮想キューをオンライン ストアに追加します。
仮想キューを追加すると、トラフィック レベルを管理し、マイクロサービスの過負荷を防ぐのに役立つ場合がありますが、倉庫システム内の在庫情報の更新の遅延の根本原因に直接対処できない場合があります。
C. Pod レプリカの数を増やします。
Podレプリカの数を増やすと、ワークロードを分散し、マイクロサービスの容量を向上させて、より多くの注文を処理できます。このアクションにより、メッセージをより迅速に処理するマイクロサービスの機能が強化され、在庫情報の更新の遅延が減り、在庫切れの製品の注文が受け入れられる影響を最小限に抑えることができます。
D. ポッドの CPU とメモリの制限を増やします。
リソース制限を増やすと、マイクロサービスにリソースの制約がある場合に役立つことがあります。ただし、ボトルネックがマイクロサービスの処理能力とスケーリングに関連している場合は、CPU とメモリの制限を増やすだけでは、問題を完全に解決できない可能性があります。
Podレプリカの数を増やすと、水平方向のスケーリングが可能になり、マイクロサービスで大量の受信注文を効率的に処理できるようになります。このアクションは、注文をタイムリーに処理するマイクロサービスの能力を強化することで在庫情報の更新の遅延に対処し、倉庫システムに反映される製品在庫の精度を向上させ、顧客への影響を最小限に抑えることができます。
</div></details>

### Q.  問題34: 回答
多数の依存システムを持つインフラストラクチャー・サービスのオンコールです。サービスがほとんどの要求を処理できず、数十万人のユーザーを持つすべての依存システムが影響を受けることを示すアラートを受け取ります。サイト信頼性エンジニアリング (SRE) インシデント管理プロトコルの一環として、インシデント指揮官 (IC) を宣言し、チームから経験豊富な 2 人をオペレーション リード (OL) とコミュニケーション リード (CL) として採用します。次に何をすべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明適切なオプション – C. インシデント対応者とリードが相互に通信できるコミュニケーション チャネルを確立します。
事前に準備する
インシデント対応トレーニングに加えて、インシデントの事前準備に役立ちます。次のヒントと戦略を使用して、より良い準備をしてください。
コミュニケーションチャネルを決定する: コミュニケーションチャネル (Slack、電話ブリッジ、IRC、HipChat など) を事前に決定し、合意します。
視聴者に情報を提供する インシデントが発生し、積極的に対処されていることを認識しない限り、人々は自動的に問題を解決するために何も行われていないと見なします。
同様に、問題が軽減または解決された後に対応を中止するのを忘れた場合、人々はインシデントが進行中であると見なします。このダイナミクスを未然に防ぐには、定期的なステータス更新でインシデント全体を通して対象者に情報を提供し続けます。
連絡先のリストを用意しておくと、貴重な時間を節約し、誰も見逃すことはありません。
https://sre.google/workbook/incident-response/
</div></details>

### Q.  問題35: 回答
アプリケーションは Google Cloud Platform(GCP)上で実行されます。アプリケーション リリースを GCP にデプロイするには、Jenkins を実装する必要があります。リリースプロセスを合理化し、運用上の労力を軽減し、ユーザーデータを安全に保つ必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明リリースプロセスを合理化し、運用上のオーバーヘッドを最小限に抑え、ユーザーデータのセキュリティを確保しながら、アプリケーションリリースをGoogle Cloud Platform(GCP)にデプロイするには、提供されているオプションの中から最適なオプションは次のとおりです。
D. Compute Engine 仮想マシンに Jenkins を実装します。
説明：
A. ローカル ワークステーションに Jenkins を実装すると、リリース プロセスを効率的に管理および自動化するために必要なスケーラビリティ、信頼性、およびアクセシビリティが提供されない可能性があります。また、必要なリソースやセキュリティ対策が不足している可能性もあります。
B. オンプレミスの Kubernetes に Jenkins を実装するという選択肢もありますが、Kubernetes インフラストラクチャの管理がさらに複雑になる可能性があり、運用上の労力を最小限に抑えるという目標に沿わない可能性があります。さらに、GCP のネイティブ サービスを効果的に活用できない可能性があります。
C. Cloud Functions は複雑なデプロイ ワークフローを管理するよりも、イベント駆動型のサーバーレス機能に適しているため、Google Cloud Functions での Jenkins の実装は、アプリケーション リリースのデプロイには適していない可能性があります。
D. Compute Engine 仮想マシンに Jenkins を実装すると、Jenkins 環境をより詳細に制御でき、カスタマイズ、スケーラビリティ、柔軟性が得られます。Compute Engine は、スケーラブルで安全な仮想マシンを GCP 上で提供し、必要なリソースとセキュリティ対策を講じた Jenkins のセットアップを可能にしながら、他の GCP サービスと統合してアプリケーション リリースを効果的にデプロイします。
そのため、Compute Engine 仮想マシンに Jenkins を実装する(オプション D)は、アプリケーション リリースを GCP にデプロイし、プロセスの合理化、運用上の労力の軽減、ユーザーデータのセキュリティの維持に最も適した選択肢となります。
インサイト - https://plugins.jenkins.io/google-compute-engine/
</div></details>

### Q.  問題36: 回答
あなたの会社は、サイト信頼性エンジニアリングのプラクティスに従っています。あなたは、顧客向けアプリケーションに影響を与える大規模で進行中のインシデントのコミュニケーション担当者です。停止の解決に要する時間はまだ決まっていません。停止に関する最新情報を求める社内の関係者からのメールや、何が起こっているのかを知りたい顧客からのメールを受信しています。停止の影響を受けるすべてのユーザーに更新プログラムを効率的に提供する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明顧客向けアプリケーションに影響を与える重大な停止が発生し、複数の利害関係者が更新を求めている状況では、最適なアプローチがオプションです
B: すべての利害関係者にタイムリーに定期的な更新を提供し、すべてのコミュニケーションで次の更新時間にコミットします。
オプションBが最も効果的である理由は次のとおりです。
1. 透明性とタイムリーな更新: 停止時には、社内の関係者と顧客の両方に対する定期的かつタイムリーな更新が重要です。このアプローチにより、透明性が確保され、インシデントの進行状況と進展について全員に情報を得ることができます。
2. コミュニケーション スケジュールへのコミットメント: すべてのコミュニケーションで次の更新時間を約束することで、利害関係者が次の更新を期待できる時期を明確に設定します。これは、期待を管理し、進行中のインシデント中に特に重要な不確実性を減らすのに役立ちます。
3. 社内外のコミュニケーションの優先順位を同等にする: 停止時には、社内の関係者と顧客の両方が等しく重要です。すべての関係者に定期的に最新情報を提供することで、関係者全員に状況、進捗状況、および問題を解決するために講じられているアクションについて常に情報を得ることができます。
オプションA、C、およびDには、潜在的な欠点があります。
· オプションA:30分ごとに社内の関係者への対応だけに集中すると、顧客とのコミュニケーションが不足し、顧客の間でフラストレーションや不確実性が生じる可能性があります。
· オプションC:社内の関係者への対応を別のチーム メンバーに委任すると、コミュニケーションのギャップが生じたり、インシデントに等しく投資している社内の関係者に必要な最新情報の提供が遅れたりする可能性があります。
· オプションD:すべての内部関係者とのコミュニケーションをインシデント指揮官に引き継ぐと、コミュニケーションフローにボトルネックが生じ、社内の関係者との情報共有が遅れたり制限されたりする可能性があります。
要約すると、顧客向けアプリケーションに影響を与える大規模なインシデントが発生した場合、すべての利害関係者にタイムリーに定期的な更新を提供し、すべてのコミュニケーションで次の更新時間を約束することで、透明性を確保し、期待を管理し、インシデントの進行状況について社内の利害関係者と顧客の両方に十分な情報を提供します。
インサイト - https://sre.google/workbook/incident-response/
</div></details>

### Q.  問題37: 回答
新しいサービスを運用環境にデプロイする必要があります。このサービスは、マネージド インスタンス グループ (MIG) を使用して自動的にスケーリングする必要があり、複数のリージョンにデプロイする必要があります。サービスにはインスタンスごとに多数のリソースが必要であり、容量を計画する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明複数のリージョンにまたがり、大きなリソース要件を持つマネージド インスタンス グループ (MIG) を使用して自動的にスケーリングする必要があるサービスをデプロイするには、次の手順を実行する必要があります。
C. リソース要件が各リージョンの使用可能なクォータ制限内にあることを検証します。
説明：
1. n1-highcpu-96 マシンタイプの使用: マシンタイプの選択は重要ですが、n1-highcpu-96 を選択することはオプションですが、特に複数のリージョンや容量計画を扱う場合は、要件のすべてをカバーしているわけではありません。
2. Stackdriver Trace の結果のモニタリング: Stackdriver Trace は、主にアプリケーションのレイテンシのトレースに使用されます。容量計画やスケーリングのためのリソース要件に関する情報が直接提供されない場合があります。
3. リージョン間の容量計画とリソース割り当て: リソース要件が各リージョンで使用可能なクォータ制限と一致していることを検証することが重要です。これにより、インスタンスを複数のリージョンにデプロイするときに、サービスの需要を満たすために必要なリソースを確保できます。
4. 1 つのリージョンにデプロイし、グローバル ロード バランサーを使用する: このアプローチでは、トラフィックがグローバルに分散される可能性がありますが、各リージョンで十分なリソースを確保しながら、サービスを複数のリージョンにデプロイするという要件には対応できません。
したがって、オプション C (リソース要件が各リージョンの使用可能なクォータ制限内にあることを検証する) は、サービスに必要なリソースをクォータ制限に合わせて複数のリージョンにプロビジョニングできるようにするのに最も適しています。
インサイト - https://cloud.google.com/compute/quotas#understanding_quotas https://cloud.google.com/compute/quotas
</div></details>

### Q.  問題38: 回答
会社のセキュリティチームは、_Requiredバケット内のデータアクセス監査ログへの読み取り専用アクセス権を持っている必要があります。セキュリティ チームには、最小権限の原則と Google が推奨するプラクティスに従って、必要な権限を付与する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明「_Required」バケット内のデータアクセス監査ログへの読み取り専用アクセスをセキュリティ チームに提供するには、最小権限の原則と Google が推奨するプラクティスに従いながら、次の方法をお勧めします。
D. roles/logging.privateLogViewer ロールを、すべてのセキュリティ チーム メンバーを含むグループに割り当てます。
説明：
1. roles/logging.privateLogViewer ロール: このロールは、標準ログよりも機密性の高いプライベートログ (データ アクセス監査ログなど) への読み取り専用アクセスを提供します。これは、最小特権の原則に沿って、必要以上の追加のアクセス許可を与えることなく、ログを表示するために必要なアクセス許可を付与します。
2.グループベースのアクセス制御:グループを使用して権限を管理すると、アクセスの管理と管理が容易になります。roles/logging.privateLogViewer ロールを、すべてのセキュリティ チーム メンバーを含む特定のグループに割り当てると、一貫性が確保され、アクセス制御管理が簡素化されます。
この方法は、Google が推奨するプラクティスに従って、機密性の高いログを表示するために必要な権限のみを付与する特定のロールを提供すると同時に、グループベースのアクセス制御によって構造化された管理しやすいアプローチを確保します。
オプション A と B は roles/logging.viewer ロールを割り当て、すべてのログへの広範なアクセスを提供しますが、必要以上のアクセス許可を付与する可能性があり、最小特権の原則に違反する可能性があります。
オプション C では、roles/logging.privateLogViewer ロールが各メンバーに個別に割り当てられますが、これはグループを使用するよりも管理が面倒で、時間の経過とともにアクセス制御の不整合やエラーにつながる可能性があります。
インサイト - https://cloud.google.com/iam/docs/job-functions/auditing
logging.privateLogViewer ロールは、データ アクセス ログを表示する機能を提供します。
</div></details>

### Q.  問題39: 回答
Cloud Build を使用して新しい Docker イメージをビルドし、Docker Hub にプッシュする CI / CD パイプラインがあります。コードのバージョン管理には Git を使用します。Cloud Build の YAML 構成を変更した後、パイプラインによって新しいアーティファクトがビルドされていないことに気付きます。この問題を解決するには、サイト
信頼性エンジニアリングのプラクティスに従って解決する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明サイト信頼性エンジニアリング(SRE)のプラクティスに従って Cloud Build の YAML 構成を変更した後、CI / CD パイプラインで新しいアーティファクトがビルドされない問題を解決するには、次の方法が最適です。
D. 以前の Cloud Build 構成ファイルと現在の Cloud Build 構成ファイルを Git で比較して、バグを見つけて修正します。
説明：
以前の Cloud Build 構成ファイルと現在の Cloud Build 構成ファイルの間で Git 比較を実行する: この手順では、構成ファイルに加えられた変更を特定できます。これにより、問題の原因となった可能性のある変更を特定し、バグや設定ミスを見つけて修正することができます。
このアプローチは、バージョン管理 (Git) を通じて問題の根本原因を特定し、構成ファイルの変更を比較することに重点を置いているため、SRE のプラクティスと一致しています。システムの信頼性を維持および向上させるためのSRE手法の基本である、構造化された体系的なトラブルシューティングプロセスを促進します。
</div></details>

### Q.  問題40: 回答
Google Cloud リソースの Terraform デプロイを実行するための CI / CD パイプラインを作成しています。CI / CD ツールは Google Kubernetes Engine(GKE)で実行されており、パイプラインの実行ごとにエフェメラル Pod を使用します。ポッドで実行されるパイプラインに、Terraformデプロイメントを実行するための適切なIdentity and Access Management(IAM)権限があることを確認する必要があります。ID 管理について、Google が推奨するプラクティスに従う必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明選択したオプションの説明:
A. 新しいKubernetesサービスアカウントを作成し、そのサービスアカウントをPodに割り当てます。Workload Identity を使用して、Google サービス アカウントとして認証します。
KubernetesサービスアカウントとWorkload Identityを組み合わせると、PodはGoogleサービスアカウントのIDを引き継ぐことができます。このアプローチは、GKE 内で ID と権限を安全に管理し、Google Cloud リソースへの適切なアクセスを確保するための Google 推奨のプラクティスです。
C. 新しい Google サービス アカウントを作成し、適切な IAM アクセス許可を割り当てます。
Googleサービスアカウントを作成し、必要なIAM権限を割り当てることは、必要なステップです。ただし、GKE で実行されている Pod の場合は、Kubernetes サービス アカウントと Workload Identity を使用して、Google Cloud リソースにアクセスするためにサービス アカウントを Pod に関連付ける方が適切です。
オプション B、D、E が、Terraform デプロイ用のエフェメラル Pod を使用して GKE 上で実行される CI / CD パイプラインでの ID 管理に関する Google 推奨のプラクティスと一致しない理由を評価してみましょう。
B. Googleサービスアカウントの新しいJSONサービスアカウントキーを作成し、そのキーをKubernetesシークレットとして保存し、キーをPodに挿入して、GOOGLE_APPLICATION_CREDENTIALS環境変数を設定します。
サービスアカウントの鍵をPodに注入することは可能な方法ですが、鍵をKubernetesのシークレットとして保存し、Podに注入すると、セキュリティ上のリスクが生じる可能性があります。このアプローチは、Kubernetes シークレット内の鍵管理に関する潜在的なセキュリティ上の懸念があるため、Google が推奨するプラクティスと完全には一致しない可能性があります。
D. Googleサービス・アカウント用の新しいJSONサービス・アカウント・キーを作成し、CI/CDツールのシークレット管理ストアにキーを格納し、このキーを認証に使用するようにTerraformを構成します。
サービス アカウント キーを CI/CD ツールのシークレット管理ストアに格納することは、安全なストレージの良い方法です。ただし、Terraform内での認証にこれらのキーを直接使用することは、セキュリティに影響を与える可能性があるため、理想的ではない場合があります。一般に、サービス アカウント キーは慎重に使用し、アプリケーションまたはデプロイの構成に直接埋め込まないようにすることをお勧めします。
E. Pod を実行する Compute Engine VM インスタンスに関連付けられた Google サービス アカウントに適切な IAM 権限を割り当てます。
基盤となる Compute Engine VM インスタンスに関連付けられた Google サービス アカウントに IAM 権限を直接割り当てても、GKE で CI / CD パイプラインを実行しているエフェメラル Pod に権限を付与するのに効果的でない場合があります。Pod で使用されるサービス アカウントは、理想的には Kubernetes サービス アカウントと GKE の Workload Identity を使用して、個別に管理および権限を付与する必要があります。
要約すると、オプション A と C は、IAM 権限の管理に Kubernetes サービス アカウント、ワークロード ID、Google サービス アカウントを使用することで、GKE ベースの CI / CD パイプライン内の ID 管理に関する Google が推奨するプラクティスとより密接に連携しています。
インサイト - https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform
</div></details>

### Q.  問題41: 回答
チームは、すべての CI / CD パイプラインに Cloud Build を使用しています。Cloud Build の kubectl ビルダーを使用して、新しいイメージを Google Kubernetes Engine
(GKE)にデプロイしたい。開発の労力を最小限に抑えながら、GKE への認証を行う必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Kubernetes Engine(GKE)への認証を行いながら、Cloud Build に kubectl ビルダーを使用し、開発の労力を最小限に抑えるには、次の方法が最適です。
オプション A: コンテナ開発者ロールを Cloud Build サービス アカウントに割り当てます。
このオプションが推奨される理由は次のとおりです。
1. 最小権限の原則: 必要な権限(コンテナ開発者ロール)を Cloud Build サービス アカウントに直接割り当てることで、最小権限の原則が確保されます。GKE 内のリソースを管理するために必要な権限のみが Cloud Build に付与されます。
2. シンプルさと効率性: Cloud Build サービス アカウントにロールを割り当てることで、複雑さを増したり、新しいサービス アカウントを作成したりすることなく、プロセスを合理化できます。
オプション B、C、および D には、潜在的な問題があります。
· オプションB:cloudbuild.yaml ファイルでロールを指定することは、CI/CD 構成内でアクセス許可を直接指定する必要があるため、最適なアプローチではない可能性があります。アクセス許可は、IAM ロールを使用して個別に管理することをお勧めします。
· オプションC:Cloud Build 専用のコンテナ デベロッパー ロールで新しいサービス アカウントを作成すると、不必要な複雑さが増し、サービス アカウントの数が増え、管理オーバーヘッドにつながる可能性があります。
· オプションD:Cloud Build でサービス アカウントの認証情報を取得する別のステップを作成して kubectl に渡すと、CI / CD パイプライン構成内で認証情報を処理することでセキュリティ リスクが生じる可能性があります。一般に、パイプライン ステップ内で資格情報を直接処理することはお勧めしません。
要約すると、コンテナ デベロッパー ロールを Cloud Build サービス アカウントに直接割り当てる(オプション A)と、最小権限の原則を遵守しながら GKE を操作するために必要な権限を Cloud Build に付与することで、安全で効率的なアプローチが保証されます。
インサイト - https://cloud.google.com/build/docs/securing-builds/configure-user-specified-service-accounts
</div></details>

### Q.  問題42: 回答
インフラストラクチャを定義するTerraformテンプレートを作成および変更する必要があります。2 人の新しいエンジニアも同じコードで作業するため、プロセスを定義し、互いのコードを上書きしないようにツールを採用する必要があります。また、すべての更新プログラムを最新バージョンでキャプチャする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Terraformテンプレートを管理しながら、複数のエンジニア間のコラボレーションとバージョン管理を確保するための最も適切な選択肢は次のとおりです。
B. Git ベースのバージョン管理システムにコードを格納します。ピアによるコードレビューと単体テストを含むプロセスを確立し、コードを統合する前に整合性と機能を確認します。リポジトリに完全に統合されたコードが最新のマスターバージョンになるプロセスを確立します。
その理由は次のとおりです。
1. Git ベースのバージョン管理システム: Git は共同ソフトウェア開発用に設計されており、Terraform テンプレートなどのコードとしてのインフラストラクチャ (IaC) の管理に適しています。これにより、複数のエンジニアが互いの変更を上書きすることなく、同じコードベースで同時に作業できます。
2. コードレビューと単体テスト: ピアによるコードレビューと単体テストを含むプロセスを確立することで、変更をメインブランチに統合する前に、品質、機能、ベストプラクティスへの準拠について徹底的にレビューされます。これにより、コードの整合性が維持され、潜在的なエラーや競合が軽減されます。
3. 最新のマスター バージョンとして統合されたコード: メイン ブランチ (マスターまたはメイン ブランチ) で完全に統合およびテストされたコードが最新バージョンになるプロセスを採用すると、最新バージョンとして受け入れられる前に、すべての変更が組み込まれ、検証されます。これにより、バージョンを管理するための明確で信頼性の高い方法が提供され、安定したコードベースが促進されます。
オプション A、C、D では、コードを Cloud Storage または Google ドライブに保存し、フォルダ名の変更、zip アーカイブの作成、ファイルのアップロードなどの手動プロセスでバージョンを管理します。これらの方法では基本的なバージョン管理は提供されますが、Git のようなバージョン管理システムの堅牢性、コラボレーション機能、自動化機能には欠けています。また、チーム環境での共同開発に必要なコードレビュー、テスト、統合メカニズムと同じレベルも提供されません。
</div></details>

## 3
### Q.  問題1: 回答
App Engine で実行され、データ ストレージに CloudSQL と Cloud Storage を使用するウェブ アプリケーションをサポートしている。Web サイトのトラフィックが一時的に急増した後、すべてのユーザー要求の待機時間が大幅に増加し、CPU 使用率が増加し、アプリケーションを実行するプロセスの数が増加していることに気付きます。最初のトラブルシューティングでは、次のことが明らかになります:
1-トラフィックの最初のスパイクの後、負荷レベルは通常に戻りましたが、ユーザーは依然として高い遅延を経験します。
2- CloudSQL データベースからのコンテンツと Cloud Storage からの画像に対するリクエストは、同じ高レイテンシを示します。
3-レイテンシーが増加した前後にWebサイトに変更が加えられませんでした。
4-ユーザーへのエラーの数は増加しません。
今後数日間で Web サイトのトラフィックが再び急増することが予想され、ユーザーが遅延を経験しないようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明ウェブサイトのトラフィックが急増した結果、ユーザー リクエストのレイテンシが増加し、ウェブサイトに変更を加えずに CloudSQL と Cloud Storage のレイテンシが高かった場合、予想されるトラフィックの急増に備え、ユーザーにレイテンシが発生しないようにするには、次のような最適なアクションを使用します。
D. App Engine の設定を変更して、アイドル状態のインスタンスを追加します。
説明：
A. GCS(Google Cloud Storage)バケットをマルチリージョンにアップグレードすると、データの可用性は向上する可能性がありますが、ユーザーが経験するレイテンシの問題に直接対処できない場合があります。レイテンシの問題は、CloudSQLとCloud Storageの両方からの応答時間に関連しているようです。
B. CloudSQL インスタンスで高可用性を有効にすることは冗長性にとって重要ですが、Cloud Storage に関連するレイテンシの問題に直接対処できない場合があります。さらに、一時的な負荷スパイクの場合の待機時間の問題がすぐに解決されない可能性があります。
C. アプリケーションを App Engine から Compute Engine に移行すると、インフラストラクチャをより詳細に制御できるようになりますが、移行に多大な労力がかかる可能性があり、CloudSQL と Cloud Storage に関連するレイテンシの問題を直接解決できない可能性があります。
D. App Engine の設定を変更してアイドル状態のインスタンスを追加すると、増加したトラフィックをより効率的に処理できます。アイドル状態のインスタンスが増えると、App Engine は受信トラフィックを処理するためにすぐに利用できるリソースが増え、トラフィックの急増時にユーザーのレイテンシが短縮されます。
そのため、ウェブサイトのトラフィックが急増すると予想されるときにユーザーにレイテンシが発生しないようにするには、App Engine の設定を変更してアイドル状態のインスタンスを追加する(オプション D)の最適なアクションです。これにより、リソースの可用性が向上し、増加したトラフィックを効率的に処理し、ユーザーの待機時間を短縮できます。
インサイト - https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed
https://cloud.google.com/appengine/docs/standard/python/config/appref max_idle_instances
</div></details>

### Q.  問題2: 回答
サードパーティのアプリケーションが正しく動作するには、サービス アカウント キーが必要です。クラウド プロジェクトからキーをエクスポートしようとすると、「組織ポリシーの制約 iam.disableServiceAccounKeyCreation が適用されています」というエラーが表示されます。サードパーティ製アプリケーションを動作させながら、Google が推奨するセキュリティ対策に従う必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明このシナリオの正しいオプションは次のとおりです。
B. 組織レベルで iam.disableServiceAccountKeyCreation ポリシーを削除し、キーを作成します。
説明：
組織レベルでの iam.disableServiceAccountKeyCreation ポリシーの削除:
このアクションには、サービス アカウント キーの作成を妨げている制限を解除し、キーの生成を許可することが含まれます。
組織レベルでの変更は、組織のセキュリティ ポリシーとコンプライアンス標準に合わせる必要があることに注意することが重要です。適切な承認を確保し、この変更が全体的なセキュリティに与える影響を考慮してください。
サードパーティアプリケーションのサービスアカウントキーの作成:
ポリシーの制約を削除した後、サードパーティ アプリケーションに必要なサービス アカウント キーを生成します。
キーを安全にダウンロードし、必要に応じてサードパーティアプリケーションに提供します。
他のオプションが適さない理由:
A. デフォルトのサービスアカウントキーを有効にしてダウンロードします。
このオプションでは、既定のサービス アカウント キーが既に使用可能であることを前提としていますが、特にサービス アカウント キーの作成が適用されたポリシーによって無効になっている場合は、そうではない可能性があります。既定のキーを有効にすることは、ポリシーの制限がまだ適用されている場合、実行可能な解決策ではない可能性があります。
C. プロジェクトのフォルダーでサービス アカウント キー作成ポリシーを無効にし、既定のキーをダウンロードします。
制限が組織レベルで課されている場合、プロジェクト レベルでポリシーを無効にするだけでは不十分な場合があります。また、サービス アカウント キーの作成が禁止されている場合、既定のキーに依存しても問題が解決されない可能性があります。
D. iam.disableServiceAccountKeyCreation ポリシーを off に設定するルールをプロジェクトに追加し、キーを作成します。
プロジェクト内のポリシー設定を変更しようとしても、現在適用されている組織レベルのポリシー制約は上書きされない場合があります。組織のルールを変更しても、組織レベルのポリシーがまだ適用されている場合、根本的な問題を効果的に解決できない可能性があります。
組織レベルのポリシーに対する変更が、セキュリティのベスト プラクティスに準拠し、組織のセキュリティとコンプライアンスの標準に沿っていることを確認することが重要です。組織レベルのポリシーを変更する前に、組織のセキュリティ チームまたはコンプライアンス チームに相談することをお勧めします。
</div></details>

### Q.  問題3: 回答
Google Kubernetes Engine(GKE)クラスタ全体に複数の制約テンプレートを適用する必要があります。制約には、Kubernetes API の制限などのポリシー パラメーターが含まれます。ポリシー パラメーターが GitHub リポジトリに格納され、変更が発生したときに自動的に適用されるようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明GitHub リポジトリから Google Kubernetes Engine(GKE)クラスタ全体に制約テンプレートを適用し、変更を自動的に適用する正しいアプローチは次のとおりです。
C. GitHub リポジトリを使用して Anthos Config Management を構成します。リポジトリに変更があった場合は、Anthos Config Management を使用して変更を適用します。
説明：
Anthos Config Management は、GKE クラスタを含む複数の Kubernetes クラスタの構成を管理するために設計された強力なツールです。これにより、これらのクラスタ間でポリシーと設定を一貫して適用できます。
GitHub リポジトリで Anthos Config Management を構成すると、リポジトリに保存されているポリシー パラメータを変更すると、GKE クラスタ全体で自動更新と適用がトリガーされます。
Anthos Config Management は Git リポジトリと適切に統合され、構成の変更を自動的に同期できるため、クラスタ間の一貫性とコンプライアンスが確保されます。
他のオプションは、望ましい結果を得るのに最も適していません。
A. パラメータが変更されたときに Cloud Build をトリガーするように GitHub アクションを設定します。Cloud Build で gcloud CLI コマンドを実行して変更を適用する: Cloud Build の使用はプロセスを自動化する方法ですが、gcloud CLI コマンドを直接使用することは、複数の GKE クラスタに一貫して変更を適用するための最も効果的な方法ではない可能性があります。
イ.GitHub に変更があった場合は、Webhook を使用して Anthos Service Mesh にリクエストを送信し、変更を適用します。 Anthos Service Mesh は、主にサービス メッシュの構成とマイクロサービスの管理を扱います。Anthos Config Management と同じ方法で Kubernetes クラスタ全体の構成を管理するようには設計されていません。
D. GitHub リポジトリで Config Connector を構成します。リポジトリに変更があった場合は、Config Connector を使用して変更を適用します。 Config Connector は、Kubernetes スタイルの宣言を使用して Google Cloud リソースを管理するために使用されます。Google Cloud リソースの管理には強力ですが、GitHub リポジトリに保存されている GKE クラスタ全体にポリシー パラメータを適用するのに特化して理想的なツールではない可能性があります。Anthos Config Management は、この目的に合わせて調整されています。
インサイト - https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller
https://medium.com/@kasiarun/introduction-to-anthos-config-management-1a43917c26ae
</div></details>

### Q.  問題4: 回答
e コマース アプリケーションを Google Cloud Platform(GCP)に移行しました。次の繁忙期に備えてアプリケーションを準備します。繁忙期に備えるために、まず何をすべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明次の繁忙期に向けてGoogle Cloud Platform(GCP)でeコマースアプリケーションを準備するには、次の最も適切な最初のステップは次のとおりです。
A. アプリケーションのロード テストを行い、スケーリングのパフォーマンスをプロファイリングします。
説明：
A. 負荷テストは、シミュレートされた高トラフィック条件下でのパフォーマンスを評価するのに役立ちます。負荷テストを実施することで、潜在的なボトルネックを特定し、アプリケーションのスケーラビリティを理解し、高負荷下でのパフォーマンスを判断できます。この情報は、繁忙期に増加するトラフィックを処理するためにリソースを効果的に最適化およびスケーリングするために重要です。
B. 運用クラスターで AutoScaling を有効にすることは効果的な戦略ですが、負荷がかかった状態でのアプリケーションの動作を理解せずに有効にすると、リソース割り当てが効率的に最適化されない可能性があります。
C. 適切な分析とテストを行わずに、昨シーズンに使用したコンピューティング能力の 2 倍を事前プロビジョニングすると、過剰なプロビジョニングが発生し、予想される成長が実現しなかった場合に不要な費用が発生する可能性があります。
D. ディザスター リカバリー (DR) 環境を拡張するための Runbook を作成することは、準備のために重要ですが、繁忙期のスケーリング ニーズに直接対応できない場合があります。
したがって、アプリケーションで負荷テストを実施して、スケーリングのパフォーマンスをプロファイリングする (オプション A) ことが、最も適切な最初の手順です。これにより、高負荷時のアプリケーションの動作をよりよく理解し、リソースの最適化を支援し、繁忙期に増加したトラフィックをアプリケーションが効率的に処理できるようにします。
インサイト - https://cloud.google.com/blog/topics/retail/preparing-for-peak-holiday-season-while-wfh
https://cloud.google.com/architecture/black-friday-production-readiness#preparation_stage
</div></details>

### Q.  問題5: 回答
Compute Engine で実行されるアプリケーションを管理している。アプリケーションは、カスタム HTTP サーバーを使用して、内部 TCP / UDP ロードバランサーを介して他のアプリケーションからアクセスされる API を公開します。ファイアウォール ルールは、0.0.0.0/0 から API ポートへのアクセスを許可します。最小限のステップ数で API にアクセスする各 IP アドレスをログに記録するように Cloud Logging を設定する必要があります。最初に何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明·このシナリオで、最も少ないステップ数で API にアクセスする各 IP アドレスをログに記録する正しいオプションは次のとおりです。
· D. サブネットで VPC フローログを有効にします。
·説明：
·サブネットで VPC フローログを有効にすると、その特定のサブネット内のネットワークトラフィック情報をログに記録できます。これには、内部 TCP / UDP ロードバランサーを介して API ポートに到達するトラフィックが含まれます。VPC フローログを有効にすることで、個々のインスタンスやコンポーネントで追加の設定を行うことなく、API にアクセスする各 IP アドレスを効果的に追跡してログに記録できます。VPC フローログは、送信元と宛先の IP アドレス、ポート、プロトコルなど、トラフィックに関するメタデータ情報をキャプチャします。
·他のオプションが適さない理由:
·A. VPC で Packet Mirroring を有効にする: Packet Mirroring は、ネットワーク パケットを複製し、侵入検知システムやパケット アナライザなどの監視ツールに送信するために使用されます。ネットワーク分析には役立ちますが、API にアクセスする各 IP アドレスをログに記録するために特別に設計されたものではありません。トラフィックをキャプチャすることはできますが、API にアクセスする特定の IP アドレスを直接記録することはありません。
·B. Compute Engine インスタンスに Ops Agent をインストールする: Ops Agent は、インスタンスを管理するためのモニタリング、ロギング、その他の機能を提供します。システムログとアプリケーションログのログ記録に役立ちますが、追加の構成が必要であり、カスタムログ分析と解析が実装されていない限り、APIにアクセスする特定のIPアドレスを直接キャプチャできない可能性があります。
·C. ファイアウォール規則のログ記録を有効にする: ファイアウォール規則のログ記録を有効にすると、ファイアウォールのアクティビティに特に関連するログが提供されます。ファイアウォールルールに基づいて許可または拒否された接続をログに記録することはできますが、APIにアクセスする各IPアドレスを識別するために必要な特定の詳細をキャプチャできない場合があります。ログには、IP アドレスによる個々の API アクセスを追跡するために必要な粒度が含まれていない場合があります。
·要約すると、サブネットで VPC フローログを有効にすることは、個々のインスタンスやコンポーネントで広範な構成を必要とせずに、API にアクセスする各 IP アドレスの詳細をキャプチャしてログに記録するための最も直接的で適切なオプションです。
</div></details>

### Q.  問題6: 不正解
組織の仮想マシン (VM) のコストを削減する必要があります。さまざまなオプションを検討した後、プリエンプティブルVMインスタンスを利用することにしました。
プリエンプティブルVMに適したアプリケーションはどれですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明プリエンプティブルVMインスタンスは、一般に、フォールトトレラントで、中断を処理でき、継続的な可用性を必要としないアプリケーションまたはワークロードに適しています。提供されるオプションを考えると、次のようになります。
ある。スケーラブルなインメモリ キャッシュ システム: インメモリ キャッシュ システムには冗長性があり、重要なデータを失うことなく中断を許容できることが多いため、プリエンプティブル VM はこのアプリケーションに適しています。これらのシステムは、一時的な VM の停止を処理するように構築できます。
イ.組織の一般向け Web サイト: プリエンプティブル VM は、継続的な可用性が必要であり、ユーザー エクスペリエンスに影響を与えずに中断を適切に処理できない可能性があるため、一般向けの Web サイトには適していない可能性があります。
ウ.十分なクォーラムを備えた分散型の結果整合性のある NoSQL データベース クラスター: NoSQL データベースは、通常、特に結果整合性のために設計されている場合、ある程度の中断を許容できます。ただし、プリエンプティブルインスタンスでは、一時的な性質上、十分なクォーラムを確保することが困難な場合があります。
D.ビデオを取得してストレージバケットに保存するGPUアクセラレーションビデオレンダリングプラットフォーム:ビデオレンダリングには継続的な処理が必要になることが多く、中断はレンダリングプロセスに大きな影響を与える可能性があります。プリエンプティブル VM は、中断のない GPU アクセラレーションが必要なため、このワークロードには適していない可能性があります。
したがって、指定されたオプションの中で、プリエンプティブル VM に最も適したアプリケーションは A です。スケーラブルなインメモリ キャッシュ システムは、中断を処理でき、必ずしも継続的な可用性を必要としないためです。
https://cloud.google.com/compute/docs/instances/preemptible
</div></details>

### Q.  問題7: 回答
あなたは、サービスの 1 つで進行中のインシデントの運用リーダーです。このサービスは通常、約 70% の容量で実行されます。1 つのノードがすべての要求に対して 5xx エラーを返していることに気付きます。また、お客様からのサポートケースも顕著に増加しています。問題のあるノードをロードバランサープールから削除して、ノードを分離して調査できるようにする必要があります。Google が推奨する方法に従ってインシデントを管理し、ユーザーへの影響を軽減したい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google が推奨するプラクティスに従い、サービス内の異常なノードが関係するインシデント時のユーザーへの影響を軽減するための最も適切なアクションは次のとおりです。
オプションA:
インシデント チームに意図を伝えます。
負荷分析を実行して、残りのノードが、削除されたノードからオフロードされたトラフィックの増加を処理し、適切にスケーリングできるかどうかを判断します。
新しいノードが正常であると報告されたら、異常なノードからトラフィックをドレインし、異常なノードをサービスから削除します。
説明：
インシデントチームとのコミュニケーション:インシデント チームとの明確なコミュニケーションを維持して、実行されているアクションを全員が認識できるようにすることが重要です。
負荷分析とスケーリング:負荷分析を実行すると、異常なノードが削除された後に既存のノードが追加のトラフィックを処理できるかどうかを判断するのに役立ちます。適切にスケーリングすることで、サービスが安定し、パフォーマンスを損なうことなく負荷を処理できるようになります。
トラフィックをドレインし、新しいノードが正常であると報告された後に異常なノードを削除する:新しいノードが正常になるのを待ってから異常なノードを削除することで、容量が突然失われることがなくなり、ユーザーエクスペリエンスに大きな影響を与えることなくスムーズな移行が可能になります。
他のオプションは、次の理由により、インシデントを管理し、ユーザーへの影響を軽減するのに最適ではありません。
オプションB:新しいノードを追加してからトラフィックをドレインすると、新しいノードが増加したトラフィック負荷を処理できない場合に潜在的な問題が発生し、古いノードが削除される前にサービスが低下する可能性があります。
オプションC:スケーリングを行わずに、または新しいノードが正常と報告されるのを待たずに異常なノードを削除すると、残りのノードが過負荷になり、より多くの 5xx エラーが発生し、ユーザーに影響を与える可能性があります。
オプションD:異常なノードを削除した後に新しいノードを追加すると、トラフィックの増加をすぐにサポートできず、新しいノードが完全に動作可能になる前にパフォーマンスの問題が発生する可能性があります。
</div></details>

### Q.  問題8: 回答
あなたの会社は、サイト信頼性エンジニアリングのプラクティスに従っています。あなたは、顧客に影響を与える新しいインシデントのインシデント指揮官です。効果的なインシデント対応を支援するために、2 つのインシデント管理ロールをすぐに割り当てる必要があります。どのような役割を割り当てる必要がありますか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明サイト信頼性エンジニアリングのプラクティスに従ったインシデント対応シナリオでは、インシデント指揮官として支援するために割り当てるべき 2 つの役割は次のとおりです。
1. A. オペレーション リード: この役割は、インシデントを迅速に解決するためのアクションとリソースを調整する責任があります。インシデントの技術的側面を管理し、適切なトラブルシューティングを確保し、差し迫ったリスクを軽減し、影響を受けるシステムを安定させます。
2. C. コミュニケーション リード: インシデント発生時にはコミュニケーションが重要です。この役割は、内部対応チームのメンバー間の効果的なコミュニケーションを確保し、必要に応じて外部コミュニケーションを調整します。インシデントの進行状況、予想される解決時間、および必要な更新について関係者に通知します。
これらの役割を割り当てることで、インシデントに対する構造化された効率的な対応を維持し、技術的な解決と利害関係者との明確で一貫性のあるコミュニケーションの両方に焦点を当てることができます。
https://sre.google/workbook/incident-response/
「インシデント対応における主な役割は、インシデントコマンダー(IC)、コミュニケーションリード(CL)、オペレーションまたはオペレーションリード(OL)です。」
</div></details>

### Q.  問題9: 回答
アプリケーションを Cloud Run にデプロイしています。アプリケーションを起動するにはパスワードが必要です。組織では、すべてのパスワードを 24 時間ごとにローテーションし、アプリケーションに最新のパスワードを設定する必要があります。ダウンタイムなしでアプリケーションをデプロイする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明ダウンタイムが発生しないようにしながら 24 時間ごとにパスワードをローテーションする要件でアプリケーションを Cloud Run にデプロイするには、次の方法が最適です。
オプション B: シークレット マネージャーにパスワードを保存し、シークレットをアプリケーション内のボリュームとしてマウントします。
説明：
シークレット マネージャー:これは、APIキー、パスワード、証明書などの機密データを保存するための安全で一元管理されたサービスです。パスワードをシークレットマネージャーに保存すると、セキュリティと適切な管理が保証されます。
シークレットをボリュームとしてマウントする:Cloud Run では、アプリケーション内でマウントされたボリュームとしての Secret Manager シークレットの使用がサポートされています。これにより、アプリケーションはコンテナーを再デプロイすることなく、最新のパスワードにアクセスできます。
パスワードのローテーション:Secret Manager でパスワードを定期的に更新し、Cloud Run アプリケーションでボリュームとしてマウントすることで、アプリケーションは再デプロイを必要とせずに最新のパスワードを動的に取得できます。これは、ダウンタイムを回避しながら 24 時間ごとにパスワードをローテーションするという要件と一致しています。
他のオプションには欠点があり、ダウンタイムなしと安全なパスワード管理の要件に合わない可能性があります。
オプションA:環境変数を使用してシークレットをアプリケーションに送信すると、パスワードが公開され、アプリケーションを再起動せずにパスワードを動的に更新する方法が提供されず、ダウンタイムが発生する可能性があります。
オプションC:Cloud Build を使用してビルド時にアプリケーション コンテナにパスワードを追加すると、パスワードが変更されるたびに新しいビルドと再デプロイが発生し、ダウンタイムが発生し、パスワードのローテーション中にダウンタイムが発生しないという要件に合致しなくなる可能性があります。
オプションD:パスワードをコードに直接保存し、パスワードが変更されるたびにアプリケーションをリビルドして再デプロイすることは、非効率的であり、エラーが発生しやすく、デプロイのたびにダウンタイムが発生します。さらに、パスワードをコードに直接保存することは、安全な方法ではありません。
インサイト - https://cloud.google.com/run/docs/configuring/services/secrets
</div></details>

### Q.  問題10: 回答
Compute Engine でホストされているウェブアプリケーションをサポートしている。このアプリケーションは、何千人ものユーザーに予約サービスを提供します。新機能のリリース直後、監視ダッシュボードには、すべてのユーザーがログイン時に遅延が発生していることが表示されます。サービスのユーザーに対するインシデントの影響を軽減する必要があります。最初に何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明新機能のリリース後にログイン時に遅延が発生するユーザーへの影響を最も効果的に軽減できるオプションは次のとおりです。
A. 最近のリリースをロールバックします。
最近のリリースをロールバックすると、システムが新機能が展開される前の状態に戻るため、ユーザーへの影響を軽減できます。このアクションにより、発生した問題が解消され、アプリケーションの以前の安定した状態が復元され、ログイン中にユーザーが経験する待機時間が短縮または解決される可能性があります。
他のオプションが問題の軽減に適していない理由は次のとおりです。
B. Stackdriver のモニタリングを確認する: モニタリングの確認はレイテンシの問題の根本原因を理解する上で重要ですが、データを分析して正確な問題を特定するには時間がかかる場合があります。この分析期間中、ユーザーには引き続き待機時間が発生するため、このアクションでは影響がすぐに軽減されない可能性があります。
C. ログイン サービスを実行している仮想マシンをアップサイジングする: 仮想マシンの容量を増やすことは、負荷や需要の増加に対処するための潜在的な解決策である可能性がありますが、新しくリリースされた機能によって突然の待機時間の問題が発生したこのシナリオでは、リソースを追加するだけでは根本原因に直接対処できない可能性があります。症状を一時的に緩和することはできますが、新しいリリースによって引き起こされる問題を完全に解決するとは限りません。
D. 新しいリリースをデプロイして、問題が解決するかどうかを確認する: 待機時間の問題の原因を理解せずに別の新しいリリースをデプロイすると、問題が悪化したり、さらに複雑になったりする可能性があります。新しいリリースでより多くの問題が発生したり、遅延の根本原因に対処しなかったりすると、ユーザーエクスペリエンスが悪化する可能性があります。
要約すると、監視を確認し、インフラストラクチャの調整を行うことは、問題を解決するために必要な手順である可能性がありますが、最近のリリースをロールバックすることは、ログイン中に遅延が発生しているユーザーへの影響を軽減するための最も迅速で的を絞ったアクションです。
インサイト - https://cloud.google.com/architecture/identifying-causes-of-app-latency-with-stackdriver-and-opencensus
</div></details>

### Q.  問題11: 回答
europe-west2-a ゾーンの 1 つの Compute Engine インスタンスにデプロイされるステートレスなウェブベースの API をサポートしている。サービス可用性のサービス レベル インジケーター (SLI) が、指定されたサービス レベル目標 (SLO) を下回っています。事後分析により、API への要求が定期的にタイムアウトすることが明らかになりました。タイムアウトは、API の要求数が多く、メモリが不足していることが原因です。サービスの可用性を向上させたい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明大量の要求とメモリ不足のために API が定期的にタイムアウトするシナリオでは、サービスをスケーリングし、回復性を強化することで、サービスの可用性の向上に対処できます。これを実現するための最良のアプローチは次のとおりです。
C. 他のゾーンに追加のサービスインスタンスを設定し、すべてのインスタンス間でトラフィックの負荷を分散します。
説明：
A. 測定されたサービス レベル インジケーター (SLI) と一致するように指定されたサービス レベル目標 (SLO) を変更しても、大量の要求とメモリの制約が原因でタイムアウトを引き起こす根本的な問題に直接対処することはできません。現実的な SLO を維持することは不可欠ですが、SLO を調整しても、サービスのパフォーマンスは本質的に向上しません。
B. より多くのメモリを備えた高仕様のコンピュート・インスタンスにサービスを移動すると、一時的な修正が提供される場合があります。ただし、負荷が増加し続けると、増加したリソースが不十分なポイントに再び到達する可能性があります。さらに、可用性を向上させるために、複数のインスタンスに負荷を分散することが重要です。
C. 他のゾーンに追加のサービス インスタンスを設定し、それらのゾーン間でトラフィックを負荷分散すると、負荷を分散し、1 つのインスタンスが過負荷になるのを防ぐことができます。これにより、全体的なサービス容量が増加し、タイムアウトの可能性が低くなり、可用性が向上します。
D. 他のゾーンに追加のサービス インスタンスを設定し、それらをフェールオーバーとして使用することは、高可用性を確保するための有効なアプローチですが、要求量とメモリの制限が多いことによる定期的なタイムアウトの問題に積極的に対処することはできません。フェイルオーバー・インスタンスは、プライマリ・インスタンスに障害が発生した場合のバックアップとして機能し、トラフィックの増加を処理するために負荷を分散します。
したがって、他のゾーンに追加のサービスインスタンスを設定し、すべてのインスタンス間でトラフィックを負荷分散する(オプションC)ことが、このシナリオでサービスの可用性を向上させるための最も効果的なアプローチです。
</div></details>

### Q.  問題12: 回答
サードパーティのアプリケーションが正しく動作するには、サービス アカウント キーが必要です。クラウド プロジェクトからキーをエクスポートしようとすると、「組織ポリシーの制約 iam.disableServiceAccounKeyCreation が適用されています」というエラーが表示されます。サードパーティ製アプリケーションを動作させながら、Google が推奨するセキュリティ対策に従う必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明このシナリオの正しいオプションは次のとおりです。
B. 組織レベルで iam.disableServiceAccountKeyCreation ポリシーを削除し、キーを作成します。
説明：
組織レベルでの iam.disableServiceAccountKeyCreation ポリシーの削除:
このアクションには、サービス アカウント キーの作成を妨げている制限を解除し、キーの生成を許可することが含まれます。
組織レベルでの変更は、組織のセキュリティ ポリシーとコンプライアンス標準に合わせる必要があることに注意することが重要です。適切な承認を確保し、この変更が全体的なセキュリティに与える影響を考慮してください。
サードパーティアプリケーションのサービスアカウントキーの作成:
ポリシーの制約を削除した後、サードパーティ アプリケーションに必要なサービス アカウント キーを生成します。
キーを安全にダウンロードし、必要に応じてサードパーティアプリケーションに提供します。
他のオプションが適さない理由:
A. デフォルトのサービスアカウントキーを有効にしてダウンロードします。
このオプションでは、既定のサービス アカウント キーが既に使用可能であることを前提としていますが、特にサービス アカウント キーの作成が適用されたポリシーによって無効になっている場合は、そうではない可能性があります。既定のキーを有効にすることは、ポリシーの制限がまだ適用されている場合、実行可能な解決策ではない可能性があります。
C. プロジェクトのフォルダーでサービス アカウント キー作成ポリシーを無効にし、既定のキーをダウンロードします。
制限が組織レベルで課されている場合、プロジェクト レベルでポリシーを無効にするだけでは不十分な場合があります。また、サービス アカウント キーの作成が禁止されている場合、既定のキーに依存しても問題が解決されない可能性があります。
D. iam.disableServiceAccountKeyCreation ポリシーを off に設定するルールをプロジェクトに追加し、キーを作成します。
プロジェクト内のポリシー設定を変更しようとしても、現在適用されている組織レベルのポリシー制約は上書きされない場合があります。組織のルールを変更しても、組織レベルのポリシーがまだ適用されている場合、根本的な問題を効果的に解決できない可能性があります。
組織レベルのポリシーに対する変更が、セキュリティのベスト プラクティスに準拠し、組織のセキュリティとコンプライアンスの標準に沿っていることを確認することが重要です。組織レベルのポリシーを変更する前に、組織のセキュリティ チームまたはコンプライアンス チームに相談することをお勧めします。
</div></details>

### Q.  問題13: 回答
あなたのチームは、外部向けアプリケーションでインシデントが発生した後、事後分析を作成しています。チームは、事後分析ポリシーを改善して、インシデントに事後分析が必要かどうかを示すトリガーを含めたいと考えています。サイト信頼性エンジニアリング (SRE) のプラクティスに基づいて、事後分析ポリシーでどのようなトリガーを定義する必要がありますか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明サイト信頼性エンジニアリング (SRE) のプラクティスでは、インシデントに事後分析が必要かどうかを判断するトリガーは、通常、重大な影響、学習機会、または潜在的なリスクを示す特定の基準を中心に展開します。これらの原則と密接に連携する 2 つのトリガーは次のとおりです。
B. インシデントによりデータが失われた場合。
データ損失は、通常、ユーザーへの深刻な影響、潜在的な法的またはコンプライアンス上の問題、および再発を防ぐために根本原因を理解する必要があることを示す重大なインシデントです。
E.CD パイプラインは問題を検出し、問題のあるリリースをロールバックします。
継続的デリバリー (CD) パイプラインの自動ロールバックは、デプロイされた変更が問題を引き起こしたことを示唆しており、何が問題だったのか、今後同様の問題を防ぐ方法を理解するために事後分析の必要性を促します。
他のオプション (A、C、D) では、利害関係者の関心や技術的な障害が強調される場合がありますが、SRE プラクティスのみに基づいて事後分析が必要になるほど重大なインシデントが常に示されているとは限りません。ただし、事後分析が必要かどうかを判断する際には、これらのトリガーをより広いコンテキストで検討し、各インシデントの重大度とシステムとユーザーへの影響を評価することが重要です。
</div></details>

### Q.  問題14: 不正解
チームは、Google Cloud Platform(GCP)の内部と外部の両方にデプロイする新しいアプリケーションを設計しています。システム リソース使用率などの詳細なメトリックを収集する必要があります。一元化された GCP サービスを使用しながら、この収集システムの設定に必要な作業量を最小限に抑えたい。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Cloud Platform(GCP)の内部と外部の両方にデプロイするように設計された新しいアプリケーションのシステム リソース使用率などの詳細な指標を収集しながら、一元化された GCP サービスを利用し、セットアップ作業を最小限に抑えるには、次のようなアプローチが最も適切です。
D. 両方の場所に Application Performance Monitoring (APM) ツールをインストールし、分析のために中央のデータ ストレージの場所へのエクスポートを構成します。
説明：
· オプション A(Stackdriver Profiler パッケージをインポートし、関数のタイミング データを Stackdriver に中継してさらに分析するように構成する): Stackdriver Profiler は、システム リソース使用率の指標を収集するのではなく、コードレベルのパフォーマンス分析と最適化に重点を置いています。
· オプション B(Stackdriver Debugger パッケージをインポートし、タイミング情報を含むデバッグ メッセージを生成するようにアプリケーションを構成する): Stackdriver Debugger はアプリケーションのデバッグに使用され、主にシステム リソース使用率の指標を収集するために設計されていません。
· オプション C(タイミング ライブラリを使用してコードをインストルメント化し、Stackdriver によってスクレイピングされるヘルス チェック エンドポイントを介して指標を公開する): ヘルスチェックでは基本的な分析情報は得られる場合がありますが、複数のデプロイ場所にわたる詳細なシステム リソース使用率の指標を包括的にカバーしていない可能性があります。
したがって、オプション D (両方の場所に Application Performance Monitoring (APM) ツールをインストールし、分析のために中央のデータ ストレージの場所へのエクスポートを構成する) の方が適しています。APMツールを活用することで、さまざまな展開場所で詳細なシステムリソース使用率メトリックを収集し、このデータを中央ストレージの場所にエクスポートして包括的な分析を行うことができます。このアプローチにより、追加のセットアップ作業を最小限に抑えながら、一元的な監視が容易になります。
</div></details>

### Q.  問題15: 回答
事後分析を組織に導入する必要があります。事後分析プロセスが好評であることを確認する必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明事後分析を組織に効果的に導入し、プロセスが確実に受け入れられるようにするには、次のオプションを検討する必要があります。
C. 上級管理職に事後分析を認め、参加するよう促す。
事後分析に上級管理職を関与させることは、プロセスの重要性と深刻さを実証しています。これは、組織内の他のメンバーが事後分析プロセスを真剣に受け止める前例となり、トップレベルの管理職から下層部への透明性を促進します。
D. 効果的な事後分析を書くことは、報われ、称賛される習慣であることを確認する。
効果的な事後分析を書くために費やされた努力を認め、称賛することで、失敗から学び、継続的な改善を行う文化が育まれます。このプラクティスに報いることで、チームは時間と労力を費やして徹底的な事後分析を行い、自分の仕事が認められ、評価されるようになります。
その他のオプションの説明:
A. 新入社員に、練習を通じてチームの事後分析を行うよう促します。新入社員を含むあらゆるレベルからの参加を促すことは有益ですが、新入社員に事後分析を任せるだけでは、包括的なアプローチが保証されない可能性があります。また、徹底した分析には、経験豊富なメンバーの関与も重要です。
B. すべての事後分析の実施を担当する指定チームを作成します。専任のチームを持つことで一貫性が生まれる一方で、問題に直面しているチームと事後分析を行うチームの間に断絶が生じる可能性があります。影響を受けるチームが関与し、オーナーシップを持つことで、より詳細で関連性のある事後分析が行われることがよくあります。
E. 過去の事後分析を批評する場を組織に提供する。建設的な批判やフィードバックは改善のために重要ですが、過去の事後分析を批判することだけに焦点を当てると、成功を認め、失敗から学ぶことを促進することとのバランスが取れていないと、ネガティブな環境が生まれる可能性があります。
直観-
https://sre.google/sre-book/postmortem-culture/
https://cloud.google.com/blog/products/devops-sre/how-lowes-improved-incident-response-processes-with-sre
</div></details>

### Q.  問題16: 回答
Google Cloud Deploy でデプロイ パイプラインを確認しています。パイプラインの労力を軽減し、エンドツーエンドのデプロイを完了するのにかかる時間を最小限に抑える必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Cloud Deploy パイプラインでの労力を軽減し、デプロイ時間を最小限に抑えるには、次のアクションを検討する必要があります。
B. 自動化のステップを小さなタスクに分割します。自動化のステップをより小さく、より管理しやすいタスクに分割することで、並列化の向上、デバッグの容易化、デプロイ プロセスのより詳細な制御が可能になります。小さなタスクを同時に実行できるため、デプロイにかかる全体的な時間が短縮されます。
E. 開発環境からテスト環境へのプロモーション承認を自動化します。ある環境から別の環境(開発からテストなど)にデプロイを昇格するための承認プロセスを自動化することで、手動による介入を大幅に減らし、パイプラインを合理化し、デプロイプロセスを迅速化できます。この自動化により、手動承認の待ち時間がなくなり、デプロイのライフサイクルが短縮されます。
選択されていないオプションの説明:
A. 手動による介入が必要な場合に、次の手順を完了するように必要なチームに通知するトリガーを作成します。通知は、手動による介入についてチームに警告するのに役立ちますが、労力やデプロイ時間を直接削減するものではありません。特定の手順を手動による介入のみに依存すると、遅延が発生する可能性があります。
C. スクリプトを使用して、Google Cloud Deploy でのデプロイ パイプラインの作成を自動化します。デプロイメント・パイプライン自体の作成を自動化すると、初期セットアップと構成に役立つ場合があります。ただし、他の最適化や改善と組み合わせない限り、継続的なデプロイのデプロイ時間や労力を大幅に削減できない可能性があります。
D.手動の手順を完了するために、エンジニアを追加します。手作業の手順を処理するエンジニアを増やすと、生産性が向上するように思えるかもしれませんが、根本的に労力が減ったり、デプロイ プロセスの効率が向上したりするわけではありません。追加の人的リソースだけに頼るよりも、プロセスを自動化および最適化する方が有益です。
インサイト - https://sre.google/workbook/eliminating-toil/
</div></details>

### Q.  問題17: 回答
組織は、2,000 ドルの投資でアプリケーションの可用性目標を 99.9% から 99.99% に引き上げたいと考えています。アプリケーションの現在の収益は $1,000,000 です。可用性の向上が、1 年間の使用に対する投資に見合うかどうかを判断する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明可用性の向上が投資に見合うかどうかを判断するには、可用性の向上の価値を計算し、投資のコストと比較する必要があります。
与えられた:
現在の可用性: 99.9% (0.999)
望ましい可用性: 99.99% (0.9999)
投資:2,000ドル
経常収益: $1,000,000
可用性の向上の値を計算する式は、可用性の向上の値=年間収益×(望ましい可用性 - 現在の可用性) です。
可用性の変化 = 希望の可用性 - 現在の可用性 可用性の変化 = 0.9999 - 0.999 = 0.0009
それでは、可用性の向上の価値を計算してみましょう。 可用性の向上の価値=$1,000,000×0.0009=$900可用性の向上の価値=$1,000,000×0.0009=$900
可用性の向上による価値は、1 年間の使用で 900 ドルです。
可用性の向上 (900 ドル) と投資コスト (2,000 ドル) を比較すると、可用性の向上は投資に見合わないことがわかります。可用性の向上 (900 ドル) から得られる価値は、投資コスト (2,000 ドル) よりも少なくなります。
したがって、正解は、A. 可用性の向上の値を $900 と計算し、可用性の向上は投資に見合わないと判断します。
</div></details>

### Q.  問題18: 回答
会社では、GitOps 手法に従ってデプロイされたアプリケーションを Google Kubernetes Engine(GKE)で実行しています。アプリケーション開発者は、アプリケーションをサポートするクラウド・リソースを頻繁に作成します。デベロッパーがインフラストラクチャをコードとして管理できるようにしながら、Google が推奨するプラクティスに従うようにする必要があります。構成のずれを回避するために、コードとしてのインフラストラクチャが定期的に調整されるようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Infrastructure as Code が定期的に調整され、構成のずれを防ぎ、Google Kubernetes Engine(GKE)での GitOps セットアップで Google が推奨するプラクティスに従うようにするには、適切なアプローチは次のとおりです。
オプション A: Google Kubernetes Engine(GKE)に Config Connector をインストールして構成します。
説明：
構成コネクタ:Config Connector は、Kubernetes スタイルの宣言を使用して Google Cloud リソースを管理できる Kubernetes アドオンです。これにより、Kubernetesマニフェスト(YAML)を使用してクラウドリソースを定義および管理し、Kubernetes内のコードとしてのインフラストラクチャのプラクティスを確保できます。
調整と構成のばらつきの防止:Config Connector は、Kubernetes マニフェストで定義された望ましい状態と Google Cloud リソースの実際の状態を継続的に調整します。この調整プロセスは、構成のずれを検出して修正し、実際の状態がコードに記述されている目的の状態と一致することを確認するのに役立ちます。
GitOps 手法との統合:Config Connector は、デベロッパーが Git リポジトリを使用してバージョン管理できる YAML ファイルで Google Cloud リソースを宣言的に定義、管理できるようにすることで、GitOps 手法とうまく統合されています。これらのマニフェストに変更が加えられると、Config Connector がトリガーされ、変更が調整され、目的の状態が維持されます。
オプション B、C、D では、さまざまな Kubernetes リソース(Cloud Build、Pod、またはジョブ)内で Terraform コマンドを使用します。Terraform は強力なインフラストラクチャ プロビジョニング ツールですが、これらのオプションは、GKE などの Kubernetes 環境内で Google Cloud リソースを管理するための Google 推奨プラクティスと直接一致しない場合があります。
オプションB:Terraform ビルダーを使用して Cloud Build を構成して terraform plan コマンドと terraform apply コマンドを実行することは、Kubernetes 内で Google Cloud リソースを管理するための最も直接的なアプローチではない可能性があります。
オプション C と D:Terraform Docker イメージで Pod またはジョブを使用して Terraform コマンドを実行すると、うまくいく可能性がありますが、Config Connector のように Kubernetes スタイルの宣言を直接活用して GKE 内の Google Cloud リソースを管理することはできません。これらのオプションには、Config Connector が提供する継続的な調整の側面が欠けている可能性があります。
追記 - Config Connector は、Google Cloud リソースを Kubernetes オブジェクトとして管理できる Kubernetes 拡張機能です。GKE に Config Connector をインストールして構成することで、クラウド リソースを Kubernetes CustomResourceDefinitions(CRD)として宣言し、Kubernetes コントローラで管理することができます。
インサイト - https://cloud.google.com/config-connector/docs/overview
</div></details>

### Q.  問題19: 回答
Google Kubernetes Engine(GKE)の JVM ベースのアプリケーションとマイクロサービス アーキテクチャで構築された e コマース ウェブサイトを運営しています。アプリケーションの負荷は日中に増加し、夜間に減少します。運用チームは、夕方のピーク負荷を処理するのに十分なPodを実行するようにアプリケーションを構成しています。負荷に対して十分なPodとノードのみを実行することで、スケーリングを自動化したい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明さまざまなアプリケーションの負荷に基づいて Google Kubernetes Engine(GKE)のスケーリングを自動化するには、適切なアプローチとして、Horizontal Pod Autoscaler(HPA)と Cluster Autoscaler(CA)の両方を利用します。
D. Horizontal Pod Autoscalerを構成し、クラスタ・オートスケーラを有効にします。
説明：
ポッドの水平オートスケーラー(HPA):
HPA は、観測された CPU 使用率またはその他のカスタム メトリックに基づいて、デプロイまたはレプリカ セット内のポッドの数を自動的にスケーリングします。
これは、現在の負荷を効率的に処理するために、適切な数のPodが実行されていることを確認するのに役立ちます。
クラスター オートスケーラー (CA):
CA は、リソースの需要に基づいて、Kubernetes クラスターのノード プールのサイズを自動的に調整します。
これは、必要な数のPodに効果的に対応するために、ノードを動的に追加または削除するのに役立ちます。
HPA と CA を組み合わせると、より包括的で効率的な自動スケーリング ソリューションが実現します。
HPA は、ワークロードに応じてクラスター内の Pod の数をスケーリングします。
CA は、自動スケーリングされた Pod によって作成されるリソースの需要を満たすために、クラスター内のノード数を調整することで、基盤となるインフラストラクチャを管理します。
オプション A と C は、ノード プールのサイズを静的に保つと、さまざまな負荷要件に合わせて基になるインフラストラクチャを動的に調整できないため、理想的な選択肢ではありません。
オプションBは、Vertical Pod Autoscalerを構成し、クラスタ・オートスケーラーを有効にすることを提案するもので、アプリケーションの負荷に基づくスケーリングではなく、Podリソースの最適化に重点が置かれています。Vertical Pod Autoscalerは、コンテナによるリソースリクエストを実際の使用量に合わせて調整しますが、アプリケーションの負荷に基づくPodのスケーリングには直接対応しません。
したがって、GKE のさまざまなアプリケーション負荷に基づいてスケーリングを自動化する最善の方法は、Horizontal Pod Autoscaler を構成し、Cluster Autoscaler(オプション D)を有効にすることです。
</div></details>

### Q.  問題20: 回答
停止の事後分析でアクションアイテムを作成して割り当てています。停止は終わりましたが、根本原因に対処する必要があります。チームがアクションアイテムを迅速かつ効率的に処理できるようにする必要があります。所有者とコラボレーターをアクションアイテムにどのように割り当てる必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明事後分析シナリオで停止後のアクション アイテムを効果的に処理するには、所有者とコラボレーターの割り当てを、説明責任、コラボレーション、効率の原則に沿って行う必要があります。提供されているオプションの中で、最も適切なアプローチは次のとおりです。
A. アクションアイテムごとに 1 人の所有者と、必要なコラボレーターを割り当てます。
各アクションアイテムに 1 人の所有者を割り当てることで、特定の問題やタスクに対処するための明確な説明責任と責任が確保されます。各アクションアイテムに指定された所有者がいると、説明責任が合理化され、誰かが所有権を持ち、解決プロセスを推進できるようになります。
さらに、必要なコラボレーターをアクションアイテムに割り当てることで、根本原因に効率的に対処するためのコラボレーションと専門知識の共有が可能になります。チームメンバー間のコラボレーションは、複雑な問題に取り組むための多様な視点とスキルをもたらし、問題をより効果的に解決するのに役立ちます。
オプション B では、各項目に複数の所有者を割り当てると、特定のアクション項目の解決に主たって責任を負うのが誰なのかが不明確になる可能性があるため、混乱や責任の分散が生じる可能性があります。
オプション C では、事後分析を責めないようにするために、協力者を割り当てるが、個々の所有者は割り当てないと、明確な所有権と説明責任が欠如し、解決プロセスが遅れる可能性があります。
オプション D では、チーム リーダーをすべてのアクション アイテムの所有者として割り当てると、チーム リーダーが過負荷になり、特定の問題に対処するためにチーム メンバーの多様なスキルと知識を効果的に活用できない可能性があります。
したがって、各アクションアイテムに 1 人の所有者と必要なコラボレーターを割り当てる (オプション A) ことは、停止後のアクションアイテムの説明責任、コラボレーション、および効率的な処理を確保するための最適なアプローチです。
インサイト - https://sre.google/sre-book/example-postmortem/
https://devops.com/when-it-disaster-strikes-part-3-conducting-a-blameless-post-mortem/
</div></details>

### Q.  問題21: 回答
チームは、データのバッチに対して計算負荷の高い処理を実行するサービスを構築しています。データは、マシン上の速度と CPU の数に基づいて高速に処理されます。これらのデータのバッチはサイズが異なり、複数のサードパーティソースからいつでも到着する可能性があります。第三者がデータを安全にアップロードできるようにする必要があります。コストを最小限に抑えながら、データを可能な限り迅速に処理する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明シナリオの要件に最も適した適切なオプションは次のとおりです。
C. サードパーティがデータのバッチをアップロードできるように Cloud Storage バケットを提供し、バケットへの適切な Identity and Access Management(IAM)アクセスを提供します。google.storage.object.finalize Cloud Storage トリガーを使用して Cloud Functions の関数を作成します。関数が Compute Engine の自動スケーリング マネージド インスタンス グループをスケールアップできるようにコードを記述します。データ処理ソフトウェアにプリロードされたイメージを使用して、処理の完了時にインスタンスを終了します。
説明：
オプション A(インスタンス グループを自動スケーリングするための Cloud Function を搭載した Compute Engine 上の SFTP サーバー):
問題: Compute Engine で SFTP サーバーを提供すると、安全なデータ転送が可能になりますが、サーバーの管理と保守に追加のオーバーヘッドが発生する可能性があります。Cloud Functions の関数トリガーに基づく自動スケーリングは機能する可能性がありますが、他のオプションほど柔軟で費用対効果が高くない可能性があります。
オプション B(データ処理用の GKE クラスタを使用した Cloud Storage):
問題: GKE クラスタを利用すると、複雑さが増し、運用コストが高くなる可能性があります。データ処理用と監視用の 2 つのサービスを維持することは、最も簡単で費用対効果の高いアプローチではない可能性があります。
オプション D(データ処理用の Cloud Function を備えた Cloud Storage):
問題: 新しいデータが到着したときに Cloud Functions の関数をトリガーするのは効率的ですが、可能な限り大きな CPU を使用するように設定すると、処理速度に比例して向上することなく、コストが大幅に増加する可能性があります。これは、自動スケーリング マネージド インスタンス グループを使用した需要に基づくスケーリングほどコスト効率が高くない場合があります。
オプションCが適している理由:
安全なアップロード: IAM 制御を備えた Cloud Storage を利用し、サードパーティからの安全なデータアップロードを保証します。
費用効率: Cloud Functions の関数トリガーを利用して Compute Engine の自動スケーリング マネージド インスタンス グループをスケールアップし、需要に基づいて費用対効果の高いリソース割り当てを可能にします。
処理速度: データ バッチのサイズに基づく自動スケーリングを活用することで、リソースを動的にスケーリングすることで処理を高速化できます。
オプションCは、コスト効率、安全なアップロード、処理速度の最適化のバランスが取れているため、提供されているオプションの中で最も適した選択肢です。
</div></details>

### Q.  問題22: 回答
本番環境とテスト環境を備えた Compute Engine でリアルタイム ゲーム アプリケーションを実行しています。各環境には、独自の仮想プライベート
クラウド(VPC)ネットワークがあります。アプリケーションのフロントエンドサーバーとバックエンドサーバーは、環境の VPC 内の異なるサブネットに配置されています。運用フロントエンド サーバーで断続的に通信している悪意のあるプロセスがある可能性があります。ネットワーク トラフィックが分析のためにキャプチャされるようにする必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明運用フロントエンド サーバーで断続的に通信している悪意のあるプロセスが疑われ、分析のためにネットワーク トラフィックをキャプチャするシナリオでは、最適なアクションは次のとおりです。
B. VPC フローログは、サンプルボリュームスケールが 1.0 の本番環境 VPC ネットワークフロントエンドおよびバックエンドサブネットでのみ有効にします。
説明：
A. サンプルボリュームスケールが 0.5 の本番環境 VPC ネットワークのフロントエンドおよびバックエンドサブネットで VPC フローログを有効にすると、ネットワークトラフィックのほんの一部(50%)のみがキャプチャされます。これにより、ログの保存に関連するコストを削減できますが、徹底的な分析に必要なすべてのデータがキャプチャされず、悪意のある疑いのあるプロセスに関連する重要な情報が欠落する可能性があります。
C. サンプルボリュームスケールが 0.5 のテストと本番の両方の VPC ネットワークフロントエンドとバックエンドサブネットで VPC フローログを有効にし、本番稼働前にテストで変更を適用すると、一部のデータがキャプチャされる可能性がありますが、テスト環境からのトラフィックが不必要にログに記録され、本番環境で疑わしい問題のみに焦点を当てることなく生成されるログの量が増加します。
D. サンプルボリュームスケールが 1.0 のテストと本番環境の両方の VPC ネットワークのフロントエンドとバックエンドのサブネットで VPC フローログを有効にすることは、オプション B と似ていますが、テスト環境からのトラフィックのログ記録も含まれます。ボリューム スケールが 1.0 のログをキャプチャすると、大量のログが生成される可能性があり、テスト環境では必要ない場合があります。
したがって、本番フロントエンドサーバーで悪意のある疑いのあるプロセスを分析するためにネットワークトラフィックをキャプチャする最も効果的なアクションは、サンプルボリュームスケールが 1.0 (オプション B) の本番 VPC ネットワークフロントエンドとバックエンドサブネットで VPC フローログを有効にすることです。これにより、テスト環境からの不要なオーバーヘッドなしに、徹底的な分析のために最大量のネットワークトラフィックを確実にキャプチャできます。
インサイト - https://cloud.google.com/vpc/docs/flow-logs#log-sampling
</div></details>

### Q.  問題23: 回答
GCP で実行されているアプリケーションをサポートしており、Stackdriver Monitoring で最も重要なアラートについてチームへの SMS 通知を設定したい。これを構成するアラートポリシーはすでに特定されています。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Stackdriver Monitoring で最も重要なアラートについてチームへの SMS 通知を設定する場合、シンプルさと有効性を考慮すると、次のようなアプローチが最も適切です。
C. チーム メンバーが Stackdriver プロフィールで SMS や電話番号を設定していることを確認します。各アラート ポリシーの [SMS 通知] オプションを選択し、一覧から適切な SMS/電話番号を選択します。
説明：
· オプション A(Stackdriver Monitoring と SMS ゲートウェイ間のサードパーティ統合をダウンロードして構成する): このアプローチでは、外部ツールを追加することで複雑さが増す可能性があり、チーム メンバーは別のプラットフォームに電話番号を追加する必要があります。
· オプション B (各アラート ポリシーの [Webhook 通知] オプションを選択し、サードパーティの統合ツールを使用するように構成する): Webhook 通知を使用すると、SMS が直接サポートされない可能性があり、追加の外部ツールが導入されます。オプションAと同様に、チームメンバーは別のプラットフォームに電話番号を追加する必要があります。
· オプション D (アラート ポリシーごとに Slack 通知を構成する): このオプションでは、Slack を仲介として使用するため、複雑さが増します。さらに、Slack と SMS の連携に頼ることは、緊急のアラートに対して直接的で信頼性が高くない可能性があります。
そのため、オプション C(チームメンバーが Stackdriver プロファイルで SMS や電話番号を設定し、アラート ポリシーごとに SMS 通知オプションを選択する)が最適です。このアプローチでは、Stackdriver に組み込まれている SMS 通知機能を活用し、追加の統合や外部ツールを必要とせずに、チーム メンバーの Stackdriver プロファイルから電話番号を直接選択できます。
インサイト - https://cloud.google.com/monitoring/support/notification-options#creating_channels
https://cloud.google.com/monitoring/support/notification-options
</div></details>

### Q.  問題24: 回答
Google Kubernetes Engine(GKE)の JVM ベースのアプリケーションとマイクロサービス アーキテクチャで構築された e コマース ウェブサイトを運営しています。アプリケーションの負荷は日中に増加し、夜間に減少します。運用チームは、夕方のピーク負荷を処理するのに十分なPodを実行するようにアプリケーションを構成しています。負荷に対して十分なPodとノードのみを実行することで、スケーリングを自動化したい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明さまざまなアプリケーションの負荷に基づいて Google Kubernetes Engine(GKE)のスケーリングを自動化するには、適切なアプローチとして、Horizontal Pod Autoscaler(HPA)と Cluster Autoscaler(CA)の両方を利用します。
D. Horizontal Pod Autoscalerを構成し、クラスタ・オートスケーラを有効にします。
説明：
ポッドの水平オートスケーラー(HPA):
HPA は、観測された CPU 使用率またはその他のカスタム メトリックに基づいて、デプロイまたはレプリカ セット内のポッドの数を自動的にスケーリングします。
これは、現在の負荷を効率的に処理するために、適切な数のPodが実行されていることを確認するのに役立ちます。
クラスター オートスケーラー (CA):
CA は、リソースの需要に基づいて、Kubernetes クラスターのノード プールのサイズを自動的に調整します。
これは、必要な数のPodに効果的に対応するために、ノードを動的に追加または削除するのに役立ちます。
HPA と CA を組み合わせると、より包括的で効率的な自動スケーリング ソリューションが実現します。
HPA は、ワークロードに応じてクラスター内の Pod の数をスケーリングします。
CA は、自動スケーリングされた Pod によって作成されるリソースの需要を満たすために、クラスター内のノード数を調整することで、基盤となるインフラストラクチャを管理します。
オプション A と C は、ノード プールのサイズを静的に保つと、さまざまな負荷要件に合わせて基になるインフラストラクチャを動的に調整できないため、理想的な選択肢ではありません。
オプションBは、Vertical Pod Autoscalerを構成し、クラスタ・オートスケーラーを有効にすることを提案するもので、アプリケーションの負荷に基づくスケーリングではなく、Podリソースの最適化に重点が置かれています。Vertical Pod Autoscalerは、コンテナによるリソースリクエストを実際の使用量に合わせて調整しますが、アプリケーションの負荷に基づくPodのスケーリングには直接対応しません。
したがって、GKE のさまざまなアプリケーション負荷に基づいてスケーリングを自動化する最善の方法は、Horizontal Pod Autoscaler を構成し、Cluster Autoscaler(オプション D)を有効にすることです。
</div></details>

### Q.  問題25: 回答
Stackdriver を使用して、Google Cloud Platform(GCP)でホストされているアプリケーションを監視しています。最近新しいアプリケーションをデプロイしたが、そのログが Stackdriver ダッシュボードに表示されない。
問題のトラブルシューティングを行う必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明新しくデプロイしたアプリケーションのログが Google Cloud Platform(GCP)内の Stackdriver ダッシュボードに表示されない問題のトラブルシューティングを行うには、次の手順を実行するのが最も適切です。
D. アプリケーションが必要なクライアント ライブラリを使用していること、およびサービス アカウント キーに適切なアクセス許可があることを確認します。
説明：
A. ホスティング仮想マシンに Stackdriver エージェントがインストールされていることを確認する: Stackdriver のログ記録には、必ずしも Google Cloud サービスのエージェントは必要ありません。通常、エージェントなしでログを自動的にキャプチャします。ただし、Google Cloud 以外のサービスやカスタム アプリケーションの場合は、Stackdriver にログを送信するために適切なクライアント ライブラリの使用が必要になることがあります。
B. アカウントに Stackdriver ダッシュボードを使用するための適切な権限があることを確認する: 権限の問題は、Stackdriver ダッシュボードでログを表示したり、設定を構成したりする機能に影響を与える可能性があります。ただし、これは通常、Stackdriver へのログの取り込みには直接影響しません。
C. ファイアウォールでポート 25 が開かれ、Stackdriver へのメッセージの通過が許可されていることを確認します。 Stackdriver Logging では、ログの取り込みにポート 25 は使用されません。ポート 25 を開いても、Stackdriver ダッシュボードにログが表示されない問題は解決されません。
D. アプリケーションが必要なクライアント ライブラリを使用していること、およびサービス アカウント キーに適切なアクセス許可があることを確認する: この手順は重要です。Stackdriver にログを送信するには、アプリケーションで適切な Stackdriver クライアント ライブラリまたはエージェントを使用する必要があります。また、アプリケーションに関連付けられたサービス アカウントには、Stackdriver にログを書き込むために必要な権限が必要です。
したがって、提供されているオプションの中で、アプリケーションが必要なクライアント ライブラリを使用していることを確認し、サービス アカウント キーに適切な権限があることを確認する(オプション D)ことが、GCP の Stackdriver ダッシュボードにログが表示されない問題のトラブルシューティングに最も適した手順です。
インサイト - https://cloud.google.com/logging/docs/reference/libraries
https://cloud.google.com/logging/docs/agent/ops-agent/configuration#default
https://cloud.google.com/monitoring/agent/monitoring/installation#:~:text=The%20Ops%20Agent%20collects%20both%20metrics%20and%20logs%20by%20default.%20You%20can%20change%20this%20default%20behavior%20by%20configuring%20the%20Ops%20Agent。
</div></details>

### Q.  問題26: 回答
チームは最近、NGINX ベースのアプリケーションを Google Kubernetes Engine(GKE)にデプロイし、HTTP Google Cloud
Load Balancer(GCLB)イングレスを介して一般公開しました。適切なサービスレベル指標(SLI)を使用して、アプリケーションのフロントエンドのデプロイをスケーリングする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Kubernetes Engine(GKE)内の適切なサービスレベル インジケータ(SLI)を使用して NGINX ベースのアプリケーションのフロントエンドのデプロイをスケーリングするには、次のようなアプローチが最適です。
C. Stackdriver カスタム メトリック アダプターをインストールし、GCLB によって提供されるリクエストの数を使用するようにポッドの水平オートスケーラーを構成します
説明：
· オプション A (Liveness probe と Readiness プローブからの平均応答時間を使用するように Horizontal Pod Autoscaler を構成する): このオプションは、アプリケーションのフロントエンドの需要または負荷に基づくスケーリングと直接相関しない可能性のある応答時間のメトリックに焦点を当てています。
· オプション B(GKE で垂直ポッド オートスケーラーを構成し、ポッドの拡張に合わせてクラスタ オートスケーラーがクラスタをスケーリングできるようにする): 垂直ポッド オートスケーラーはポッドに割り当てられたリソースを調整し、クラスタ オートスケーラーはクラスタ内のノード数を調整します。これは、フロントエンドの需要に基づくスケーリングに直接対処しない可能性があります。
· オプション D (NGINX 統計エンドポイントを公開し、NGINX デプロイによって公開される要求メトリックを使用するように水平ポッド オートスケーラーを構成する): NGINX 統計にアクセスすると分析情報が得られる可能性がありますが、NGINX メトリックのみに依存すると、アプリケーションのフロントエンドの需要や負荷を完全に表すことができない可能性があります。
したがって、オプション C(Stackdriver カスタム メトリック アダプタをインストールし、GCLB によって提供されるリクエスト数を使用するように水平ポッド オートスケーラーを構成する)が最も適切なソリューションです。Stackdriver のカスタム指標アダプターを利用し、Google Cloud Load Balancer(GCLB)が提供するリクエスト数を使用するようにポッドの水平オートスケーラーを構成することで、アプリケーションの需要と直接相関する実際の受信リクエストに基づいてフロントエンドをスケーリングできます。
インサイト - https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler#overview
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics
</div></details>

### Q.  問題27: 回答
大規模な Standard Google Kubernetes Engine(GKE)クラスタにアプリケーションをデプロイしました。アプリケーションはステートレスであり、複数のポッドが同時に実行されます。アプリケーションが一貫性のないトラフィックを受信します。トラフィックの変化に関係なく、ユーザーエクスペリエンスの一貫性が保たれ、クラスターのリソース使用量が最適化されていることを確認する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明トラフィックに一貫性のない大規模な Standard Google Kubernetes Engine(GKE)クラスタで複数のポッドを実行するステートレス アプリケーションがあり、リソースの使用を最適化しながら一貫したユーザー エクスペリエンスを確保したいシナリオでは、適切なソリューションは次のとおりです。
B. Horizontal Pod Autoscaler (HPA)を構成します。
説明：
Horizontal Pod Autoscaler (HPA): HPAは、観測されたCPU使用率またはその他の選択したメトリックに基づいて、デプロイメントまたはレプリカ・セット内のレプリカ・ポッドの数を自動的に調整します。ポッドの数を水平方向にスケーリングし、現在の需要に合わせてインスタンスを追加または削除します。
ステートレス アプリケーション: HPA は、トラフィックまたはリソースの需要に基づいてレプリカを作成または削除することでポッドのスケーリングを管理するため、ステートレス アプリケーションに最適です。これにより、アプリケーションがさまざまな負荷を効果的に処理できるようにすることで、一貫したユーザーエクスペリエンスを維持できます。
リソース使用量の最適化: HPA は、ポッドの数をスケールアップまたはスケールダウンすることでリソース使用量を最適化し、現在のトラフィック レベルを処理するために必要なリソースのみが使用されるようにします。
その他のオプション:
A. スケジュールに従って展開をスケーリングするように cron ジョブを構成する: このソリューションは、トラフィックの変化に動的に適応せず、予測不可能なトラフィック パターンの管理の効率が低下します。
C. Vertical Pod Autoscalerの構成: Vertical Pod Autoscalerは、ポッドのCPUおよびメモリー・リクエストを調整し、ポッド・レベルでリソース使用量を最適化しますが、トラフィックの変動に基づいてポッドの数を直接スケーリングすることはありません。
D. ノード プールでクラスターの自動スケーリングを構成する: このオプションでは、クラスターの基になるインフラストラクチャ (ノード) がスケーリングされ、リソースの可用性に役立つ可能性がありますが、アプリケーションの需要に基づいてポッドのスケーリングを直接管理することはありません。
したがって、トラフィックが変化するステートレス・アプリケーションのリソース使用量を最適化しながら、一貫したユーザー・エクスペリエンスを確保するには、Horizontal Pod Autoscaler (オプションB)の構成が、与えられた選択肢の中で最も適切なソリューションです。
</div></details>

### Q.  問題28: 不正解
Stackdriver Logging にログを書き込むアプリケーションを管理します。一部のチーム メンバーにログをエクスポートする権限を与える必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明チームメンバーが Stackdriver Logging からログをエクスポートできるようにするには、次のような方法が最適です。
C. アクセス許可 logging.sinks.list と logging.sink.get を持つカスタム IAM ロールを作成して付与します。
説明：
· オプション A(チームメンバーに Cloud IAM の logging.configWriter の IAM ロールを付与する): このロールは、当面のタスクに対して過度に寛容であり、ログのエクスポートに必要以上の権限を提供します。これには、ログ構成の変更など、ログのエクスポート以外の権限が含まれます。
· オプション B(これらのメンバーのみにログのエクスポートを許可するように Access Context Manager を設定する): Access Context Manager は Google Cloud リソースへのアクセスを制御するのに役立ちますが、ログのエクスポートを具体的に管理するのではなく、より広範なレベルでのきめ細かなアクセス制御に適している場合があります。
· オプション D(Cloud IAM で組織ポリシーを作成して、これらのメンバーのみがログのエクスポートを作成できるようにする): 組織ポリシーは、組織のリソース全体にわたる制限と制御を定義して適用するのに役立ちますが、チームメンバーのログのエクスポートを具体的に管理するために必要な粒度を提供しない場合があります。
したがって、ログのエクスポート用に特別に調整された必要なアクセス許可 (logging.sinks.list や logging.sink.get など) を持つカスタム IAM ロールを作成して割り当てるのが最も適切なアプローチです。これにより、チームメンバーは、不必要または過剰な権限を付与することなく、ログをエクスポートするために必要なアクセス権を得ることができます。
インサイト - https://cloud.google.com/logging/docs/access-control
</div></details>

### Q.  問題29: 回答
大きなファイルを提供する n2-standard-2 Compute Engine インスタンスを使用するサービスをモニタリングしています。ユーザーからは、ダウンロードが遅いとの報告があります。Cloud Monitoring ダッシュボードには、VM がピーク ネットワーク スループットで実行されていることが表示されます。ネットワークのスループット性能を向上させたい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明大きなファイルを提供する n2-standard-2 Compute Engine インスタンスを使用し、現在、ネットワーク スループットのピークが原因でダウンロードが遅くなっているサービスのネットワーク スループット パフォーマンスを向上させるには、次のような最適なソリューションを使用します。
C. VM のマシンの種類を n2-standard-8 に変更します。
説明：
A. ネットワーク インターフェイス コントローラー (NIC) を追加すると、ネットワーク容量が増える可能性がありますが、この方法では、ピーク ネットワーク スループットで実行されている VM によって発生するダウンロードの遅延に関連するパフォーマンスの問題に直接対処できない場合があります。
B. Cloud NAT ゲートウェイをデプロイして VM のサブネットにアタッチしても、ネットワーク スループットのピークによるダウンロードの遅延の問題には直接対処できません。Cloud NAT を使用すると、パブリック IP アドレスを持たないインスタンスがインターネットにアクセスできますが、VM 内の輻輳が緩和されたり、ネットワーク パフォーマンスが向上したりしない可能性があります。
C. VM のマシンの種類をより高い仕様 (この場合は n2-standard-2 から n2-standard-8) に変更すると、VM の CPU、メモリ、およびネットワーク容量が増加します。マシンの種類をより高い構成に増やすと、VM のネットワーク スループット能力が向上し、ピーク ネットワーク スループットによって引き起こされるダウンロード速度の低下の問題が解決される可能性があります。
D. Ops エージェントをデプロイして追加の監視メトリックをエクスポートすると、システムのパフォーマンスに関するより多くの分析情報が得られる場合がありますが、VM のネットワーク スループットの制限によって引き起こされるダウンロードの遅延の問題に直接対処できない場合があります。
そのため、VM のマシンの種類をより高い構成 (n2-standard-8 など) に変更すると、ネットワーク スループットのパフォーマンスが向上し、VM で大きなファイルのダウンロードをより効率的に処理できる可能性があります。
インサイト - https://cloud.google.com/compute/docs/network-bandwidth
</div></details>

### Q.  問題30: 回答
トラフィックの多いマルチリージョン Web アプリケーションのサービス レベル目標 (SLO) を定義する必要があります。顧客は、アプリケーションが常に利用可能であり、応答時間が速いことを期待しています。お客様は現在、アプリケーションのパフォーマンスと可用性に満足しています。現在の測定に基づくと、28 日間のウィンドウで、レイテンシーの 90 パーセンタイルが 120 ミリ秒、レイテンシーの 95 パーセンタイルが 275 ミリ秒であることがわかります。パブリッシュするチームに推奨されるレイテンシー SLO はどれくらいですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明オプション C (90 パーセンタイル 150 ミリ秒、95 パーセンタイル 300 ミリ秒) を、トラフィックの多いマルチリージョン Web アプリケーションの待機時間 SLO として選択することをお勧めします。その理由は次のとおりです。
考慮 事項：
お客様の幸せ:現在の業績は満足のいくものですが、わずかな改善を目指すことは、積極的な最適化を示し、将来の成長期待と一致しています。
野心と実現可能性のバランス:目標を現在のパフォーマンスに近づけすぎると野心的ではなく、過度に厳しい目標を設定すると達成が難しくなり、チームのモチベーションが低下する可能性があります。
改善の余地:Cは、非現実的になることなく合理的な改善目標を提供し、潜在的な将来の需要や予期せぬ問題の余地を残します。
一般的なプラクティスとの整合性: 90 パーセンタイルと 95 パーセンタイルは広く使用されている SLO 指標であり、外れ値を除外しながら全体的なパフォーマンスに関する優れた洞察を提供します。
その他のヒント:
利害関係者の意見を収集します。SLO 定義プロセスに顧客担当者と主要な社内関係者を関与させ、整合性と理解を確保します。
SLO をエラーバジェットとともに定義します。許容可能なダウンタイムまたはエラー率を定義するエラーバジェットを確立し、SLOフレームワーク全体で時折発生するパフォーマンスの低下に柔軟に対応できるようにします。
監視とアラート:SLO指標を継続的に監視し、アラートを設定することで、ユーザーに大きな影響を与える前に潜在的な問題をプロアクティブに特定して対処します。
定期的な見直しと調整:SLO を定期的に見直し、変化するユーザーのニーズ、パフォーマンスの傾向、業界標準に基づいて調整します。
</div></details>

### Q.  問題31: 回答
最近、会社が Google Cloud に移行した。会社が Google Cloud で新しいプロジェクトと基本的なリソースをプロビジョニングするには、高速で信頼性が高く、反復可能なソリューションを設計する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Cloud で新しいプロジェクトと基本的なリソースを高速で信頼性が高く、反復可能な方法でプロビジョニングするための最適なオプションは次のとおりです。
D. Cloud Foundation Toolkit の Terraform リポジトリを使用します。適切なパラメータを使用してコードを適用し、Google Cloud プロジェクトと関連リソースを作成します。
説明：
A. Google Cloud Console を使用したプロジェクトの作成は、手作業で時間がかかり、エラーが発生しやすく、一貫性と再現性を確保する機能が不足している可能性があります。
B. gcloud CLI を使用してスクリプトを記述すると効果的ですが、Infrastructure-as-Code(IaC)アプローチが不足している可能性があり、他のオプションほどスケーラブルでなく、メンテナンス性も高くない可能性があります。
C. Terraform モジュールを記述してソース管理リポジトリに保存することは、Infrastructure as Code を使用するための正しい方向への一歩です。ただし、Terraform を直接使用すると、必要な構成と自動化の設定により多くの労力が必要になる場合があります。
D. Cloud Foundation Toolkit の Terraform リポジトリを利用すると、Google Cloud プロジェクトと関連リソースに合わせて事前構築および標準化されたインフラストラクチャ コードが提供されます。ベストプラクティス、コンプライアンス、標準化を促進すると同時に、構成を柔軟にカスタマイズできるように設計されています。このアプローチにより、Google Cloud でのリソースの高速かつ信頼性が高く、反復可能なプロビジョニングが保証されます。
したがって、Cloud Foundation Toolkit の Terraform リポジトリを活用するオプション D は、Google Cloud で新しいプロジェクトと基本的なリソースをプロビジョニングするための最も包括的で効率的なソリューションです。
インサイト - https://cloud.google.com/foundation-toolkit
</div></details>

### Q.  問題32: 回答
ロードバランサーを使用せずにHTTPエンドポイントを公開するアプリケーションを管理しています。HTTP応答のレイテンシーは、ユーザーエクスペリエンスにとって重要です。すべてのユーザーが経験している HTTP レイテンシーを把握する必要があります。Stackdriver Monitoring を使用します。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Stackdriver Monitoring を使用しているユーザーが経験する HTTP レイテンシを把握するには、次のようなアプローチが最適です。
C. アプリケーションで、metricKind を GAUGE に設定し、valueType を DISTRIBUTION に設定したメトリックを作成します。Stackdriver Metrics Explorer で、ヒートマップ グラフを使用して指標を視覚化します。
説明：
オプション A (metricKind を DELTA に設定し、valueType を DOUBLE に設定したメトリックの作成): DELTA メトリックは通常、レイテンシー値を表すためではなく、増分変更に使用されるため、DELTA メトリックの使用はレイテンシーの測定に適していない場合があります。
オプション B (metricKind を CUMULATIVE に設定し、valueType を DOUBLE に設定してメトリクスを作成する): CUMULATIVE メトリクスは時間の経過とともに値を累積する可能性がありますが、レイテンシーは時間の経過とともに累積されるのではなく、リクエストごとに測定される傾向があるため、レイテンシー値を正確に表さない場合があります。
オプション D (metricKind を METRIC_KIND_UNSPECIFIED に設定し、valueType を INT64 に設定してメトリクスを作成する): 未指定のメトリクスの種類を使用すると、レイテンシーデータを正確に表すために必要なセマンティクスが提供されない可能性があります。
したがって、オプション C (metricKind を GAUGE に設定し、valueType を DISTRIBUTION に設定したメトリックの作成) が最も適切です。レイテンシー値を表すために GAUGE メトリクスの種類を使用し、valueType を DISTRIBUTION に設定することで、ユーザーが経験した HTTP レイテンシーの分布を正確にキャプチャして視覚化できます。Stackdriver Metrics Explorer でヒートマップ グラフを使用すると、さまざまな期間のレイテンシ分布を視覚化し、レイテンシ指標に基づいてユーザー エクスペリエンスをより詳細に把握できます。
フォームの上部
注 - レイテンシーは分布として測定され、ゲージは分布として測定されます。
GAUGE Metric: 特定の瞬間を測定する値。
DELTAメトリック: 値は、最後に記録されてからの変化を測定します。CUMULATIVE メトリック: 値が時間の経過とともに常に増加します。
参考 - https://cloud.google.com/monitoring/api/v3/kinds-and-types?hl=en
</div></details>

### Q.  問題33: 回答
アプリケーションの CI/CD パイプラインを会社のマルチクラウド環境に実装しています。アプリケーションは、カスタム Compute Engine イメージと、他のクラウド プロバイダの同等のイメージを使用してデプロイされます。イメージをビルドして現在の環境にデプロイでき、将来の変更に適応できるソリューションを実装する必要があります。どのソリューションスタックを使用すべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明カスタム Compute Engine イメージを構築して現在の環境にデプロイし、マルチクラウド環境における将来の変更に適応できるソリューション スタックは次のとおりです。
A. Packer を使用した Cloud Build
説明：
Cloud Build:
Cloud Build は、柔軟かつスケーラブルな方法でビルドとデプロイを自動化できるマネージド CI / CD プラットフォームです。
さまざまなGoogle Cloud Platform(GCP)サービスと統合され、他のクラウドプロバイダーと連携するように拡張できます。
パッカー：
Packer は、単一のソース構成から複数のプラットフォーム用のマシン イメージの作成を自動化するオープンソース ツールです。
さまざまなクラウドプロバイダーをサポートしているため、特定の要件に合わせたカスタムマシンイメージを構築できます。
Cloud Build と Packer を併用すると、Compute Engine のカスタム イメージや、他のクラウド プロバイダで同等のイメージを柔軟に構築できます。これにより、将来の変更や追加のクラウド環境への拡張に適応できる標準化されたイメージ構築プロセスを作成できます。
オプション B、C、D には Google Cloud Deploy または Google Kubernetes Engine が含まれますが、マルチクラウド環境用のカスタム Compute Engine イメージの構築には直接適用できない場合があります。一方、Cloud Build with Packer は、複数のクラウド プラットフォームでカスタム イメージを作成するために必要な汎用性を提供します。
インサイト - https://cloud.google.com/build/docs/building/build-vm-images-with-packer
</div></details>

### Q.  問題34: 回答
インフラストラクチャDevOpsエンジニアのチームが成長し、Terraformを使用してインフラストラクチャを管理し始めています。コードのバージョン管理を実装し、他のチーム メンバーとコードを共有する方法が必要です。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明インフラストラクチャ管理にTerraformを使用する場合に、コードのバージョン管理を実装し、チームメンバー間でコードを共有するための最も適切で広く受け入れられているアプローチは次のとおりです。
A. Terraformコードをバージョン管理システムに格納します。新しいバージョンをプッシュし、マスターとマージするための手順を確立します。
説明：
Git(GitHub、GitLab、Bitbucketなどのプラットフォームでホストされている)などのバージョン管理システム(VCS)は、コードのバージョン管理、コラボレーション、チーム全体の変更管理のための業界標準です。Terraformコードをバージョン管理システムに格納すると、次のことが可能になります。
バージョン管理: コードベースに加えられた各変更が追跡されるため、履歴バージョンに簡単にアクセスでき、チームは必要に応じて変更をロールバックできます。
コラボレーション: 複数のチーム メンバーが同じコードベースで同時に作業でき、プル/マージ要求を使用して変更をマージおよびレビューできます。
ブランチとマージ: 特定の機能や修正に取り組むために異なるブランチを作成でき、準備ができたら変更をメイン/マスターブランチにマージして戻すことができます。
オプションB、CおよびDは、Terraformコードの管理に適していない、または効率的ではない代替方法を提案します。
B. 異なるバージョンのネットワーク共有フォルダにTerraformコードを格納すると、すぐに管理が煩雑になり、エラーが発生しやすくなり、適切なバージョン管理やコラボレーションが促進されない可能性があります。
C. オブジェクトのバージョニングを使用して Cloud Storage バケットに Terraform コードを保存すると、バージョニングは提供される可能性がありますが、Git などのバージョン管理システムが提供するコラボレーション機能や使いやすさに欠けます。
D. Terraform コードを共有 Google ドライブ フォルダに保存しても、コードとしてのインフラストラクチャの効果的な管理に必要なバージョン管理、ブランチ、マージ、またはコラボレーション機能は提供されません。また、チーム環境でコード変更を管理するためのベスト プラクティスも適用されません。
したがって、バージョン管理システム (オプション A) の使用は、成長するインフラストラクチャ DevOps チーム内で Terraform コードのバージョン管理、コラボレーション、管理を行うための推奨および標準的なアプローチです。
インサイト - https://www.terraform.io/docs/cloud/guides/recommended-practices/part3.3.html
</div></details>

### Q.  問題35: 回答
グローバルな組織に勤務し、Compute Engine でモノリシック アプリケーションを実行している。最も少ないステップ数で CPU 使用率を最適化するアプリケーション用のマシンタイプを選択する必要があります。ヒストリカル・システム・メトリックを使用して、アプリケーションが使用するマシン・タイプを識別する必要があります。Google が推奨する方法に従う場合。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明過去のシステム メトリックを使用して Compute Engine のモノリシック アプリケーションに適したマシンタイプを特定することで CPU 使用率を効率的に最適化するには、次の方法をお勧めします。
A. Recommender API を使用して、提案されたレコメンデーションを適用します。
説明：
Recommender API: Google Cloud の Recommender API は、過去のシステム指標を分析し、使用パターンとパフォーマンス データに基づいて具体的な推奨事項を提供します。この API は、リソース使用率を最適化できるマシンタイプを提案します。
Google が推奨するプラクティス: Google Cloud の Recommender API を活用することで、機械学習と履歴データを使用してリソースを最適化するためのカスタマイズされた推奨事項を提供するため、ベスト プラクティスと一致します。
効率性と精度:APIの提案は詳細な分析に基づいており、効率的なリソース割り当てを保証し、適切なマシンタイプを選択する際の手作業を最小限に抑えます。
その他のオプションは、過去のシステム指標を使用したり、Google が推奨する方法に従ったりするのとは一致しません。
B. すべての VM に Ops エージェントを自動的にインストールするエージェント ポリシーを作成する: このオプションは、監視用のエージェントのインストールに対応しますが、マシンの種類を最適化するために履歴データを特に使用しません。
C. gcloud CLI を使用して VM のフリートに Ops Agent をインストールする: 前のオプションと同様に、エージェントのインストールに重点が置かれていますが、マシンタイプの選択に履歴指標を直接利用することはありません。
D. VM の Cloud Monitoring ダッシュボードを確認し、CPU 使用率が最も低いマシンタイプを選択する: ダッシュボードを確認することで分析情報を得ることができますが、現在の CPU 使用率のみに基づいてマシンタイプを手動で選択することは、パフォーマンスを最適化するための最も効果的な方法ではなく、他の過去のパフォーマンス要因を考慮しない可能性があります。
したがって、Recommender API(オプション A)を利用することは、Google が推奨するプラクティスに従って、過去のシステム指標を使用して、最小限の手順で CPU 使用率を最適化するマシンタイプを特定するという要件に最も適しています。
フォームの上部
インサイト - https://cloud.google.com/recommender/docs/overview
</div></details>

### Q.  問題36: 回答
Cloud Build を使用してアプリケーションをビルドします。コストと開発労力を最小限に抑えながら、ビルド時間を短縮したい。あなたは何をすべきか
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Cloud Build を使用してアプリケーションを構築する際に、コストと開発労力を最小限に抑えながらビルド時間を短縮するには、次のような最も効果的なアプローチがあります。
A. Cloud Storage を使用して中間アーティファクトをキャッシュします。
説明：
A. Cloud Storage を使用して中間アーティファクトをキャッシュすると、以前にビルドおよびキャッシュされたコンポーネントを再利用できるため、後続のビルド時にコンポーネントを再構築する必要性が軽減されます。キャッシュから必要な成果物をフェッチすることでビルド プロセスを高速化し、完全な再構築を必要としない特定の手順に必要な時間を最小限に抑えます。これにより、冗長な作業が回避され、ビルド時間が短縮され、その結果、コストが削減されます。
B. 複数の Jenkins エージェントを実行してビルドを並列化すると、複数のエージェントにワークロードを分散することで、ある程度役立つ場合があります。ただし、Jenkins インフラストラクチャの追加のセットアップ、構成、管理が必要であり、複雑さとコストが増加する可能性があります。
C. 複数の小さなビルド ステップを使用すると、ビルド プロセスの可読性と管理性が向上する可能性がありますが、必ずしも実行時間が大幅に短縮されるとは限りません。ビルド ステップを小さくすると、複雑さとメンテナンスのオーバーヘッドが増加する可能性があります。
D. machine-type オプションを使用して大規模な Cloud Build 仮想マシン (VM) を使用すると、より多くのリソースが提供されるため、ビルド プロセスが高速化される可能性があります。ただし、このアプローチでは、大規模な VM では料金が高くなることが多く、ビルド プロセス自体を最適化するほど費用対効果が高くない可能性があるため、コストが増加する可能性があります。
したがって、Cloud Storage を活用して中間アーティファクトをキャッシュする(オプション A)ことは、Cloud Build 環境での費用と開発労力を最小限に抑えながらビルド時間を短縮するための最も効率的で費用対効果の高いアプローチです。
インサイト - https://cloud.google.com/storage/docs/best-practices
https://cloud.google.com/build/docs/speeding-up-builds#caching_directories_with_google_cloud_storage
</div></details>

### Q.  問題37: 回答
サポートする実動システムで多数の停止が発生しました。夜間に目が覚めるすべての停止に関するアラートを受信します。アラートは、1 分以内に自動的に再起動される異常なシステムが原因です。サイト
信頼性エンジニアリングのプラクティスに従いながら、スタッフの燃え尽き症候群を防ぐプロセスを設定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明多数の停止によって夜間に目が覚めるアラートがトリガーされる状況で、サイト信頼性エンジニアリング (SRE) のプラクティスに従っている間にスタッフの燃え尽き症候群を防ぐには、最も効果的なアプローチは次のとおりです。
A. 対処できないアラートを排除します。
説明：
A. 実行不可能なアラートを排除するには、アラート システムを調整して、重要で実用的なアラートのみがオンコール エンジニアに送信されるようにする必要があります。アラートルールを継続的に改善することで、重要でないアラートや誤ったアラートによって引き起こされるノイズを減らすことができ、エンジニアは早急な対応が必要な実用的な問題に集中できます。これにより、睡眠が不必要に妨げられるのを防ぎ、アラート疲れを軽減します。
B. アラートごとにインシデント レポートを作成することは、問題の文書化とパターンの分析に役立つ場合がありますが、アラートの疲労と燃え尽き症候群の差し迫った問題に必ずしも対処せずに、追加のオーバーヘッドとドキュメントを追加する可能性もあります。
C. 異なるタイム ゾーンのエンジニアにアラートを配信すると、負荷が分散される可能性がありますが、すべての組織にとって実行可能なソリューションではなく、アラートがエンジニアのウェルビーイングに与える影響を完全に防ぐことはできません。
D. 関連するサービス レベル目標 (SLO) を再定義して、エラー予算の枯渇を防ぐことは、より広範な戦略の一部になる可能性があります。ただし、SLO を再定義するだけでは、チーム内の燃え尽き症候群や疲労の原因となるアラートの根本原因にすぐに対処できない場合があります。
したがって、アクション不可能なアラート(オプションA)を排除することは、SREプラクティスに従いながらスタッフの燃え尽き症候群を防ぐための最も実用的なステップです。これにより、チームは重大で実用的なアラートに集中でき、重大ではないインシデント中の不要な障害を減らし、チーム全体のウェルビーイングを向上させることができます。
インサイト - https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles
</div></details>

### Q.  問題38: 回答
アプリケーション・ログを7年間アーカイブすることを義務付ける政府機関と仕事をしているとします。ストレージのコストを最小限に抑えながらログをエクスポートして保存するように Stackdriver を設定する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明ストレージ コストを最小限に抑えながらログを 7 年間エクスポートして保存するように Stackdriver を設定するには、次のような最適な方法があります。
D. Stackdriver でシンクを作成して名前を付け、アーカイブされたログを保存するためのバケットを Cloud Storage に作成し、ログのエクスポート先としてバケットを選択します。
説明：
A. Cloud Storage バケットを作成し、そのバケットにログを直接送信するアプリケーションを開発すると、ログを保存できますが、ログの長期保存と取得に Stackdriver の機能を効果的に活用できない可能性があります。
B. Stackdriver からログを取得して BigQuery に保存する App Engine アプリケーションを開発することもできますが、この方法では、ログを取得するために App Engine アプリケーションを継続的に実行すると、複雑さとコストが増す可能性があります。
C. Stackdriver でエクスポートを作成し、ログを永続ストレージに 7 年間保存するように Cloud Pub/Sub を設定することは、Pub/Sub がログの長期保存用に最適化されていない可能性があるため、費用対効果の高いソリューションではない可能性があります。
D. Stackdriver でシンクを作成し、そのシンクに名前を付けると、特定のエクスポート構成を作成できます。アーカイブされたログを保存するためのCloud Storageバケットを作成し、ログのエクスポート先として選択することで、クラウドストレージにログを効率的かつ費用対効果の高い形で長期保存することができ、ストレージコストを最小限に抑えながらログを7年間アーカイブするという要件を満たします。
そのため、Stackdriver でシンクを作成し、Cloud Storage バケットをログのエクスポート先として設定する(オプション D)ことが、ストレージ コストを最小限に抑えながらログを 7 年間エクスポートして保存するように Stackdriver を構成するのに最適なアプローチです。
インサイト - https://cloud.google.com/logging/docs/routing/overview
https://cloud.google.com/logging/docs/routing/overview#destinations
</div></details>

### Q.  問題39: 回答
CI / CD パイプラインを Google Cloud でネイティブに構成している。本番環境の Google Kubernetes Engine(GKE)前環境でのビルドを、本番環境の GKE 環境に昇格する前に自動的に負荷テストを行う必要がある。このテストに合格したビルドのみが運用環境にデプロイされるようにする必要があります。Google が推奨する方法に従う場合。このパイプラインをバイナリ承認でどのように構成する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明バイナリ認証を使用して Google Cloud で CI / CD パイプラインをネイティブに構成し、Google が推奨するプラクティスに従いながら、負荷テストに合格したビルドのみが本番環境にデプロイされるようにするには、次の方法が最も適しています。
オプション C: Workload Identity で認証された Cloud Key Management Service(Cloud KMS)に保存されている秘密鍵を使用して、ロード テストに合格したビルドの認証を作成します。
説明：
Cloud Key Management Service(Cloud KMS):これは、サービスの暗号化キーを管理するための安全な方法を提供します。秘密鍵を Cloud KMS に保存すると、セキュリティとアクセス制御が確保されます。
ワークロード ID:これにより、Kubernetes ワークロードは、Google が管理するサービス アカウントを使用して Google Cloud サービスに対して認証と承認を行うことができます。Workload Identity による認証により、Cloud KMS 鍵にアクセスするための安全でシームレスなプロセスが保証されます。
ビルドに合格するための構成証明:Workload Identity で認証された Cloud KMS に保存されている秘密鍵を使用して構成証明を作成することで、ロードテストに合格したビルドをマークするための安全で承認された方法を確立できます。この構成証明は、運用環境へのデプロイ前の検証手順として使用でき、検証済みのビルドのみが運用環境に進行するようにします。
その他のオプションは、バイナリ認証の設定に関する Google が推奨するプラクティスとはあまり一致しません。
オプションA:品質保証エンジニアに個人の秘密鍵を使用して証明に署名することを要求すると、鍵の管理や個々の人間の行動への依存に関連する潜在的なリスクが生じ、必要な自動化およびセキュリティ標準に合致しない可能性があります。
オプションB:Kubernetes シークレットとして保存されたサービス アカウントの JSON キーを使用して Cloud KMS に秘密鍵を保存すると、セキュリティが確保される可能性がありますが、Kubernetes ワークロードでは Google Cloud 内で認証に Workload Identity を使用する方が推奨される傾向があります。
インサイト - https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
https://cloud.google.com/iam/docs/best-practices-for-using-workload-identity-federation
</div></details>

### Q.  問題40: 回答
アプリケーション サービスは Google Kubernetes Engine(GKE)で実行されます。altostrat-images プロジェクトの一元管理された Google Container Registry
(GCR)イメージレジストリのイメージのみをクラスターにデプロイできるようにし、開発時間を最小限に抑えたい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明開発時間を最小限に抑えながら、altostrat-images プロジェクト内の一元管理された Google Container Registry(GCR)のイメージのみを Google Kubernetes Engine(GKE)クラスタにデプロイできるようにするには、次のような最適なオプションを使用します。
B. ホワイトリスト名のパターンを含むバイナリ認証ポリシーを使用する gcr.io/altostrat-images/
説明：
· オプション A(イメージを gcr.io/altostrat-images にのみプッシュする Cloud Build のカスタム ビルダーを作成する): このアプローチはイメージ構築プロセスに関連していますが、指定したレジストリの特定のイメージのみが GKE にデプロイされることを保証するものではありません。
· オプション C (すべてのマニフェストに gcr.io/altostrat-images のイメージのみが含まれていることを確認するロジックをデプロイ パイプラインに追加する): デプロイ パイプライン内でイメージ チェックを適用するのは良いことですが、この方法は複雑でエラーが発生しやすく、開発時間が長くなる可能性があります。
· オプション D (gcr.io/altostrat-images の各イメージにタグを追加し、イメージのデプロイ時にこのタグが存在することを確認する): このアプローチでは、デプロイ中に手動でタグ付けとタグを確認する必要がありますが、エラーが発生しやすく、時間がかかる可能性があります。
したがって、オプション B (ホワイトリスト名パターン gcr.io/altostrat-images/ を含むバイナリ認証ポリシーを使用する) が最も適切なソリューションです。Binary Authorization を使用すると、特定のレジストリの特定のコンテナ イメージのみを GKE クラスタにデプロイできるようにするなど、デプロイ時のセキュリティ制御を適用するポリシーを作成できます。これにより、指定した GCR レジストリのイメージのみが許可され、イメージのデプロイを厳密に制御しながら開発時間が最小限に抑えられます。
インサイト - https://cloud.google.com/binary-authorization/docs/example-policies
https://cloud.google.com/binary-authorization/docs/cloud-build
</div></details>

### Q.  問題41: 回答
Python で記述され、App Engine フレキシブル環境でホストされている取引アプリケーションをサポートしている。Stackdriver Error Reporting に
送信されるエラー情報をカスタマイズする場合。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Python で記述され、App Engine フレキシブル環境でホストされている取引アプリケーションの Stackdriver Error Reporting に送信されるエラー情報をカスタマイズするには、次の方法が最適です。
C. Python 用の Stackdriver Error Reporting ライブラリをインストールし、App Engine フレキシブル環境でコードを実行します。
説明：
A. Stackdriver Error Reporting ライブラリのインストール後に Compute Engine VM でコードを実行すると、現在のホスティング環境(App Engine フレキシブル環境)と一致しない可能性があります。これには移行作業が必要であり、最も簡単なソリューションではない可能性があります。
B. Stackdriver Error Reporting ライブラリのインストール後に Google Kubernetes Engine(GKE)でコードを実行することもできますが、アプリケーションを GKE に移行すると不必要に複雑になり、現在のホスティング環境に合わない可能性があります。
C. Python 用の Stackdriver Error Reporting ライブラリをインストールし、App Engine フレキシブル環境でコードを実行するのが最も適切な方法です。App Engine フレキシブル環境では、カスタム エラー報告設定がサポートされているため、Python コード内で Stackdriver Error Reporting ライブラリを利用して、Stackdriver Error Reporting に送信されるエラー情報を調整できます。
D. Stackdriver Error Reporting API を使用して ReportedErrorEvent にエラーを書き込み、Stackdriver Logging でログエントリを生成すると、App Engine フレキシブル環境内の Stackdriver Error Reporting ライブラリが提供する組み込み機能を利用する場合に比べて、手作業による処理が複雑になる場合があります。
そのため、Python 用の Stackdriver Error Reporting ライブラリをインストールし、App Engine フレキシブル環境(オプション C)でコードを実行するのが、App Engine でホストされている Python ベースの取引アプリケーションの Stackdriver Error Reporting に送信されるエラー情報をカスタマイズするのに最適な方法です。
インサイト - https://cloud.google.com/error-reporting/docs/setup/python
https://cloud.google.com/error-reporting/docs/setup/python#app-engine
https://cloud.google.com/error-reporting/docs/setup/app-engine-flexible-environment
https://cloud.google.com/error-reporting/docs/setup/python#installing_the_client_library
</div></details>

## 4
### Q.  問題1: 回答
開発チームがサービスの API の新しいバージョンを作成しました。新しいバージョンの API は、サードパーティの開発者やサードパーティがインストールしたアプリケーションのエンドユーザーへの影響を最小限に抑えてデプロイする必要があります。あなたは何をするべきか?
1. 
2. 古いバージョンの API の廃止をアナウンスします。新しいバージョンの API を導入します。古い API で残りのユーザーに連絡します。古いバージョンの API を非推奨にします。古いバージョンの API をダウンします。古い API のユーザーにベスト エフォート サポートを提供します。
3. 
4. 
<details><div>
    答え：2
説明サードパーティの開発者とサードパーティがインストールしたアプリケーションのエンド ユーザーへの影響を最小限に抑えて、サービスの API の新しいバージョンをデプロイするための最も適切なアプローチは次のとおりです。
B. 古いバージョンの API の廃止をアナウンスします。新しいバージョンの API を導入します。古い API で残りのユーザーに連絡します。古いバージョンの API を非推奨にします。古いバージョンの API をダウンします。古い API のユーザーにベスト エフォート サポートを提供します。
このアプローチには、構造化された一連の手順が含まれます。
1. 古いバージョンの API の廃止をアナウンスする: 古い API バージョンの廃止が間近に迫っていることを開発者とユーザーに通知します。これにより、移行の準備に時間を割くことができます。
2. 新しいバージョンの API を導入する: 新しいバージョンの API を利用できるようにし、十分に文書化され、アクセスしやすいようにします。
3. 古い API で残っているユーザーに連絡する: 古いバージョンをまだ使用しているユーザーに連絡を取り、新しいバージョンへの移行を奨励し、支援します。
4. 古いバージョンの API を非推奨にする: 古いバージョンを非推奨として明確にマークし、ユーザーに新しいバージョンへの切り替えを促します。
5. 古いバージョンの API を廃止する: ユーザーが移行するための十分な時間を確保し、廃止を通知した後、古いバージョンの API を無効にするか削除して、ユーザーに新しいバージョンへの切り替えを求めます。
6. 古い API のユーザーにベスト エフォート サポートを提供する: 移行中に困難に直面したユーザーにサポートと支援を提供し、新しい API バージョンにスムーズに移行するための支援を受けられるようにします。
この系統的なアプローチにより、古いバージョンから新しいバージョンへの明確な移行パスをユーザーに提供し、移行プロセス全体を通じてサポートとコミュニケーションを提供することで、中断を最小限に抑えます。
インサイト - https://httptoolkit.tech/blog/how-to-turn-off-your-old-apis/
</div></details>

### Q.  問題2: 回答
CI パイプラインを構成しています。CI パイプライン統合テストのビルドステップでは、プライベート VPC ネットワーク内の API にアクセスする必要があります。セキュリティチームは、API トラフィックを一般公開しないことを要求しています。管理オーバーヘッドを最小限に抑えるソリューションを実装する必要があります。あなたは何をするべきか?
1. 
2. Cloud Build プライベートプールを使用してプライベート VPC に接続します。
3. 
4. 
<details><div>
    答え：2
説明管理オーバーヘッドを最小限に抑えながら、API トラフィックを公開することなくプライベート VPC ネットワーク内の API へのアクセスを許可する最も適切なソリューションは次のとおりです。
A. Cloud Build プライベートプールを使用してプライベート VPC に接続します。
説明：
A. Cloud Build のプライベートプールを使用すると、プライベート VPC ネットワーク内のリソースにアクセスできるワーカー リソースの専用プライベート プールを作成できます。Cloud Build のプライベートプールにより、CI パイプラインの統合テストのステップで、API トラフィックを一般公開することなく、プライベート ネットワーク内の API に安全にアクセスできます。このセットアップでは、プライベート リソースへのアクセスを簡単に管理および制御する方法が提供されるため、管理オーバーヘッドが最小限に抑えられます。
オプション B(Spinnaker for Google Cloud を使用)は、デプロイと管理の機能を提供する場合がありますが、API トラフィックを公開せずにプライベート VPC リソースへのアクセスに直接対処するものではありません。
オプション C と D では、Cloud Build をパイプライン ランナーとして使用し、API アクセスの負荷分散を構成します。ただし、どちらのオプションも負荷分散構成を提案しており、トラフィックを公開せずにプライベート VPC 内の API に安全にアクセスするには直接適用できない場合があります。
したがって、オプション A(Cloud Build プライベート プールを使用してプライベート VPC に接続する)は、統合テストのためにプライベート VPC ネットワーク内の API にアクセスするという要件に最も適しており、API トラフィックを一般公開せず、管理オーバーヘッドを最小限に抑えるというセキュリティ チームの要件を満たしています。
インサイト - https://cloud.google.com/build/docs/private-pools/private-pools-overview
</div></details>

### Q.  問題3: 回答
Google Kubernetes Engine(GKE)にデプロイされた一般的なモバイルゲームアプリケーションを、複数の Google Cloud リージョンでサポートしています。各リージョンには複数の
Kubernetes クラスターがあります。特定のリージョンのどのユーザーもアプリケーションに接続できないというレポートを受け取ります。サイト信頼性エンジニアリングのプラクティスに従ってインシデントを解決する必要があります。最初に何をすべきですか?
1. 
2. 
3. 影響を受けるリージョンから、問題を報告していない他のリージョンにユーザー トラフィックを再ルーティングします。
4. 
<details><div>
    答え：3
説明特定の地域のユーザーが Google Kubernetes Engine(GKE)にデプロイされたモバイルゲームアプリケーションに接続できないシナリオでは、サイト信頼性エンジニアリング(SRE)のプラクティスに従いながら、まずユーザー アクセスを復元することが優先されるため、問題を効果的に診断することに当面の焦点を当てる必要があります。したがって、ユーザー アクセスの復元の優先順位を考慮して、最初に実行する最も適切なアクションは次のとおりです。
A. 影響を受けるリージョンから、問題を報告していない他のリージョンにユーザー トラフィックを再ルーティングします。
影響を受けるリージョンから、アプリケーションが正しく機能している他のリージョンにユーザー トラフィックをリダイレクトすることで、問題のトラブルシューティング中にユーザーのサービスが中断されることはありません。このアクションは、ユーザー エクスペリエンスを優先し、インシデント解決時の中断を最小限に抑えるという SRE の原則に沿っています。
ユーザーがアプリケーションにアクセスできるようにトラフィックを再ルーティングしたら、次の手順は、接続の問題の根本原因を調査することです。Stackdriver Logging などのツールを利用してログ内のエラー メッセージを調べる(オプション D)ことは、根本的な問題を理解して解決するために重要です。同時に、Stackdriver Monitoring を使用してリソースの異常な使用状況を確認したり(オプション B)、スケーリング調整を検討したり(オプション C)したりすることも、ユーザー アクセスが復元されたら並行して行うことができます。
ただし、トラフィックを機能リージョンに一時的にリダイレクトすると、ユーザーはアクティビティを再開し、調査が根本原因を特定して問題を効果的に解決し続ける間、エクスペリエンスを優先できます。
</div></details>

### Q.  問題4: 回答
パブリック IP アドレスを持つ Compute Engine インスタンスで実行される新しいアプリケーション用に Cloud Logging を構成しています。ユーザー管理サービス アカウントがインスタンスにアタッチされます。必要なエージェントがインスタンスで実行されていることを確認しましたが、Cloud Logging でインスタンスのログエントリを表示できません。Google が推奨する方法に従って問題を解決したい。あなたは何をするべきか?
1. Logs Writer ロールをサービス アカウントに追加します。
2. 
3. 
4. 
<details><div>
    答え：
説明Google が推奨する方法に従っているときに Cloud Logging にインスタンスのログエントリが表示されないという問題を解決するには、次の方法を使用します。
C. Logs Writer ロールをサービス アカウントに追加します。
説明：
Cloud Logging には、サービスにアクセスしてログを書き込むために必要な権限が必要です。Compute Engine インスタンスにアタッチされたユーザー管理サービス アカウントに Logs Writer ロールを付与すると、Cloud Logging にログを書き込むことが許可されます。
Logs Writer ロールは、Cloud Logging にログを書き込むために必要な権限を提供します。インスタンスにアタッチされたサービス アカウントにこのロールを付与すると、インスタンスで実行されているエージェントによって生成されたログを Cloud Logging に送信して表示と分析を行うことができます。
オプション A、B、D は、Cloud Logging でログエントリが見つからない問題の解決には直接関係ありません。
A. サービス アカウント キーをエクスポートし、そのキーを使用するようにエージェントを構成することは、Google が推奨するアプローチではない可能性があり、セキュリティ上のリスクが生じる可能性があります。Google Cloud Platform(GCP)のロールとサービスアカウントを使用してアクセスと権限を管理することをお勧めします。
B. デフォルトの Compute Engine サービス アカウントを使用するようにインスタンスを更新することは、アプリケーションが他の GCP サービスとやり取りするために適切な権限を持つ特定のサービス アカウントを必要とする場合、理想的なソリューションではない可能性があります。
D. インスタンスが存在するサブネットで限定公開の Google アクセスを有効にすることは、パブリック IP アドレスなしで Google API やサービスにアクセスするインスタンスに関連しています。Cloud Logging へのログの送信とは直接関係ありません。
</div></details>

### Q.  問題5: 回答
会社で Google Kubernetes Engine(GKE)でアプリケーションを実行している。いくつかのアプリケーションは、エフェメラルボリュームに依存しています。ワーカー・ノード上のDiskPressureノードの状態が原因で、一部のアプリケーションが不安定になっていることに気付きました。問題の原因となっているPodを特定する必要がありますが、ワークロードとノードへの実行アクセス権がありません。あなたは何をするべきか?
1. メトリックス エクスプローラーを使用して container/ephemeral_storage/used_bytes メトリックを確認します。
2. 
3. 
4. 
<details><div>
    答え：1
説明このシナリオでのオプションとその適合性を詳しく見ていきましょう。
B. メトリックス エクスプローラーを使用して container/ephemeral_storage/used_bytes メトリックを確認します。
説明：
メトリックス・エクスプローラーのcontainer/ephemeral_storage/used_bytesメトリックを使用すると、Pod内のコンテナによって使用されるエフェメラル・ストレージの量を監視できます。このメトリクスは、Pod内のどのコンテナがかなりの量のエフェメラルストレージを消費しているかを特定するのに役立ちます。
このオプションは、コンテナー レベルのメトリックに焦点を当て、ストレージの過剰使用を引き起こし、DiskPressure 状態に寄与している可能性のあるコンテナーに関する分析情報を提供するため、適している可能性があります。
それでは、このコンテキストで他のオプションが適していない理由について説明しましょう。
A. メトリックス エクスプローラーを使用して、ノード/ephemeral_storage/used_bytes メトリックを確認します。
ノードレベルのエフェメラルストレージの使用状況を監視することは有用ですが、どのPodやコンテナが高い使用率を引き起こしているかを直接特定するものではありません。ノードストレージの全体像を提供しますが、問題の原因となっている問題のあるPodを特定できない可能性があります。
C. emptyDir ボリュームを持つすべての Pod を見つけます。df -h コマンドを使用して、ボリュームのディスク使用量を測定します。D. emptyDirボリュームを持つすべてのPodを見つけます。df -sh * コマンドを使用して、ボリューム ディスクの使用状況を測定します。
オプションCとDでは、Podを手動で検査してemptyDirボリュームを使用しているものを特定し、Pod内でdfコマンドを使用します。ただし、ワークロードとノードへの実行アクセス権がないため、このシナリオではポッド内でコマンドを実行することは現実的ではないとおっしゃいました。
アクセスの制限と、エフェメラルストレージの使用によるDiskPressureの原因となる特定のコンテナを特定する必要があることを考えると、オプションB(コンテナレベルのエフェメラルストレージメトリクスの確認)は、Pod自体に直接アクセスすることなく、問題のあるコンテナを特定する最良の機会を提供します。
インサイト - https://cloud.google.com/monitoring/api/metrics_kubernetes
</div></details>

### Q.  問題6: 回答
Cloud Run を使用してサーバーレス アプリケーションを構築し、そのアプリケーションを本番環境にデプロイしました。コストを最適化するために、アプリケーションのリソース使用率を特定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. Cloud Monitoring を使用して、アプリケーションのコンテナの CPU とメモリの使用率をモニタリングします。
<details><div>
    答え：4
説明費用最適化のために Cloud Run にデプロイされたサーバーレス アプリケーションのリソース使用率をモニタリングするには、提供されるオプションの中で最適なアプローチは次のとおりです。
C. Cloud Monitoring を使用して、アプリケーションのコンテナの CPU とメモリの使用率をモニタリングします。
説明：
Cloud Monitoring は、CPU やメモリの使用率などのコンテナレベルの指標など、Google Cloud Platform 上のアプリケーションのパフォーマンスと健全性に関するさまざまな指標と分析情報を提供します。
Cloud Run アプリケーションはコンテナで実行され、Cloud Monitoring を使用して CPU とメモリの使用率をモニタリングすると、アプリケーションによって消費されるリソースを把握するのに役立ちます。
この情報は、実際の使用パターンに基づいてリソース割り当てを調整することにより、コストの最適化について十分な情報に基づいた意思決定を行うために重要です。
Cloud Trace、Cloud Profiler with Ops Agent、Cloud Ops などのオプションは、貴重なモニタリング機能とプロファイリング機能を提供しますが、Cloud Monitoring(オプション C)を使用してコンテナの CPU とメモリの使用率をモニタリングすると、Cloud Run デプロイのコンテキストでコスト最適化のためのリソース使用率を特定する必要性に直接対応できます。
インサイト - https://cloud.google.com/run/docs/monitoring?hl=ja
https://cloud.google.com/profiler/docs/about-profiler
</div></details>

### Q.  問題7: 回答
Google Cloud でのアプリケーションのデプロイ手法を設計している。デプロイ計画の一環として、ライブ トラフィックを使用して、アプリケーションの新しいバージョンのパフォーマンス メトリックを収集する必要があります。アプリケーションを起動する前に、運用環境の全負荷に対してテストする必要があります。あなたは何をするべきか?
1. 
2. 
3. 継続的デプロイでシャドウ テストを使用します。
4. 
<details><div>
    答え：3
説明正しいオプション - D. 継続的デプロイでシャドウ テストを使用します。
シャドウ テストでは、新しいバージョンを現在のバージョンと一緒にデプロイして実行できますが、ユーザーからは非表示になります。このアプローチでは、受信要求がミラーリングされ、テスト環境で再生されるため、運用環境への影響がゼロになります。トラフィックを複製することで、エンドユーザーに影響を与えることなく、本番環境の全負荷に対する新機能と改善点の包括的なテストが可能になります。
さらに、継続的デプロイの側面により、デプロイ プロセスが継続的に維持され、完全なロールアウトが行われる前に、実際のパフォーマンス メトリックとユーザーの操作に基づいて反復的なテストと調整が可能になります。
https://cloud.google.com/architecture/application-deployment-and-testing-strategies#shadow_test_pattern
</div></details>

### Q.  問題8: 回答
異なる VPC の Google Kubernetes Engine(GKE)クラスタ間の接続を設定しています。クラスタ A のノードがクラスタ B のノードにアクセスできないことに気付きました。ワークロード アクセスの問題は、ネットワーク構成が原因であると思われます。問題のトラブルシューティングを行う必要があるが、ワークロードとノードへの実行アクセス権がない。ネットワーク接続が切断されているレイヤーを特定します。あなたは何をするべきか?
1. Network Connectivity Center を使用して、クラスタ A からクラスタ B への接続テストを実行します。
2. 
3. 
4. 
<details><div>
    答え：1
説明オプション B: Network Connectivity Center を使用して、クラスタ A からクラスタ B への接続テストを実行します。
説明: Network Connectivity Center は、さまざまな Google Cloud リソース間のネットワーク接続を一元的に管理、モニタリング、トラブルシューティングする方法を提供します。Network Connectivity Center を使用して接続テストを実行すると、ワークロードやノードに直接アクセスすることなく、クラスタ A とクラスタ B 間の接続ステータスと潜在的な問題を確認できます。これは、接続の問題を引き起こしている可能性のあるネットワークレベルの問題を特定するのに役立ちます。
他のオプションが最良の選択ではない可能性がある理由:
オプション A: クラスター A のノードにツールボックス コンテナーをインストールして、クラスター B へのルート構成を確認します。
ルート構成の確認は便利ですが、クラスター A のノードにツールボックス コンテナーをインストールしても、ネットワーク パスに関する包括的な情報が得られなかったり、クラスター間の潜在的な問題が特定されなかったりする可能性があります。
オプション C: デバッグ コンテナーを使用して、クラスター A からクラスター B へ、およびクラスター B からクラスター A へ traceroute コマンドを実行します。
クラスタ内のデバッグコンテナからtracerouteコマンドを実行すると、ネットワークパスのホップと潜在的な障害点を特定するのに役立ちます。ただし、クラスタ間接続の問題をトラブルシューティングするために Network Connectivity Center が提供する一元的な情報や詳細情報が提供されない場合があります。
オプション D: VPC の両方で VPC フローログを有効にし、パケットドロップを監視します。
VPC フローログを有効にすると、パケットのドロップを含むネットワーク トラフィックのログ記録に役立ち、問題の診断に役立つ可能性があります。ただし、クラスタ間のネットワーク接続のレイヤーまたは障害点を正確に特定できない場合があります。
このコンテキストでは、オプション B は、ワークロードやノードに直接アクセスすることなく、異なる VPC 内の GKE クラスタ間のネットワーク接続の問題を特定するための一元化された包括的なアプローチを提供することに重点を置いています。
インサイト - https://cloud.google.com/network-intelligence-center/docs/connectivity-tests/concepts/overview
</div></details>

### Q.  問題9: 回答
サイト信頼性エンジニアは、Google Kubernetes Engine(GKE)で実行される Go で記述されたアプリケーションを本番環境でサポートします。アプリケーションの新しいバージョンをリリースした後、アプリケーションが約 15 分間実行され、再起動することがわかります。Cloud Profiler をアプリケーションに追加することにしたところ、アプリケーションが再起動するまでヒープ使用量が絶えず増加していることに気付きました。あなたは何をするべきか?
1. 
2. 
3. アプリケーション展開のメモリ制限を増やします。
4. 
<details><div>
    答え：3
説明約 15 分間の実行後にアプリケーションが再起動し、再起動が発生するまで Cloud Profiler のヒープ使用量が継続的に増加するシナリオでは、この問題に対処するための最も適切なアクションは次のとおりです。
C. アプリケーション展開のメモリ制限を増やします。
説明：
Cloud Profiler が示すように、ヒープ使用量が増加し続けていることは、メモリ関連の問題を示唆しています。アプリケーションが割り当てられたメモリよりも多くのメモリを消費している可能性があり、メモリ制限に達してクラッシュまたは再起動します。
Kubernetes 内のアプリケーション・デプロイメント構成でメモリー制限を増やすことで、アプリケーションのワークロードを処理するためのメモリー・リソースを増やすことができます。これにより、アプリケーションの絶え間ない増加とその後の再起動につながるメモリ枯渇の問題を回避できる可能性があります。
オプションAとBは、目前の問題に直接対処する可能性が低くなります。
A. アプリケーションで CPU スロットリングや枯渇が発生している場合は、CPU 制限を増やすと役立つ場合がありますが、この場合、問題はヒープ使用量の増加とメモリの制約に関連しています。
B. 高メモリのコンピューティング ノードをクラスターに追加すると、クラスターの全体的な容量が向上する可能性がありますが、アプリケーション自体内の特定のメモリ使用量の問題を直接解決するわけではありません。
オプション D (Cloud Trace をアプリケーションに追加して再デプロイする) は、Cloud Profiler によって示されたメモリ関連の問題に対処するのに最も適したアクションではありません。Cloud Trace は、分散トレースとパフォーマンス モニタリングに使用され、アプリケーションのレイテンシの分析に役立つ可能性がありますが、メモリ関連の問題は直接解決されない場合があります。
</div></details>

### Q.  問題10: 回答
ユーザー向けの Web アプリケーションをサポートしている。過去 6 か月間のアプリケーションのエラー バジェットを分析すると、特定の時間枠でアプリケーションがエラー バジェットの 5% 以上を消費したことがないことがわかります。ビジネス関係者とサービス レベル目標 (SLO) のレビューを行い、SLO が適切に設定されていることを確認します。アプリケーションの SLO は、観察された信頼性をより厳密に反映する必要があります。速度、信頼性、ビジネスニーズのバランスを取りながら、その目標を推進するためにどのようなステップを踏むことができますか?(2つ選択してください。
1. 
2. 
3. SLO を厳しくして、アプリケーションで観察された信頼性に一致させます。
4. アプリケーションの追加のサービス レベル インジケーター (SLI) を実装して測定します。
5. 
<details><div>
    答え：3,4
説明アプリケーションのサービス レベル目標 (SLO) を、速度、信頼性、およびビジネス ニーズを考慮しながら、観察された信頼性とより緊密に調整するという目標に基づいて、次の 2 つの最適な手順があります。
C. SLO を厳しくして、アプリケーションで観察された信頼性に一致させます。観測された信頼性が設定された SLO を一貫して大幅に下回っている場合は、アプリケーションの実際の信頼性により適した、より現実的な値に SLO を調整することを検討してください。これにより、SLO はアプリケーションのパフォーマンスをより正確に表し、信頼性の現実的な目標を設定します。
D. アプリケーションの追加のサービス レベル インジケーター (SLI) を実装して測定します。追加のSLIを導入して監視することで、アプリケーションのパフォーマンスをより包括的に把握できます。追加のSLIを実装することで、システムのさまざまな側面をより微妙に理解し、改善が必要な領域を特定するのに役立ちます。このステップは、既存の指標を超えて信頼性の全体像を取得するのに役立ち、アプリケーションのパフォーマンスをより正確に評価することに貢献します。
オプション A、B、および E は、速度、信頼性、およびビジネス ニーズのバランスを取りながら、SLO を観測された信頼性とより緊密に連携させるという目標の達成に適していないか、助長しません。
· A. すべてのアプリケーション ゾーンに供給能力を追加します。これにより、容量と潜在的な信頼性が向上する可能性がありますが、必ずしも SLO が観察された信頼性とより緊密に連携するとは限りません。
· イ.より頻繁な、または潜在的に危険なアプリケーション リリースを行う: 危険なリリースの頻度を増やすと、ばらつきと不安定さが増し、信頼性に悪影響を与える可能性があります。
· E. より多くのエラーバジェットを消費するために計画的なダウンタイムを発表します。意図的にダウンタイムを引き起こしてエラーバジェットを消費することは、ユーザーの信頼を損ない、ビジネスに悪影響を与える可能性があり、信頼性とビジネスニーズのバランスをとるという目的と矛盾するため、適切なアプローチではありません。
したがって、SLO を実際の信頼性に合わせて調整し、追加の SLI を実装することは、さまざまなビジネスおよび信頼性の要因を考慮しながら、SLO をアプリケーションの実際のパフォーマンスに合わせるための効果的な手順です。
https://sre.google/sre-book/service-level-objectives/
</div></details>

### Q.  問題11: 回答
Compute Engine マネージド インスタンス グループにデプロイされたウェブ アプリケーションを実行しています。Ops Agent はすべてのインスタンスにインストールされます。最近、特定のIPアドレスからの不審なアクティビティに気付きました。最小限の運用オーバーヘッドで、その特定の IP アドレスからのリクエスト数を表示するように Cloud Monitoring を構成する必要があります。あなたは何をするべきか?
1. Ops エージェントをロギング レシーバで構成します。ログベースのメトリクスを作成します。
2. 
3. 
4. 
<details><div>
    答え：1
説明Ops Agent がインストールされた Compute Engine マネージド インスタンス グループで、運用上のオーバーヘッドを最小限に抑えながら、特定の IP アドレスからのリクエスト数をモニタリングする最適なソリューションは次のとおりです。
A. Ops エージェントをロギング レシーバで構成します。ログベースのメトリクスを作成します。
説明：
1. Ops Agent はインスタンスからログを収集し、Cloud Logging に転送できます。
2. Ops Agent をロギング レシーバーで構成することで、IP アドレスなどの受信要求に関する情報を含む Web サーバー ログを収集できます。
3. Cloud Logging でログが収集されたら、不審な IP アドレスをターゲットとするログベースの指標を作成できます。
4. Cloud Monitoring でログベースの指標を作成すると、アプリケーションを大幅に変更したり、複雑さを増したりすることなく、特定の IP アドレスからのリクエスト数をモニタリングして可視化できます。
オプション A は、運用上のオーバーヘッドを最小限に抑えながら、Ops Agent と Cloud Monitoring を使用して特定の IP アドレスからのリクエスト数をモニタリングするための、最も適切で侵入の少ないソリューションです。
フォームの上部
インサイト - https://cloud.google.com/monitoring/agent/ops-agent/configuration#logging-receivers
https://cloud.google.com/monitoring/agent/ops-agent/configuration
</div></details>

### Q.  問題12: 回答
組織は Google Cloud でコンテナ化を開始しています。コンテナー イメージと Helm チャート用のフル マネージド ストレージ ソリューションが必要です。Google Kubernetes Engine(GKE)、Cloud Run、VPC Service Controls、Identity and Access Management(IAM)など、既存の Google Cloud サービスにネイティブに統合されているストレージ ソリューションを特定する必要があります。あなたは何をするべきか?
1. アーティファクト・レジストリを、Helmチャートとコンテナ・イメージの両方に対してOCIベースのコンテナ・レジストリとして構成します。
2. 
3. 
4. 
<details><div>
    答え：1
説明Google Kubernetes Engine(GKE)、Cloud Run、VPC Service Controls、Identity and Access Management(IAM)などの既存の Google Cloud サービスにネイティブに統合された、コンテナ イメージと Helm チャートのフルマネージド ストレージ ソリューションに適したソリューションは次のとおりです。
C. アーティファクト・レジストリを、Helmチャートとコンテナ・イメージの両方に対してOCIベースのコンテナ・レジストリとして構成します。
説明：
C. Artifact Registry は、コンテナ イメージや Helm チャートなどのソフトウェア パッケージ アーティファクトを保存するために特別に設計された Google Cloud 内のマネージド サービスです。Google Kubernetes Engine(GKE)、Cloud Run、VPC Service Controls、IAM など、さまざまな Google Cloud サービスとシームレスに統合されます。Artifact Registry は OCI (Open Container Initiative) 標準をサポートしているため、コンテナー イメージと Helm チャートを安全に保存できます。バージョン管理、アクセス制御、脆弱性スキャン、監査ログなどの機能を提供し、完全に管理されています。
それでは、他のオプションが最も適切ではない理由について説明しましょう。
A. Docker を使用して、組織が所有するバケットをポイントする Cloud Storage ドライバを構成することは、コンテナ イメージと Helm チャートを管理するためのネイティブの Google Cloud ソリューションではありません。Artifact Registryのような専用サービスによって提供される直接的な統合と機能が欠けています。
B. 制限の厳しい RBAC を使用して GKE で実行するようにオープンソースのコンテナ レジストリ サーバーを構成することは実現可能なオプションですが、追加のセットアップと管理が必要であり、Artifact Registry などの専用の Google Cloud サービスと同じレベルのネイティブ統合とマネージド機能が提供されない可能性があります。
D. Container Registryは、かつてはコンテナイメージの保存に使用されるサービスでしたが、推奨されるソリューションとしてArtifact Registryに置き換えられました。Container Registry はレガシ サービスと見なされるようになり、徐々に非推奨になっています。さらに、これは主にコンテナー イメージ用に設計されており、Helm チャートやその他のアーティファクトの種類を格納するのには理想的ではない場合があります。
したがって、Google Cloud サービスと統合され、コンテナ イメージと Helm チャートの両方に適したフルマネージド ストレージ ソリューションには、Artifact Registry(オプション C)が最適です。
インサイト - https://cloud.google.com/artifact-registry/docs/helm
</div></details>

### Q.  問題13: 回答
Google Kubernetes Engine で実行されているアプリケーションがあります。アプリケーションは要求ごとに複数のサービスを呼び出しますが、応答が遅すぎます。遅延の原因となっているダウンストリーム サービスを特定する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Kubernetes Engine で実行されているアプリケーションで遅延の原因となっているダウンストリーム サービスを特定する必要があるシナリオでは、次のようなアクションが最も適切です。
D. OpenTelemetry や Stackdriver Trace などの分散トレース フレームワークを使用します。
OpenTelemetry や Stackdriver Trace などの分散トレース フレームワークは、分散システムのさまざまなサービスを介して伝播するリクエストをトレースするように特別に設計されています。これにより、さまざまなサービスやコンポーネント間で要求のパスを追跡し、遅延やパフォーマンスの問題が発生している場所に関する分析情報を得ることができます。
ディストリビューティッド(分散)トレーシングを実装することで、リクエストごとに詳細なトレースを生成し、異なるサービス間のフローを示すことができます。これは、特定のダウンストリーム サービスによって引き起こされるアプリケーション内のボトルネック、遅延の問題、または遅延を特定するのに役立ちます。この情報により、全体的な速度低下の原因となっているサービスを特定し、それらのサービスのパフォーマンスの問題を最適化または対処することに集中できます。
オプション A、B、C は、システムに関するいくつかの分析情報を提供する可能性がありますが、遅延の原因となっているダウンストリーム サービスを具体的に特定しない場合があります。
A. VPC フローログの分析:ネットワーク トラフィック パターンの理解には役立つ場合がありますが、アプリケーションの遅延の原因となっているサービスを直接特定できない場合があります。
B. Liveness and Readiness プローブの調査:これらのプローブは、サービスの正常性を検査しますが、要求処理中に特定のサービスによって引き起こされた遅延を直接示さない場合があります。
C. Dataflow パイプラインを作成する:サービス メトリックに関する分析情報を提供することはできますが、各要求に対して特定のダウンストリーム サービスによって発生する待機時間に関する詳細情報は提供されない場合があります。
対照的に、ディストリビューティッド(分散)トレーシングフレームワーク(オプションD)の使用は、サービス間のリクエストのパスとタイミングを正確に識別するように調整されているため、アプリケーションのサービス内の遅延の原因を分離するのに最も適した選択肢です。
インサイト – https://cloud.google.com/trace/docs/overview
</div></details>

### Q.  問題14: 回答
会社では、CI / CD に Google Cloud VM インスタンスで実行されている Jenkins を使用しています。Terraform を使用して、Infrastructure as Code オートメーションを使用するように機能を拡張する必要があります。Terraform Jenkins インスタンスが Google Cloud リソースを作成する権限を持っていることを確認する必要があります。Google が推奨する方法に従う場合。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Terraform Jenkins インスタンスが Google が推奨するプラクティスに従って Google Cloud リソースを作成する権限を確実に取得するには、次の方法が最適です。
A. Jenkins VM インスタンスに、適切な Identity and Access Management (IAM) アクセス許可を持つサービス アカウントがアタッチされていることを確認します。
説明：
サービス アカウント: このアプローチは、Google Cloud のベスト プラクティスに沿ったものです。Jenkins VM インスタンスにサービス アカウントがアタッチされていることを確認する必要があります。サービス アカウントは、Google Cloud サービスへのアクセスを認証および承認するために使用されます。
IAM アクセス許可: Jenkins VM にアタッチされているサービス アカウントに必要な IAM アクセス許可が付与されていることを確認します。この権限により、Jenkins で実行されている Terraform スクリプトで、Infrastructure-as-Code の自動化に必要な Google Cloud リソースを作成、管理、削除できるようになります。
他のオプションが最適ではない理由の説明:
B. Terraform モジュールを使用して、Secret Manager が資格情報を取得できるようにする。
認証情報の管理に Secret Manager を使用することはセキュリティ上の優れた方法ですが、この方法のみに依存すると、Google Cloud リソースを作成するための Jenkins インスタンスの承認に対処できない可能性があります。
C. Terraformインスタンス専用のサービス・アカウントを作成し、秘密鍵の値をJenkinsサーバー上のGOOGLE_CREDENTIALS環境変数にダウンロード/コピーします。
秘密キーの値を手動で管理して環境変数にコピーすると、セキュリティ上のリスクが発生する可能性があり、資格情報を安全に処理するための推奨される方法ではありません。
D. Terraform コマンドを実行する前に、gcloud auth application-default login コマンドを Jenkins のステップとして追加します。
このコマンドは Jenkins インスタンスを認証する可能性がありますが、gcloud CLI の認証情報に依存しているため、自動化のベスト プラクティスではない可能性があり、実行時間の長いプロセスや非インタラクティブでの使用には適していない可能性があります。
要約すると、オプション A は、アタッチされたサービス アカウントに適切な IAM 権限を確保することで、Terraform を使用して Google Cloud リソースを作成する Jenkins インスタンスを安全に承認するための Google 推奨プラクティスに最も合致しています。
インサイト - https://github.com/sassoftware/viya4-iac-gcp/blob/main/docs/user/TerraformGCPAuthentication.md
</div></details>

### Q.  問題15: 未回答
クライアント用に新しい Google Cloud 組織を設計しています。クライアントは、Google Cloud で作成された有効期間の長い認証情報に関連するリスクを懸念しています。運用上のオーバーヘッドを最小限に抑えながら、JSON サービス アカウント キーの使用に関連するリスクを完全に排除するソリューションを設計する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明オプション A が適切と見なされる理由と、他のオプションが長期間有効な資格情報に関する懸念に完全に対処しない理由について説明します。
A. constraints/iam.disableServiceAccountKeyCreation 制約を組織に適用します。
この制約により、Google Cloud 組織内での新しいサービス アカウント キーの作成が制限されます。追加のキーの生成を防ぐことで、攻撃対象領域と、新しく作成された有効期間の長い資格情報に関連する潜在的なリスクを大幅に軽減します。既存のキーには対応していませんが、新しいキーの作成に関連する継続的なリスクを効果的に最小限に抑え、リスクの軽減に役立ちます。
他のオプションが適していない理由:
B. 事前定義されたロールのカスタムバージョンを使用して、すべての iam.serviceAccountKeys.* サービスアカウントロールのアクセス許可を除外します。
特定のサービス アカウント キーのアクセス許可を除外するようにロールをカスタマイズすることは複雑になる可能性があり、可能なすべてのキー関連のアクセス許可をカバーできない場合があります。キー関連のアクセス許可が見落とされた場合、サービス アカウント キーに関連するリスクに包括的に対処できない可能性があります。
C. constraints/iam.disableServiceAccountKeyUpload 制約を組織に適用します。
この制約は、サービス アカウント キーのアップロードの防止と新しいキーの追加の制限に重点を置いていますが、既存の有効期間の長い資格情報に関連するリスクには対処していません。これは、新しいキーの生成を減らすのに効果的ですが、以前に作成された資格情報によってもたらされるリスクはカバーしていません。
D. roles/iam.serviceAccountKeyAdmin IAM ロールを組織管理者のみに付与します。
serviceAccountKeyAdmin ロールを特定の管理者に割り当てると、サービス アカウント キーを管理できるようになり、新しいキーを作成したり、既存のキーを変更したりする可能性があり、リスクを最小限に抑えるどころか、リスクが高まる可能性があります。広範なキー管理特権を付与することは、有効期間の長い資格情報に関連するリスクを完全に排除するという目標と一致しない可能性があります。
要約すると、オプション A (constraints/iam.disableServiceAccountKeyCreation) は、新しいサービス アカウント キーの作成を制限することに重点を置いていますが、既存の有効期間の長い認証情報に関連するリスクを完全に排除するわけではありません。ただし、新しいキーの生成を防ぐことで、継続的なリスクを効果的に最小限に抑え、時間の経過とともに攻撃対象領域と潜在的な脆弱性を減らすために重要です。その他のオプションでは、JSON サービス アカウント キーに関連するリスクを完全に排除するには制限があります。
インサイト - https://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts
</div></details>

### Q.  問題16: 回答
お客様は、大容量のエンタープライズ・アプリケーションの信頼性に責任を負います。多くのユーザーから、アプリケーションの機能の重要なサブセットである「データ集約型のレポート機能」がHTTP 500エラーで一貫して失敗していると報告されています。アプリケーションのダッシュボードを調査すると、エラーと、レポートの生成に使用される内部キューのサイズを表すメトリックとの間に強い相関関係があることに気付きます。I/O待機時間が長いレポートバックエンドに障害を追跡します。バックエンドの永続ディスク (PD) のサイズを変更することで、問題をすばやく修正します。ただし、レポート生成機能の可用性サービスレベル指標(SLI)を作成する必要があります。どのように定義しますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明説明されているシナリオでは、レポート生成機能の可用性サービス レベル インジケーター (SLI) の最も適切な定義は次のとおりです。
B.応答が成功したレポート生成要求の割合。
ここでの焦点は、HTTP 500エラーが発生しているレポート生成機能の可用性です。エラーとレポートを生成するための内部キューのサイズとの間に強い相関関係があることから、この問題がレポート生成要求の正常な完了に影響を与えていることを示唆しています。
可用性の SLI は、レポート生成要求の正常な完了を測定する必要があります。これは、応答が成功した要求の割合と、行われたレポート生成要求の合計数を反映します。これは、内部キューのサイズやバックエンドの問題に関係なく、レポート機能が使用可能であり、ユーザーの意図したとおりに機能するようにするという目標と一致しています。
したがって、SLIを、応答が成功したレポート生成要求の割合(オプションB)として定義することは、このコンテキストでデータ集約型レポート機能の可用性を測定するのに最も適した選択です。
インサイト - https://sre.google/workbook/implementing-slos/
ヒント：
サービスの種類: 要求主導型
SLI のタイプ: 可用性
説明: 応答が成功した要求の割合。
</div></details>

### Q.  問題17: 回答
会社では、Google Kubernetes Engine(GKE)にデプロイされるアプリケーションを開発しています。各チームは異なるアプリケーションを管理します。コストを最小限に抑えながら、チームごとに開発環境と運用環境を作成する必要があります。異なるチームが他のチームの環境にアクセスできないようにする必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明正しいオプション – D. 開発と本番環境の GKE クラスタを別々のプロジェクトに作成します。各クラスターで、チームごとに Kubernetes 名前空間を作成し、各チームが独自の名前空間にのみアクセスできるように Kubernetes ロールベースのアクセス制御 (RBAC) を構成します
オプション D は、コストを最小限に抑え、異なるチームが他のチームの環境にアクセスできないようにしながら、各チームの開発環境と運用環境を作成するための優れたアプローチです。このアプローチでは、開発と本番環境の GKE クラスタを別々の GCP プロジェクトに作成します。各クラスターでは、チームごとに Kubernetes 名前空間が作成されます。次に、Kubernetes のロールベースのアクセス制御 (RBAC) が構成され、各チームが独自の名前空間にのみアクセスできるようになります。これにより、チームは互いに分離され、必要なリソースにのみアクセスでき、異なるチームに同じクラスターを使用することでコストを最小限に抑えることができます。
さらに、GKE には、Identity and Access Management(IAM)とロールベースのアクセス制御(RBAC)の 2 つのアクセス制御システムがあります。IAM は、Google Cloud リソースの認証と承認を管理するための Google Cloud のアクセス制御システムです。IAM を使用して、GKE リソースと Kubernetes リソースへのアクセス権をユーザーに付与します。RBAC は Kubernetes に組み込まれており、クラスター内の特定のリソースと操作に対してきめ細かなアクセス許可を付与します。
インサイト - https://cloud.google.com/architecture/prep-kubernetes-engine-for-prod#roles_and_groups
</div></details>

### Q.  問題18: 回答
組織は Helm を使用してコンテナー化されたアプリケーションをパッケージ化しています。アプリケーションは、公開チャートとプライベートチャートの両方を参照します。セキュリティ チームは、パブリック Helm リポジトリを依存関係として使用することはリスクであるとフラグを立てました。ネイティブのアクセス制御と VPC Service Controls を使用して、すべてのチャートを一元的に管理する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明パブリックチャートとプライベートチャートの両方を一様に管理し、ネイティブアクセス制御を実装し、VPC Service Controls を採用するという要件を考えると、最も適切なソリューションは次のとおりです。
A. アーティファクト・レジストリを使用して、パブリック・チャートとプライベート・チャートをOCI形式で格納します。
説明：
1. アーティファクト レジストリ: Google Artifact Registry は、コンテナ イメージと Helm チャートを保存するための OCI(Open Container Initiative)形式をサポートしています。これにより、パブリックとプライベートの両方の Helm チャートを管理リポジトリに格納できます。
2. 統一された管理: アーティファクト レジストリは、Helm チャートを管理するための一元的な場所を提供し、パブリック チャートとプライベート チャートの両方の一貫性とアクセスの容易さを保証します。
3. ネイティブ アクセス制御: Artifact Registry は Google Cloud IAM(Identity and Access Management)と統合されているため、きめ細かなアクセス制御を設定して、Artifact Registry 内に保存されているチャートにアクセスして変更できるユーザーを管理できます。
4. VPC Service Controls: VPC Service Controls を適用して、指定した境界内のアクセスと通信を保護し、保存されたチャートのセキュリティ レイヤーを追加できます。
オプションAは、HelmチャートをOCI形式で保存するための管理リポジトリを提供し、IAMを介してネイティブアクセス制御を提供し、セキュリティを強化するためのVPC Service Controlsの実装を可能にすることで、要件とよく一致します。
インサイト - https://cloud.google.com/artifact-registry/docs/helm
</div></details>

### Q.  問題19: 回答
これで、Web ベース アプリケーションの新機能を運用環境にデプロイする準備が整いました。Google Kubernetes Engine(GKE)を使用して、ウェブサーバーポッドの半分に対して段階的なロールアウトを実行したい。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明ウェブベースのアプリケーションの新機能を Google Kubernetes Engine(GKE)上のウェブサーバーポッドの半分にデプロイするための段階的なロールアウトを実行する場合、最適なオプションは次のとおりです。
A. パーティション分割されたローリング更新を使用します。
パーティション分割されたローリング更新を使用すると、ポッドを段階的に更新し、特に新機能のロールアウトのレプリカの割合をターゲットにすることで、制御されたデプロイを実行できます。この方法では、ポッドの一部を更新し、新しいバージョンの安定性に自信が持てるまで残りを変更しないでおくことができます。
Kubernetes Deploymentのパーティション化されたローリング更新機能を使用すると、更新するポッドの割合または数を指定することで、ロールアウトプロセスを制御できます。これにより、ポッドの一部のみが新しいバージョンに置き換えられるため、段階的なデプロイを実行し、すべてのポッドにデプロイする前に新機能の動作を監視できます。
これは、残りのポッドを更新する前に、制御された方法で Web サーバー ポッドの半分のみに新機能をロールアウトするという要件と一致しています。また、段階的なロールアウト中に問題が発生した場合に、簡単にロールバックできます。
インサイト - https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset#partitioning_rolling_updates
https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#partitioning_a_rollingupdate
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions
</div></details>

### Q.  問題20: 回答
組織では最近、アプリケーション開発にコンテナベースのワークフローを採用しました。チームは、自動化されたビルド パイプラインを介して運用環境の Kubernetes クラスターに継続的にデプロイされる多数のアプリケーションを開発します。セキュリティ監査人は、開発者やオペレーターが自動テストを回避し、承認なしにコードの変更を本番環境にプッシュする可能性があることを懸念しています。承認を強制するにはどうすればよいですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明承認を強制し、Kubernetesクラスタにデプロイされたコンテナベースのワークフローで不正なコード変更が本番環境にプッシュされるリスクを軽減するには、次のソリューションが最も適切なソリューションです。
D. Kubernetes クラスター内でバイナリ承認を有効にし、ビルド パイプラインを構成証明者として構成します。
バイナリ承認は、検証および承認されたコンテナー イメージのみを Kubernetes クラスターにデプロイできるようにするのに役立ちます。バイナリ承認を有効にし、ビルド パイプラインを構成証明者として構成することで、承認されていない、または検証されていないコンテナー イメージがデプロイされないようにする厳格なポリシーを確立します。
その他のオプションの説明:
A. プル要求の承認を必要とする保護されたブランチを使用してビルド システムを構成することは、バージョン管理システム (VCS) のプラクティスです (Git など)。コードのバージョン管理とコラボレーションには不可欠ですが、Kubernetes 環境での本番環境への不正なコードのデプロイを直接防ぐことはできません。
B. アドミッションコントローラーを使用して、受信リクエストが承認されたソースから発信されていることを確認することは有効なセキュリティ対策ですが、Kubernetes環境内の不正なコード変更が本番環境にプッシュされるのを防ぐという特定の懸念に直接対処できない場合があります。
C. Kubernetesのロールベースアクセス制御(RBAC)の活用は、Kubernetesクラスタ内のさまざまなリソースへのアクセスを制御するために重要です。RBAC は、特定のユーザーまたはエンティティへのアクセスを制限するのに役立ちますが、承認されていないコード変更がデプロイされるのを明示的に防ぐことはできません。
セキュリティ監査人が提起した、不正なコード変更が本番環境にプッシュされるという懸念の文脈では、Kubernetes内でバイナリ認証を有効にし、ビルドパイプラインをアテスターとして構成することが、承認を強制し、承認されたコンテナイメージのみが本番環境にデプロイされるようにするための最も効果的なアプローチです。
インサイト - https://cloud.google.com/binary-authorization
</div></details>

### Q.  問題21: 回答
機密情報にアクセスする必要があるアプリケーションをデプロイしています。この情報が暗号化され、侵害が発生した場合の漏洩のリスクが最小限に抑えられていることを確認する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明機密情報が暗号化され、侵害が発生した場合の漏洩のリスクを最小限に抑えるには、次のアプローチが最も適切です。
A. 暗号化キーを Cloud Key Management Service(KMS)に保存し、キーを頻繁にローテーションします。
説明：
A. Cloud Key Management Service(KMS)などの安全なサービスに暗号鍵を保存し、これらの鍵を定期的にローテーションすることは、機密情報を保護するためのベスト プラクティスです。KMS は堅牢なキー管理機能を提供し、データを暗号化し、暗号化キーを安全に管理できるようにします。キーを定期的にローテーションすると、キーが侵害された場合の露出ウィンドウが制限されるため、セキュリティが強化されます。
B. インスタンスの作成時に、暗号化された構成管理システムを介してシークレットを挿入すると、ある程度のセキュリティが提供される場合があります。ただし、暗号鍵の保存と管理に Cloud KMS などの専用の鍵管理サービスを使用するほど安全ではない可能性があります。
C. アプリケーションをシングル サインオン (SSO) システムと統合すると、ユーザー認証の管理に役立ちますが、機密情報の暗号化には直接対処しません。SSOシステムは、保存データの暗号化ではなく、主にユーザーアクセスと認証を管理します。
D. 継続的なビルド パイプラインを利用して、アプリケーションのインスタンスごとに複数のバージョンのシークレットを生成することは、機密情報をセキュリティで保護するための最適なアプローチではない可能性があります。セキュリティで保護されたキー管理に重点を置かずにシークレットの複数のバージョンを作成することで、攻撃対象領域が増加する可能性があります。
したがって、暗号化キーを Cloud Key Management Service(KMS)に保存し、キーを頻繁にローテーションする(オプション A)ことが、機密情報を確実に暗号化し、侵害が発生した場合の漏洩のリスクを最小限に抑えるための最適なアプローチです。この方法は、機密データをセキュリティで保護し、暗号化キーを安全に管理するための業界のベスト プラクティスに準拠しています。
インサイト - https://cloud.google.com/security-key-management
</div></details>

### Q.  問題22: 回答
会社では、HTTPS リクエストを使用して、https://booking-engine-abcdef.a.run.app URL でアクセス可能な Cloud Run でホストされる一般公開サービスをトリガーしています。サービスを顧客に公開する前に、サービスの最新リビジョンをテストできる機能を開発者に提供する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Cloud Run でホストされる一般公開サービスの最新リビジョンを顧客に公開する前にデベロッパーにテストしてもらいたい場合、最適なアプローチは次のとおりです。
A. gcloud run deploy booking-engine --no-traffic --tag dev コマンドを実行します。テストには https://dev--booking-engine-abcdef.a.run.app URL を使用します。
説明：
デプロイ中に --no-traffic フラグを使用すると、新しくデプロイされたリビジョンがライブトラフィックを受信しなくなります。
「dev」とタグ付けすることで、開発者はURL https://dev--booking-engine-abcdef.a.run.app を使用してこの特定のタグ付きリビジョンにアクセスし、テストできます。
このアプローチでは、テスト専用の個別の URL が作成され、開発者がライブ サービスに影響を与えることなく最新の変更をテストできる安全な環境が提供されます。
他のオプションが最良の選択ではない理由の説明:
オプション B(gcloud run services update-traffic booking-engine --to-revisions LATEST=1): テスト用の URL を別途作成せずに、トラフィックを最新のリビジョンに誘導するため、メイン URL にアクセスするユーザーにテストされていない変更が公開される可能性があります。
オプション C (ID トークンで curl を使用する): この方法では、ID トークンを渡す必要がありますが、開発者が特定のリビジョンをテストするための個別のテスト URL や環境は提供されません。
オプション D (roles/run.invoker ロールの付与): アクセス許可の付与は不可欠ですが、開発者が特定のリビジョンを安全にテストするための個別のテスト環境や URL は作成されません。
したがって、オプション A は、デベロッパーが分離されたテスト環境で Cloud Run がホストするサービスの最新リビジョンをテストできるようにするための最も適切な選択肢です。
インサイト - https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#deploy-with-tags
https://cloud.google.com/sdk/gcloud/reference/run/deploy
</div></details>

### Q.  問題23: 回答
Compute Engine でアプリケーションを実行し、Stackdriver でログを収集している。個人を特定できる情報(PII)が特定のログエントリフィールドに漏洩していることがわかりました。これらのフィールドが新しいログエントリにできるだけ早く書き込まれないようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明個人を特定できる情報(PII)ができるだけ早く新しいログエントリに書き込まれるのを防ぐための最も迅速で効果的なアプローチは次のとおりです。
A. filter-record-transformer Fluentdフィルタプラグインを使用して、処理中のログエントリからフィールドを削除します。
filter-record-transformer Fluentd フィルタ プラグインは、ログエントリを Stackdriver に送信する前に変更するように設定できます。このプラグインを使用すると、ログレコードを転送中に変更できるため、PII を含む特定のフィールドを Stackdriver Logging に保存される前に、ログエントリから削除または操作できます。
このアプローチにより、PIIがログエントリからリアルタイムで削除され、アプリケーションの変更や追加の処理手順を待たずに、機密情報がログに保存されたり表示されたりすることを防ぎます。
オプションB、C、およびDには、戦略や待機時間が異なるため、すぐには解決できない場合があります。
オプション B (fluent-plugin-record-reformer):この出力プラグインは、ログエントリを変更する可能性がありますが、PIIを特にターゲットにして削除しない可能性があります。
オプションC(アプリケーション開発者の待機中):アプリケーション開発者だけに頼ってアプリケーションにパッチを適用すると、時間がかかり、検証がすぐに行われない可能性があります。
オプション D(ログエントリを Cloud Storage にステージングし、Cloud Function を使用する):これには追加の手順とプロセスが含まれるため、遅延が発生したり、ログエントリでの PII の露出をリアルタイムで防止できなかったりする可能性があります。
したがって、filter-record-transformer Fluentd フィルタ プラグイン(オプション A)を使用することは、Stackdriver Logging に送信される前に機密性の高いフィールドを転送中に削除することで、ログエントリ内の PII の露出を軽減するための最も迅速で直接的なソリューションです。
インサイト - https://docs.fluentd.org/filter/record_transformer
https://cloud.google.com/logging/docs/agent/logging/configuration#modifying_log_records
</div></details>

### Q.  問題24: 回答
あなたの会社は、すべての組織ログを 7 年間保存する必要がある、規制の厳しいドメインで運営されています。マネージド サービスを使用して、ログ記録インフラストラクチャの複雑さを最小限に抑える必要があります。構成ミスや人為的ミスによるログキャプチャや保存されたログの将来的な損失を回避する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明インフラストラクチャの複雑さを最小限に抑え、構成ミスや人為的ミスによる潜在的な損失を回避しながら、すべての組織ログを 7 年間保存するという要件を満たすための正しいオプションは次のとおりです。
B. Cloud Logging を使用して、組織レベルで集約シンクを構成し、7 年間の保持ポリシーとバケット ロックを使用してすべてのログを Cloud Storage にエクスポートします。
ジャスティフィケーション：
組織レベルでの集約シンク: 集約シンクを使用すると、複数のプロジェクトまたはリソースのログを 1 つの宛先に統合できるため、管理が簡素化されます。
保持ポリシーを使用した Cloud Storage: 7 年間の保持ポリシーを使用して Cloud Storage にログを保存すると、指定した期間ログを保持するための規制要件に準拠できます。
バケットロック: Cloud Storage でバケットロックを有効にすると、指定した保持期間内のログの偶発的または意図的な削除や変更が防止され、人為的ミスや設定ミスによるログ損失のリスクが軽減されます。
複雑さの最小化: Cloud Logging と Cloud Storage をマネージド サービスとして利用することで、インフラストラクチャの複雑さが軽減され、設定ミスやログ損失の可能性が減少します。
それでは、間違ったオプションについて説明しましょう。
A. Cloud Logging を使用して組織レベルで集約シンクを構成し、すべてのログを BigQuery データセットにエクスポートします。
BigQuery にログをエクスポートすると分析機能が提供されますが、ログの 7 年間の長期保持は本質的に保証されません。BigQuery のストレージ費用とクエリ機能は、ログの長期保持に関する単純な要件と一致しない可能性があります。
C. Cloud Logging を使用して各プロジェクト レベルでエクスポート シンクを構成し、すべてのログを BigQuery データセットにエクスポートします。
オプション A と同様に、ログを BigQuery にエクスポートすると分析が可能になりますが、ログ保持期間が 7 年という要件に直接対応しないため、異なるプロジェクト間でのログ管理が複雑になる可能性があります。
D. Cloud Logging を使用して各プロジェクト レベルでエクスポート シンクを構成し、7 年間の保持ポリシーとバケット ロックを使用してすべてのログを Cloud Storage にエクスポートします。
このオプションでは、正しいポリシーとバケットロックを使用して Cloud Storage に保持できますが、さまざまなプロジェクトに複数のシンクがあるため、複雑さが増し、設定ミスやログの一元管理が困難になる可能性があります。
</div></details>

### Q.  問題25: 回答
本番環境サービスの一部は、eu-west-1 リージョンの Google Kubernetes Engine(GKE)で実行されています。ビルドシステムは us-west-1 リージョンで実行されます。コンテナー イメージをビルド システムからスケーラブルなレジストリにプッシュして、イメージをクラスターに転送するための帯域幅を最大化する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明正しいオプションは C です。 eu.gcr.io ホスト名を使用してイメージを Google Container Registry(GCR)にプッシュします。
イメージをクラスターに転送するための帯域幅を最大化するには、レジストリをデプロイする必要がある運用環境またはシステムに近づける必要があります。
eu.gcr.io ホスト名を使用して Google Container Registry(GCR)にイメージをプッシュすると、可能な限り最高のネットワーク パフォーマンスで eu-west-1 リージョンの GKE クラスタにイメージを転送できます。これにより、クラスターがレジストリからイメージをプルするときの待機時間が最小限に抑えられ、イメージをクラスターに転送するための帯域幅が最大化されます。
https://cloud.google.com/container-registry/docs/pushing-and-pulling#add-registry-eu.gcr.io
</div></details>

### Q.  問題26: 回答
Cloud Build で CI / CD パイプラインを作成して、アプリケーション コンテナ イメージを構築します。アプリケーション コードは GitHub に格納されています。会社では、運用イメージのビルドをメイン ブランチに対してのみ実行し、変更管理チームがメイン ブランチへのすべてのプッシュを承認する必要があります。イメージのビルドは、できるだけ自動化する必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明運用イメージのビルドがメイン ブランチに対して排他的にトリガーされ、変更管理チームがメイン ブランチへのすべてのプッシュを承認するようにするには、次の手順を実行する必要があります。
C. Cloud Build ジョブでトリガーを作成します。リポジトリのイベント設定を「ブランチにプッシュ」に設定します。
この設定により、いずれかのブランチへのプッシュが発生したときに Cloud Build ジョブがトリガーされます。
D. リポジトリのメインブランチのブランチ保護ルールを設定します。
ブランチ保護ルールは、変更をマージする前に承認を要求したり、合格するためにステータスチェックを要求したりするなど、ブランチに特定の制御と制限を適用するのに役立ちます。
説明：
オプション C では、任意のブランチへのプッシュ時に Cloud Build ジョブがトリガーされます。ただし、運用イメージのビルドがメイン ブランチに対してのみ行われるようにするには、追加の手順が必要です。
オプション D では、メイン ブランチ専用のブランチ保護ルールを設定し、変更をメイン ブランチにマージする前に変更管理チームからの承認を要求するなどの要件を適用できます。
オプション A、B、および E は、運用イメージのビルドをメイン ブランチに制限したり、メイン ブランチへのプッシュの承認を強制したりする要件に直接対処しません。pull request のトリガーを有効にしたり (オプション A)、ファイル フィルターを設定したり (オプション B) しても、運用イメージのビルドがメイン ブランチに特に制限されない場合があり、トリガーで直接承認を有効にしても (オプション E)、ブランチ固有の制御は適用されません。
</div></details>

### Q.  問題27: 回答
ユーザーに深刻な影響を与えたインシデントの事後分析を作成しています。今後、同様のインシデントを防止したい。次の 2 つのセクションのうち、事後分析に含めるべきセクションはどれですか?
(2つ選択してください。
1. 
2. 
3. インシデントの再発を防ぐためのアクション項目の一覧
4. ある。インシデントの根本原因の説明
<details><div>
    答え：
説明今後同様のインシデントを防ぐために事後分析に含める必要がある 2 つのセクションは次のとおりです。
ある。インシデントの根本原因の説明: インシデントの根本原因を理解し、文書化することが重要です。このセクションでは、何が問題になったのか、なぜそれが起こったのか、そしてどのような特定の要因がインシデントの発生に寄与したのかを詳細に分析する必要があります。根本原因を特定することは、今後同様のインシデントを回避するための効果的な予防策を実施するための基本です。
ウ.インシデントの再発を防ぐためのアクション項目の一覧: このセクションでは、インシデントの分析から派生した特定のアクション アイテム、タスク、または推奨事項の概要を説明する必要があります。根本原因に対処し、同様のインシデントの再発を防ぐための実行可能な手順を含める必要があります。これらのアクションアイテムには、プロセスの変更、インフラストラクチャの改善、コードの更新、またはシステムの回復力を高めて再発を防ぐためのその他の関連措置が含まれる場合があります。
セクションB、D、およびEは、通常、同様のインシデントを防ぐことを目的とした効果的な事後分析には推奨されず、必要もありません。
イ.インシデントの原因となった従業員のリスト: 個人を非難することに焦点を当てることは逆効果であり、将来のインシデントの防止に建設的に貢献しません。代わりに、体系的な問題を理解し、改善を実施することに重点を置く必要があります。
D.過去のインシデントと比較したインシデントの重大度に関するあなたの意見: インシデントの重大度を理解することはコンテキストにとって重要ですが、重大度を過去のインシデントと比較するのではなく、再発を防ぐために必要な是正措置に焦点を当てることがより重要です。
E. インシデントの影響を受けたすべてのサービスの設計文書のコピー:ドキュメントは貴重ですが、事後分析ですべての設計ドキュメントを含める必要はないかもしれません。ただし、インシデントの原因となった設計ドキュメントの関連情報を参照して根本原因を説明することはできますが、すべてのドキュメントを含めると過剰になる可能性があります。
</div></details>

### Q.  問題28: 回答
Git ブランチの更新時に Terraform コードをデプロイする Cloud Build ジョブをデプロイしています。テスト中に、ジョブが失敗することに気付きました。ビルドログに次のエラーが表示されます:
バックエンドを初期化しています...
エラー: 既存のワークスペースを取得できませんでした: Cloud Storage のクエリに失敗しました: googleapi: エラー 403
この問題を解決するには、Google が推奨する方法に従ってください。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明このエラー メッセージは、Cloud Storage へのアクセス中に権限に問題があり、特に Terraform の状態ファイルのクエリまたはアクセスに関連していることを示しています。Google が推奨する方法に従ってこの問題を解決するには、次のような適切なアクションを行います。
D. roles/storage.objectAdmin Identity and Access Management(IAM)ロールを状態ファイルバケットのCloud Buildサービスアカウントに付与します。
説明：
このエラーは、Cloud Storage をクエリする際の権限関連の問題を示しており、Cloud Build サービス アカウントに、Terraform 状態ファイルが保存されているストレージ バケットにアクセスするために必要な権限がないことを示唆しています。
Terraform状態ファイルの保存に使用されるバケットのCloud Buildサービスアカウントにroles/storage.objectAdmin IAMロールを付与することで、Cloud Buildサービスアカウントがバケット内のオブジェクトの読み取り/書き込みに必要なアクセス権を付与します。このロールは、Terraformがその状態で操作を実行するために必要なバケット内のオブジェクトの管理を可能にします。
オプション A、B、C は、Cloud Storage の権限関連の問題に直接対処していません。
A. ローカル状態を使用するように Terraform コードを変更することは、インフラストラクチャのデプロイを目的とした Cloud Build ジョブでは実現できない場合があります。ローカル状態を利用すると、チーム メンバー間の同期とコラボレーションに関連する問題が発生する可能性があります。
B. Terraform構成で指定された名前でストレージ・バケットを作成しても、権限関連の問題が解決されない場合があります。この問題は、バケットの不在ではなく、権限に関連しているようです。
C. プロジェクト全体で Cloud Build サービス アカウントにロール/所有者の IAM ロールを付与することは過剰であり、推奨される方法ではありません。必要以上のリソースへの広範なアクセスを提供し、セキュリティリスクを高め、最小特権の原則に違反する可能性があります。
インサイト - https://cloud.google.com/storage/docs/access-control/iam-roles
</div></details>

### Q.  問題29: 回答
明確に定義されたサービス レベル目標 (SLO) でサービスをサポートします。過去 6 か月間、サービスは一貫して SLO を満たし、顧客満足度は一貫して高くなっています。サービスの運用タスクのほとんどは自動化されており、反復的なタスクが頻繁に発生することはほとんどありません。信頼性と展開速度のバランスを最適化しながら、サイト信頼性エンジニアリングのベスト プラクティスに従う必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明このシナリオで B と C 以外のオプションが最適な選択ではない理由について説明します。
A. サービスの SLO をより厳格にする: SLO を厳格にすることは、信頼性を高める方法のように思えるかもしれませんが、そうすると、信頼性とデプロイ速度のバランスが崩れる可能性があります。SLO を厳しくすると、チームに過度のプレッシャーがかかり、燃え尽き症候群になったり、デプロイを急いだりして、長期的には信頼性が損なわれるリスクがあります。
D. 製品チームに新機能よりも信頼性の作業を優先させる: 信頼性の作業を優先させることは不可欠ですが、新機能を犠牲にして信頼性のみに焦点を当てると、サービスの成長と競争力が停滞する可能性があります。信頼性の向上と機能強化のバランスを取ることは、持続的な成功のために不可欠です。
E. サービス レベル インジケーター (SLI) の実装を変更してカバレッジを増やす: SLI カバレッジを増やすことは一般的に有益ですが、スタンドアロン アクションとしてそれを行うと、信頼性と展開速度のバランスが直接最適化されない場合があります。より多くのデータを提供できますが、信頼性とデプロイ速度のトレードオフを管理するための特定のニーズには対応できない可能性があります。
選択したオプションの説明:
B. サービスのデプロイ速度やリスクを高める: このオプションが選択されているのは、現在の設定では、サービスがスムーズかつ確実に実行されていることが示唆されているためです。デプロイ速度の制御と計算された増分を導入することは、信頼性を損なうことなく実現できる可能性があります。段階的な変更により、テストと調整が可能になり、リスクが最小限に抑えられます。
C. 信頼性を高める必要がある他のサービスにエンジニアリング時間をシフトする: 現在のサービスは正常に実行されており、ほとんどのタスクは自動化されているため、信頼性の向上が必要な他のサービスにエンジニアリング リソースを再割り当てすることで、バランスを維持できます。この移行により、インフラストラクチャ全体でリソースが最適に利用されます。
要約すると、オプション B と C は、制約を不必要に厳しくしたり、サービスの現在のパフォーマンスを危険にさらしたりすることなく、信頼性とデプロイ速度のバランスを最適化する戦略を提供するため、推奨されます。
</div></details>

### Q.  問題30: 回答
明確に定義されたサービス レベル目標 (SLO) を持つ大規模なサービスをサポートします。開発チームは、サービスの新しいリリースを週に複数回デプロイします。重大なインシデントによってサービスが SLO を満たさなくなった場合、開発チームは機能の作業からサービスの信頼性の向上に重点を移す必要があります。
重大インシデントが発生する前に何をすべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明重大なインシデントが発生する前に、SLO の失敗が発生した場合に、開発チームが機能の作業からサービスの信頼性の向上に適切に焦点を移すようにするには、次の最善の行動方針があります。
A. すべてのサービス関係者と協力して、適切なエラー予算ポリシーを策定します。
説明：
明確に定義されたエラーバジェットポリシーは、イノベーション(新機能)と信頼性(サービスの安定性)のバランスを維持する上で重要です。このオプションが最も適している理由は次のとおりです。
· エラーバジェット:エラー予算ポリシーの策定は、サービスの中断や停止の許容レベルのしきい値を定義する戦略的なステップです。エラーバジェットポリシーは、ユーザーエクスペリエンスに影響を与えることなく、特定の時間枠内でどの程度のダウンタイムやサービスの低下を許容できるかについて、明確なガイドラインを設定します。
· ステークホルダーの協力:開発チーム、運用チーム、製品チームなど、すべてのサービス関係者との調整は、包括的で合意可能なエラー予算ポリシーを確立する上で不可欠です。このポリシーにより、新機能のリリースとサービスの信頼性の維持の間のトレードオフを全員が理解し、合意できるようになります。
· 信頼性と機能のバランス:エラー予算ポリシーは、作業に優先順位を付けるためのガイドとして機能します。重大なインシデントによってサービスがエラーバジェットを超えると、サービスの信頼性の向上に焦点が移ります。このアプローチにより、サービスの安定性を維持するために、必要に応じてチームの注意が適切に向けられるようになります。
オプション B、C、D は、定義されたしきい値に基づいて機能リリースとサービスの信頼性のバランスを取るという全体的なアプローチに直接対処していません。
· オプションB:機能よりもサービスの信頼性を優先するために製品チームと交渉しても、エラー予算ポリシーに関連する必要な技術的および運用上の側面をカバーできない可能性があります。
· オプションC:リリース頻度を減らすことでリスクを軽減できるかもしれませんが、エラー予算ポリシーの側面や SLO の失敗に対するチームの対応戦略に必ずしも対処できるわけではありません。
· オプションD:SLO の失敗時に新しいリリースを防ぐためにプラグインを追加すると、サービスの信頼性を管理するためのより広範なフレームワークがなければ、開発チームの柔軟性が制限される可能性があります。
したがって、すべてのサービス関係者と協力して適切なエラー予算ポリシーを策定すること (オプション A) は、SLO の失敗に備えるための最も効果的なアクションであり、インシデントを処理し、必要に応じてサービスの信頼性を優先するための適切に構造化されたアプローチを確保します。
インサイト - https://sre.google/workbook/error-budget-policy/
</div></details>

### Q.  問題31: 不正解
実稼働中の Java アプリケーションを分析しています。すべてのアプリケーションには、Cloud Profiler と Cloud Trace がデフォルトでインストールされ、構成されています。どのアプリケーションにパフォーマンス・チューニングが必要かを判断する必要があります。あなたは何をするべきか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Cloud Profiler と Cloud Trace を使用してパフォーマンス チューニングが必要なアプリケーションを特定するには、次の手順を検討する必要があります。
A. アプリケーションの実時間と CPU 時間を調べます。差が大きい場合は、CPU リソースの割り当てを増やします。
これは、CPU がアプリケーションのボトルネックになっているかどうかを特定するのに役立ちます。CPU 時間が実時間よりも大幅に長い場合は、アプリケーションが CPU リソースの待機に多くの時間を費やしていることを示しています。このような場合は、CPU 割り振りを増やすとパフォーマンスの向上に役立つ可能性があります。
B. アプリケーションの実時間と CPU 時間を調べます。差が大きい場合は、メモリ リソースの割り当てを増やします。
同様に、実時間の方が CPU 時間よりも大幅に長い場合は、アプリケーションがメモリからのデータを待機していることを示している可能性があります。メモリ割り当てを増やすと、このようなシナリオでパフォーマンスが向上する可能性があります。
その他のオプション(C、D、E)は、Cloud Profiler と Cloud Trace を使用したパフォーマンス チューニングのニーズを判断するのには適していません。
C. ローカル ディスク ストレージの割り当てを増やしても、実時間や CPU 時間で示されるように、パフォーマンスに直接影響することはありません。ディスク ストレージは、通常、CPU やメモリのパフォーマンスではなく、I/O 操作とストレージ容量に影響を与えます。
D. 待機時間、エラー バジェット、および実時間と CPU 時間のわずかな差は問題を示唆する可能性がありますが、これらのメトリックだけでは、必ずしもパフォーマンス最適化の必要性を特定できるとは限りません。
E. ヒープの使用状況を調べるだけでは、アプリケーションのパフォーマンスの全体像を把握できない場合があります。ヒープ使用率が低いからといって、必ずしも最適化の必要性を示すわけではありません。CPU 時間、実時間、メモリ使用量など、その他の要因も考慮する必要があります。
要約すると、オプション A と B は、Cloud Profiler と Cloud Trace を使用した実時間と CPU 時間の分析に基づいて、パフォーマンス チューニングの恩恵を受ける可能性のあるアプリケーションを判断するための最適な手順です。
インサイト - https://cloud.google.com/profiler/docs/concepts-profiling
</div></details>

### Q.  問題32: 回答
組織では、複数の Google Cloud プロジェクトからのすべてのアプリケーションログを中央の Cloud Logging プロジェクトに保存しています。セキュリティ チームは、各プロジェクト チームのみがそれぞれのログを表示でき、運用チームのみがすべてのログを表示できるというルールを適用したいと考えています。コストを最小限に抑えながら、セキュリティチームの要件を満たすソリューションを設計する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明オプションC、「各プロジェクトチームのログビューを作成し、各プロジェクトチームのアプリケーションログのみを表示します。運用チームに_AllLogs中央ログ プロジェクトのビューへのアクセス権を付与する」というメッセージは、セキュリティ チームの要件に合致する適切なソリューションです。このオプションが正しい理由は次のとおりです。
C. 各プロジェクトチームのログビューを作成し、各プロジェクトチームのアプリケーションログのみを表示します。運用チームに、中央ロギング プロジェクトの _AllLogs ビューへのアクセス権を付与します。
このオプションでは、プロジェクトチームごとに特定のログビューを作成し、各チームがそれぞれのアプリケーションに関連付けられたログにのみアクセスして表示できるようにします。これらのビューを作成することで、ログビューレベルでアクセス制御を適用し、異なるプロジェクトチームのログを分離できます。
運用チームに中央ロギングプロジェクトの_AllLogsビューへのアクセス権を付与することで、すべてのログを可視化し、個々のプロジェクトチームのログのセキュリティを損なうことなく、すべてのプロジェクトのログを表示するという要件を満たすことができます。
他のオプションが正しくない理由:
A. 各プロジェクト チームに、中央ログ プロジェクトのプロジェクト _Default ビューへのアクセス権を付与します。中央ログ プロジェクトの運用チームにログ閲覧者アクセス権を付与します。
中央ログ プロジェクトの _Default ビューへのアクセス権を付与すると、すべてのプロジェクト チームが他のチームのログを表示できるようになり、異なるプロジェクト チーム間でログを分離する要件と矛盾します。
B. プロジェクトチームごとに Identity and Access Management(IAM)ロールを作成し、個々の Google Cloud プロジェクトの _Default ログビューへのアクセスを制限します。中央ロギングプロジェクトの運用チームへの閲覧者アクセス権を付与します。
このオプションには IAM ロールと個々のプロジェクト内のアクセスの制限が含まれますが、プロジェクト間でのログの分離は保証されません。中央ログ プロジェクトの運用チームに閲覧者アクセス権を付与すると、他のプロジェクト チームのログが分離されていないため、他のプロジェクト チームのログを表示できる場合があります。
D. 各プロジェクトチームの BigQuery テーブルにログをエクスポートします。プロジェクトチームにテーブルへのアクセス権を付与します。ログライターに、中央ログ記録プロジェクトの運用チームにアクセス権を付与します。
各チームの BigQuery テーブルにログをエクスポートし、それに応じてアクセス権を付与しても、Cloud Logging 自体でのログの可視性と分離は保証されません。さらに、ログ作成者に中央ログ プロジェクトの運用チームへのアクセス権を付与すると、ログの変更が許可されるため、セキュリティが損なわれる可能性があります。
要約すると、オプションCは、運用チームが中央ログプロジェクト内のすべてのログにアクセスできるようにしながら、各プロジェクトチームのログを分離するという要件に具体的に対処するため、最も適切です。
インサイト - https://cloud.google.com/logging/docs/logs-views
注 - Cloud Logging では、すべてのバケットのビューが自動的に作成され、すべてのログが表示されます。_AllLogs
</div></details>

### Q.  問題33: 回答
Google Kubernetes Engine(GKE)で実行され、ブルー / グリーン デプロイ手法を使用するアプリケーションを管理します。Kubernetes マニフェストの抜粋を以下に示します。
Deployment app-green は、新しいバージョンのアプリケーションを使用するように更新されました。デプロイ後の監視中に、ユーザー要求の大部分が失敗していることに気付きます。テスト環境では、この動作は確認されませんでした。ユーザーに対するインシデントの影響を軽減し、開発者が問題のトラブルシューティングを行えるようにする必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明特定のシナリオでは、デプロイ アプリ グリーンを新しいバージョンのアプリケーションに更新した後、ユーザー要求の大部分が失敗していますが、これはテスト環境では観察されませんでした。ユーザーに対するインシデントの影響を軽減し、開発者が問題のトラブルシューティングを行えるようにするための最善のアクションは次のとおりです。
オプション D: サービス app-svc のセレクタを app: my-app, version: blue に変更します。
説明：
サービスセレクタの変更:トラフィックを特に青のデプロイに転送するようにサービスセレクタを変更することで、問題のあるグリーンの展開から既知の安定バージョン(青の展開)にユーザートラフィックをすぐにリダイレクトできます。
ユーザーへの影響の軽減:このアクションにより、問題のあるデプロイ (新しいバージョンでは app-green が有効) が効果的に分離され、開発者は大多数のユーザーに影響を与えることなく問題の調査とトラブルシューティングを行うことができます。
安定性の維持:トラフィックを最後の既知の安定バージョン(ブルーデプロイ)に転送することで、新しいバージョンの問題に対処している間、ユーザーが作業バージョンにルーティングされるようにします。
他のオプションが適さない理由:
A. 新しいバージョンのアプリケーションを使用するように Deployment app-blue を更新します。
このオプションでは、新しいバージョンがブルー デプロイに導入され、ユーザー要求の失敗という差し迫った問題には対処されません。新しいデプロイで問題が解決しない場合は、問題が悪化する可能性があります。
B. 以前のバージョンのアプリケーションを使用するように Deployment app-green を更新する:
この選択により、以前のバージョンにロールバックすることで問題が直接解決されますが、問題のある状態がデバッグに存在しなくなるため、開発者が新しいバージョンで問題のトラブルシューティングと修正を行う能力が妨げられる可能性があります。
C. サービス app-svc のセレクタを app: my-app に変更します。
バージョンを指定せずにセレクタを変更しても、問題のあるデプロイからトラフィックが特定されるわけではないため、問題に効果的に対処できません。
したがって、オプション D は、トラフィックを最後の既知の安定バージョン(青色のデプロイ)に再ルーティングすることでユーザーへの影響を最小限に抑えると同時に、開発者が新しいバージョンのアプリケーション(app-green でデプロイ)の問題のトラブルシューティングに集中できるようにするための最適な即時アクションです。
インサイト - https://blog.devgenius.io/blue-green-deployment-with-kubernetes-b7595b17fe17
</div></details>

### Q.  問題34: 回答
開発、品質保証 (QA)、および運用の 3 つの異なる環境でシステムを設計しています。各環境は Terraform を使用してデプロイされ、アプリケーション チームがアプリケーションをデプロイできるように Google Kubernetes Engine(GKE)クラスタが作成されます。Anthos Config Management を使用してテンプレートを作成し、各 GKE クラスタにインフラストラクチャ レベルのリソースをデプロイします。すべてのユーザー (インフラストラクチャ オペレーターやアプリケーション所有者など) は GitOps を使用します。コードとしてのインフラストラクチャ (IaC) とアプリケーション コードの両方のソース管理リポジトリをどのように構成する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Anthos Config Management、GitOps を使用し、さまざまな環境(開発、品質保証(QA)、本番環境)を管理しながら、Infrastructure as Code(IaC)とアプリケーション コードの両方のソース管理リポジトリを構築するには、次のオプションが最適です。
オプションB:
クラウド・インフラストラクチャ(Terraform)リポジトリは共有されます。ディレクトリが異なれば、環境も異なります。
GKE Infrastructure(Anthos Config Management Kustomize マニフェスト)のリポジトリは次のように分離されています。ブランチが異なれば、環境も異なります。
アプリケーション (アプリのソース コード) リポジトリは分離されています。ブランチが異なれば、表される機能も異なります。
説明：
クラウド・インフラストラクチャ(Terraform)リポジトリは、異なるディレクトリと共有されます。これにより、Terraformコードを、異なる環境(開発、QA、本番)を表すディレクトリを持つ単一のリポジトリに一元化できます。この一元化は、さまざまな環境のインフラストラクチャ コードの管理とバージョン管理に役立ちます。
GKE Infrastructure(Anthos Config Management Kustomize マニフェスト)リポジトリをブランチで区切ったもの:このアプローチでは、ブランチを使用して Anthos Config Management の構成マニフェストを環境ごとに分離します。これにより、各環境の構成が個別にバージョン管理され、分離と管理が容易になります。
ブランチで区切られたアプリケーション (アプリのソース コード) リポジトリ:アプリのソース コードを、機能ごとにブランチを持つさまざまなリポジトリに分離することで、アプリケーションへの変更が機能ごとに分離されます。このアプローチは、バージョン管理されたブランチを使用して変更とデプロイを管理することで、GitOps プラクティスとよく一致します。
このアプローチにより、インフラストラクチャ、GKE 構成、アプリケーション コードの適切な分離とバージョン管理が可能になり、さまざまな環境や機能の管理が容易になり、分離が強化され、トレーサビリティが向上します。
インサイト - https://cloud.google.com/anthos-config-management/docs/concepts/gitops-best-practices#use-folders
</div></details>

### Q.  問題35: 回答
最近停止したサービスをサポートします。この停止は、新しいリリースによってサービスメモリリソースが枯渇したことが原因でした。ユーザーへの影響を軽減するために、リリースを正常にロールバックしました。これで、停止の事後分析を担当します。事後分析を開発する際には、サイト信頼性エンジニアリングのプラクティスに従う必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明停止後にサイト信頼性エンジニアリング (SRE) のプラクティスに従って事後分析を開発する場合は、学習と再発防止に重点を置くことが重要です。適切なアクションは次のとおりです。
B. 原因の責任者ではなく、インシデントの原因を特定することに重点を置きます。
SRE プラクティスのコンテキストでは、事後分析の主な目的は、停止につながった要因と根本原因を理解することです。誰も責めることのない事後分析は、個人に責任を負わせるのではなく、対処が必要な体系的な問題、プロセスの失敗、または技術的な欠点を明らかにすることを目的としています。
非難の余地のない文化は、オープンな議論を奨励し、個人に対する懲罰的措置を恐れることなく、インシデントに寄与した要因の特定を促進します。このアプローチは、学習環境を促進し、将来の同様のインシデントを防ぐためのコラボレーションを促進します。
提供される他の選択肢(A、C、D)は、誰も責めることのない事後分析の原則に沿っておらず、学習と再発防止に効果的に貢献しない可能性があります。
· A. 停止の再発を避けるのではなく、新機能の開発に重点を置きます。このアプローチでは、障害から学習し、信頼性を向上させることの重要性が無視されています。
· C. 関係するすべてのエンジニアとの個別のミーティングを計画します。誰が新しいリリースを承認し、本番環境にプッシュしたかを判断する: 個人に責任を転嫁することに焦点を当てることは、誰も責めない事後分析の文化に反し、透明性とオープンな議論を妨げる可能性があります。
· D. Git 履歴を使用して、関連するコードのコミットを見つけます。そのコミットを行ったエンジニアが運用サービスで作業できないようにする: この懲罰的なアプローチは、学習と改善を妨げ、システム上の問題やインシデントの根本原因に対処しません。
したがって、推奨されるアプローチは、インシデントの寄与する原因を特定することに集中して (オプション B)、学習を促進し、個人に責任を負わせることなく必要な改善を行うことです。
</div></details>

### Q.  問題36: 回答
Terraform を使用して、Google Cloud 環境にデプロイされたアプリケーションを管理します。アプリケーションは、マネージド・インスタンス・グループによってデプロイされたインスタンスで実行されます。Terraform コードは、CI/CD パイプラインを使用してデプロイされます。マネージド・インスタンス・グループで使用されるインスタンス・テンプレートのマシン・タイプを変更すると、パイプラインはterraform適用ステージで失敗し、次のエラー・メッセージが表示されます:
インスタンス テンプレートを更新し、アプリケーションの中断とパイプラインの実行回数を最小限に抑える必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明このシナリオの正しいオプションは次のとおりです。
D. インスタンステンプレートのライフサイクルブロックでcreate_before_destroyメタ引数をtrueに設定します。
説明：
create_before_destroyはTerraformのメタ引数で、リソースのライフサイクル・ブロックでtrueに設定すると、Terraformが古いリソースを破棄する前に新しいリソースを作成します。
マネージド・インスタンス・グループで使用されるインスタンス・テンプレートを更新するコンテキストでは、create_before_destroyをtrueに設定すると、Terraformは古いインスタンス・テンプレートを削除する前に新しいインスタンス・テンプレートを作成できます。これにより、マネージド・インスタンス・グループを直ちに中断することなく、スムーズな移行を確保できます。
それでは、間違ったオプションと、それらがこのシナリオに適していない理由について説明しましょう。
A. 管理対象インスタンス・グループを削除し、インスタンス・テンプレートの更新後に再作成します。
このアプローチでは、マネージド・インスタンス・グループ全体を削除する必要がありますが、実行中のインスタンスを削除してグループを再作成する必要があり、ダウンタイムが発生するため、アプリケーションに大きな中断が発生します。
B. 新しいインスタンス・テンプレートを追加し、新しいインスタンス・テンプレートを使用するように管理対象インスタンス・グループを更新し、古いインスタンス・テンプレートを削除します。
このアプローチでは、新しいインスタンス・テンプレートを作成し、それを使用するようにマネージド・インスタンス・グループを更新しますが、更新プロセス中に短時間の中断が発生する可能性があり、中断を最小限に抑えるという要件に反します。
C. Terraform状態ファイルから管理対象インスタンス・グループを削除し、インスタンス・テンプレートを更新して、管理対象インスタンス・グループを再インポートします。
このアプローチは、Terraform内のインスタンス・テンプレートなどのリソースを更新するようには設計されていません。これには、Terraform状態からのマネージド・インスタンス・グループの削除、テンプレートの更新、およびインスタンス・グループの再インポートが含まれますが、インスタンス・テンプレート自体の更新の問題には直接対処しません。
</div></details>

### Q.  問題37: 回答
Cloud Build を使用して、アプリケーションをビルドしてデプロイします。データベース資格情報とその他のアプリケーション シークレットをビルド パイプラインに安全に組み込む必要があります。また、開発作業も最小限に抑える必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明開発作業を最小限に抑えながら、データベースの認証情報やその他のアプリケーション シークレットを Cloud Build パイプラインに安全に組み込むには、次の方法が最適です。
D. Cloud Key Management Service(Cloud KMS)を使用してシークレットを暗号化し、Cloud Build のデプロイ構成に含めます。Cloud Build に KeyRing へのアクセス権を付与します。
Cloud KMS を使用すると、データベースの認証情報やアプリケーション シークレットなどの機密データを暗号化し、暗号鍵を管理できます。これらのシークレットを Cloud KMS で暗号化することで、これらの鍵へのアクセスを制御し、Cloud Build パイプラインなどの許可されたエンティティのみがデプロイ中にシークレットを復号して使用できるようにすることができます。
シークレットをリポジトリに直接保存すると、暗号化されていても、リポジトリが侵害された場合にセキュリティ上のリスクが生じる可能性があります。そのため、Cloud KMS などの専用サービスを使用して暗号鍵を管理し、暗号化されたシークレットを安全に保存することは、より堅牢で安全なソリューションです。
Cloud Build に KeyRing へのアクセス権を付与すると、デプロイ プロセス中にシークレットをプレーンテキストで公開したり、セキュリティを損なったりすることなく、シークレットにアクセスして復号するために必要な権限がビルド パイプラインに付与されます。
このアプローチにより、機密データを暗号化しながら、Cloud Build パイプラインなどの承認されたプロセスがデプロイ中に安全にアクセスして利用できるようにすることで、セキュリティを維持できます。
インサイト - https://cloud.google.com/build/docs/securing-builds/use-encrypted-credentials#configuring_builds_to_use_encrypted_data
https://cloud.google.com/build/docs/securing-builds/use-encrypted-credentials
</div></details>

### Q.  問題38: 回答
停止に対するアクションアイテムをポストモダンで作成して割り当てています。停止は終わりましたが、根本原因に対処する必要があります。チームがアクションアイテムを迅速かつ効率的に処理できるようにする必要があります。所有者とコラボレーターをアクションアイテムにどのように割り当てる必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明正しいオプションは [A] です。各アクションアイテムに 1 人のオーナーと必要なコラボレータを割り当てます。
各アクションアイテムには 1 人のオーナーが必要です。
</div></details>

### Q.  問題39: 回答
Google Cloud でのアプリケーションのパフォーマンスが前回のリリース以降に低下している。ダウンストリームの依存関係が原因で、一部の要求の完了に時間がかかる可能性があります。原因を特定するには、アプリケーションの問題を調査する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明Google Cloud のアプリケーションのパフォーマンス低下の問題を調査する場合、ダウンストリームの依存関係が遅延の原因である可能性があると思われる場合、最適なアクションは次のとおりです。
D. アプリケーションで Cloud Trace を構成します。
説明：
クラウドトレース:
Cloud Trace は、アプリケーションのレイテンシをトレースしてプロファイリングできる Google Cloud サービスです。アプリケーションの各部分が要求に応答するのにかかる時間に関する詳細な分析情報を提供します。
これは、アプリケーション内およびその依存関係全体のボトルネック、待機時間の問題、およびパフォーマンスの低下を特定するのに役立ちます。
ダウンストリームの依存関係の遅延がリクエストの完了時間に影響を与えていると思われる場合、Cloud Trace は、アプリケーションまたは外部サービスのどの部分が遅延やレイテンシを引き起こしているかを特定するのに役立ちます。さまざまなサービス間で要求を追跡することで、フローを視覚化し、速度低下が発生している場所を特定できます。
オプション A、B、C は貴重な分析情報を提供する可能性がありますが、分散システムのパフォーマンス関連の問題を調査し、レイテンシの問題を特定するには、Cloud Trace は特にリクエスト パスのトレースと遅延が発生している場所の分析に重点を置いているため、このシナリオに最適です。
</div></details>

### Q.  問題40: 回答
組織では最近、アプリケーション開発にコンテナベースのワークフローを採用しました。チームは、自動化されたビルド パイプラインを通じて運用環境に継続的にデプロイされる多数のアプリケーションを開発します。最近のセキュリティ監査では、本番環境にプッシュされたコードに脆弱性が含まれている可能性があり、仮想マシン (VM) の脆弱性に関する既存のツールがコンテナー化された環境に適用されなくなったことがチームに警告されました。パイプラインを通過するすべてのコードのセキュリティとパッチレベルを確保する必要があります。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明コンテナベースのワークフローでは、パイプラインを通過するコードのセキュリティとパッチレベルを確保するには、コンテナ化された環境の脆弱性に対処するための特定の戦略が必要です。実行する最も適切なアクションは次のとおりです。
A. Common Vulnerabilities and Exposures(CVE)をスキャンして報告するようにContainer Analysisを設定します。
説明：
A. 共通脆弱性識別子 (CVE) をスキャンして報告するように Container Analysis を設定することは、既知の脆弱性についてコンテナー イメージを評価する効果的な方法です。コンテナ分析ツールは、イメージとその依存関係を調べ、見つかった脆弱性を報告し、これらの脆弱性を軽減するための実用的な洞察を提供することができます。Google Cloud には、コンテナ レジストリと統合してイメージをスキャンしてセキュリティ上の問題を検出する Container Analysis などのツールが用意されています。
B. リリース前に常に自身を更新するようにビルド パイプライン内のコンテナーを構成することは、コンテナーを最新のパッチとセキュリティ修正プログラムで最新の状態に保つための良い方法です。ただし、これはコードとその依存関係内の特定の脆弱性に対処しない可能性があります。
C. コンテナはより分離された方法で動作し、独自のファイルシステム、依存関係、およびプロセスを持つため、既存のオペレーティングシステムの脆弱性ソフトウェアをコンテナ内に存在するように再構成しても、それほど効果的ではない可能性があります。VM 用に設計された従来のオペレーティング システムの脆弱性ツールは、コンテナー環境には適していない可能性があります。
D. コンテナーの作成に使用される Docker ファイルに対して静的コード分析ツールを実装することは、Dockerfile 内の潜在的なセキュリティの問題や構成ミスを検出するための優れた方法です。ただし、脆弱性のすべての側面、特にコンテナイメージ自体とその依存関係の脆弱性をカバーしているわけではありません。
したがって、共通脆弱性識別子 (CVE) をスキャンして報告するように Container Analysis を設定する (オプション A) は、コンテナー イメージとそれに関連するコンポーネント内の脆弱性の特定と対処に重点を置いて、コンテナーベースのビルド パイプラインで実行されるコードのセキュリティとパッチ レベルを確保するための最適なアプローチです。
インサイト - https://cloud.google.com/container-analysis/docs/container-analysis
</div></details>

### Q.  問題41: 回答
同じ Google Cloud Platform(GCP)プロジェクト内の Compute Engine で実行される複数の本番環境システムを管理しています。各システムには、専用の Compute Engine インスタンスの独自のセットがあります。各システムの実行にかかるコストを知りたい。あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：
説明同じ GCP プロジェクト内の Compute Engine インスタンスで実行されている各システムに関連する費用を決定するための最も適切なオプションは次のとおりです。
B. すべてのインスタンスに、実行するシステムに固有のラベルを割り当てます。BigQuery の請求、エクスポート、ラベルごとのクエリ費用を設定します。
インスタンスにラベルを割り当て、請求データを BigQuery にエクスポートすると、そのラベルに基づいて費用をフィルタリングして分析できます。この方法では、割り当てられたラベルに基づいて請求データを照会することで、個々のシステムまたはインスタンスのグループに関連するコストを正確に追跡できます。これにより、ラベル付けされた各システムに関連するコストの明確な内訳が提供されます。
オプション A はコストの内訳を示す場合がありますが、システムでフィルター処理する機能がなく、一般的なコストの内訳に基づいているため、各システムほど詳細または固有ではない可能性があります。
オプション C と D では、メタデータや VM 名を使用して費用を追跡しますが、ラベルを割り当てることで、Compute Engine インスタンスで実行されているさまざまなシステムやアプリケーションなど、さまざまな側面で費用を整理、分析するための、よりスケーラブルで柔軟なアプローチが提供されます。
したがって、システムごとに費用を効果的に追跡および分析するには、ラベルと BigQuery 請求のエクスポートを利用するオプション B が最適な方法です。
インサイト - https://cloud.google.com/billing/docs/how-to/export-data-bigquery
https://cloud.google.com/billing/docs/how-to/bq-examples#query-with-labels
</div></details>
