## 1
### Q. 問題1: 回答
Web ベースのアプリケーションの新機能を運用環境にデプロイする準備が整いました。目的は、Google Kubernetes Engine(GKE)を利用して、ウェブサーバーポッドの半分に段階的にロールアウトすることです。
どのようなステップを踏むべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. NoExecute で Node テイントを使用します。
アプリケーション更新の段階的なロールアウトよりも、ノード管理に適しています。
C. 展開仕様のレプリカ セットを使用します。
指定された数のポッド レプリカを確保しますが、段階的なロールアウト戦略は促進しません。
D. 並列ポッド管理ポリシーでステートフル セットを使用します。
通常、ステートフル アプリケーションに使用され、Web サーバーなどのステートレス アプリの段階的なロールアウトには特に使用されません。
正解：
A. パーティション分割されたローリング更新を使用します。
段階的なロールアウトに効果的で、一度に更新されるポッドの数を制御できます。このアプローチは、新機能を段階的にテストおよび監視する場合に最適です。
リンクス：
Kubernetesでのステートフルセットのアップグレード戦略の探索
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions
RollingUpdate のパーティション分割
</div></details>

### Q. 問題2: 回答
Compute Engine でアプリケーションを運用し、ログ収集に Cloud logging を利用している。個人を特定できる情報 (PII) が意図せずに特定のログ エントリ フィールドに表示されていることを確認しました。目的は、これらのフィールドが新しいログエントリに書き込まれるのを迅速に防ぐことです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. fluent-plugin-record-reformer Fluentd 出力プラグインを使用して、実行中のログエントリからフィールドを削除します。
この Fluentd 出力プラグインは、ログ レコードのリフォームに使用されますが、通常、filter-record-transformer と比較して、特定のフィールドを削除するのにはあまり適していません。
C. アプリケーション開発者がアプリケーションに修正プログラムを適用するのを待ってから、ログ エントリが PII を公開していないことを確認します。
このアプローチは事後対応型であり、問題への対処が遅れる可能性があります。パッチを待っても、ログ内の PII の既存の問題に対する即時の解決策は提供されません。
D. ログエントリを Cloud Storage にステージングし、Cloud Functions の関数をトリガーしてフィールドを削除し、Cloud Logging API を介してエントリを Cloud Logging に書き込みます。
ログを Cloud Storage にステージングし、Cloud Functions の関数で処理すると、複雑さが増し、遅延が生じる可能性があります。この方法は、Fluentdプラグインを使用する場合と比較して、リアルタイムのログ処理の効率が低くなります。
正解：
A. filter-record-transformer Fluentdフィルタプラグインを使用して、処理中のログエントリからフィールドを削除します。
このオプションを使用すると、ログデータを Cloud Logging に送信する前に、即座にフィルタリングして変換できます。filter-record-transformer プラグインは、PII を含む特定のフィールドを削除またはマスクするように構成して、機密情報がログに記録されないようにすることができます。この方法は、ログをリアルタイムで処理し、アプリケーションのパッチを待つ必要がないため効率的です。
リンクス：
ログ・レコードの変更
record_transformer
Google Cloud fluentd 出力プラグインの構成
</div></details>

### Q. 問題3: 回答
お客様は、Compute Engine で実行されるアプリケーションを担当します。このアプリケーションは、カスタム HTTP サーバーを使用して、内部 TCP / UDP ロードバランサーを介して他のアプリケーションからアクセスされる API を公開します。現在、ファイアウォール ルールでは、0.0.0.0/0 から API ポートへのアクセスが許可されています。ここで行う作業は、最小のステップ数で API にアクセスする各 IP アドレスをログに記録するように Cloud Logging を設定することです。
最初に何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. VPC で Packet Mirroring を有効にします。
Packet Mirroring はネットワーク トラフィックをコピーして分析用に送信しますが、より複雑で、高度なネットワーク監視とセキュリティ分析の目的で使用されます。APIにアクセスするIPアドレスを単に記録するだけではやり過ぎです。
B. Compute Engine インスタンスに Ops Agent をインストールします。
Ops Agent は、アプリケーションログを含む詳細なログをインスタンスからキャプチャできますが、API にアクセスするネットワークトラフィックや IP アドレスは本質的にログに記録されません。これには、追加のアプリケーションレベルのログ設定が必要になります。
D. サブネットで VPC フローログを有効にします。
VPC フローログは、VPC のネットワークフローデータをキャプチャします。これには API にアクセスする IP アドレスに関するデータが含まれますが、他のすべてのネットワーク フローもキャプチャされるため、API アクセス ログのみに関心がある場合は、必要以上のデータになる可能性があります。
正解：
C. ファイアウォール ルールのログ記録を有効にします。
API ポートへのアクセスを許可する既存のファイアウォール ルールでログ記録を有効にすることで、IP アドレスを含む各接続試行のログをキャプチャできます。この方法は、追加の設定やリソースのオーバーヘッドなしで、関心のある特定のトラフィックを直接ターゲットにします。
リンクス：
ファイアウォール ルールのログ記録
</div></details>

### Q. 問題4: 回答
グローバル HTTP/S Cloud Load Balancer(CLB)によって管理される Google Kubernetes Engine(GKE)で動作するマルチリージョン ウェブサービスのサポートを提供します。従来の理由により、ユーザー要求は、CLB に送信される前に、最初にサードパーティのコンテンツ配信ネットワーク (CDN) によって処理されます。可用性サービス レベル インジケーター (SLI) は CLB レベルで既に実施されていますが、ロード バランサーの構成ミス、CDN の障害、その他のグローバル ネットワークの問題などの潜在的なリスクに対処するために、監視を強化する必要があります。
この新しいSLIはどこに実装すべきでしょうか?(2 つのオプションを選択します。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。アプリケーションサーバーのログ:アプリケーションサーバーのログだけに頼っていては便利ですが、CDNまたはグローバルネットワークレベルでの問題をキャプチャできない可能性があります。これらのログは、主にアプリケーションの内部動作とパフォーマンスに関する洞察を提供し、ユーザーアクセスに影響を与える外部要因ではありません。
C. アプリケーションサーバーからエクスポートされたメトリック:オプションAと同様に、アプリケーション・サーバーからエクスポートされたメトリックは、リソース使用率や応答時間など、サーバーの内部パフォーマンスに重点が置かれます。CDNやアプリケーションサーバーの外部にあるネットワークインフラストラクチャから発生する問題を効果的にキャプチャすることはできません。
D. アプリケーション サーバーの GKE ヘルス チェック:GKE のヘルスチェックは、主に Kubernetes 環境内のアプリケーション サーバーの健全性をモニタリングします。これらは GKE クラスタが正しく機能していることを確認するために重要ですが、CDN やグローバル ネットワーキングに関連する問題を監視するようには設計されていません。
正解：
B. クライアントで直接コーディングされるインストゥルメンテーション:このアプローチにより、CDNの障害やグローバルネットワークの問題による影響など、ユーザーエクスペリエンスを直接測定できます。クライアントをインストルメント化することで、これらの要因がエンドユーザーにどのような影響を与えるかについて、実際のデータを取得できます。
E.シミュレートされたユーザー要求を定期的に送信する合成クライアント: この方法では、ユーザーの動作を模倣する合成クライアントを作成します。これは、ロードバランサーの設定ミスやCDNの障害など、従来の監視では見えない問題を検出するのに役立ちます。
リンクス：
SLO の採用
測定方法を選択する
</div></details>

### Q. 問題5: 未回答
あなたは、今後のサービス開始に向けて Cloud Monitoring SLO を確立する任務を負っています。目標は、サービス要求が 300 ミリ秒未満で、毎月 90% 以上の効率で処理されるようにすることです。
この SLO の適切な指標と評価方法を決定するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. ウィンドウベースの評価方法の待機時間メトリックを選択します。
ウィンドウベースの評価では、時間枠でデータが集計されますが、特定のしきい値に対する個々のリクエストのレイテンシーを追跡するには、それほど正確ではない可能性があります。
C. 要求ベースの評価方法の可用性メトリックを選択します。
可用性メトリクスは、通常、SLO の焦点である要求処理の速度ではなく、稼働時間や正常な応答を測定します。
D. ウィンドウベースの評価方法の可用性メトリックを選択します。
C と同様に、これは待機時間ではなく可用性に重点を置いているため、要求処理時間を測定するという特定の目標とは一致しません。
正解：
A. 要求ベースの評価方法の待機時間メトリックを選択します。
目標は要求処理時間 (300 ミリ秒未満) に重点を置いているため、待機時間メトリックが適切です。
要求ベースの評価方法では、待機時間の目標を達成する個々の要求の割合を測定し、毎月少なくとも 90% の効率を維持するという目標に沿っています。
リンクス：
コンプライアンス期間
要求ベースと Windows ベースの SLO のコンプライアンス
</div></details>

### Q. 問題6: 未回答
Cloud Run にデプロイされたアプリケーションがあり、グローバル ユーザーベースがあります。ユーザーが最も近いリージョンにルーティングされ、レスポンシが低くなるようにするには、Cloud Run をどのように構成すればよいのでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーションを任意の 1 つのリージョンにデプロイし、グローバル外部 HTTP(S) ロード バランサーの背後にあるグローバル エニーキャスト IP アドレスでアプリケーションを提供します。
グローバル エニーキャスト IP を使用しても、アプリケーションを 1 つのリージョンにデプロイすると、すべての要求がその 1 つのインスタンスにルーティングされ、遠く離れたリージョンのユーザーに遅延や輻輳が生じる可能性があります。
C. 各リージョンのリージョンの外部 HTTP(S) ロードバランサーを使用して、大陸ごとに 1 つのリージョンにアプリケーションをデプロイし、すべてのリージョンのロードバランサーに同じグローバル IP アドレスをアタッチします。
大陸ごとに 1 つのリージョンにデプロイすると待機時間が短縮されますが、複数のリージョン ロード バランサーを使用すると複雑で効率が低下する可能性があります。また、ユーザーを最も近いインスタンスにルーティングするための統一されたアプローチも提供されません。
D. アプリケーションを任意の 1 つのリージョンにデプロイし、すべての IP アドレスからのトラフィックを許可するようにフロントエンドで Google Cloud Armor を構成します。
Google Cloud Armor の許可ルールでは、トラフィックをグローバルに許可できますが、1 つのリージョンからすべてのユーザーにサービスを提供すると、選択したリージョンから遠く離れたユーザーのレイテンシが長くなります。Google Cloud Armor は、グローバル トラフィックのレスポンス タイムの最適化ではなく、主にセキュリティに重点を置いています。
正解：
B. アプリケーションを各リージョンにデプロイし、グローバル外部 HTTP(S) ロード バランサーの背後にあるグローバル エニーキャスト IP アドレスでアプリケーションを提供します。
アプリケーションを複数のリージョンにデプロイし、エニーキャスト IP アドレスを持つグローバル外部 HTTP(S) ロードバランサーを使用すると、アプリケーションが実行されている最も近いリージョンにリクエストをルーティングできます。このアプローチにより、世界中のユーザーに低遅延の応答が保証されます。
リンクス：
複数のリージョンからのトラフィックを処理する
コンテナランタイムコントラクト
</div></details>

### Q. 問題7: 回答
Git ブランチが更新されるたびに Terraform コードをデプロイする Cloud Build ジョブをデプロイ中である。テスト中にジョブでエラーが発生し、ビルド ログに次のエラーが示されています。
"Initializing the backend...
Error: Failed to get existing workspaces: querying Cloud Storage failed: googleapi: Error 403"
Google が推奨する方法に従いながらこの問題を解決するには、どうすればよいですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ローカル状態を使用するようにTerraformコードを変更します。
ローカル状態を使用すると、特定のエラーを回避できる可能性がありますが、運用環境での使用はお勧めしません。ベスト プラクティスは、コラボレーションと管理の目的でリモート状態ストレージを使用することです。
B. Terraform構成で指定された名前でストレージ・バケットを作成します。
特定の名前でストレージバケットを作成しても、必ずしもアクセス許可の問題が解決するとは限りません。この問題は、既存の状態ファイルバケットに対するCloud Buildサービスアカウントの権限に関連しています。
C. Cloud Buildサービスアカウントに、プロジェクト内のロール/所有者のIdentity and Access Management(IAM)ロールを付与します。
ロール/所有者のIAMロールをCloud Buildサービスアカウントに割り当てることは、過度に寛容であり、最小権限の原則に反します。通常、特定の必要性がない限り、このような広範なアクセス許可を付与することはお勧めしません。
正解：
D. Cloud Buildサービスアカウントに、状態ファイルバケットのroles/storage.objectAdminIDおよびアクセス管理(IAM)ロールを割り当てます。
これは推奨される方法です。これにより、Cloud Build が状態ファイル バケットを操作するために必要な権限が、過度に広範な権限なしで付与されます。roles/storage.objectAdmin ロールを使用すると、Terraform の状態管理に必要なバケット内のオブジェクトを Cloud Build で管理できます。
リンクス：
Cloud Storage の IAM ロール
</div></details>

### Q. 問題8: 回答
アプリケーションをビルドして Google Kubernetes Engine(GKE)にデプロイするための複数ステップの Cloud Build パイプラインがあります。このパイプラインをサードパーティの監視プラットフォームと統合するには、HTTP POST 要求を介してビルド情報を Webhook に送信します。目標は、この統合に必要な開発作業を最小限に抑えることです。
推奨されるアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Build の各ステップにロジックを追加して、ビルド情報を Webhook に HTTP POST します。
このアプローチでは、Webhook にデータを送信するためにすべてのビルド ステップを変更する必要があります。手間がかかり、エラーが発生しやすく、特に複数のステップを持つ複雑なパイプラインではスケーラブルではありません。
B. Cloud Build のパイプラインの最後に新しいステップを追加して、ビルド情報を Webhook に HTTP POST します。
これには、パイプラインの最後に通知を送信するためのステップを 1 つ追加することが含まれます。A よりも簡単ですが、パイプライン全体の実行後にのみトリガーされるため、中間状態が欠落する可能性があります。
C. Cloud Logging を使用して、Cloud Build のログからログベースの指標を作成します。Webhook 通知タイプでアラートを作成します。
Google Cloud のオペレーション スイート(Stackdriver)を活用して Cloud Build ログをモニタリングし、アラートを設定するのは、どちらかというとモニタリングの目的です。通知をトリガーすることはできますが、ビルドの状態をサードパーティのサービスと継続的に統合するようには設計されていません。
正解：
D. Cloud Build への Cloud Pub/Sub プッシュ サブスクリプションを作成し、PubSub トピックをクラウドビルドして、ビルド情報を Webhook に HTTP POST します。
このユースケースに最適です。Cloud Build は Cloud Pub/Sub と統合されており、ビルドイベントごとに Webhook に自動通知できます。効率的で、追加コードが最小限で済み、リアルタイムのビルド状態の更新が提供されます。
リンクス：
プッシュ サブスクリプション
Cloud Run による Cloud Build の通知
</div></details>

### Q. 問題9: 回答
Cloud Build で CI / CD パイプラインを設定して、アプリケーション コンテナ イメージを構築します。アプリケーション コードは GitHub に格納されています。会社のポリシーでは、運用イメージのビルドはメイン ブランチに対して排他的に実行され、メイン ブランチへのすべてのプッシュは変更管理チームによって承認される必要があります。
イメージのビルド プロセスを可能な限り自動化することを目指しています。あなたは何をするべきか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Build ジョブでトリガーを作成します。リポジトリ イベントの設定を "Pull request" に設定します。
B. トリガーの [含まれるファイル] フィルターに OWNERS ファイルを追加します。
E. トリガーの [承認] オプションを有効にします。
オプション A、B、および E は、特定のシナリオでは役立ちますが、変更管理チームによる承認後にメイン ブランチから構築するという要件に特に適合しません。
正解：
C. Cloud Build ジョブでトリガーを作成します。リポジトリのイベント設定を「ブランチにプッシュ」に設定します。
これにより、指定したブランチ (この場合はメイン ブランチ) に変更がプッシュされたときに、ビルド プロセスが自動的に開始されます。
D. リポジトリのメインブランチのブランチ保護ルールを設定します。
GitHub でブランチ保護ルールを設定することで、メイン ブランチへのすべてのプッシュが、マージされる前に変更管理チームによってレビューおよび承認されるように強制できます。これにより、変更管理に関する会社のポリシーと一致し、承認された変更のみが Cloud Build プロセスをトリガーするようになります。
リンクス：
ビルドトリガーの作成と管理 |Cloud Build ドキュメント |グーグルクラウド
ブランチ保護ルールの管理
</div></details>

### Q. 問題10: 回答
大きなファイルを提供するために Compute Engine インスタンスに依存するサービスを監視しています 。ユーザーからダウンロード速度が遅いという報告があります。Cloud Monitoring ダッシュボードには、VM がピーク時のネットワーク スループットで動作していることが示されます。n2-standard-2
ネットワークのスループットパフォーマンスを向上させるには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. VM にネットワーク インターフェイス コントローラー (NIC) を追加します。NIC を追加すると、1 つの VM で使用できるネットワーク帯域幅を増やすことができますが、Google Cloud は通常、個々の NIC から抽象化されたレベルでネットワーク スループットを処理するため、これはすぐに適用できるソリューションではない可能性があります。
B. Cloud NAT ゲートウェイを実装し、VM に関連付けられたサブネットに接続します。Cloud NAT を使用すると、パブリック IP アドレスを持たない VM をインターネットに接続できますが、すでにピーク容量で動作している VM のネットワーク スループットは向上しません。
D. Ops エージェントをデプロイして、追加の監視メトリックのエクスポートを有効にします。Ops エージェントをデプロイすると、より詳細な監視メトリックを提供できますが、それだけでは VM のネットワーク パフォーマンスは向上しません。
正解：
C. VM のマシンの種類を n2-standard-8 に変更します。マシンタイプをリソース割り当ての高いマシンタイプに増やすと、ネットワークスループットが向上します。Google Cloud VM では、VM のサイズに合わせてネットワーク パフォーマンスがスケーリングされるため、これはスループットを向上させる直接的な方法です。
リンクス：
ネットワーク帯域幅
</div></details>

### Q. 問題11: 未回答
Cloud Run と Cloud Functions でクライアント アプリケーションを構築して実行している。クライアントでは、ログをログサービスにインポートできるように、すべてのログを 1 年間使用できる必要があります。必要なコード変更は最小限に抑える必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Run のすべてのイメージと Cloud Functions のすべての関数を更新して、Cloud Logging とクライアントのロギング サービスの両方にログを送信します。ログの送信に必要なすべてのポートが VPC ファイアウォールで開いていることを確認します。
このアプローチでは、各アプリケーションと関数にデュアル ログ機能を実装するために、大幅なコード変更が必要です。
VPC ファイアウォールで開いているポートを管理してログ送信を有効にすると、複雑さが増し、セキュリティ上の懸念が生じる可能性があります。
保持要件を満たすことはできますが、かなりの運用オーバーヘッドが伴い、コードの変更を最小限に抑えるという目標から逸脱します。
B. Pub/Sub トピック、サブスクリプション、ログ シンクを作成します。すべてのログをトピックに送信するようにログ シンクを構成します。ログを取得するためのトピックへのアクセス権をクライアントに付与します。
Pub/Sub トピックとサブスクリプションをログ シンクで設定すると、アプリケーション コードを変更することなくログがキャプチャされます。
Pub/Sub はリアルタイムのログ ストリーミングには効率的ですが、本質的に長期保存や直接的な保持管理は提供されません。
クライアントは、Pub/Sub からログを積極的に取得し、独自のストレージと保持を管理する必要があるため、クライアント側で複雑さが生じる可能性があります。
C. ストレージ バケットと適切な VPC ファイアウォール ルールを作成します。Cloud Run のすべてのイメージと Cloud Functions のすべての関数を更新して、ストレージ バケット内のファイルにログを送信します。
これには、Cloud Storage バケットにログを送信するためにすべての画像と関数を更新することが含まれますが、これには大幅なコード変更が必要です。
ログ送信のファイアウォール規則を管理すると、セキュリティ上のリスクと追加の管理オーバーヘッドが発生する可能性があります。
このアプローチは、コードの変更を最小限に抑えるという目的から逸脱し、ログ管理の複雑さを増します。
正解：
D. ログ バケットとログ シンクを作成します。ログバケットの保持期間を 365 日に設定します。バケットにログを送信するようにログシンクを設定します。ログを取得するためのバケットへのアクセス権をクライアントに付与します。
このオプションでは、Cloud Logging を利用してログをキャプチャし、ログシンクを利用して Cloud Storage バケットに自動的に送信します。
バケットに 365 日間の保持ポリシーを設定すると、必要な期間ログが保存されます。
ログのキャプチャとエクスポートは、アプリケーション コード内ではなくインフラストラクチャ レベルで処理されるため、コードの変更が最小限に抑えられます。
ログ取得のためにバケットへのクライアントアクセスを提供することは、簡単で安全であり、運用上のオーバーヘッドを最小限に抑えるという要件に合致しています。
要約すると、オプション A と C は目的を達成できますが、大幅なコード変更が必要であり、運用が複雑になります。オプションBは、リアルタイムのログストリーミングソリューションを提供しますが、固有の長期ストレージと保持管理に欠けています。オプションDは、クラウドネイティブなログ管理のベストプラクティスと連携し、コードの変更を最小限に抑え、必要なログ保持を確保する、最も効率的で簡単なアプローチとして際立っています。
リンクス：
https://cloud.google.com/logging/docs/routing/overview#:~:text=URL%3A%20https%3A%2F%2Fcloud
</div></details>

### Q. 問題12: 回答
インフラストラクチャDevOpsエンジニアのチームが拡大しており、インフラストラクチャ管理にTerraformの使用を開始しました。コードのバージョン管理を実装し、他のチーム メンバーとコードを共有する方法が必要です。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Terraformコードを、各バージョン・リリースの子フォルダを含むネットワーク共有フォルダに格納します。全員が異なるファイルで作業するようにします。
バージョン管理された子フォルダを含むネットワーク共有フォルダにTerraformコードを格納すると、効率が低下します。高度なバージョン管理やコラボレーション機能がなく、複数の人が同じファイルで作業する場合、混乱や競合につながる可能性があります。
C. オブジェクトのバージョニングを使用して、Terraform コードを Cloud Storage バケットに格納します。すべてのチームメンバーにバケットへのアクセス権を付与して、ファイルをダウンロードできるようにします。
オブジェクトのバージョニングで Cloud Storage バケットを使用すると、異なるバージョンのファイルを維持できますが、コラボレーションには最適ではありません。ブランチング、プルリクエスト、コードレビュープロセスなど、チーム環境に不可欠な機能が欠けています。
D. Terraform コードを Google ドライブの共有フォルダに保存して、すべてのチーム メンバーのパソコンに自動的に同期されるようにします。新しい各バージョンを識別する命名規則を使用してファイルを整理します。
Google ドライブの共有フォルダにコードを保存すると、自動同期が可能になりますが、コード管理には適していません。適切なバージョン管理、差分表示、および開発チームに必要なその他のコラボレーション機能が欠けています。
正解：
A. Terraformコードをバージョン管理システムに格納します。新しいバージョンをプッシュし、マスターとマージするための手順を確立します。
これが最も効果的なアプローチです。Git のようなバージョン管理システムは、変更の追跡、バージョンの管理、共同作業のための堅牢なツールを提供します。新しいバージョンをプッシュし、マスターブランチとマージするための手順を確立することで、構造化され制御されたプロセスが保証されます。
リンクス：
https://www.terraform.io/docs/cloud/guides/recommended-practices/part3.3.html
</div></details>

### Q. 問題13: 回答
Standard 構成を使用して、大規模な Google Kubernetes Engine(GKE)クラスタ内にステートレス アプリケーションをデプロイしました。このアプリケーションは、複数のポッドを同時に実行し、さまざまなレベルの受信トラフィックを経験します。主な目的は、トラフィックの変動に関係なく一貫したユーザーエクスペリエンスを維持し、クラスター内のリソース使用率を最適化することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. スケジュールに従ってデプロイをスケーリングするように cron ジョブを構成する
スケーリング操作のスケジュール設定は、予測可能な負荷パターンに役立ちますが、予期しないトラフィックの急増やドロップにはうまく適応できない場合があります。
このアプローチでは、トラフィック レベルの変動に必要な動的な応答性が欠けています。
C. Vertical Pod Autoscalerの構成
VPA は、ポッドの CPU とメモリの割り当てを自動的に調整しますが、その数は調整しません。
このオプションは、ポッドの数をスケーリングしないため、変動するトラフィックの処理にはあまり適していません。
D. ノード プールでクラスターの自動スケーリングを構成します。
これにより、ポッドの要求に基づいてクラスター内のノード数が調整されます。
ノード レベルでのリソース使用率の最適化に役立ちますが、HPA のようにポッドの数を直接スケーリングすることはありません。
正解：
B. ポッドの水平オートスケーラーを構成します。
HPA は、CPU 使用率などのリアルタイム メトリックに基づいて、デプロイ内のポッドの数を動的に調整します。
トラフィックレベルが変化するアプリケーションに最適で、リソースを効率的に使用し、ユーザーエクスペリエンスの一貫性を維持します。
このシナリオでは、ポッドの数をスケーリングすることでさまざまなトラフィック レベルに適応するニーズに直接対処するため、オプション B (HPA) が最適です。
リンクス：
ポッドの水平自動スケーリング
</div></details>

### Q. 問題14: 回答
担当する実動システム内で大量の停止が発生しています。これらの停止は、主に 1 分以内に自動的に再起動される異常なシステムが原因でアラートをトリガーします。
この状況に対処し、サイト信頼性エンジニアリング (SRE) のプラクティスに準拠するには、何をすべきでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 関連する SLO を再定義して、エラーバジェットを使い果たさないようにする
サービス レベル目標 (SLO) を調整することで、より現実的になったり、システムの現在の機能に合わせたりすることができます。ただし、これでは停止の原因となる根本的な問題は解決されません。システムの安定性を向上させることよりも、期待値を管理することが大切です。
C. 異なるタイムゾーンのエンジニアにアラートを配信する
このアプローチにより、24時間週7日体制で対応し、アラートに即座に対応することができます。ただし、アラートの根本原因には対処されず、アラートの量が多い場合、分散したチームが圧倒される可能性があります。
D. 各アラートのインシデント レポートを作成する
インシデントの文書化は、事後分析と学習のための優れたプラクティスです。ただし、システムが大量のアラートを生成している場合、これは現実的ではなく、問題を解決するのではなく、文書化タスクでチームを圧倒する可能性があります。
正解：
A. 実用的でないアラートを排除する
この手順により、チームはすぐに注意が必要なアラートや実際の問題を示すアラートに集中できます。実行不可能なアラートは、アラート疲れにつながり、実際のシステムの問題への対処から注意をそらす可能性があります。
オプションAの「実行不可能なアラートを排除する」は、アラート疲れを軽減し、真正かつ重大な問題に注意を向けることに重点を置いているため、最も効果的な即時アプローチです。
リンクス：
SLO に関するアラート
</div></details>

### Q. 問題15: 回答
トラフィックの多いWebアプリケーションを担当し、ホームページの読み込みが速いことを保証することに熱心です。このプロセスを開始するには、ホーム ページ要求の待機時間を測定するサービス レベル インジケーター (SLI) を確立し、許容可能なページ読み込み時間の目標値を 100 ミリ秒に設定します。この SLI を計算するために Google が推奨するアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 要求の待機時間を範囲に分割し、100 ミリ秒でパーセンタイルを計算します。
リクエストのレイテンシーをバケット化することは詳細な分析に役立ちますが、特定のパーセンタイル(100ミリ秒)のみに焦点を当てても、全体的なパフォーマンスを包括的に把握できない場合があります。
B. 要求の待機時間を範囲にバケット化し、中央値と 90 パーセンタイルを計算します。
パフォーマンスのより広いビューを提供しますが、ホームページの読み込みを 100 ミリ秒未満にするという特定の目標に直接一致しない場合があります。
D. 読み込まれたホーム ページ要求の数を 100 ミリ秒未満でカウントし、すべての Web アプリケーション要求の合計数で割ります。
これにより、すべての Web アプリケーション要求が計算に含まれるため、ホーム ページのパフォーマンスに特に重点を置く必要がなくなります。
正解：
C. 読み込まれる時間が 100 ミリ秒未満のホーム ページ要求の数をカウントし、ホーム ページ要求の合計数で割ります。
特定の目標を達成するリクエストの数を直接測定し、設定された目標に対するパフォーマンスを明確かつ焦点を絞った評価を提供します。
リンクス：
SLO の実装
SLO ドキュメントの例
</div></details>

### Q. 問題16: 回答
アプリケーションを Cloud Run にデプロイしている最中で、このアプリケーションを起動するにはパスワードが必要です。組織では、すべてのパスワードを 24 時間ごとにローテーションすることが義務付けられており、アプリケーションでは常に最新のパスワードを使用する必要があります。
ダウンタイムを発生させずにこれを実現するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. パスワードを Secret Manager に保存し、環境変数を使用してシークレットをアプリケーションに送信します。
あなたのケースでは、主に環境変数がインスタンスの起動時に解決されるため、推奨されません。つまり、必要に応じて 24 時間ごとにパスワードをローテーションする場合、環境変数メソッドでは、再起動せずに実行中のアプリケーションのパスワードが自動的に更新されません。これに対し、シークレットをボリュームとしてマウントすると (オプション B)、アプリケーションは最新バージョンのシークレットに継続的にアクセスでき、これは毎日のパスワードローテーションという組織のポリシーに準拠するために重要です。
C. Cloud Build を使用して、ビルド時にアプリケーション コンテナにパスワードを追加します。アーティファクト・レジストリがパブリック・アクセスから保護されていることを確認します。
ビルド時にパスワードを統合すると、頻繁な再構築と再デプロイが必要になり、毎日のパスワードローテーションの効率が悪くなります。
D. パスワードをコードに直接保存します。Cloud Build を使用して、パスワードが変更されるたびにアプリケーションを再構築してデプロイします。
機密情報をコードに直接格納することはセキュリティ上のリスクであり、シークレット管理のベスト プラクティスに反します。また、更新のために頻繁に再デプロイする必要があります。
正解：
B. パスワードを Secret Manager に保存し、シークレットをアプリケーション内のボリュームとしてマウントします。
このアプローチにより、アプリケーションは常に Secret Manager から直接最新バージョンのシークレットにアクセスできます。これは、アプリケーションが再起動せずに最新のシークレットを読み取ることができるため、頻繁にローテーションされるシークレットに特に効果的です。
リンクス：
https://cloud.google.com/run/docs/configuring/services/secrets
</div></details>

### Q. 問題17: 回答
お客様は、Python で記述され、App Engine フレキシブル環境でホストされる取引アプリケーションを担当します。目的は、Cloud Error Reporting(旧称 Stackdriver Error Reporting)に送信されるエラー情報をカスタマイズすることです。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Python 用の Cloud Error Reporting ライブラリをインストールし、Compute Engine VM でコードを実行します。
これはうまくいくかもしれませんが、アプリケーションを App Engine から Compute Engine に移行する必要があるため、現在のインフラストラクチャに合わず、不必要な複雑さが生じる可能性があります。
B. Python 用 Cloud Error Reporting ライブラリをインストールし、Google Kubernetes Engine でコードを実行します。
オプション A と同様に、デプロイ プラットフォームを GKE に移行する必要がありますが、アプリケーションがすでに App Engine 用に最適化されている場合は、このプラットフォームは必要ない可能性があります。
D. Cloud Error Reporting API を使用して、アプリケーションから ReportedErrorEvent にエラーを送信し、適切な形式のエラー メッセージを含むログエントリを Cloud Logging(旧称 Stackdriver Logging)で生成します。
カスタム エラー報告に API を直接利用すると、エラー ログをより詳細に制御できます。ただし、API のより深い統合と理解が必要であり、アプリケーションのエラー処理が複雑になる可能性があります。
正解：
C. Python 用 Cloud Error Reporting ライブラリをインストールし、App Engine フレキシブル環境でコードを実行します。
この手順により、アプリケーション環境内で Cloud Error Reporting を直接統合し、エラー情報を Cloud Error Reporting に送信する前にカスタマイズできます。これは、App Engine フレキシブル環境とアプリケーションの Python 言語とうまく連携します。
リンクス：
https://cloud.google.com/error-reporting/docs/setup/python#app-engine
https://cloud.google.com/error-reporting/docs/setup/app-engine-flexible-environment
</div></details>

### Q. 問題18: 回答
異なる VPC の Google Kubernetes Engine(GKE)クラスタ間の接続を設定しています。クラスタ A のノードがクラスタ B のノードにアクセスできないことが確認されました。この問題はネットワーク構成に関連していると思われ、トラブルシューティングが必要です。ただし、ワークロードとノードへの実行アクセス権はありません。目的は、接続が切断されている特定のネットワーク層を特定することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. クラスタ内のノードにツールボックス コンテナをインストールする クラスタ B へのルートが適切に構成されていることを確認します。
このオプションでは、ツールボックス コンテナーをノードに直接配置してルートを検査します。ただし、ワークロードとノードへの実行アクセス権がないとおっしゃったため、この方法は状況によっては実行できません。それには、あなたが持っていないレベルのアクセスが必要です。
C. デバッグ コンテナを使用して、クラスタ A からクラスタ B へ、およびクラスタ B からクラスタ A へ traceroute コマンドを実行します。
オプション A と同様に、このアプローチでもコンテナーをデプロイし、その中でコマンドを実行する必要があります。tracerouteを実行すると、ネットワークパスと接続が切断される場所を特定するのに役立ちます。ただし、繰り返しになりますが、ワークロードとノードへの実行アクセス権がないため、このオプションは実用的ではありません。
D. 両方の VPC で VPC フロー ログを有効にし、パケット ドロップを監視します。
VPC フローログを有効にし、パケットドロップを監視することは、ネットワークの問題を診断するための有効な方法です。ただし、ネットワーク トラフィックのより一般的なビューを提供するため、特定の GKE クラスタ間の接続のトラブルシューティングには直接役立たない場合があります。このアプローチは、より広範なネットワーク トラフィック分析に適しており、2 つのクラスター間の特定の問題を特定するために広範なフィルタリングと分析が必要になる場合があります。
正解：
B. Network Connectivity Center を使用して、クラスタ A からクラスタ B への接続テストを実行します。
Google Cloud の Network Connectivity Center は、オンプレミスとクラウドの異種ネットワークを作成、接続、管理する方法を提供します。これには、異なる VPC 間でネットワーク構成の問題を特定するのに役立つ接続テストを実行する機能が含まれています。
このツールを使用すると、2 つのエンドポイント間(この場合はクラスタ A とクラスタ B のノード間)のネットワーク パスと構成をテストおよび診断し、接続が切断されている可能性のある場所に関する分析情報を得ることができます。
ネットワーク接続センターを使用して接続テストを実行することは、シナリオに最も適した選択肢であることに変わりはありません。これにより、ノードやワークロードへの実行アクセスを必要とせずに、2 つの GKE クラスタ間の接続の問題を直接テストして診断できます。
リンクス：
接続テストの概要
</div></details>

### Q. 問題19: 回答
アプリケーション サービスは Google Kubernetes Engine(GKE)でホストされます。「altostrat-images」プロジェクト内の一元管理された Google Container Registry(GCR)イメージ リポジトリのイメージのみをクラスタへのデプロイに利用し、開発時間を最小限に抑えます。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 画像を gcr.io/altostrat-images にプッシュするだけの Cloud Build 用のカスタム ビルダーを作成します。
Cloud Build のカスタム ビルダーでは、イメージが特定の GCR リポジトリにのみプッシュされるように強制できますが、他のリポジトリから GKE クラスタへのイメージのデプロイは妨げられません。このオプションは、デプロイを指定されたリポジトリからのイメージのみに制限するという要件を満たしていません。
C. デプロイ パイプラインにロジックを追加して、すべてのマニフェストに gcr.io/altostrat-images のイメージのみが含まれていることを確認します。
デプロイメント・パイプラインにカスタム・ロジックを追加すると、指定したリポジトリーのイメージのみが使用されるようになります。ただし、これには追加の開発と保守作業が必要であり、バイナリ承認を使用する場合ほど合理化または一元化されません。このアプローチは効率が悪く、人為的ミスが発生しやすくなります。
D. gcr.io/altostrat-images の各イメージにタグを追加し、イメージの展開時にこのタグが存在することを確認します。
GCR リポジトリ内のイメージにタグを付け、デプロイ中にこれらのタグを検証することで、承認されたイメージのみがデプロイされるようにすることができます。ただし、この方法は、バイナリ承認を使用するよりも安全性が低く、面倒です。手動チェックが必要であり、バイナリ承認のような一元化された適用メカニズムは提供されません。
正解：
B. ホワイトリスト名パターン gcr.io/altostrat-images/ を含むバイナリ認証ポリシーを使用します。
Binary Authorization は GKE と統合してデプロイ時のセキュリティ制御を適用し、信頼できるコンテナ イメージのみがデプロイされるようにする GCP サービスです。特定の GCR リポジトリ(つまり、gcr.io/altostrat-images/)をホワイトリストに登録するバイナリ認証ポリシーを作成することで、このリポジトリのイメージのみをデプロイできるようにすることができます。このアプローチは、最小限の追加開発作業で安全で信頼できるデプロイを確保するという目標に直接合致しています。
リンクス：
ポリシーの例
Cloud Build パイプラインで Binary Authorization 構成証明を作成するCreate a Binary Authorization attestation in a Cloud Build pipeline
</div></details>

### Q. 問題20: 未回答
ロードバランサーを使用せずにHTTPエンドポイントを提供するアプリケーションを監視する責任があります。HTTP応答のレイテンシーは、ポジティブなユーザーエクスペリエンスを確保する上で重要な役割を果たします。すべてのユーザーが経験する HTTP レイテンシに関する分析情報を取得し、この目的のために Cloud Monitoring を利用することを目指しています。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
オプションA: 積み上げ棒グラフは特定の指標を視覚化できますが、応答時間の範囲と分散に関する洞察を提供するため、遅延は分布を使用するとより効果的に表されます。
オプションB:累積メトリクスでは、グラフが増加し続けるため、個々のレイテンシーインスタンスの追跡には適していません。HTTP 応答時間の動作を正確に反映することはできません。
オプションD:METRIC_KIND_UNSPECIFIED の使用は、待機時間などの特定の測定には適していません。レイテンシーメトリクスを効果的に監視および分析するために必要な精度が不足しています。
正解：
オプションC:レイテンシーは通常、分布として測定され、過去の 99 パーセンタイルよりも遅いリクエスト数を特定するなど、さまざまなパーセンタイルを分析できます。このアプローチにより、レイテンシのパフォーマンスを包括的に把握できます。
リンクス：
https://cloud.google.com/monitoring/api/v3/kinds-and-types
https://sre.google/workbook/implementing-slos/
https://cloud.google.com/architecture/adopting-slos/
</div></details>

### Q. 問題21: 回答
最近、Google Kubernetes Engine(GKE)にアプリケーションをデプロイし、新しいバージョンのアプリケーションをリリースする必要があります。新しいバージョンに問題がある場合に備えて、以前のバージョンのアプリケーションに即座にロールバックする機能が必要です。
どのデプロイ モデルを使用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ローリング デプロイを実行し、デプロイの完了後に新しいアプリケーションをテストします。
ポッドを新しいバージョンのアプリケーションで段階的に更新し、古いバージョンを段階的に置き換えます。この方法では、デプロイ中もサービスが引き続き使用できるため、ダウンタイムが最小限に抑えられます。ただし、新しいバージョンで問題が発生した場合、ロールバックはすぐには行われず、複雑になる可能性があります。このプロセスにはロールアウトの取り消しが含まれますが、これには時間がかかり、即時のフォールバックが重要なシナリオには適していない可能性があります。
B. A/B テストを実行し、デプロイの完了後にアプリケーションを定期的にテストします。
アプリケーションの 2 つ以上のバージョンが同時にデプロイされ、それらの間でトラフィックが分割される戦略です。このモデルは、通常、新機能や変更に対するユーザーの応答をテストするために使用されます。これは、迅速なロールバック機能よりも、データと分析情報の収集に関するものです。新しいバージョンで問題が発生した場合、古いバージョンへの即時の切り替えはA/Bテストの主な焦点ではないため、デプロイの問題に迅速に対応する必要がある状況にはあまり理想的ではありません。
C. カナリア デプロイを実行し、新しいバージョンがデプロイされた後、新しいアプリケーションを定期的にテストします。
新しいバージョンが最初に少数のユーザーにリリースされます。この最初のリリースが成功すると、新しいバージョンがユーザーベース全体にロールアウトされます。このアプローチの利点は、最初の小規模なリリース中に問題を特定して修正できるため、すべてのユーザーに一度に影響する誤った更新のリスクが軽減されることです。ただし、A/B テストと同様に、カナリア デプロイでは即時ロールバックは重視されません。新しいバージョンで問題が発生した場合、ユーザー ベース全体で古いバージョンにロールバックしても、必要なほどすぐには実行されません。
正解：
D. ブルー/グリーン デプロイを実行し、デプロイの完了後に新しいアプリケーションをテストします。
2 つの同一の環境 (1 つはアプリケーションの現在のバージョン (青) を実行し、もう 1 つは新しい (緑) バージョンを実行します。グリーンバージョンを展開すると、トラフィックはブルーからグリーンに切り替わります。新しいバージョンで問題が発生した場合、トラフィックを即座にブルー環境に戻すことができます。このアプローチでは、古いバージョンが引き続き動作し、必要に応じて迅速に復元できるため、ダウンタイムとリスクが大幅に最小限に抑えられます。主な欠点は、両方の環境が同時に実行されるため、2 倍のリソースが必要になることです。それにもかかわらず、特に即時ロールバックの場合、それが提供する制御と安全性のレベルは、多くの場合、追加のリソース要件を正当化します。
リンクス：
アプリケーションのデプロイとテストの戦略
</div></details>

### Q. 問題22: 回答
チームは、開発、ステージング、本番環境の 3 つの Google Kubernetes Engine(GKE)環境にアプリケーションをデプロイします。GitHub リポジトリを信頼できる情報源として利用します。目的は、3 つの環境間で一貫性を確保しながら、ネットワーク ポリシーを適用し、それらの環境内のすべての GKE クラスタにロギング DaemonSet をデプロイするための Google 推奨プラクティスに従うことです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Google Cloud Deploy を使用して、ネットワーク ポリシーと DaemonSet をデプロイします。Cloud Monitoring を使用して、ネットワーク ポリシーと DaemonSet がリポジトリ内のソースからずれた場合にアラートをトリガーします。
デプロイとドリフト検出に Google Cloud Deploy と Cloud Monitoring を使用しますが、プロアクティブな適用はありません。
B. Google Cloud Deployを使用してDaemonSetをデプロイし、Policy Controllerを使用してネットワークポリシーを設定します。Cloud Monitoring を使用してリポジトリ内のソースからのドリフトを検出し、Cloud Functions を使用してドリフトを修正します。
ネットワーク ポリシーには Google Cloud Deploy と Policy Controller を組み合わせ、ドリフトの検出と修正には Cloud Monitoring と Cloud Functions を使用します。このオプションは近いですが、DaemonSet のポリシー・コントローラーの使用は指定されません。
C. Cloud Build を使用して、ネットワーク ポリシーと DaemonSet をレンダリングしてデプロイします。Config Sync をセットアップして、3 つの環境の設定を同期します。
Cloud Build と Config Sync を利用して構成のレンダリング、デプロイ、同期を行いますが、アクティブなポリシーの適用はありません。
正解：
D. Cloud Build を使用して、ネットワーク ポリシーと DaemonSet をレンダリングしてデプロイします。ポリシーコントローラを設定して、3 つの環境の設定を適用します。
レンダリングとデプロイには Cloud Build を、すべての環境に構成を適用するには Policy Controller を利用し、自動化とポリシー適用のバランスを取ります。
オプション D は、複数の GKE 環境で一貫性を維持し、ポリシーを適用するための最も強力な選択肢です。
リンクス：
構成コントローラーの概要
</div></details>

### Q. 問題23: 不正解
サードパーティ製アプリケーションが正しく機能するには、サービス アカウント キーが必要です。ただし、クラウド プロジェクトからキーをエクスポートしようとすると、次のようなエラー メッセージが表示されます。'The organization policy constraint iam.disableServiceAccountKeyCreation is enforced.'
目標は、Google が推奨するセキュリティ対策を遵守しながら、サードパーティ製アプリケーションが適切に機能することを確認することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. デフォルトのサービス アカウント キーを有効にし、キーをダウンロードします。
既定のサービス アカウント キーの使用に関するセキュリティ上の懸念があるため、通常はお勧めできません。
B. 組織レベルで iam.disableServiceAccountKeyCreation ポリシーを削除し、キーを作成します。
これにより、サービス アカウント キーを作成できますが、このポリシーを削除すると、組織全体のセキュリティ制御が弱くなる可能性があり、理想的ではありません。
C. プロジェクトのフォルダーでサービス アカウント キー作成ポリシーを無効にし、既定のキーをダウンロードします。
オプション A と同様に、これには既定のサービス アカウント キーを使用し、より高いレベルでポリシーを変更することが含まれ、セキュリティに影響を与える可能性があります。
正解：
D. iam.disableServiceAccountKeyCreation ポリシーを off に設定するルールをプロジェクトに追加し、キーを作成します。
これには、特定のプロジェクトで iam.disableServiceAccountKeyCreation ポリシーを "off" に設定するルールを追加することが含まれ、より広範な組織のセキュリティポリシーを維持しながらサービスアカウントキーを作成できるようにします。このターゲットを絞ったアプローチは、機能を有効にすることとセキュリティプラクティスを順守することのバランスと見なされています。
リンクス：
サービス アカウントでは GCP マネージド キーのみを使用する必要がある
</div></details>

### Q. 問題24: 回答
現在、組織の Google Cloud プロジェクトの Cloud Monitoring 指標の表示を計画中です。組織は 3 つのフォルダーで構成されており、それぞれに次の 2 つのプロジェクトが含まれています。
目標は、指定した 1 つのフォルダ内のプロジェクトの指標のみを表示する Cloud Monitoring ダッシュボードを構成することです。これらのダッシュボードでは、他の 2 つのフォルダーにあるプロジェクトからメトリックを除外することが不可欠です。Google が推奨するベスト プラクティスを遵守しながらこれを実現するには、次のことを行う必要があります。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 新しいスコープ プロジェクトを 1 つ作成します。
このオプションでは、Google Cloud で新しいプロジェクトを 1 つ作成し、構成を監視するための中心点として使用します。ただし、このアプローチでは、スコープ プロジェクトでは本質的に異なるフォルダーのメトリックが区別されないため、フォルダーごとにメトリックを分離するために必要な粒度が得られない可能性があります。
C. 現在の app-one-prod プロジェクトをスコープ プロジェクトとして使用します。
フォルダーの 1 つから既存のプロジェクトをスコープ プロジェクトとして使用すると、メトリックが明確に分離されないため、理想的ではない場合があります。「app-one-prod」プロジェクトの指標は、フォルダーの他の部分の指標と混在しているため、混乱を招き、1つのプロジェクトに固有の問題を特定するのが難しくなる可能性があります。
D. 現在の app-one-dev、app-one-staging、app-one-prod の各プロジェクトを各フォルダーのスコープ プロジェクトとして使用します。
このオプションでは、各フォルダーの既存のプロジェクトをスコープ プロジェクトとして使用することを提案します。これにより、ある程度の分離が可能になるかもしれませんが、それでも、専用のスコーピング プロジェクトが行うようなメトリックの明確な分離はできません。これにより、スコープ プロジェクトのメトリックが、同じフォルダー内の他のプロジェクトのメトリックと混同される可能性があります。
正解：
B. フォルダーごとに新しいスコープ プロジェクトを作成します。
これは推奨される方法であり、各フォルダーの監視スコープとして機能する新しいプロジェクトを作成する必要があります。各スコープ プロジェクトは、対応するフォルダーのリソースとメトリックのみを含むように構成されます。これにより、ダッシュボードが組織構造の特定のセグメント専用になり、監視データがより整理され、分析しやすくなります。
リンクス：
複数のプロジェクトのメトリックスコープを構成する
プロジェクトのスコープ設定に関するベスト プラクティス
</div></details>

### Q. 問題25: 未回答
組織では、大量のデータを処理するフォールト トレラントなバッチ処理アプリケーションを運用し、並列処理のために数百の VM に計算を分散しています。
このデータ処理を大規模に実行するための最も費用対効果の高いソリューションは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. データ処理には、テナントのマシンのみを使用します。
単一テナントの VM は、専用の物理サーバーを提供しますが、Standard VM やスポット VM に比べてコストが高いため、大規模なバッチ処理にはコスト効率が良くありません。
C. 自動修復を有効にしたマネージド・インスタンス・グループを確立します。
自動修復機能を備えたマネージド インスタンス グループでは、VM の正常性は確保されますが、特に標準 VM が使用されている場合は、本質的にコスト効率の高いソリューションは提供されません。
D. アクセラレータ最適化 VM と自動スケーリングを使用してマネージド インスタンス グループを設定します。
マネージド インスタンス グループでの自動スケーリングでは、需要に基づいてリソースが動的に調整されますが、これらの種類の VM は通常より高価であり、ハードウェア アクセラレータを必要とする特定のワークロード用に設計されているため、アクセラレータ最適化 VM の使用はバッチ処理に最もコスト効率の高い選択肢ではない可能性があります。
正解：
A. データ処理にはスポット VM を使用します。
スポット VM は、フォールト トレラントなバッチ処理タスクのための経済的な選択肢です。これらは標準の VM よりも安価であり、フォールト トレラントな環境でよく見られる中断を処理できるワークロードに適しています。
リンクス：
スポット VM
スポット VM の作成と使用
</div></details>

### Q. 問題26: 回答
Cloud Run アプリケーションは、非構造化ログをテキスト文字列として Cloud Logging に書き込みます。非構造化ログを JSON ベースの構造化ログに変換する。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Logging ソフトウェア開発キット(SDK)を使用するようにアプリケーションを変更し、jsonPayload フィールドを含むログエントリを送信します。
実行可能なオプションですが、アプリケーション コードを変更する必要があります。
B. Fluent Bit サイドカー コンテナーをインストールし、JSON パーサーを使用します。
Cloud Run はサイドカー コンテナをサポートしていないため、適用されません。
C. Cloud Run コンテナ イメージにログ エージェントをインストールし、ログ エージェントを使用してログを Cloud Logging に転送します。
Cloud Run はログを Cloud Logging に自動的に転送するため、この処理は不要になります。
正解：
D. ログ テキスト ペイロードを JSON ペイロードに変換するようにログ エージェントを構成します。
ドキュメントに従ってサポートされており、ペイロードを構造化されたJSONログとして処理するようにロギングエージェントを構成できます。この方法では、アプリケーション コードを変更する必要はありません。
リンクス：
ペイロードの処理
</div></details>

### Q. 問題27: 回答
規制の厳しい業界では、会社の業務は厳しい規制の対象となります。セキュリティ チームは、信頼できるコンテナ イメージのみを Google Kubernetes Engine(GKE)にデプロイすることを義務付けています。あなたの仕事は、管理オーバーヘッドを最小限に抑えながら、セキュリティチームの要件を満たすソリューションを実装することです。
これを達成するには、どのようなステップを踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. roles/artifactregistry.writer ロールを Cloud Build サービス アカウントに付与します。アーティファクト・レジストリの書込み権限を持つ従業員がいないことを確認します。
このロールにより、Cloud Build はアーティファクト レジストリにイメージを書き込むことができます。ただし、デプロイできるイメージにポリシーを適用したり、承認されていないイメージのデプロイを防止したりすることはありません。
C. Cloud Run を使用してカスタム バリデーターを作成し、デプロイします。Eventarc トリガーを有効にして、新しい画像がアップロードされたときに検証を実行します。
これはイメージを検証するためのカスタム ソリューションですが、Binary Authorization などの組み込みソリューションを使用する場合と比較して、より多くの管理と開発のオーバーヘッドが必要です。
D. GKE クラスタで実行するように Kritis を設定し、デプロイ時のセキュリティ ポリシーを適用します。
Kritisは、デプロイ時のセキュリティポリシーを適用するためのオープンソースソリューションです。これはバイナリ承認と同様の目的を果たしますが、管理と構成の作業が多くなる可能性があります。
正解：
A. GKE クラスタでバイナリ認証を構成して、デプロイ時のセキュリティ ポリシーを適用します。
Binary Authorization は、デプロイ時のセキュリティ制御を提供する Google Cloud サービスで、信頼できるコンテナ イメージのみが GKE にデプロイされるようにします。GKE とシームレスに統合され、セキュリティ ポリシーの適用を自動化することで管理オーバーヘッドを削減します。
このアプローチは、規制の厳しい業界における安全で自動化されたコンプライアンスに準拠した導入プロセスの必要性に直接対応しています。
リンクス：
Binary Authorization の概要
</div></details>

### Q. 問題28: 回答
会社では、Pub/Sub、App Engine 標準環境、Go でコーディングされたアプリケーションを使用して、IoT データを大規模に処理しています。負荷のピーク時に一貫性のないパフォーマンスの低下が観察され、ローカル ワークステーションでこの問題を再現できませんでした。目的は、運用アプリケーションを継続的に監視して、パフォーマンスへの影響と管理オーバーヘッドを最小限に抑えながら、低速なコード パスを特定することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Monitoring を使用して、App Engine の CPU 使用率指標を評価します。
Cloud Monitoring はリソースの使用状況に関する有用な分析情報を提供しますが、特定のスローコードパスを特定するために必要な詳細なプロファイリング データは提供しません。
B. 継続的プロファイリング ツールを Compute Engine にインストールする。プロファイリング データをツールに送信するようにアプリケーションを構成します。
このアプローチでは、追加のセットアップと管理のオーバーヘッドが必要です。また、App Engine とシームレスに統合されない可能性もあります。
C. アプリケーションインスタンスに対して go tool pprof コマンドを定期的に実行します。フレーム グラフを使用して結果を解析します。
この手動アプローチはプロファイリングには効果的ですが、特に予測不可能なピーク負荷時の継続的な監視には適していません。これは、継続的なパフォーマンス分析よりも、アドホックなプロファイリングに適しています。
正解：
D. Cloud Profiler を構成し、アプリケーションで cloud.google.com/go/profiler ライブラリを初期化します。
Cloud Profiler は、本番環境でのパフォーマンス監視用に設計されています。最小限のオーバーヘッドでプロファイリングデータを継続的に収集します。
Cloud Profiler ライブラリを Go アプリケーションに統合することで、CPU とメモリの使用状況に関する貴重なインサイトを収集し、負荷のピーク時に低速なコードパスを特定して分析することができます。
リンクス：
https://cloud.google.com/profiler/docs/profiling-go#app-engine
</div></details>

### Q. 問題29: 回答
最近、サービスの 1 つが、現在のローリング ウィンドウ期間に割り当てられたエラー バジェットを超えていることを確認しました。一方、あなたの会社の製品チームは、新機能のリリースを間近に控えています。サイト信頼性エンジニアリング (SRE) の原則に従って、この状況でどのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. エラーバジェットの不足をチームに通知し、すべてのテストが成功することを確認して、ローンチがエラーバジェットをさらに危険にさらさないようにします
テストを確実に成功させることは重要ですが、新しい機能よりもシステムの安定性が必要であることを示唆する、使い果たされたエラーバジェットという主な懸念事項には対処していません。
C. 状況をエスカレーションし、追加のエラー予算を要求します。
エラーバジェットは、信頼性とイノベーションのペースのバランスを維持するために定義されます。追加のエラー バジェットを要求することは、SRE の原則と矛盾し、根本的な信頼性の問題は解決されません。
D. 製品に関連する他の指標に目を通し、エラーバジェットが残っている SLO を見つけます。エラーバジェットを再割り当てし、機能の起動を許可します。
異なるSLO間でエラーバジェットをシフトすると、根本的な信頼性の問題が覆い隠され、システムパフォーマンスとユーザーエクスペリエンスの説明責任と現実的な評価を重視するSREのベストプラクティスと一致しません。
正解：
B. エラー予算を使い切ったことをチームに通知します。ローンチの凍結についてチームと交渉するか、ユーザーエクスペリエンスが少し悪化することを許容します。
このアプローチは、エラーバジェットに基づいてリスクを管理する SRE のプラクティスと一致しています。エラーバジェットが枯渇した場合は、システムを安定させるために新機能のリリースを停止するのが賢明です。
ローンチを進めるかどうかは、ユーザーエクスペリエンスとシステムの現在の状態に対する潜在的な影響を考慮して決定する必要があります。
リンクス：
メンテナンス期間がエラーバジェットに及ぼす影響 - SRE のヒント
</div></details>

### Q. 問題30: 未回答
最近 Google Cloud に移行した会社では、組織が Google Cloud 内で新しいプロジェクトや重要なリソースを一貫してプロビジョニングできるように、高速で信頼性の高いソリューションを設計する任務を負っています。
どのような行動を取るべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Google Cloud コンソールを使用して、手動でプロジェクトを作成します。
これは簡単ですが、特に複数のプロジェクトの場合、一貫性のあるプロビジョニングにはスケーラブルでも効率的でもありません。
B. gcloud CLI を使用してスクリプトを開発し、リクエストから必要なパラメータを渡して、そのスクリプトを Git リポジトリに保存します。
これにより、プロセスが自動化され、高速になりますが、状態管理やモジュール性など、コードとしてのインフラストラクチャの利点が十分に発揮されません。
C. Terraformモジュールを作成してソース管理リポジトリに格納し、terraform applyコマンドを実行して新しいプロジェクトを確立します。
信頼性と再現性に優れたInfrastructure-as-Codeソリューションを提供します。スケーラブルで、バージョン管理をサポートしています。
正解：
D. Cloud Foundation Toolkit が提供する Terraform リポジトリを活用し、適切なパラメータを使用してコードを適用し、Google Cloud プロジェクトとその関連リソースを生成します。
Google Cloud 向けに標準化されたベスト プラクティス構成を提供し、一貫性のある信頼性の高いプロジェクトとリソースのプロビジョニングを実現します。
このオプションにより、すべてのプロジェクトとリソースにベストプラクティスを一貫して適用できるため、要件に対して最も堅牢で効率的な選択肢になります。
リンクス：
https://cloud.google.com/docs/terraform/blueprints/terraform-blueprints
</div></details>

### Q. 問題31: 未回答
主サービスに対して半年ごとのキャパシティ プランニングを実施しています。今後 6 か月間、前月比 10% のユーザー増加率を見込んで、十分な準備をすることを目指します。完全にコンテナ化されたサービスは、Google Cloud Platform(GCP)上で動作し、3 つのゾーンにまたがる Google Kubernetes Engine(GKE)スタンダード リージョン クラスタと有効なクラスタ オートスケーラーを利用します。現在、サービスはデプロイされた合計 CPU 容量の約 30% を使用しており、ゾーン障害が発生した場合の回復性を維持することが不可欠です。目標は、この成長と潜在的なゾーン障害にもかかわらず、シームレスなユーザーエクスペリエンスを確保しながら、コストを効果的に管理することです。
予想される成長に対応するために、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B.GKE にデプロイされ、クラスタ オートスケーラーを使用しているため、GKE クラスタは増加率に関係なく自動的にスケーリングされます。
オートスケーラーはスケーリングに役立ちますが、事後対応型であり、予想される成長を先制的に計画することはありません。また、潜在的なゾーン障害にも対処しません。
C. 使用率が 30% にとどまっているため、かなりのヘッドルームがあり、この増加率に合わせて容量を追加する必要はありません。
このアプローチにはリスクが伴います。前月比10%の伸びは、特にゾーン障害時の回復力の必要性を考慮すると、現在の30%の使用率をすぐに上回る可能性があります。
D. 6 か月間の 10% の増加率を考慮して、ノード容量を 60% 増やす前に事前に追加し、ロード テストを実行して十分な容量があることを確認します。
これはプロアクティブなアプローチですが、コスト効率が悪い可能性があります。正確な分析を行わないと、過剰なプロビジョニングにつながる可能性があります。負荷テストは有益ですが、より計算された容量計画に従う必要があります。
正解：
A. ノード プールの最大サイズを確認し、ポッドの水平オートスケーラーを有効にしてから、ロード テストを実行して、予想されるリソースのニーズを確認します。
この方法により、スケーラビリティと効率的なリソース使用率が保証されます。ノード プールの最大サイズを確認することで、クラスターのスケーリング機能を確認できます。ポッドの水平自動スケーリングを有効にすると、負荷の変化に応じてポッドの数が自動的に調整されます。負荷テストでは、予想される成長に対してセットアップを検証し、需要の増加に備えることができます。
リンクス：
ポッドの水平自動スケーリング
</div></details>

### Q. 問題32: 回答
貴社は、サイト信頼性エンジニアリングの原則を遵守しています。ソフトウェアの変更によって引き起こされ、ユーザーに大きな影響を与えたインシデントの事後分析を準備しているところです。今後、このような重大インシデントが発生しないように対策を講じることが第一の目標です。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. インシデントの原因となるエンジニアを特定し、上級管理職にエスカレーションします。
このアプローチは、SRE の中核をなす誰も責めない事後分析の文化に反しています。誰も責めることのない事後分析は、個人的な責任を負わずにインシデントの原因を理解することに重点を置き、罰よりも学習と改善が優先される環境を育みます。
C. 変更を確認した従業員をフォローアップし、今後従うべき慣行を規定します。
インシデントから学ぶことは重要ですが、変更を確認した従業員のみに焦点を当てると、体系的な問題を見逃す可能性があります。個々のプラクティスだけに焦点を当てるのではなく、プロセスとツールの改善を含むより包括的なアプローチは、SRE の原則に沿っています。
D. インシデントが発生した場合に、オンコール チームがすぐにエンジニアと経営陣に電話して行動計画について話し合うことを要求するポリシーを設計します。
インシデント中のコミュニケーションは重要ですが、このオプションは将来のインシデントの防止に直接対処するものではありません。これは、システムやプロセスを改善するためのプロアクティブな予防やインシデントからの学習ではなく、インシデント対応に関するものです。
正解：
B. この種類のエラーをキャッチするテスト ケースが、新しいソフトウェア リリースの前に正常に実行されることを確認します。
このオプションは、テスト プロセスを改善することで、将来の同様のインシデントを防ぐことに重点を置いています。これは、潜在的なエラーを本番環境に移行する前に特定できる堅牢なテストメカニズムの必要性を強調しています。このプロアクティブなアプローチは、障害から学び、システムの信頼性を継続的に向上させることを優先するSREの原則と一致しています。
</div></details>

### Q. 問題33: 未回答
現在、仮想マシン(VM)の使用率を Cloud Logs に保存しており、インタラクティブな VM 使用率ダッシュボードを作成する必要があります。このダッシュボードは、共有が簡単で、リアルタイムの更新を提供し、四半期ごとに集約された情報を表示する必要があります。このタスクには、Google Cloud Platform ソリューションを利用することをお勧めします。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. VM 使用率を Cloud Pub/Sub にエクスポートし、次に SIEM システムにエクスポートします。
ログを Cloud Pub/Sub にエクスポートする: リアルタイムのデータ ストリーミングに適していますが、複雑さが増します。
ダッシュボードにSIEMを使用する:堅牢なセキュリティとイベント管理を提供しますが、外部ツールの統合が必要です。ダッシュボードの作成だけを行うための最も単純なソリューションではありません。
C. BigQuery にエクスポートし、次に Google スプレッドシート for Dashboard にエクスポートします。
BigQuery へのエクスポート: データの保存と分析に効果的です。
Google スプレッドシートへの転送: 手動の手順が導入されます。静的なレポートには適していますが、リアルタイムのデータ視覚化には適していません。
D. Cloud Storage へのエクスポート、カスタム アプリのビルド:
クラウドストレージへのエクスポート:データストレージに適していますが、直接分析ツールは提供していません。
カスタム視覚化アプリの構築: 最大限の柔軟性を提供しますが、かなりの開発リソースが必要です。迅速なセットアップやソフトウェア開発機能のない人には理想的ではありません。
正解：
A. BigQuery へのエクスポート、データポータルでの作成、ダッシュボードの共有:
1.VM 使用率データを Cloud Logs から BigQuery にエクスポートします。
2.BigQuery データを使用して、Google データポータルでインタラクティブなダッシュボードを作成します。
3. ダッシュボードを関係者と共有して、リアルタイムの更新や四半期ごとの集計情報を簡単に取得できます。
この方法では、Google Cloud の強力な分析ツールと可視化ツールを効率的に活用し、外部システムや複雑な手動プロセスを必要とせずに、VM 使用率データを効率的に分析して表示できます。
リンクス：
サポートされている宛先へのログのルーティング
</div></details>

### Q. 問題34: 回答
機密情報へのアクセスを必要とするアプリケーションをデプロイしている最中です。目的は、この情報の暗号化を保証し、侵害が発生した場合の漏洩のリスクを最小限に抑えることです。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. インスタンスの作成時に、暗号化された構成管理システムを介してシークレットを挿入します。
インスタンス作成時のシークレットの挿入は、特に構成管理システムが暗号化されている場合に安全です。ただし、このアプローチは柔軟性が低く、インスタンスが侵害された場合に危険になる可能性があります。
C. アプリケーションをシングル サインオン (SSO) システムと統合し、シークレットをアプリケーションに公開しないでください。
SSOを統合すると、ユーザーアクセスを制御することでセキュリティを強化できますが、アプリケーション内の機密データの暗号化には直接対処できません。
D. アプリケーションのインスタンスごとに複数のバージョンのシークレットを生成する継続的なビルド パイプラインを活用します。
アプリケーション・インスタンスごとに複数のシークレット・バージョンを生成すると、シークレット管理が複雑になり、堅牢なキー管理システムと比較してセキュリティ上の大きな利点が得られない可能性があります。
正解：
A. 暗号化キーを Cloud Key Management Service(KMS)に保存し、キーを頻繁にローテーションします
これは強く推奨されるアプローチです。Cloud KMS は安全な鍵管理を提供し、鍵のローテーションは長期にわたってセキュリティを維持するのに役立ちます。
他のオプションは、特定のコンテキストでは有益である可能性がありますが、機密情報を暗号化し、それらの暗号化キーを安全に管理するという基本的な要件に特に対応していません。
リンクス：
https://cloud.google.com/security-key-management
</div></details>

### Q. 問題35: 回答
組織は、パブリックチャートとプライベートチャートの両方を参照するHelmを使用してコンテナ化されたアプリケーションをパッケージ化します。セキュリティチームは、パブリックな Helm リポジトリを依存関係として使用することをリスクとしてフラグを立てています。ネイティブ アクセス制御と VPC Service Controls を実装しながら、すべてのチャートを統一的に管理するには、何をすべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. GitHub Enterprise と Google Workspace を ID プロバイダーとして使用して、パブリック チャートとプライベート チャートを格納します。
チャートを制御できますが、VPC Service Controls などの Google Cloud のインフラストラクチャとのネイティブ統合がありません。また、パブリックチャートとプライベートチャートの統一管理にも、Artifact Registryほど効果的に対応していません。
C. Git リポジトリを使用してパブリック チャートとプライベート チャートを格納します。リポジトリのコンテンツを Cloud Storage バケットに同期するように Cloud Build を構成します。Helm リポジトリとして https://[bucket].storage-googleapis.com/[helmchart] を使用して、Helm をバケットに接続します。
は、同期と管理のための追加の手順を含む、より複雑なセットアップです。Cloud Storage はセキュリティで保護して Google Cloud と統合できますが、このアプローチは Artifact Registry を使用する場合に比べて合理化されていません。
D. Cloud Storage バケットをストレージ バックエンドとして使用して、Google Kubernetes Engine(GKE)で実行するように Helm チャート リポジトリ サーバーを構成します。
運用が複雑になります。サーバーの管理と可用性とセキュリティの確保は、Artifact Registryなどのマネージドサービスを使用するよりも厳しい場合があります。
正解：
A. アーティファクト・レジストリを使用して、パブリック・チャートとプライベート・チャートをOCI形式で格納します。
アーティファクト レジストリは、組織がコンテナー イメージと言語パッケージ (Helm チャートなど) を管理するための 1 つの場所です。OCI形式のHelmチャートをサポートし、パブリックチャートとプライベートチャートの両方を安全に管理できます。
Artifact Registry を使用すると、Google Cloud のアクセス制御と VPC Service Controls とのネイティブ統合が実現し、アーティファクトのセキュリティとガバナンスが強化されます。
このアプローチにより、Helm チャートを安全でスケーラブルな統合サービスに一元化することで管理が簡素化され、パブリックチャートとプライベートチャートに別々のシステムが不要になります。
リンクス：
Helm チャートの操作
</div></details>

### Q. 問題36: 回答
最近、e コマース アプリケーションの Google Cloud への移行が完了し、トラフィックのピーク シーズンが近づいています。
Google が推奨するプラクティスに確実に従うには、繁忙期に備えるための最初のステップは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーションを Cloud Run に移行し、自動スケーリングを採用します。
これは、コンテナ化が可能で、トラフィックに基づいて自動的にスケーリングされる Cloud Run のサーバーレス モデルのメリットを享受できるアプリケーションに適したアプローチです。ただし、特にアプリケーションが移行されたばかりで、さらなる移行を検討する前に最適化が必要な場合、これは最初の手順ではない可能性があります。
B. アプリケーションの基盤インフラストラクチャの Terraform 構成を開発し、複数のリージョンへの迅速なデプロイを可能にします。
Terraform 構成は、リージョン間でのレプリケーションと管理を容易にするコードとしてのインフラストラクチャ (IaC) のベスト プラクティスですが、ピーク シーズンの準備のための即時のステップではなく、戦略的で長期的な最適化です。
D. 成長を予測して、前のピーク シーズンに使用された追加のコンピューティング リソースを先制的に割り当てます。
このオプションは履歴データに基づいているため、実用的に思えるかもしれませんが、アプリケーションのパフォーマンス特性が前回のピークシーズンから変わっていないことを前提としています。前回のトラフィック急増以降に実装された可能性のある新機能、コード変更、または最適化は考慮されていません。
正解：
C. アプリケーションの負荷テストを実行して、スケーリングのパフォーマンス特性を評価します。
負荷テストは、トラフィックの多い状況でアプリケーションがどのように動作するかを理解するために不可欠です。ボトルネックとパフォーマンスの制限を特定するのに役立ち、ピークシーズン中に増加した負荷を処理するためのスケーリング戦略とリソース割り当てを通知できます。
他のオプションもピーク時のトラフィックを処理するための重要な考慮事項ですが、ロード テストでは、スケーリングとリソースのプロビジョニングについて十分な情報に基づいた決定を下すために必要なデータが提供されるため、通常は最初の手順として推奨されます。
Google のベスト プラクティスでは、スケーリングを決定する前に、負荷がかかった状態でのアプリケーションの動作を理解することの重要性が強調されています。これは通常、負荷テストの実施に関するガイダンスを含む、トラフィックの多いイベントに対するアプリケーションの準備に関する Google Cloud のドキュメントによってサポートされます。
リンクス：
デベロッパーとオペレーターのための Google Cloud での費用の最適化
コンテナの運用に関するベストプラクティス
</div></details>

### Q. 問題37: 未回答
組織では、Google Cloud で多数のビルドを頻繁に実行しています。セキュリティを強化するには、仮想マシン (VM) にパブリック IP アドレスがなく、データ流出から保護されていることを確認する必要があります。スケーラブルでカスタマイズ可能で管理しやすいビルドのソリューションを開発するために Google が推奨するベスト プラクティスは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Cloud Build でビルドをデフォルトのプールを使用して構成し、プライバシーを強化します。
Cloud Build でデフォルト プールを使用するのは簡単に思えるかもしれませんが、パブリック IP アドレスの制限など、VM 設定をカスタマイズする柔軟性に欠けています。このオプションでは、複雑な組織のニーズに必要なレベルのセキュリティとカスタマイズを提供するには不十分です。
C. カスタム VPC で Compute Engine VM を使用し、ビルド環境には Jenkins を使用します。
Compute Engine VM と Jenkins を使用してビルド環境を作成すると、カスタマイズが可能になりますが、作業と管理が大幅に必要になります。このアプローチは、主にビルドのスケーリングと管理の複雑さが増し、非効率になる可能性があるため、Google Cloud の推奨プラクティスに沿っていません。
D. カスタム VPC の Compute Engine スポット VM インスタンスをビルド環境に Jenkins で利用します。
オプション C と同様に、ビルド環境にスポット VM インスタンスと Jenkins を使用する方が、より手間のかかるアプローチです。この方法はコスト面でのメリットもありますが、Google Cloud では推奨されません。これにより、一貫したスケーリングとビルド プロセスの効率的な管理が課題になる可能性があります。
正解：
A. VPC サービス コントロールを使用して Cloud Build にプライベート プールを実装します。
Cloud Build にプライベートプールを実装し、VPC サービス制御と組み合わせることで、セキュリティとカスタマイズのオプションを強化できます。このアプローチにより、VM にパブリック IP アドレスが割り当てられなくなるだけでなく、スケーラビリティと管理の容易さも提供され、Google の推奨プラクティスに合致します。プライベート プールでは、既定のプールで見られる制限とは異なり、よりカスタマイズされた構成が可能です。
リンクス：
プライベートプールの概要
VPC Service Controls の使用
VPC ネットワークでプライベートプールを使用するための環境を設定する
</div></details>

### Q. 問題38: 回答
組織には、現在オンプレミスで実行されているコンテナー化された Web アプリケーションがあります。Google Cloud への移行計画の一環として、次の受け入れ基準を満たすデプロイ戦略とプラットフォームを選択する必要があります。
プラットフォームは、Android デバイスから Android 固有のマイクロサービスにトラフィックを転送できる必要があります。
プラットフォームでは、任意の割合ベースのトラフィック分割を許可する必要があります。
デプロイ戦略では、マイクロサービスの複数のバージョンを継続的にテストできるようにする必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーションのカナリア リリースを Cloud Run にデプロイします。トラフィック分割を使用して、リビジョンタグに基づいてユーザートラフィックの 10% を Canary リリースに転送します。
Cloud Run はリビジョンタグに基づくトラフィック分割をサポートしていますが、重要な要件であるユーザーエージェント ヘッダー(Android デバイスなど)に基づいてトラフィックを転送する特定の機能は提供していません。
B. アプリケーションのカナリア リリースを App Engine にデプロイします。トラフィック分割を使用して、IP アドレスに基づいてユーザー トラフィックのサブセットを新しいバージョンに転送します。
App Engine では、IP アドレスに基づく場合など、トラフィックの分割が可能です。ただし、Android 固有のマイクロサービスにトラフィックを誘導するために必要な、特定のユーザー エージェント ヘッダーに基づいてトラフィックをターゲットにする機能は提供されません。
C. アプリケーションのカナリア リリースを Compute Engine にデプロイします。Compute Engine で Anthos Service Mesh を利用し、仮想サービスを構成することで、ユーザー トラフィックの 10% を Canary リリースに転送します。
Compute Engine の Anthos Service Mesh は堅牢なトラフィック管理を提供しますが、このセットアップは GKE に比べて複雑で、コンテナ化されたウェブ アプリケーションには適していない可能性があります。また、通常、Kubernetes ベースのソリューションよりも管理が複雑になります。
正解：
D. Anthos Service Mesh を使用して Canary リリースを Google Kubernetes Engine にデプロイします。トラフィック分割を使用して、仮想サービスで構成されたユーザ エージェント ヘッダーに基づいて、ユーザ トラフィックの 10% を新しいバージョンに転送します。
Google Kubernetes Engine(GKE)と Anthos Service Mesh を併用すると、パーセンテージベースのトラフィック分割や HTTP ヘッダー(ユーザー エージェントなど)に基づくルーティングなど、高度なトラフィック管理機能を利用できるようになります。このセットアップでは、任意のマイクロサービスの複数のバージョンの継続的なテストがサポートされ、特に Android デバイスから Android 固有のマイクロサービスにトラフィックを転送できます。
リンクス：
Google Cloud で Google Kubernetes Engine(GKE)マルチクラスタ ソリューションを選択する
HTTP トラフィックの分割
</div></details>

### Q. 問題39: 未回答
クラウド チャートを作成して、ワークスペース プロジェクト内のダッシュボードで CPU 使用率を監視しました。目標は、最小権限の原則を遵守しながら、このチャートをサイト信頼性エンジニアリング (SRE) チームとのみ共有することです。
これを達成するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ワークスペースのプロジェクト ID を SRE チームと共有します。SRE チームに、ワークスペース プロジェクトの監視ビューアー IAM ロールを割り当てます。
より広範なアクセス権を付与し、SRE チームがプロジェクト内のすべての監視データを表示できるようにします。より包括的ですが、特定のグラフの共有はあまり対象としていません。
B. ワークスペースのプロジェクト ID を SRE チームと共有します。SRE チームに、ワークスペース プロジェクトのダッシュボード ビューアー IAM ロールを割り当てます。
このロールは Google Cloud には存在しないため、このオプションは無効です。
D. [Share chart by URL] をクリックし、SRE チームに URL を提供します。SRE チームに、ワークスペース プロジェクトのダッシュボード ビューアー IAM ロールを割り当てます。
オプション B と同様に、このオプションは Google Cloud に存在しないロールを参照しているため、無効になります。
正解：
C. 「Share chart by URL」をクリックし、SREチームにURLを提供します。SRE チームに、ワークスペース プロジェクトの監視ビューアー IAM ロールを割り当てます。
特定のチャートの共有に重点を置き、それを表示するために必要な権限を付与するため、要件に最も的を絞った適切なオプションになります。
リンクス：
IAM 基本ロールと事前定義ロールのリファレンス
ロールの監視
</div></details>

### Q. 問題40: 回答
お客様は、Google Cloud Platform(GCP)でホストされているトラフィックの多いウェブアプリケーションを維持する責任があります。あなたの仕事は、エンジニアリング上の変更を行うことなく、ユーザーの視点からアプリケーションの信頼性を評価することです。
どのような手順を踏む必要がありますか?(2 つのオプションを選択します。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 現在のアプリケーション メトリックを確認し、必要に応じて新しいメトリックを追加します。
現在のメトリックを確認することは良い方法ですが、新しいメトリックを追加するには、監視設定の変更が必要であり、関連するスコープとツールによっては、エンジニアリングの変更と見なされる場合があります。
B. コードを変更して、ユーザー操作のための追加情報をキャプチャします。
追加情報を取得するためにコードを変更することは、エンジニアリング上の変更を行わないという規定と直接矛盾します。
C. Web プロキシ ログのみを分析し、各要求の応答時間をキャプチャします。
Web プロキシ ログを分析すると、応答時間に関する貴重な情報が得られますが、範囲は限られています。ユーザーの観点から信頼性を包括的に理解するために不可欠な、アプリケーション全体のユーザーエクスペリエンスやジャーニーをキャプチャするものではありません。
正解：
D. 新しい合成クライアントを作成して、アプリケーションを使用したユーザージャーニーをシミュレートします。
シンセティッククライアントは、ユーザージャーニーとアプリケーションとのインタラクションをシミュレートできます。このアプローチにより、実際のユーザーが経験したアプリケーションのパフォーマンスと信頼性を評価できます。重要なのは、合成クライアントの作成は、アプリケーションのコードを変更する必要のないテストおよび監視アクティビティであるということです。
E. 現在および過去の要求ログを使用して、顧客とアプリケーションとの対話を追跡します。
要求ログを分析すると、アプリケーションに対する実際のユーザー操作に関する分析情報が得られます。これらのログを調べることで、パターンを理解し、一般的な問題を特定し、ユーザーの視点から信頼性を評価できます。このプロセスでは、既存のデータの分析が行われ、アプリケーション自体を変更する必要はありません。
リンクス：
SLO の採用
</div></details>

### Q. 問題41: 未回答
組織では最近、アプリケーション開発用のコンテナーベースのワークフローを実装しました。チームは、自動ビルド パイプラインを介して運用環境の Kubernetes クラスターに継続的にデプロイされる複数のアプリケーションを作成します。セキュリティ監査人は、開発者やオペレーターが自動テストを迂回し、適切な承認なしにコード変更を本番環境に持ち込む可能性について懸念を表明しています。
承認を強制するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. プル要求の承認を必要とする保護されたブランチを使用してビルド システムを構成します。
プル要求の承認を必要とする保護されたブランチは良い習慣ですが、シナリオにとって重要なパイプラインの使用の適用は保証されません。
B. アドミッション・コントローラーを使用して、受信要求が承認されたソースから発信されていることを確認します。
Kubernetesのアドミッションコントローラーは、セキュリティ上の目的でAPIサーバーへのリクエストをインターセプトすることに重点を置いていますが、コードデプロイの承認プロセスを直接強制するものではありません。
C. Kubernetes のロールベースのアクセス制御 (RBAC) を活用して、承認されたユーザーのみにアクセスを制限します。
Kubernetes RBAC はクラスター内のアクセスを制御しますが、特にオペレーターがこれをバイパスできるため、運用環境にプッシュされた変更の承認プロセスは適用されません。
正解：
D. Kubernetes クラスター内でバイナリ承認を有効にし、ビルド パイプラインを構成証明者として構成します。
ビルド パイプラインをアテスターとして使用する Kubernetes のバイナリ承認により、承認および署名されたコードのみが運用環境にデプロイされます。承認を強制する必要性に効果的に対処し、未承認のコードが本番環境にプッシュされるのを防ぎます。
リンクス：
https://cloud.google.com/binary-authorization
</div></details>

### Q. 問題42: 回答
データ ストレージに CloudSQL と Cloud Storage を利用する App Engine 上で動作するウェブ アプリケーションを担当します。Web サイトのトラフィックが一時的に急増した後、すべてのユーザー要求の待機時間が大幅に増加し、CPU 使用率が高くなり、アプリケーション プロセスの数が増加していることに気付きます。最初のトラブルシューティングでは、次のことがわかります。
最初のトラフィックの急増後、負荷レベルが通常に戻ったにもかかわらず、ユーザーは引き続き高い遅延を経験しています。
CloudSQL データベースからのコンテンツと Cloud Storage からの画像に対するリクエストは、どちらも高いレイテンシを示しています。
レイテンシーが増加した前後にWebサイトに変更が加えられませんでした。
ユーザーエラーは増加していません。
今後数日間で Web サイトのトラフィックが再び急増することが予想され、ユーザーが遅延の問題に遭遇しないようにする必要があります。
次のステップは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. GCS バケットをマルチリージョンにアップグレードします。
負荷テストは重要ですが、インフラストラクチャがすでに逼迫している場合は、繁忙期に必要な即時のソリューションではない可能性があります。
B. CloudSQL インスタンスで高可用性を有効にします。
自動スケーリングは重要な戦略ですが、負荷テストまたは履歴データに基づいて正しく調整する必要があります。
C. アプリケーションを App Engine から Compute Engine に移行します。
コンピューティング能力を 2 倍に事前プロビジョニングすることは簡単なアプローチですが、コスト効率が悪く、必要ではない場合があります。
正解：
D. App Engine の設定を変更して、アイドル状態のインスタンスを追加します。
コミュニティでは、App Engine の設定を変更してアイドル状態のインスタンスを追加することを強くサポートしています。これにより、負荷の増加を効果的に処理でき、トラフィックの急増時に遅延の問題が発生する可能性が低くなります。
リンクス：
インスタンスの管理方法
App Engine の app.yaml リファレンス
</div></details>

### Q. 問題43: 回答
会社では、Google Kubernetes Engine(GKE)にデプロイするアプリケーションを開発中です。各チームは、異なるアプリケーションを担当します。あなたの仕事は、コストを最小限に抑えながら、チームごとに個別の開発環境と運用環境を確立することです。さらに、異なるチームが互いの環境にアクセスできないようにすることが重要です。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. チームごとに 1 つの GCP プロジェクトを作成します。各プロジェクトで、Development 用のクラスターと Production 用のクラスターを 1 つずつ作成します。チームにそれぞれのクラスターへの IAM アクセス権を付与します。
これにより、強力な分離が保証されますが、複数のクラスターによるコストが高くなる可能性があります。
B. チームごとに 1 つの GCP プロジェクトを作成します。各プロジェクトで、開発用の Kubernetes 名前空間と運用環境用の Kubernetes 名前空間を持つクラスターを作成します。チームにそれぞれのクラスターへの IAM アクセス権を付与します。
1 つのクラスター内である程度の分離を提供しますが、完全に分離されたクラスターほど安全ではない可能性があります。
C. 開発と本番環境の GKE クラスタを別々のプロジェクトに作成します。各クラスターで、チームごとに Kubernetes 名前空間を作成し、各チームが独自の名前空間にのみアクセスできるように Identity Aware Proxy を構成します。
環境を一元化しますが、同じクラスター内の名前空間では、チーム間の分離が十分でない可能性があります。
正解：
D. 開発と本番環境の GKE クラスタを別々のプロジェクトに作成します。各クラスターで、チームごとに Kubernetes 名前空間を作成し、各チームが独自の名前空間にのみアクセスできるように Kubernetes ロールベースのアクセス制御 (RBAC) を構成します。
堅牢な分離とアクセス制御を提供しながら、費用対効果に優れています。このアプローチは、Kubernetesのマルチテナント環境のベストプラクティスとよく一致しています。
リンクス：
クラスター管理の概要
</div></details>

### Q. 問題44: 回答
組織では、さまざまな Google Cloud プロジェクトのすべてのアプリケーションログが、一元化された Cloud Logging プロジェクトに保存されます。セキュリティ チームには、各プロジェクト チームが自分のログにのみアクセスでき、運用チームのみがすべてのログにアクセスできるというルールを適用する必要があります。
コストを最小限に抑えながらこのセキュリティ要件を満たすには、ソリューションをどのように設計する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 各プロジェクト チームに、中央ログ プロジェクトのプロジェクト _Default ビューへのアクセス権を付与します。中央ロギングプロジェクトの運用チームにトギング閲覧者アクセス権を付与します。
このアプローチでは、プロジェクトごとにログが効果的に分離されないため、チームが他のプロジェクトのログにアクセスできる可能性があります。
B. プロジェクトチームごとに Identity and Access Management(IAM)ロールを作成し、個々の Google Cloud プロジェクトの _Default ログビューへのアクセスを制限します。中央ログ プロジェクトの運用チームへの閲覧者アクセス権を付与します。
すべてのログは個々のプロジェクトログではなく、一元化されたログ記録プロジェクトに保存されるため、これは実現できない場合があります。
D. 各プロジェクトチームの BigQuery テーブルにログをエクスポートします。プロジェクトチームにテーブルへのアクセス権を付与します。ログ作成者のアクセス権を中央ログ プロジェクトの運用チームに付与します。
ログを BigQuery にエクスポートすると分離されますが、BigQuery リソースを使用するため、コストが高くなる可能性があります。
このアプローチはより複雑で、単にログにアクセスするには最も効率的ではない可能性があります。
正解：
C. 各プロジェクトチームのログビューを作成し、各プロジェクトチームのアプリケーションログのみを表示します。運用チームに、中央ロギング プロジェクトの_AllLogsビューへのアクセス権を付与します。
各プロジェクトチームのログビューを作成し、各プロジェクトチームのアプリケーションログのみを表示します。これにより、ログアクセスをきめ細かく制御できます。特定のログビューを作成することで、各プロジェクトチームが一元化された Cloud Logging プロジェクト内の関連するログにのみアクセスできるようにすることができます。
運用チームに、中央ロギング プロジェクトの _AllLogs ビューへのアクセス権を付与します。この設定により、運用チームはすべてのログを包括的に把握できるため、すべてのプロジェクトでログを効果的に監視および管理できます。
このアプローチは、BigQuery などの追加リソースを必要とせずに既存の Cloud Logging インフラストラクチャを利用するため、費用対効果に優れています。また、Google Cloud 内のアクセス制御とデータ セキュリティに関するベスト プラクティスとも一致しています。
リンクス：
バケットレベルのアクセスを設定する
</div></details>

### Q. 問題45: 未回答
小規模な会社で、Cloud Build の CI / CD パイプラインでオープンソースの Java パッケージを利用しているとします。セキュリティチームは、これらのパッケージの潜在的な脆弱性について懸念を表明しています。
本番システムの依存関係のセキュリティを確保するために、どのような費用対効果の高いソリューションを実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 常に最新バージョンの GitHub パッケージを Cloud Build パイプラインにプルしてください。
最新バージョンのパッケージをプルしても、これらのバージョンには未発見のバグや脆弱性が含まれている可能性があるため、セキュリティは保証されません。
C. セキュリティ チームによる検証のために、パッケージを Cloud Source Repositories にプルします。
Cloud Source Repositories でパッケージを手動で検証することは、特に Google Cloud が Assured OSS を通じてキュレーションおよび事前検証済みのオープンソース パッケージを提供している場合、費用対効果が高くありません。
D. オープンソース パッケージをローカルにダウンロードし、Cloud Build でスキャンして、フラグが立てられたパッケージを削除します。
各ビルドですべてのパッケージをローカルにスキャンすると、リソースを大量に消費し、費用対効果が高くありません。さらに、スキャン中にフラグが立てられた重要なパッケージを削除すると、ビルド プロセスが中断される可能性があり、非現実的なアプローチになります。
正解：
B. Cloud Build パイプラインで Assured Open Source Software(Assured OSS)パッケージを使用します。
Google Cloud の安全なパイプラインに構築され、脆弱性を定期的にスキャンする Assured OSS パッケージを利用することで、信頼性と費用対効果の高いソリューションが実現します。これらのパッケージは徹底的なセキュリティチェックを受け、パイプラインに脆弱性を持ち込むリスクを軽減します。
リンクス：
リモートリポジトリを使用した Assured OSS パッケージのダウンロード
リポジトリへの直接アクセスを使用したJavaパッケージのダウンロード
ソフトウェアサプライチェーンを保護
</div></details>

### Q. 問題46: 回答
CI パイプラインを構成中です。CI パイプラインのビルド ステップ(特に統合テスト用)では、プライベート VPC ネットワーク内にある API にアクセスする必要があります。セキュリティチームは、APIトラフィックをパブリックに公開してはならないと規定しています。目的は、管理オーバーヘッドを最小限に抑えるソリューションを実装することです。
どのような行動を取るべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Spinnaker for Google Cloudを使用して、プライベートVPCへの接続を確立します。
Spinnakerは強力な継続的デリバリープラットフォームですが、この特定のニーズに必要以上に複雑になる可能性があります。
C. Cloud Build をパイプライン ランナーとして活用し、API アクセス用に内部 HTTP(S) 負荷分散を構成します。
このオプションは実行可能ですが、ロードバランサーの追加構成が必要になるため、管理オーバーヘッドが増加する可能性があります。
D. Cloud Build をパイプライン ランナーとして利用し、API アクセス用に Google Cloud Armor ポリシーを使用して外部 HTTP(S) 負荷分散を構成します。
このオプションでは、API がパブリック インターネットに公開されるため、API トラフィックをプライベート ネットワーク内に保持するという要件と矛盾します。
正解：
A. Cloud Buildプライベートプールを利用して、プライベートVPCへの接続を確立します。
これにより、Cloud Build はプライベート VPC ネットワーク内のリソースに直接アクセスできるため、セキュリティ要件を満たし、管理オーバーヘッドを最小限に抑えることができます。
リンクス：
https://cloud.google.com/build/docs/private-pools/private-pools-overview
</div></details>

### Q. 問題47: 未回答
チームは、データ バッチに対して計算負荷の高い処理を実行するサービスを開発しています。処理速度と効率は、マシンのCPU速度と数量によって異なります。これらのデータバッチにはさまざまなサイズがあり、複数のサードパーティソースからいつでも到着できます。目的は、コストを最小限に抑え、データ処理速度を最適化しながら、サードパーティの安全なデータアップロードを可能にすることです。
これを達成するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. スケーリング用の Compute Engine + Cloud Function 上の SFTP サーバー:
SFTPサーバーを提供することで、安全なデータアップロードが可能になりますが、サーバーの資格情報を管理する必要があり、より効率的なクラウドネイティブソリューションを活用しません。
Cloud Functions の関数を使用して Compute Engine の自動スケーリング グループをスケーリングすることは、変動するワークロードを効率的に処理するための優れた方法です。
必要なソフトウェアをインスタンスを事前にロードし、完了時に終了するように設定することで、コストを最適化できます。
B. Cloud Storage + GKE クラスタと 2 つのサービス:
Cloud Storage は、サードパーティが適切な IAM 権限でデータをアップロードするための安全でスケーラブルな方法を提供します。
GKE クラスタは計算負荷の高いタスクを効率的に処理できますが、処理とモニタリングの両方にサービスを維持すると、特にデータがない期間にサービスが十分に活用されていない場合、コストが高くなる可能性があります。
コストを節約するために処理サービスを停止することは有益ですが、新しいデータが到着したときに起動に遅延が生じます。
D. Cloud Storage + Cloud Monitoring で Cloud Functions の関数をトリガーします。
アップロードに Cloud Storage を使用すると、IAM で効率的かつ安全になります。
Cloud Functions の関数のトリガーとしての Cloud Monitoring は革新的なアプローチですが、Cloud Storage のトリガーほど直接的でも効率的でもない場合があります。
最大の CPU を使用するように Cloud Functions を設定しても、コールド スタートの遅延が発生する可能性があるため、実行時間が常に最小化されるとは限らず、大規模な処理効率が考慮されません。
正解：
C. Cloud Storage + Cloud Functions の Compute Engine を自動スケーリングするための関数:
これはオプション A と似ていますが、よりスケーラブルで管理しやすい SFTP サーバーの代わりに Cloud Storage を使用します。
Compute Engine グループをスケーリングする Cloud Functions の関数トリガーは、変動するワークロードに対応する費用対効果と応答性に優れたソリューションです。
インスタンスの後処理と終了にプリロードされたイメージを使用すると、コストが最小限に抑えられ、リソース使用量が最適化されます。
これらの点を考慮すると、オプションCはニーズに対して最も費用対効果が高く効率的である可能性があります。Cloud Storage のスケーラビリティとセキュリティ、Cloud Functions のイベントドリブンな応答性、Compute Engine の動的なスケーリングを組み合わせて、さまざまなワークロードを処理します。このセットアップでは、クラウドネイティブ機能を活用し、処理が不要な場合はスケールダウンし、事前に読み込まれたイメージを使用してセットアップ時間を短縮することで、コストを最小限に抑えます。
リンクス：
カスタム Cloud Monitoring メトリクスを使用したインスタンスグループの自動スケーリング
</div></details>

### Q. 問題48: 未回答
あなたは、サービスの 1 つに関連する進行中のインシデント中に運用リーダーの役割を果たします。通常、このサービスは約 70% の容量で動作します。ただし、1 つの特定のノードがすべての受信要求に対して一貫して 5xx エラーを返し、カスタマー サポートのケースが著しく急増していることがわかりました。目的は、問題のあるノードをロードバランサ プールから削除して問題を切り分けて調査すると同時に、Google が推奨するプラクティスに従ってインシデントを効果的に管理し、ユーザーへの影響を最小限に抑えることです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 異常なノードを削除する前に、新しいノードを追加します。
この方法では、問題のあるノードを削除する前に追加の容量が確保されますが、新しいノードが正常になるまでに時間がかかる場合は、差し迫った問題への対処が遅れる可能性があります。
C. 最初にドレインし、次に監視とスケーリングを行います。
異常なノードをすぐにドレインすると、残りのノードにさらに負荷がかかる可能性があります。また、このオプションでは、アクションの後にコミュニケーション ステップが配置されるため、チームの調整には適さない場合があります。
D. 削除、追加、監視:
Cと同様に、これは問題のあるノードを削除することから始まりますが、新しい容量の追加が遅れ、サービスのパフォーマンスを危険にさらす可能性があります。最後のコミュニケーションは、インシデント対応の有効性に影響を与える可能性があります。
正解：
ある。
1.インシデントチームに意図を伝えます。
2.負荷分析を実行して、削除されたノードからオフロードされたトラフィックの増加を残りのノードが処理できるかどうかを判断し、適切にスケーリングします。
3. 新しいノードが正常であると報告されたら、異常なノードからトラフィックをドレインし、異常なノードをサービスから削除します。
このアプローチでは、負荷を処理するために残りのノードの容量を最初に評価できますが、これは、サービスが通常 70% の容量で動作することを考えると重要です。また、インシデント管理のベストプラクティスに沿って、変更を加える前のコミュニケーションと慎重な計画の重要性も強調しています。
リンクス：
インシデントの管理
https://cloud.google.com/products/operations
</div></details>

### Q. 問題49: 未回答
組織は、サイト信頼性エンジニアリング (SRE) の文化と原則を取り入れている最中です。管理しているサービスの最近の限定的な停止に続いて、別のチームのマネージャーが、是正措置を容易にするためにインシデントの正式な説明を要求しました。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 根本原因、解決策、教訓、優先順位付けされたアクションアイテムのリストを含む事後分析を作成します。マネージャーとのみ共有します。
これでは、学習と改善の機会が 1 人のマネージャーに限定され、透明性と組織全体の学習という SRE の文化を完全には受け入れていません。
C. 根本原因、解決策、教訓、責任者のリスト、各担当者のアクションアイテムのリストを含む事後分析を作成します。マネージャーとのみ共有します。
個人の責任に焦点を当てたこのアプローチは、SRE の原則に反する非難の文化を生み出す可能性があります。
D. 根本原因、解決策、学んだ教訓、責任者のリスト、各人のアクションアイテムのリストを含む事後分析を作成します。エンジニアリング組織のドキュメントポータルで共有します。
C と同様に、このオプションは非難の文化を生み出すリスクがあり、将来のインシデントでオープンなコミュニケーションとコラボレーションを妨げる可能性があります。
正解：
B. 根本原因、解決策、教訓、およびアクションアイテムの優先順位付けされたリストを含む事後分析を作成します。エンジニアリング組織のドキュメントポータルで共有します。
このアプローチは、SRE のベスト プラクティスと一致しており、透明性と継続的な改善に重点を置いています。事後分析は、個人の責任ではなく、体系的な問題と解決策に焦点を当てるべきです。(1 人のマネージャーだけでなく) 広く共有することで、組織の学習と改善が促進されます。
リンクス：
GKE ログについて
</div></details>

### Q. 問題50: 未回答
企業は、本番環境でのバグ、システム停止、パフォーマンスの遅れなどの問題に直面しています。開発者は現在、新機能の開発やバグの修正にこの環境を利用しています。さらに、運用環境で構成の変更や実験を行うと、ユーザーが停止します。テスト担当者は、同じ環境で負荷テストも実行するため、システムの速度が低下することがよくあります。環境を再設計して、運用環境でのバグや停止の発生を減らし、テスト担当者に新機能の負荷テスト機能を提供する必要があります。
どのような行動を取るべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 本番環境で自動テスト スクリプトを作成して、障害が発生したらすぐに検出します。
障害を早期に検出することは有益ですが、本番環境で直接自動テストを実施すると、ユーザーが直面する停止のリスクが高まり、環境管理の誤りという根本的な問題に対処できません。
B. サーバー容量の小さい開発環境を作成し、開発者とテスト担当者のみにアクセス権を付与します。
これは、別の開発スペースを作成することで問題に部分的に対処しますが、包括的なテスト、特に運用環境と同様の環境を必要とするロード テストには十分ではない可能性があります。
C. 運用環境をセキュリティで保護して、開発者が変更できないようにし、1 年に 1 回の制御された更新プログラムを設定します。
これは制限が厳しすぎて、現代の開発状況では実用的ではありません。これにより、市場の変化に対応し、バグを修正し、機能をタイムリーに更新する能力が大幅に制限されます。
正解：
D. コードを記述するための開発環境と、構成、実験、およびロード テスト用のテスト環境を作成します。
このアプローチでは、環境を分離し、運用環境の安定性を確保します。独立した開発環境により、開発者は本番環境に影響を与えることなくコーディングとデバッグを行うことができますが、専用のテスト環境により、本番環境の安定性を損なうことなく、負荷テストを含む徹底的なテストが可能になります。このセットアップにより、より信頼性が高く効率的な開発およびテストプロセスが促進されます。
</div></details>

## 2
### Q. 問題1: 回答
企業のデータサービスと製品の監督を担当するサイト信頼性エンジニアは、予測不可能なデータ量の管理やデータ取り込みプロセスのコスト管理など、運用上の課題に日常的に対処しています。最近、Google Cloud 内で新しいデータ インジェスト プロダクトが開発されていることに気付きました。
製品開発チームに運用上の洞察を提供するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. プロトタイプ製品をテスト環境に展開し、ロード テストを実行して、結果を製品開発チームと共有します。
このアプローチには実用的なテストが含まれますが、製品の基本的な設計面に影響を与えるには遅すぎる可能性があります。
B.最初の製品バージョンが品質保証フェーズとコンプライアンス評価に合格したら、製品をステージング環境にデプロイします。エラーログとパフォーマンス指標を製品開発チームと共有します。
QA後にステージング環境にデプロイすると、貴重な洞察が得られますが、オプションAと同様に、製品の初期の設計を形作る機会を逃す可能性があります。
C. 新製品が運用環境で少なくとも 1 つの内部顧客によって使用されている場合は、エラー ログと監視メトリックを製品開発チームと共有します。
製品が本番環境で使用されるまで待つと、フィードバックが遅れ、後でコストがかさんだり、より複雑な変更が発生したりする可能性があります。
正解：
D. 製品開発チームと一緒に製品の設計をレビューし、設計段階の早い段階でフィードバックを提供します。
設計段階で運用上の洞察を提供することで、スケーラビリティ、費用対効果、信頼性などの実用的な考慮事項が最初から製品に統合されます。
早期のコラボレーションは、潜在的な運用上の課題を予測し、それらに積極的に対処するのに役立ちます。
このプロアクティブなアプローチは、設計プロセスの早い段階で影響を与えることで堅牢で効率的なシステムを構築することに重点を置いた、サイト信頼性エンジニアリングの原則と一致しています。
リンクス：
https://cloud.google.com/architecture
</div></details>

### Q. 問題2: 未回答
開発、品質保証 (QA)、および運用の 3 つの異なる環境を含むシステムを設計している最中です。これらの各環境は Terraform を使用してデプロイされ、それぞれに Google Kubernetes Engine(GKE)クラスタが確立されます。これらの GKE クラスタは、アプリケーション チームがそれぞれのアプリケーションをデプロイするためのプラットフォームとして機能します。Anthos Config Management は、各 GKE クラスタ内にインフラストラクチャ レベルのリソースをデプロイするために利用され、テンプレート化されます。インフラストラクチャ オペレーターやアプリケーション所有者を含むすべてのユーザーは、GitOps アプローチを採用します。
このセットアップに最も適した状態にするには、コードとしてのインフラストラクチャ (IaC) とアプリケーション コードの両方のソース管理リポジトリをどのように整理する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B.
異なるディレクトリを持つ共有クラウド・インフラストラクチャ(Terraform)リポジトリ:A と似ていますが、GKE インフラストラクチャを異なるリポジトリに分離し、異なる環境用のブランチを使用します。これにより、複雑さが増し、Anthos Config Management の管理効率が低下する可能性があります。
機能のブランチを持つ個別のアプリケーション(アプリのソースコード)リポジトリ:これはオプションAと同じで、アプリケーションコードを独立して管理できます。
C.
クラウド・インフラストラクチャ(Terraform)とアプリケーション・コードの両方の共有リポジトリ:このアプローチでは、インフラストラクチャとアプリケーション開発の異なる側面を同じリポジトリ内で管理する際に複雑になる可能性があります。
アプリケーションリポジトリ内の機能の異なるディレクトリ:GitOps では一般的ではなく、通常は機能開発に異なるブランチが使用されます。
D.
環境ごとに個別のクラウドインフラストラクチャ(Terraform)リポジトリ:これにより、Terraform構成が重複し、環境間での一般的な変更の管理が複雑になる可能性があります。
個別の GKE インフラストラクチャ(Anthos Config Management Kustomize マニフェスト)リポジトリ:Terraform と同様に、これらを異なるリポジトリに分離すると、構成の管理と同期が困難になる可能性があります。
機能のブランチを持つ個別のアプリケーション(アプリのソースコード)リポジトリ:オプションAおよびBと一致しているため、独立したアプリケーション開発と機能の分岐が可能です。
正解：
ある。
クラウド・インフラストラクチャ(Terraform)リポジトリは、環境ごとに異なるディレクトリで共有されます。この構造は、異なるディレクトリが異なる環境を表すTerraform構成を管理する場合に論理的です。
GKE Infrastructure(Anthos Config Management Kustomize マニフェスト)リポジトリは、環境ごとに異なるオーバーレイ ディレクトリと共有されます。Kustomize と Anthos Config Management を使用するためのベスト プラクティスに準拠し、環境をディレクトリ別に表します。
アプリケーション (アプリのソース コード) リポジトリは、機能ごとに異なるブランチで区切られています。このセットアップにより、アプリケーション チームは、機能開発用のブランチを使用して、コードを個別に管理できます。
このアプローチは、さまざまな環境やアプリケーションのリポジトリを明確かつ効率的に編成し、GitOps の方法論とうまく連携させます。
リンクス：
ブランチの代わりにフォルダーを使用する
リポジトリのアーキテクチャ
</div></details>

### Q. 問題3: 回答
Google Cloud Platform(GCP)の 3 つのゾーンに製品をデプロイし、これらのゾーンにユーザーを分散させます。現在、あるゾーンから別のゾーンにフェールオーバーすると、影響を受けるユーザーのサービスが 10 分間中断されます。通常、システムでデータベース障害は四半期に 1 回発生し、5 分以内に検出できます。リアルタイム チャット機能の導入に関連する信頼性リスクを評価するときは、各リスクについて次のメトリックを文書化します。
平均検出時間 (MTTD) (分)
平均修復時間 (MTTR) (分)
平均故障間隔 (MTBF) (日数)
ユーザー影響率
新しいチャット機能では、ゾーン間のフェールオーバーを正常に行うのに 2 倍の時間がかかるデータベース システムが必要です。1 つのゾーンで新しいデータベースのフェールオーバーのリスク値を計算する必要があります。
新しいシステムでのデータベース・フェイルオーバーのリスクに対する適切な値はいくつですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. MTTD:5 MTTR:10 MTBF:90 影響:33%
これは、新しいシステムでのフェールオーバー時間が 2 倍になり、MTTR が 20 分に増加することは考慮されていません。
C. MTTD: 5 MTTR: 10 MTBF: 90 影響: 50%
ユーザーへの影響は 50% と高くなりますが、フェールオーバー時間が長くなるため、MTTR は 20 分にする必要があります。
D. MTTD: 5 MTTR: 20 MTBF: 90 影響: 50%
このオプションでは、MTTR の延長は正しく考慮されますが、新しいシステムの障害がより多くのユーザーに影響を与えるという証拠がなく、ユーザーへの影響の割合が高いことを前提としています。
正解：
B. MTTD: 5 MTTR: 20 MTBF: 90 影響: 33%
これは、同じ平均検出時間(MTTD)が 5 分、フェールオーバー時間が長いため平均修復時間(MTTR)が 20 分に倍増し、同じ平均故障間隔(MTBF)が 90 日、3 つのゾーン間でユーザーが均等に分布すると仮定されているため、1 つのゾーンの障害に対するユーザー影響率は 33% になります。
リンクス：
https://www.atlassian.com/incident-management/kpis/common-metrics
https://linkedin.github.io/school-of-sre/
</div></details>

### Q. 問題4: 回答
あなたは、多くの依存システムを持つインフラストラクチャー・サービスのオンコール担当者です。サービスがほとんどのリクエストを満たせず、すべての依存システムとその数十万人のユーザーに影響を与えているというアラートを受け取ったら、サイト信頼性エンジニアリング(SRE)インシデント管理プロトコルに従ってインシデントコマンダー(IC)の役割を引き受けます。さらに、オペレーションリード (OL) とコミュニケーションリード (CL) として 2 人の経験豊富なチームメンバーを参加させました。
次に取るべきステップは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ユーザーへの影響を軽減し、軽減策を運用環境に展開する方法を探します。
これは重要なステップですが、通常は通信チャネルを確立した後に続きます。インシデント指揮官の主な役割は、対応を調整することであり、必ずしも技術的な解決策を直接実装することではありません。
B. 影響を受けるサービス所有者に連絡し、インシデントの状態を更新します。
この手順は重要ですが、通常、通信チャネルを確立し、最初の軽減策を開始した後に行われます。これは継続的なコミュニケーションの一部ですが、最初のアクションではありません。
D. 事後分析を開始し、インシデント情報を追加し、ドラフトを社内に回覧し、社内の関係者に意見を求めます。
事後分析の開始は、インシデント対応プロセスにおける重要なステップですが、インシデントが軽減され、解決された後に行われます。最初の焦点は、差し迫った問題に対処し、対応チームと効果的にコミュニケーションをとることです。
正解：
C. インシデント対応者とリーダーが相互に通信できるコミュニケーション チャネルを確立します。
これは、インシデント発生時に効果的なコミュニケーションを行うための専用チャネルを持つことが重要であるため、サイト信頼性エンジニアリング (SRE) のプラクティスと一致しています。これにより、インシデント指揮官、オペレーション リーダー、コミュニケーション リード、その他の対応者の間での協調的な取り組み、迅速な情報共有、意思決定が容易になります。明確なコミュニケーションを確立することは、ユーザーへの影響の軽減やサービス所有者への連絡など、他のタスクに進む前の基本的なステップです。
リンクス：
https://sre.google/workbook/incident-response/
</div></details>

### Q. 問題5: 回答
貴社はサイト信頼性エンジニアリングのプラクティスを順守しており、顧客向けアプリケーションに影響を与える重大かつ継続的なインシデントが発生した場合のコミュニケーションを担当します。現在、停止の解決にかかる予定時間はありません。状況に関する最新情報を求める社内の関係者からのメールや、何が起こっているのかを知りたいという顧客からの問い合わせを受け取っています。目標は、停止の影響を受けるすべての関係者に更新プログラムを効率的に提供することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 少なくとも 30 分ごとに社内の関係者に対応することに集中します。「次の更新」時間にコミットします。
インシデントによって直接影響を受ける顧客のニーズを無視する可能性があります。両方のグループとの定期的かつ一貫したコミュニケーションを維持する必要があります。
C. 社内の関係者の電子メールへの応答をインシデント対応チームの別のメンバーに委任します。顧客に直接応答を提供することに重点を置きます。
応答プロセスが断片化され、配信されるメッセージに不整合が生じる可能性があります。コミュニケーション リードは、コミュニケーションをまとまって管理することが重要です。
D. すべての内部関係者の電子メールをインシデント指揮官に提供し、内部コミュニケーションを管理できるようにします。顧客に直接応答を提供することに重点を置きます。
彼らを圧倒し、インシデントを管理するという本来の役割を損なう可能性があります。コミュニケーションリードは、これらのコミュニケーションを処理するのに適しています。
正解：
B. すべての利害関係者にタイムリーに定期的な更新を提供します。すべてのコミュニケーションで「次の更新」時間を約束します。
この戦略により、社内の利害関係者と顧客の両方との一貫性のある明確なコミュニケーションが保証され、平等に情報を得ることができます。これは、期待値を管理し、インシデントの状態に関する透明性を維持するのに役立ちます。また、「次の更新」時間を約束することで、コミュニケーションプロセスを構造化し、より効率的で予測可能なものにすることができます。
他のオプションでは、コミュニケーションが不均一になったり、特定のチームメンバーに過度の負担がかかったり、あるグループの利害関係者が別のグループよりも軽視されたりする可能性があります。
リンクス：
https://sre.google/workbook/incident-response/
</div></details>

### Q. 問題6: 回答
Google Cloud Platform(GCP)でホストされているアプリケーションのメンテナンスを担当し、Cloud Monitoring で最も重要なイベントについてチームに警告する SMS 通知を設定したいと考えています。この構成を必要とするアラート ポリシーは既に決定されています。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Monitoring と SMS ゲートウェイ間のサードパーティ統合をダウンロードして構成します。チームメンバーがSMS/電話番号を外部ツールに追加していることを確認します。
これには、Cloud Monitoring とサードパーティの SMS ゲートウェイの統合が含まれます。この方法では SMS サービスへの直接接続が可能ですが、外部ツールの追加構成と管理が必要です。これは、最も単純で、最も統合されたソリューションではありません。
B. 各アラート ポリシーの [Webhook 通知] オプションを選択し、サードパーティの統合ツールを使用するように構成します。チームメンバーがSMS/電話番号を外部ツールに追加していることを確認します。
このオプションでは、Cloud Monitoring 内の Webhook を使用して、SMS 通知用のサードパーティ ツールに接続します。これは柔軟なアプローチですが、Webhook 構成と外部サービスの管理が複雑になります。この方法は、カスタマイズされた通知ワークフローや複雑な通知ワークフローに適しています。
D. アラート ポリシーごとに Slack 通知を構成します。Slackメッセージを受信したときにSMSメッセージを送信するようにSlackとSMSの統合を設定します。チームメンバーがSMS/電話番号を外部統合に追加していることを確認します。
この方法では、Slack通知を設定してから、SlackとSMSの統合を使用してメッセージを送信します。これは、SMS通知を実現するためのより遠回りな方法であり、SlackとSlackとSMS間の統合の両方の機能と信頼性に依存しています。このアプローチは、チームが Slack に大きく依存しているが、SMS 通知の最も直接的な方法ではない場合に役立つ可能性があります。
正解：
C. チームメンバーが Cloud Monitoring で SMS / 電話番号を設定していることを確認します。各アラート ポリシーの [SMS 通知] オプションを選択し、一覧から適切な SMS/電話番号を選択します。
このオプションは、Google Cloud のドキュメントで概説されているように、直接的で統合された方法です。これには、Cloud Monitoring 内で SMS / 電話番号を設定し、アラート ポリシーで SMS 通知用にこれらを選択することが含まれます。これは最も簡単なアプローチであり、外部のツールやサービスを必要とせずに Google Cloud エコシステム内でシームレスな統合を提供します
リンクス：
https://cloud.google.com/monitoring/support/notification-options#creating_channels
https://cloud.google.com/monitoring/support/notification-options
</div></details>

### Q. 問題7: 回答
会社には、本番環境、テスト、開発用のフォルダを含む Google Cloud リソース階層があります。サイバーセキュリティ チームは、セキュリティ問題の特定と解決を迅速化するために、会社の Google Cloud のセキュリティ体制を確認する必要があります。アラートとほぼリアルタイムの分析を可能にするには、すべてのプロジェクトから Google Cloud サービスによって生成されたログを本番フォルダ内でのみ一元化する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Workflows API を有効にし、すべてのログを Cloud Logging にルーティングします。
Workflows API を有効にしてログを Cloud Logging にルーティングするには、この API を使用してログのフローを自動化する必要があります。Workflows API は、複雑なサービス間プロセスを管理するための強力なツールですが、主にログの集計用に設計されているわけではありません。その使用は、ログの収集と分析の特定の目的に対して、特にほぼリアルタイムの分析の要件を考慮する場合、最も単純または効率的ではない可能性があります。
B. 中央の Cloud Monitoring ワークスペースを作成し、関連するすべてのプロジェクトをアタッチします。
中央の Cloud Monitoring ワークスペースを作成し、関連するすべてのプロジェクトをアタッチすることで、これらのプロジェクト全体の指標と運用の健全性を統一的に表示できます。この設定は、リソースの全体的な状態を監視するのに有利であり、貴重な洞察を提供できます。ただし、その主な焦点は、ログデータの集計と分析ではなく、メトリックの監視です。ログ集約のニーズに特に対応したり、サイバーセキュリティチームに必要なセキュリティに重点を置いた詳細なログ分析を可能にしたりするものではありません。
D. Cloud Logging バケットを宛先として使用する本番フォルダに関連付けられた集約ログシンクを作成します。
Cloud Logging バケットをコピー先として本番フォルダのログシンクを設定すると、すべてのログが一元化されたストレージスペースに収集されます。Cloud Logging バケットは、大規模なログ ストレージに最適で、Cloud Monitoring ツールやアラート ツールと適切に統合されているため、包括的なログ分析が容易になります。ただし、リアルタイム分析のログ可用性に若干の遅延が発生する可能性があり、脅威の検出と対応を迅速なデータ分析に依存しているサイバーセキュリティチームにとっては懸念事項になる可能性があります。
正解：
C. Pub/Sub トピックを宛先として使用する本番フォルダーに関連付けられた集約ログ シンクを作成します。
このアプローチは、セキュリティイベントの迅速な分析と対応に不可欠なリアルタイムのログストリーミングに特化しています。Google Cloud Pub/Sub をログシンクのデスティネーションとして使用すると、分析やアラートにログをすぐに利用できるようになり、サードパーティの SIEM システムと統合してより高度なセキュリティ分析を行うこともできます。
リンクス：
Google Cloud Platform と Logentries によるほぼリアルタイムのログストリーミングと分析
</div></details>

### Q. 問題8: 回答
現在、CI / CD パイプラインを Google Cloud 内で直接設定しているところです。目標は、本番環境の Google Kubernetes Engine(GKE)環境内のビルドを、本番環境への昇格を検討する前に、自動負荷テストを行うことです。目的は、このロード テストに合格したビルドのみが運用環境に配置されるようにすることです。
Google の推奨プラクティスに合わせるには、バイナリ認証を使用してこのパイプラインをどのように構成する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ロード テストに合格したビルドの構成証明を作成するには、リード品質保証エンジニアに個人の秘密キーを使用して構成証明に署名するように要求します。
このアプローチは手動による介入に依存しており、個人の秘密キーを使用するため、拡張性が低く、安全性が低下する可能性があります。
B. Cloud Key Management Service(Cloud KMS)に保存されている秘密鍵と、Kubernetes Secret として保存されているサービス アカウントの JSON 鍵を使用して、ロード テストに合格したビルドの認証を作成します。
サービスアカウントのJSONキーをKubernetesシークレットとして保存することは、セキュリティ上の懸念から推奨されません。
D. Cloud Key Management Service(Cloud KMS)に保存されている鍵を使用して、リード品質保証エンジニアに構成証明への署名を要求して、負荷テストに合格したビルドの証明を作成します。
オプションAと同様に、個人による手動署名が必要であり、自動化されたプロセスほど効率的またはスケーラブルではない可能性があります。
正解：
C. Workload Identity で認証された Cloud Key Management Service(Cloud KMS)に保存されている秘密鍵を使用して、ロード テストに合格したビルドの構成証明を作成します。
この方法では、安全な鍵管理に Cloud KMS を、認証に Workload Identity を活用して、ビルドを証明するための堅牢で安全な方法を提供します。
構成証明プロセスが自動化され、ロード テストに合格したビルドのみが運用環境にデプロイできるようになり、運用環境のデプロイで高水準を維持するという目的に沿っています。
リンクス：
ワークロード ID の使用
ワークロード ID フェデレーションを使用するためのベスト プラクティスBest practices for using workload identity federation
</div></details>

### Q. 問題9: 未回答
Google Cloud VPC 内の VM へのネットワーク トラフィックを調査しており、VPC フローログを有効にしています。ただし、ログをフィルタリングしても、探している特定のネットワークトラフィック情報は表示されません。
問題を特定するために何をする必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. TCPプロトコルでフィルタリングしているかどうかを確認します。
VPC フローログは TCP プロトコルをサポートしており、TCP を使用するトラフィックはログにキャプチャする必要があります。この問題は、フィルタリングされているプロトコルの種類とは関係がない可能性があります。
C. [VPC Flow Logs] 設定でサンプルレートを下げることを検討してください。
VPC フローログのサンプルレートを下げると、キャプチャするログエントリは増えるどころか少なくなります。サンプルレートが高いほど、特に少量のトラフィックの場合、すべての関連データをキャプチャできる可能性が高くなります。
D. トラフィックが VPC 外の VM から VPC 内の VM に発信されているかどうかを調査します。
VPC フローログは、外部ソースから Google Cloud VPC 内の VM へのトラフィックをキャプチャできます。トラフィックの送信元は、VPC の外部か内部かにかかわらず、ログデータの可用性に影響を与えてはなりません。
正解：
B.フィルタリングしているトラフィックの量が非常に少ないかどうかを確認します。
VPC フローログはサンプリングされるため、すべてのパケットがログに記録されるわけではありません。トラフィック量が非常に少ない場合、一部のパケットがキャプチャされず、ログで使用できない可能性があります。これは、特定のトラフィックデータが欠落している理由を説明できる可能性があります。
リンクス：
https://cloud.google.com/vpc/docs/using-flow-logs#some_flows_are_missing
https://cloud.google.com/vpc/docs/using-flow-logs
</div></details>

### Q. 問題10: 未回答
Google Cloud Deploy でデプロイ パイプラインを確認しています。パイプラインの労力を軽減し、エンドツーエンドのデプロイを完了するのにかかる時間を最小限に抑える必要があります。
あなたは何をするべきか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 自動化ステップを小さなタスクに分割します。
C. スクリプトを使用して、Google Cloud Deploy でのデプロイ パイプラインの作成を自動化します。
D.手動の手順を完了するために、さらにエンジニアを追加します。
タスクを分割 (オプション B) すると、プロセスが管理しやすくなりますが、本質的に労力やデプロイ時間が短縮されるわけではありません。パイプラインの作成を自動化すると (オプション C) 効率が 1 回だけ向上しますが、エンドツーエンドのデプロイ時間には影響しません。エンジニアを追加しても(オプションD)、労力は軽減されず、調整のオーバーヘッドにより労力が増加する場合もあります。
正解：
E. 開発環境からテスト環境へのプロモーション承認を自動化します。
プロモーション承認の自動化により、手作業による介入が大幅に減り、全体的な展開時間が短縮されます。
A. 手動による介入が必要な場合に、次の手順を完了するように必要なチームに通知するトリガーを作成します。
これにより、手動の手順が不要になるわけではありませんが、アクションが必要なときにチームに迅速に通知されるため、プロセスが合理化され、待ち時間と潜在的な遅延が削減されます。
リンクス：
Google Cloud Deploy による継続的デリバリー
</div></details>

### Q. 問題11: 未回答
貴社は、サイト信頼性エンジニアリング (SRE) のプラクティスを遵守しています。あなたは、顧客に影響を与えている新しいインシデントのインシデント指揮官の役割を引き受けました。効率的なインシデント対応の実行をサポートするために、2 つのインシデント管理ロールを迅速に指定する必要があります。
どの役割を割り当てるべきか?(2 つのオプションを選択します。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. エンジニアリングリード
エンジニアリングリードはシステムの開発と保守において重要な役割を果たしますが、インシデントへの即時対応では、運用面とコミュニケーションに重点が置かれます。エンジニアリングリードは、より深い技術的洞察や長期的なソリューションのために、後の段階に関与する場合があります。
D. 顧客インパクト評価者
顧客への影響を評価することは重要ですが、一般的にはオペレーション リードまたはインシデント指揮官の役割の一部です。即時対応フェーズでは、これに専任の役割を割り当てることは、運用やコミュニケーションの役割ほど重要ではない場合があります。
E. 外部カスタマー・コミュニケーション・リード
外部コミュニケーションは重要ですが、通常、内部コミュニケーションと外部コミュニケーションの両方を処理するコミュニケーション リードが管理できます。大規模なインシデントや組織では、外部コミュニケーション用に別の役割を持つことが必要な場合がありますが、通常、すぐに対応して割り当てられる最初の役割ではありません。
正解：
A. オペレーションリード
オペレーション リードは、インシデント中の技術的な調査と軽減の取り組みを主導する責任があります。インシデント指揮官と緊密に連携して、影響を理解し、技術リソースを指示し、問題のトラブルシューティングを行い、ソリューションを実装します。この役割は、インシデントの技術的な側面に対処し、その解決に向けて取り組むために重要です。
C. コミュニケーションリード
コミュニケーション リードは、インシデントに関連するすべてのコミュニケーションを、組織内および外部の顧客や利害関係者との間で管理します。この役割には、インシデントの状態、進捗状況、予想される解決時間について、すべての関係者に情報を提供することが含まれます。明確でタイムリーなコミュニケーションは、インシデントの影響を管理し、信頼を維持する上で不可欠です。
リンクス：
https://sre.google/workbook/incident-response/
</div></details>

### Q. 問題12: 未回答
Google Kubernetes Engine(GKE)クラスタで動作する携帯電話ゲームのバックエンドを監督し、ユーザーからの HTTP リクエストを処理します。
ネットワークコストを削減するには、どのようなソリューションを実装する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. VPC を共有 VPC ホスト プロジェクトとして構成します。
このセットアップは、主にプロジェクト間でのリソース共有とネットワーク編成のためのものです。GKE アプリケーションへのユーザー トラフィックに関連するネットワーク コストには影響しません。
C. Kubernetesクラスタをプライベート・クラスタとして構成します。
プライベート クラスターは、パブリック IP をノードに割り当てないことでセキュリティを強化しますが、ユーザー トラフィックに関連するネットワーク コストを必ずしも削減するわけではありません。下り(外向き)には Cloud NAT などの追加設定が必要になる場合があり、その場合、費用が増加する可能性があります。
d. Google Cloud HTTP Load Balancer を Ingress として構成します。
ロードバランサーはトラフィック分散を最適化し、パフォーマンスと可用性を向上させることができます。ただし、本質的にネットワークコストが下がるわけではありません。実際、ロードバランサーでは、特にトラフィックの処理と転送に追加コストが発生し、ネットワーク全体の費用が増加する可能性があります。
正解：
B. Standard レベルでネットワーク サービスを構成します。
Google Cloud のスタンダード ティアは、ネットワーク サービスに費用対効果の高いソリューションを提供し、特に下り(外向き)トラフィックの費用を削減します。Google Cloud HTTP ロードバランサ(オプション D)はトラフィック ルーティングを最適化できますが、ネットワーク コストを直接削減するものではありません。ここでの焦点はネットワーク価格レベルであり、通常、Standard レベルはプレミアム レベルよりも経済的であり、ネットワーク コストを削減するためのより良い選択肢となっています。
リンクス：
ネットワーク サービス層
負荷分散のための Standard レベルの構成
</div></details>

### Q. 問題13: 未回答
SRE のプラクティスと原則を順守する組織のメンバーとして、開発チームから新しいサービスの管理を引き受けます。運用準備レビュー (PRR) の結果、現在の状態ではサービスがサービス レベル目標 (SLO) を達成できないことがわかりました。目標は、サービスが運用環境で SLO を確実に達成することです。
次に何をすべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. SLO の目標をサービスで達成できるように調整して、運用環境に導入できるようにします。
このアプローチは誤解を招く可能性があります。準備ができていないサービスで SLO を調整して達成しやすくすると、ユーザーが期待する品質と信頼性が損なわれる可能性があります。SLO は、サービスの現在の機能だけでなく、ユーザーのニーズと期待を反映する必要があります。
B. 開発チームに、サービスの運用サポートを提供する必要があることを通知します。
開発チームと SRE チーム間のコラボレーションは重要ですが、特定された欠点に対処するための具体的なガイダンスや計画なしに、単に開発者にサービスを渡すだけでは、必要な改善につながらない可能性があります。運用環境に対するサービスの準備を強化するために、協力することが重要です。
D. SLO なしでサービスを運用環境に導入し、運用データを収集したら構築します。
SLO はサービスの信頼性とパフォーマンスを測定するためのベンチマークとなるため、明確な SLO なしでサービスをデプロイするとリスクが伴う可能性があります。SLO なしで開始すると、許容できるパフォーマンスの構成要素を明確に理解できないため、サービスの成功を測定し、改善が必要な領域を特定することが困難になります。
正解：
C. 引き渡し前に完了すべきサービスの信頼性向上の推奨項目を特定する。
このオプションは、サービスを運用する準備が整っており、責任を引き継ぐ前に SLO を満たせるようにするという SRE のプラクティスに沿ったものです。これには、開発チームと協力してサービスの信頼性を高めることが含まれます。
リンクス：
改善とリファクタリング
</div></details>

### Q. 問題14: 回答
お客様は、1 つの Compute Engine インスタンスで動作する本番環境サービスを維持する責任があります。現在、サービスの手動再作成にかなりの時間が費やされています。これには、誤動作しているインスタンスを削除し、適切なイメージを使用して新しいインスタンスを設定することが含まれます。
サイト信頼性エンジニアリングの原則を遵守しながら、これらの手動タスクに費やす時間を減らすには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
C. Compute Engine インスタンスの前にロードバランサを追加し、ヘルスチェックを使用してシステム ステータスを判断します。
これによりヘルスチェックを行うことができますが、複数のインスタンス間のトラフィックを管理するのに適しており、誤動作しているインスタンスを再作成する問題は解決しません。
A. 開発チームにバグを報告して、クラッシュしたインスタンスの根本原因を見つけられるようにします。
D. SMS アラートを含む Cloud Monitoring ダッシュボードを作成して、クラッシュしたインスタンスがクラッシュした直後に再作成を開始できるようにします。
A と D は、根本原因の特定や問題の監視に重点が置かれていますが、これらは重要ですが、インスタンスの再作成を自動化する問題には直接対処していません。
正解：
B. 単一のインスタンスで管理対象インスタンス・グループを作成し、ヘルス・チェックを使用してシステム・ステータスを判断します。
マネージド・インスタンス・グループは、障害発生時にインスタンスを自動的に再作成できます。これは、高可用性を確保し、手動による介入を減らすための最も効率的な方法です。MIG では、ヘルス チェックを使用してインスタンスのヘルスを自動的に判断し、必要に応じて置き換えることができます。
リンクス：
自動修復と自動修復
</div></details>

### Q. 問題15: 回答
組織では最近、アプリケーション開発にコンテナベースのワークフローを採用しました。チームは、自動ビルド パイプラインを通じて運用環境に一貫してデプロイされる複数のアプリケーションを開発する責任があります。最近のセキュリティ監査では、本番環境にプッシュされたコードに脆弱性が含まれている可能性があり、仮想マシン(VM)の脆弱性に対処するための既存のツールがコンテナ化された環境に適用できなくなるという懸念が提起されています。目的は、パイプラインを通過するすべてのコードのセキュリティとパッチコンプライアンスを保証することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. ビルド パイプライン内のコンテナーを構成して、リリース前に常に自身を更新します。
リリース前にコンテナー自体を更新するように構成すると、更新によって破壊的変更や競合が発生すると、予期しない状態になる可能性があります。通常、運用の安定性のためにはお勧めしません。
C. 既存のオペレーティング システムの脆弱性ソフトウェアを、コンテナー内に存在するように再構成します。
従来のVM脆弱性ツールは、コンテナのセキュリティモデルと依存関係が異なるため、コンテナ内で互換性や効果がない可能性があります。
D. コンテナーの作成に使用される Docker ファイルに対して静的コード分析ツールを実装します。
静的コード分析は、Docker ファイルの潜在的な問題を特定するのに役立ちますが、コンテナー自体のランタイムの脆弱性に完全に対処するわけではありません。
正解：
A. Common Vulnerabilities and Exposuresをスキャンして報告するようにContainer Analysisを設定します。
これが最も効果的なアプローチです。Container Analysis は、コンテナの既知の脆弱性 (Common Vulnerabilities and Exposures) をスキャンできますが、これは、従来の VM ベースのツールが適用できないコンテナ化された環境では非常に重要です。
リンクス：
https://cloud.google.com/container-analysis/docs/container-analysis
</div></details>

### Q. 問題16: 未回答
Google Kubernetes Engine にデプロイされたアプリケーションがあります。このアプリケーションは、要求ごとに複数のサービス呼び出しを行いますが、応答時間が遅くなります。どのダウンストリーム サービスが遅延の原因であるかを判断する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. リクエストのパスに沿って VPC フロー ログを分析します。
これにより、ネットワークレベルでのネットワークトラフィックとパフォーマンスの問題に関する洞察を得ることができますが、特定のアプリケーションサービスの問題を効率的に切り分けられない場合があります。
B. 各サービスの Liveness probe と Readiness probe を調査します。
これらのプローブは、個々のサービスの正常性を理解するのに役立ちますが、複数のサービス呼び出しにわたる待機時間の問題を効果的に特定できない場合があります。
C. Dataflow パイプラインを作成して、サービス指標をリアルタイムで分析します。
サービス指標のリアルタイム分析は便利ですが、この目的で Dataflow パイプラインを設定するのは複雑すぎて、個々のサービス呼び出しのトレースに直接焦点を当てていない可能性があります。
正解：
D. OpenTelemetry や Cloud Trace などの分散トレース フレームワークを使用します。
ディストリビューティッド(分散)トレーシングを使用すると、さまざまなサービス間でリクエストのパスを追跡し、サービスコールチェーンの各部分のレイテンシを測定できます。これにより、遅延が発生している場所を簡単に特定できます。たとえば、Cloud Trace は GKE とシームレスに統合され、サービスのパフォーマンスに関する詳細な分析情報を提供できます。
リンクス：
https://cloud.google.com/trace/docs/overview
https://cloud.google.com/architecture/processing-logs-at-scale-using-dataflow?hl=en
</div></details>

### Q. 問題17: 回答
企業は、グローバルに分散した複数の Google Kubernetes Engine(GKE)クラスタでサービスを運用しています。運用チームは、メトリック、アラート、ダッシュボード生成のための Prometheus ベースのツールを使用して、ワークロードの監視を確立しました。ただし、このセットアップでは、すべてのクラスターでメトリックをグローバルに表示する方法は提供されません。
この問題に対処し、管理オーバーヘッドを最小限に抑えながらスケーラブルなソリューションを実装するには、何をすべきでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Prometheus のサービス間フェデレーションを構成して、一元化されたデータ アクセスを有効にします。
このアプローチでは、複数の Prometheus サーバーをリンクして、一元化されたデータ アクセスを可能にします。複数のソースからデータを集約できますが、特に大規模な場合は、設定と管理が複雑になる傾向があります。また、パフォーマンスのボトルネックが発生するリスクも高まり、クラウド サービスが提供するマネージド スケーラビリティが本質的に提供されません。
B. Cloud Operations for GKE 内にワークロード指標を実装します。
Cloud Operations(旧称 Stackdriver)内にワークロード指標を実装することは実行可能なオプションであり、GKE とうまく統合されています。ただし、特にチームがすでに Prometheus 固有の機能やダッシュボードを利用している場合は、Prometheus と同じレベルの互換性と機能セットが提供されない可能性があります。
C. 集中データアクセスのために Prometheus 階層フェデレーションを設定します。
サービス間フェデレーションと同様に、階層型フェデレーションでは、Prometheus メトリックを集計するための階層的なアプローチが可能になります。これは、サービス間フェデレーションよりも効率的ですが、複雑さと管理オーバーヘッドの増加という負担が伴います。データの重複やクエリのパフォーマンスの問題などの問題を回避するには、慎重な計画と構成が必要です。
正解：
d. Google Cloud Managed Service for Prometheus をデプロイします。
このソリューションは、運用チームによって既に使用されている Prometheus の管理された環境を提供するため、理想的です。複数のクラスターからのメトリックを集計して表示するプロセスを簡素化し、統合されたスケーラブルなマネージド サービスを提供します。これにより、特にグローバルに分散された環境での Prometheus セットアップの管理に通常関連するオーバーヘッドが削減されます。
リンクス：
https://cloud.google.com/stackdriver/docs/managed-prometheus
</div></details>

### Q. 問題18: 回答
ユーザー向けの Web アプリケーションを担当します。過去 6 か月間のアプリケーションのエラー バジェットを分析したところ、特定の時間枠でアプリケーションのエラー バジェットの 5% を超えたことがないことがわかりました。最近、ビジネス関係者とサービス レベル目標 (SLO) のレビューを実施し、SLO が適切に設定されていることを確認しました。ただし、ここでは、アプリケーションの SLO を、観察された信頼性とより緊密に一致させることを目指します。
速度、信頼性、ビジネス要件のバランスを保ちながら、この目標を達成するにはどうすればよいでしょうか。(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーションのすべてのゾーンでサービス容量を増やします。
これは、SLO をサービスの観察された信頼性に必ずしも合わせるとは限りません。パフォーマンスは向上する可能性がありますが、既存のエラーバジェットをイノベーションやリスクテイクに活用することはありません。
C. SLO を調整して、アプリケーションの観察された信頼性に一致させます。
SLO は適切に設定されており、調整は必要ない場合があります。このオプションでは、十分に活用されていないエラーバジェットによって提供される機会を活用しません。
E. 計画的なダウンタイムを発表して、より多くのエラー バジェットを消費し、ユーザーがより厳しい SLO に依存しないようにします。
これはエラーバジェットの戦略的な使用法である可能性がありますが、SLOを観察された信頼性に合わせるための最良のアプローチではない可能性があり、ユーザーの認識に悪影響を与える可能性があります。
正解：
B. より頻繁な、または潜在的に危険なアプリケーション リリースを実装します。
エラーバジェットが十分に活用されていないことを考えると、イノベーションの余地やリリース頻度の増加の余地があり、これはリスクとイノベーションのバランスをとるという原則と一致しています。
D. アプリケーションに追加のサービス レベル インジケーター (SLI) を導入して測定します。
サービスのより多くの側面を測定することで、その信頼性をより明確に把握でき、SLO をサービスの観察されたパフォーマンスに合わせるのに役立ちます。
リンクス：
サービス・レベル目標
メンテナンス期間がエラーバジェットに及ぼす影響 - SRE のヒント
</div></details>

### Q. 問題19: 未回答
カスタム Debian イメージを使用する仮想マシン (VM) で実行されているアプリケーションがあります。このイメージには Cloud Logging エージェントが含まれており、VM のスコープは cloud-platform です。アプリケーションは syslog を介して情報を記録しています。ただし、Google Cloud Platform Console のログ ビューアの [すべてのログ] プルダウン リストに syslog が表示されないという問題が発生しました。
この問題に対処するための最初のステップは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ログビューアでエージェントのテストログエントリを探します。
これにより、エージェントがアクティブにログを送信しているかどうかが確認されます。ただし、ログが表示されない場合は、エージェントが最初に実行されているかどうかを確認するのがより基本的です。
B. 最新バージョンのクラウドエージェントをインストールします。
最新のエージェントを使用することは重要ですが、特に既にインストールされている場合は、エージェントが動作していることを確認することは二次的です。
C. VM サービス アカウントのアクセス スコープに monitoring.write スコープが含まれていることを確認します。
クラウドプラットフォームのスコープはログをカバーする必要がありますが、特に問題がエージェント自体にある可能性があるため、ログが表示されない場合の検証は最初のステップではありません。
正解：
D. VM に SSH 接続し、VM で次のコマンドを実行します。grep fluentdです。
このアプローチは、Google Cloud の Cloud Logging エージェントのトラブルシューティング ガイドで推奨される最初の手順と一致しています。エージェントは fluentd に基づいているため、ログを収集して Cloud Logging に転送するには、エージェントが実行されていることを確認することが重要です。コミュニティのインサイトでは、エージェントの動作ステータスを確認することが、ログの問題のトラブルシューティングの最初のステップである必要があることが強調されています。
リンクス：
https://cloud.google.com/logging/docs/agent/logging/troubleshooting#checklist
インスタンスへのサービスアカウントのアタッチ
</div></details>

### Q. 問題20: 回答
インフラストラクチャを形成するTerraformテンプレートの作成および変更を監督します。2人の新しいエンジニアが同じコードベースで作業するため、コードの競合を回避し、すべての更新が最新バージョンに組み込まれるようにするためのプロセスを確立し、ツールを選択する必要があります。
どのようなアプローチを取るべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。 Git ベースのシステムにコードを保存するのは良いことですが、開発者がピア レビューなしで毎日の終わりに変更をマージできるようにすると、競合や品質の問題につながる可能性があります。パッケージ化して Cloud Storage にアップロードするプロセスは、標準的なバージョン管理の方法と一致していません。
ウ. Google ドライブを使用してコードをテキスト ファイルとして保存することは、共同開発環境では現実的ではありません。バージョン管理、分岐、マージなど、コード変更を効果的に管理するために不可欠な機能が欠けています。
D. このアプローチは、オプションCと同様に、Googleドライブを使用し、適切なバージョン管理を欠いています。.zipアーカイブを毎日作成することは非効率的であり、変更の追跡や共同開発を容易にしません。また、コードの品質と機能も保証されません。
正解：
B.
Git ベースのバージョン管理システムにコードを格納します。
ピアによるコードレビューと単体テストを含むプロセスを確立し、コードを統合する前に整合性と機能を確認します。リポジトリに完全に統合されたコードが最新のマスターバージョンになるプロセスを確立します。
この方法では、コラボレーション コーディング環境に不可欠なバージョン管理システム (Git) を活用します。これにより、変更を追跡し、コードをレビューし、最新バージョンのインフラストラクチャコードの信頼できる唯一の情報源を維持できます。このアプローチでは、ピアレビューや単体テストなどのベストプラクティスも促進され、コードの品質と機能が保証されます。
</div></details>

### Q. 問題21: 回答
新しいサービスを運用環境にデプロイするタスクがあります。このサービスは、マネージド インスタンス グループ (MIG) を使用して自動的にスケーリングできる必要があり、複数のリージョンにデプロイする必要があります。さらに、サービスの各インスタンスには大量のリソースが必要であり、容量を慎重に計画する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. MIG の設定で n1-highcpu-96 マシン タイプを使用します。
この選択は、高い CPU 能力を持つ特定のマシンタイプを選択することに重点が置かれています。ただし、複数リージョンのデプロイの側面や容量の全体的なリソース計画には対応していないため、特定のシナリオに対する包括的なソリューションではありません。
B. Cloud Trace の結果をモニタリングして、必要なリソース量を決定します。
Cloud Trace はパフォーマンスの問題を特定するのに役立ちますが、主にデプロイ後にサービスの動作を監視およびトレースするために使用されます。このオプションでは、デプロイ前の容量計画や、複数リージョンのセットアップに関するクォータに関する考慮事項には直接対応していません。
D. サービスを 1 つのリージョンにデプロイし、グローバル ロード バランサーを使用してトラフィックをこのリージョンにルーティングします。
サービスを 1 つのリージョンにデプロイすると、複数リージョンのデプロイの要件と矛盾します。グローバルロードバランサーはトラフィックをグローバルに分散できますが、真の地理的冗長性と高可用性に不可欠な、複数のリージョンでリソースの可用性を確保する必要性は軽減されません。
正解：
C. リソース要件が各リージョンの使用可能なクォータ制限内にあることを検証します。
マネージド・インスタンス・グループを使用して複数のリージョンに新しいサービスをデプロイする前に、リソース要件が各リージョンで使用可能なクォータ制限と一致していることを確認することが重要です。この手順は、クォータの超過によるデプロイの失敗を防ぐための基本です。これは、特定のマシンタイプの可用性だけでなく、サービスに必要なリソースのより広範な可用性についても重要です。
コミュニティのコンセンサスは、重要なステップとして割り当て制限の検証に大きく傾いており、計画されたデプロイが Google Cloud Platform の地域リソースの制約内で実現可能であることを確認しています。
リンクス：
https://cloud.google.com/compute/quotas
https://cloud.google.com/compute/quotas#understanding_quotas
</div></details>

### Q. 問題22: 未回答
同じ Google Cloud Platform(GCP)プロジェクト内で Compute Engine でホストされている複数の本番環境システムを監督します。各システムは、専用の Compute Engine インスタンス セットで動作します。目的は、これらの各システムの実行コストを決定することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。Google Cloud Platform Console の [Cost Breakdown] セクションを使用して、システムあたりのコストを視覚化します。
この機能では、コストの内訳を視覚的に把握できますが、特に請求データ内でシステムが明確に定義されていない場合は、同じプロジェクト内のシステムごとのコストを区別するために必要な粒度が得られない可能性があります。
C. すべてのインスタンスを、実行するシステムに固有のメタデータでエンリッチします。BigQuery にエクスポートするように Cloud Logging を設定し、メタデータに基づいて費用をクエリします。
メタデータでインスタンスをエンリッチメントすると、インスタンスの識別に役立ちますが、Cloud Logging をコスト分析に使用するのは簡単ではありません。Cloud Logging はログデータ用に設計されているため、直接的な費用分析機能を簡単に提供できない場合があります。
D. 各仮想マシン (VM) には、実行するシステムの名前を付けます。Cloud Storage バケットへの使用状況レポートのエクスポートを設定します。バケットを BigQuery のソースとして構成し、VM 名に基づいて費用をクエリします。
VM に名前を付けると識別に役立ちますが、コスト分析に名前に依存すると、ラベルを使用するよりも精度が低くなります。使用状況レポートは、一般的な使用状況データを提供しますが、特に複数のシステムがある複雑な環境では、正確なコスト属性に必要な詳細を提供しない場合があります。
正解：
B. すべてのインスタンスに、実行するシステムに固有のラベルを割り当てます。BigQuery の請求、エクスポート、ラベルごとのクエリ費用を設定します。
この方法により、コストを効果的に追跡し、特定のシステムに帰属させることができます。インスタンスにラベルを付け、請求データを BigQuery にエクスポートすることで、詳細なクエリを実行して、システムごとのコストを分析して分類できます。このアプローチは、複数のシステムにわたるコスト管理のためにスケーラブルで正確です。
リンクス：
Cloud Billing データを BigQuery にエクスポートする
ラベルを使用したクエリの例
</div></details>

### Q. 問題23: 未回答
貴社は、サイト信頼性エンジニアリングの原則を遵守しています。ソフトウェアの変更によってトリガーされ、ユーザーに大きな影響を与えたインシデントの事後分析レポートを作成しています。目標は、今後同様の重大なインシデントが発生しないようにすることです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. インシデントの原因となるエンジニアを特定し、上級管理職にエスカレーションします。
これは、SRE の「無責任」の原則に反します。個人の責任に焦点を当てると、オープンな議論やインシデントからの学習が妨げられる可能性があります。
C. 変更を確認した従業員をフォローアップし、今後従うべき慣行を規定します。
プロセスを確認することは重要ですが、レビュー担当者のみに焦点を当てると、インシデントにつながった体系的な問題に対処できない可能性があります。
D. インシデントが発生した場合に、オンコール チームがすぐにエンジニアと経営陣に電話して行動計画について話し合うことを要求するポリシーを設計します。
即時のコミュニケーションは重要ですが、これだけを中心としたポリシーの設計は事後対応的になり、インシデントの根本的な原因に対処したり、将来の発生を防いだりできない可能性があります。
正解：
B. この種類のエラーをキャッチするテスト ケースが、新しいソフトウェア リリースの前に正常に実行されることを確認します。
将来同様のエラーをキャッチできる包括的なテストケースを実装することは、プロアクティブなステップです。このアプローチでは、同じ問題の再発を防ぐために、開発と展開のプロセスを改善することに重点を置いています。
このオプションは、個人の責任や事後対応的な対策に焦点を当てるのではなく、システムやプロセスの改善に重点を置くことで、SRE の原則に沿ったものです。
リンクス：
事後分析文化:失敗から学ぶ
信頼性のテスト
</div></details>

### Q. 問題24: 未回答
グローバルな組織で働いており、99%の可用性目標でサービスを管理していますが、エンジニアリングリソースは限られています。現在の暦月には、サービスの可用性が 99.5% に達していることを確認しました。目的は、サービスが定義された可用性の目標を一貫して満たし、新機能のリリースなどのビジネスの変化に適応できるようにすると同時に、運用コストを最小限に抑えながら技術的負債を削減することです。Google が推奨する方法に従うことを目標としている。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. サービスにコンピューティング リソースを追加することで、サービスに N+1 の冗長性を追加します。
冗長性を追加すると可用性が向上しますが、特にサービスが既に可用性の目標を満たしている場合は、最も費用対効果の高いアプローチではない可能性があります。不必要なリソース使用率とコストの増加につながる可能性があります。
C. サービス レベルの可用性のエラー バジェットを定義し、残りのエラー バジェットを最小限に抑えます。
エラーバジェットを定義することは、イノベーションのペースと信頼性のバランスを取るための良い方法ですが、信頼性の低いリスクを管理することが大切です。このアプローチは、技術的負債や運用コストを削減するという目標に直接対処するものではありません。
D. 対応可能なエンジニアを機能バックログに割り当て、サービスが可用性目標内にとどまるようにします。
運用効率を考慮せずに機能開発のみに集中すると、技術的負債と運用上の課題が増加する可能性があります。このアプローチでは、サービスの保守と信頼性の重要な側面が無視される可能性があります。
正解：
B. 反復的なタスクを自動化することで、労力を特定、測定、排除します。
労力の削減:労苦とは、サービスにあまり価値を付加しないが、その運用に必要な反復的で平凡なタスクを指します。これらのタスクを自動化することで、エンジニアリングチームの作業負荷を大幅に軽減し、より影響力のあるアクティビティに集中できるようになります。
可用性の目標達成における一貫性:手間が省けることで、チームは、監視、パフォーマンスチューニング、ベストプラクティスの実装など、サービスの可用性を確保するためのプロアクティブな対策により多くの時間を費やすことができます。
ビジネスの変化への適応:労力が減ったことで、チームは新機能のリリースなど、ビジネスの変化に適応するためにより多くのリソースを割り当てることができます。自動化は、新しい変更が導入されても、安定した運用環境を維持するのに役立ちます。
運用コストの最小化: 反復的なタスクを自動化することで、運用コストの削減にもつながります。手動による介入の必要性が最小限に抑えられるため、人為的ミスのリスクとそれに関連するコストが削減されます。
要約すると、反復的なタスクの自動化に重点を置くことで、現在のレベルのサービス可用性を維持できるだけでなく、運用コストを抑えながら、ビジネスの変化への適応や技術的負債の削減など、より戦略的なタスクに貴重なエンジニアリングリソースを解放することができます。
リンクス：
https://sre.google
</div></details>

### Q. 問題25: 未回答
会社では、すべてのチーム メンバーに、一般的な統合開発環境 (IDE) を含む、一貫性のあるテンプレート化された開発環境を使用する必要があります。お客様は、これらの開発環境のセキュリティパッチとアップデートを維持する役割を担っています。さらに、各チームのすべてのデベロッパーが一貫した環境を使用できるように、信頼できる Google 推奨の方法を見つける必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. すべてのセキュリティ パッチを適用した「ゴールデン」マシンの Compute Engine スナップショットを毎日作成し、デベロッパーが自分のマシンを更新できるようにします。
スナップショットを使用してマシンを手動で更新する開発者に依存すると、すべての開発者が一様にプロセスに従うわけではないため、不整合が生じる可能性があります。
B. 更新スクリプトを使用してすべてのソフトウェア更新プログラムのチェックリストを作成し、開発者が自分のマシンを更新できるようにします。
チェックリストのアプローチは、更新を個々の開発者に大きく依存しているため、チーム全体で実装が多様で一貫性がなくなる可能性があります。
C. すべての最新の依存関係を持つイメージを作成し、開発者が使用できるようにラップトップにイメージをインストールします。
各開発者のラップトップで更新を直接管理することは、特に重要なパッチの場合、すべてのマシンが一貫して更新されるようにするには信頼できません。
正解：
D. 各チームに固有の Cloud Workstations 構成を作成します。
Cloud Workstations の構成は、一貫性のある開発環境を構築するためのテンプレートベースのアプローチを提供します。これらの構成では、マシン・タイプ、ディスク・サイズ、ツール、およびプリインストール・ライブラリーを指定できます。マシンタイプやコンテナイメージの更新など、ワークステーション構成に変更が加えられると、これらの変更は次回の起動時に各開発者のワークステーションに自動的に反映されます。これにより、統一性が確保され、開発環境の管理が簡素化されます。
リンクス：
https://cloud.google.com/workstations/docs/overview
https://cloud.google.com/workstations/docs/create-configuration
https://www.youtube.com/watch?v=E1cblFqb8nk
</div></details>

### Q. 問題26: 回答
Google Cloud リソースの Terraform デプロイ用に CI / CD パイプラインを設定しています。CI / CD ツールは Google Kubernetes Engine(GKE)内で実行され、パイプラインの実行ごとにエフェメラル Pod を利用します。目標は、これらのポッドで実行されているパイプラインに、Terraformデプロイを実行するために必要なIdentity and Access Management(IAM)権限があることを確認することです。
ID 管理について、Google が推奨するプラクティスに従う必要があります。
どのような手順を踏む必要がありますか?(2 つのオプションを選択します。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. GoogleサービスアカウントのJSONサービスアカウントキーを作成し、そのキーをKubernetesシークレットとして保存し、そのキーをPodに挿入して、GOOGLE_APPLICATION_CREDENTIALS環境変数を設定します。
このアプローチでは、サービス アカウント キーを JSON 形式で生成し、Kubernetes 内に安全に保存してから、各パイプラインの実行に挿入します。この方法は効果的ですが、一般的には Workload Identity を使用するよりも安全性が低く、煩雑です。サービス アカウント キーの管理には、キーが安全に保管およびローテーションされるように慎重に取り扱う必要があり、潜在的なセキュリティ リスクをもたらします。
D. Googleサービス・アカウントのJSONサービス・アカウント・キーを作成し、CI/CDツールのシークレット管理ストアにキーを格納し、このキーを認証に使用するようにTerraformを構成します。
オプション B と同様に、これにはサービス アカウント キーの管理が含まれますが、CI/CD パイプラインに固有のシークレット管理ツールに格納されます。これにより、Kubernetesに直接キーを保存する場合と比較して、セキュリティのレイヤーが追加されます。ただし、キー管理に関連する複雑さと潜在的なセキュリティの問題が依然として伴います。
E. Pod を実行する Compute Engine VM インスタンスに関連付けられた Google サービス アカウントに適切な IAM 権限を割り当てます。
この方法では、GKE クラスタを実行している Compute Engine インスタンスに関連付けられたサービス アカウントに必要な権限が直接割り当てられます。これは実行可能なオプションですが、VM 全体に必要以上に広範なアクセス許可が付与される可能性があり、セキュリティ上の懸念につながる可能性があります。一般的には、特定のロールとアクセス許可を必要とするエンティティにのみ付与される最小特権の原則に従う方が安全です。
要約すると、オプション B、D、E は、GKE で CI / CD パイプラインの認証と権限を管理するさまざまな方法を提供しますが、Workload Identity(オプション A)を使用したり、特定のロール専用の Google サービス アカウントを作成したり(オプション C)するのと同じレベルのセキュリティ、シンプルさ、ベスト プラクティスの整合性は提供されません。特に Workload Identity は、Kubernetes サービス アカウントと Google サービス アカウントを安全に結び付けるように設計されているため、JSON キー ファイルの管理など、安全性の低いプラクティスの必要性が軽減されます。
正解：
A. 新しいKubernetesサービスアカウントを作成し、そのサービスアカウントをPodに割り当てます。Workload Identity を使用して、Google サービス アカウントとして認証します。
C. 新しい Google サービス アカウントを作成し、適切な IAM アクセス許可を割り当てます。
新しいGoogleサービスアカウント(オプションC)を作成し、適切なIAM権限を割り当てます。このサービス アカウントは、Terraform のデプロイ専用に使用され、付与されるアクセス許可をきめ細かく制御できます。
新しいKubernetesサービスアカウントを作成し、そのサービスアカウントをPodに割り当てます(オプションA)。次に、Workload Identity を使用して、ポッドを Google サービス アカウントとして認証します。Workload Identity は、Kubernetes サービス アカウントと Google サービス アカウントを関連付け、クラウド リソースにアクセスするためのより安全で管理しやすい方法を提供するため、GKE 内で実行されているアプリケーションから Google Cloud サービスにアクセスするための推奨される方法です。
リンクス：
サービス アカウントを使用した Google Cloud への認証
</div></details>

### Q. 問題27: 回答
会社のマルチクラウド環境内でアプリケーションのCI/CDパイプラインを設定しています。アプリケーションはカスタム Compute Engine イメージを使用してデプロイされており、将来の変更に適応しながら、現在の環境でこれらのイメージを構築してデプロイできるソリューションを確立したいと考えています。
どのソリューションスタックを使用すべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Google Cloud Deploy を使用した Cloud Build
C. Google Kubernetes Engine と Google Cloud Deploy の組み合わせ
D. kpt を使用した Cloud Build
その他のオプション B、C、D は、Kubernetes のデプロイとコンテナ管理を含むシナリオに適していますが、現在 Compute Engine イメージに重点を置くものとはあまり一致しません。
正解：
A. Packer を使用した Cloud Build
Cloud Build は、Google Cloud のインフラストラクチャ上でビルドを実行するサービスです。ビルド、テスト、デプロイのプロセスを自動化できるため、CI/CD パイプラインに最適です。
Packer は、単一のソース構成から複数のプラットフォーム用の同一のマシン イメージを作成するためのオープンソース ツールです。これは、シナリオの要件であるカスタム イメージを事前に構成する場合に特に役立ちます。
この組み合わせにより、Cloud Build を活用して CI / CD パイプライン内で自動化と統合を行うことができ、Packer は異なるクラウド環境間で一貫性のあるイメージを作成する柔軟性を提供します。このセットアップは、マルチクラウド戦略の将来の変更への適応性をサポートします。
リンクス：
Packer を使用した VM イメージのビルド
</div></details>

### Q. 問題28: 回答
規制の厳しいドメインで事業を行っている場合、会社はすべての組織ログを 7 年間保持する必要があります。マネージドサービスによるログ記録インフラストラクチャの複雑さを軽減し、設定ミスや人為的ミスによる将来のログキャプチャやストレージの損失を防ぐには、どのような対策を講じ、何をすべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Logging を使用して組織レベルで集約シンクを構成し、すべてのログを BigQuery データセットにエクスポートします。
BigQuery へのログのエクスポートは分析には便利ですが、特に 7 年間にわたって大量のログを処理する場合は、すべてのログを長期間保存するには費用対効果が低くなる可能性があります。
C. Cloud Logging を使用して各プロジェクト レベルでエクスポート シンクを構成し、すべてのログを BigQuery データセットにエクスポートする
D. Cloud Logging を使用して各プロジェクト レベルでエクスポート シンクを構成し、7 年間の保持ポリシーとバケット ロックを使用してすべてのログを Cloud Storage にエクスポートします。
各プロジェクト レベルでエクスポート シンクを構成すると、複雑さが増し、個々のプロジェクトの構成ミスによるログの欠落のリスクが高まり、ログ インフラストラクチャを簡素化し、完全なログ キャプチャを確保するという目標と矛盾します。
正解：
B. Cloud Logging を使用して、組織レベルで集約シンクを構成し、7 年間の保持ポリシーとバケット ロックを使用してすべてのログを Cloud Storage にエクスポートします。
Cloud Logging で組織レベルで集約シンクを構成すると、組織内のすべてのプロジェクトからログをキャプチャできます。この一元化されたアプローチにより、管理が簡素化され、個々のプロジェクトレベルでの設定ミスによるログの欠落がなくなります。
ログを Cloud Storage にエクスポートすると、大量のログデータを保存するためのスケーラブルで費用対効果の高いソリューションが提供されます。
Cloud Storage バケットに 7 年間の保持ポリシーを設定すると、必要な期間、ログが保持されます。
バケットロックを有効にすると、保持期間内のログの削除が防止され、規制の厳しいドメインで重要な偶発的または不正な変更から保護されます。
リンクス：
https://bluexp.netapp.com/blog/gcp-cvo-blg-google-cloud-storage-retention-policy-a-how-to-guide
</div></details>

### Q. 問題29: 回答
Cloud Monitoring のカスタム ダッシュボードをパートナー チームと共有したい。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. パートナー チームにダッシュボードの URL を提供して、パートナー チームがダッシュボードのコピーを作成できるようにします。
ダッシュボードの URL を指定すると、パートナー チームは Google Cloud プロジェクトにアクセスできる場合にのみダッシュボードを表示できます。ダッシュボードを環境に直接コピーまたはインポートすることはできません。
B. 指標を BigQuery にエクスポートします。Looker Studio を使用してダッシュボードを作成し、そのダッシュボードをパートナー チームと共有します。
指標を BigQuery にエクスポートし、Looker Studio を使用してダッシュボードを作成するプロセスは複雑で、元のダッシュボードの構成とレイアウトを正確に複製できない可能性があります。
C. ダッシュボードからMonitoring Query Language (MQL)クエリをコピーし、MLクエリをパートナーチームに送信します。
MQLクエリのコピーと送信は、クエリロジックの共有に役立ちますが、ダッシュボードの構成や視覚化設定全体は含まれていません。
正解：
D. ダッシュボードの JSON 定義をダウンロードし、パートナー チームに JSON ファイルを送信します。
この方法により、パートナー チームはダッシュボードを独自の Google Cloud Monitoring 環境にインポートできます。JSONファイルを共有することで、ダッシュボードの構成やダッシュボードに含まれる可能性のあるMonitoring Query Language (MQL)問合せなど、ダッシュボードの完全な表現を提供します。このアプローチにより、パートナー チームは、手動で再作成することなく、ダッシュボードを最初に設計したとおりに表示および操作できます。
リンクス：
カスタムダッシュボードを共有する
</div></details>

### Q. 問題30: 回答
CTO は、すべてのインシデントについて、社内で使用する事後分析ポリシーを確立することを指示しました。
社内でポリシーを確実に成功させるには、取るべき 2 つのアクションは何ですか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. すべての事後分析に、インシデントの原因、インシデントの原因、インシデントの将来の発生を防ぐ方法が含まれていることを確認します。
責任者の特定に重点を置くと、非難の文化が生まれる可能性があり、効果的な事後分析の実践では一般的に推奨されません。体系的な問題と解決策に焦点を当てる方が有益です。
B. すべての事後分析に、インシデントの原因、インシデントがどのように悪化したか、およびインシデントの将来の発生を防ぐ方法が含まれていることを確認します。
インシデントがどのように悪化したかを分析することは有用な場合もありますが、一般的には、実際のイベントと直接的な改善に焦点を当てることがより重要です。
D. すべての事後分析に、顧客情報の名前を挙げずに、インシデントがどのように解決されたか、およびインシデントの原因が含まれていることを確認します。
このオプションでは、プライバシーと機密性が重要になりますが、予防戦略など、包括的な事後分析に必要なすべての側面を網羅していない可能性があります。
正解：
C. すべての事後分析に、インシデントの重大度、インシデントの将来の発生を防ぐ方法、および内部システム コンポーネントの名前を挙げずにインシデントの原因が含まれていることを確認します。
このアプローチは、事後分析に不可欠なセキュリティと機密性を維持しながら、中核的な問題を理解して対処することに重点を置いています。
E. すべての事後分析に、事後分析の作成にすべてのインシデント参加者が含まれていることを確認し、事後分析をできるだけ広く共有します。
これにより、複数の視点からインシデントを包括的にカバーし、組織内で透明性と学習の文化を促進します。
リンクス：
事後分析文化:失敗から学ぶ
</div></details>

### Q. 問題31: 不正解
クライアント用に新しい Google Cloud 組織を設計しています。クライアントは、Google Cloud で作成された有効期間の長い認証情報に関連するリスクを懸念しています。運用上のオーバーヘッドを最小限に抑えながら、JSON サービス アカウント キーの使用に関連するリスクを完全に排除するソリューションを設計する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. constraints/iam.disableServiceAccountKevCreation 制約を組織に適用します。
似ていますが、オプション C を優先して非推奨です。
B. 事前定義されたロールのカスタムバージョンを使用して、すべての iam.serviceAccountKeys.* サービスアカウントロールのアクセス許可を除外します。
役割をカスタマイズし、特定のアクセス許可を除外するには、かなりの手作業が必要になります。これは、キー関連のリスクを排除するための最も簡単で効率的なアプローチではありません。
D. roles/iam.serviceAccountKeyAdmin IAM ロールを組織管理者のみに付与します。
キー管理が制限される可能性がありますが、管理者は引き続きキーを管理できるため、サービス アカウント キーに関連するリスクを完全に排除するわけではありません。
正解：
C. constraints/iam.disableServiceAccountKeyUpload 制約を組織に適用します。
constraints/iam.disableServiceAccountKeyUpload 制約は、新しいサービスアカウントキーの作成を完全に防止します。これにより、組織内のサービス アカウントの新しいキーをアップロードまたは生成する機能が無効になります。
この制約を組織レベルで適用すると、新しい JSON サービス アカウント キーの作成が制限され、既存のキーのみを使用できます。これにより、新しいキーの作成に関連するリスクが排除され、攻撃対象領域が縮小されます。
これはポリシーベースのアプローチであり、サービス アカウントの特定のロールやアクセス許可を手動で管理する必要がないため、運用上のオーバーヘッドが最小限に抑えられます。
リンクス：
https://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts#disable_service_account_key_creation
</div></details>

### Q. 問題32: 未回答
セキュリティチームは、システム侵害が疑われる場合にテストするために、本番インフラストラクチャのレプリカを作成する必要があります。
レプリカを迅速に配信するための Google 推奨のプラクティスに合わせるには、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Asset Inventory を利用して本番リソースを一覧表示し、Google Cloud コンソールを使用して手動で再作成します。
コンソールから技術環境全体を毎回手動で作り直すのは非効率的であり、Googleは推奨していません。この方法では、一貫性のある環境を維持するために不可欠なスケーラビリティと再現性が欠けています。
B. Security Command Center でスキャンを実行し、本番環境のリソースを一覧表示し、gcloud CLI を使用して手動で再作成します。
Security Command Center は貴重な分析情報を提供しますが、レプリケーションに必要な詳細なリソース構成は提供しません。また、gcloud CLI を使用した手動による再作成は時間がかかり、エラーが発生しやすいため、高速で信頼性の高い環境レプリケーションには適していません。
D. bash シェル スクリプトを記述して運用環境を複製し、必要に応じてスクリプトを実行します。
環境のレプリケーションに bash スクリプトを使用することは可能ですが、最も効率的で信頼性の高い方法ではありません。スクリプトは複雑で保守が困難になる可能性があり、状態管理やモジュール性など、Terraform などの IaC ツールに固有の利点が欠けています。
正解：
C. Terraformスクリプトを開発して本番環境を複製し、必要に応じてスクリプトを実行します。
コードとしてのインフラストラクチャ (IaC) は、技術環境を管理および複製するための推奨されるアプローチです。Terraformは、IaCツールとして、インフラストラクチャの自動化された反復可能な作成を可能にし、一貫性のあるエラーのない方法で環境を迅速に複製するための理想的なソリューションになります。
リンクス：
再現性のあるスケールが必要ですか?Infrastructure as Code を GCP に導入する
Terraformを使用するためのベスト・プラクティス
</div></details>

### Q. 問題33: 回答
「europe-west2-a」ゾーンの 1 つの Compute Engine インスタンスにデプロイされたステートレスなウェブベースの API を維持する必要があります。サービス可用性のサービス レベル インジケーター (SLI) が、指定されたサービス レベル目標 (SLO) を下回っています。事後分析の結果、API への要求が頻繁にタイムアウトになることが確認されており、これは主に API で大量の要求が発生し、メモリが不足していることが原因です。目標は、サービスの可用性を高めることです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 測定された SLI と一致するように、指定した SLO を変更します
これは、サービスが利用できないという根本的な問題には対処しておらず、サービスを改善せずにターゲットを変更するだけです。
B. より多くのメモリを備えた高仕様のコンピューティング インスタンスにサービスを移動する
より強力なインスタンスにアップグレードすると、メモリの問題を解決できますが、トラフィックの多い課題に完全には対処できない可能性があります。
D. 他のゾーンに追加のサービスインスタンスを設定し、プライマリインスタンスが使用できない場合のフェイルオーバーとして使用します
これにより回復性は向上しますが、負荷が積極的に分散されないため、プライマリインスタンスが依然として高トラフィックに圧倒される可能性があります。
正解：
C. 他のゾーンに追加のサービスインスタンスを設定し、すべてのインスタンス間でトラフィックの負荷を分散します
さすがに最も効果的です。他のゾーンに追加のサービスインスタンスを設定し、ロードバランサーを使用してトラフィックを分散すると、トラフィックとメモリの制約が高いという差し迫った問題に対処できるだけでなく、冗長性も追加されます。1 つのインスタンスに障害が発生したり、過負荷になったりした場合、ロードバランサーはトラフィックを他の正常なインスタンスに再ルーティングします。このアプローチにより、サービスの可用性と回復性が大幅に向上し、API のサービスの可用性を向上させるという目標に沿っています。
</div></details>

### Q. 問題34: 回答
組織は、2,000 ドルのコストで、アプリケーションの可用性目標を 99.9% から 99.99% に引き上げることを目指しています。アプリケーションによって生成された現在の収益は $1,000,000 です。可用性の向上への投資が 1 年間の使用に正当化されるかどうかを評価するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 可用性の向上の価値を $1,000 と計算し、可用性の向上は投資に見合わないと判断します。
このオプションでは、可用性の向上から $1,000 の金銭的利益を計算することが提案されています。しかし、前述の計算方法に基づくと、経済的利益は1,000ドルではなく900ドルに近かった。したがって、このオプションの推定は正しくありません。
さらに、メリットが 1,000 ドルであっても、可用性を向上させるコスト (2,000 ドル) はメリットよりも高いため、投資は正当化されません。
C. 可用性の向上の価値を 1,000 ドルと計算し、可用性の向上が投資に見合う価値があると判断します。
オプション B で説明したように、$1,000 の計算値は、特定のデータに基づいて正確ではありません。より正確な計算では、約900ドルの利益が得られます。
たとえ利益が1,000ドルであったとしても、改善のコストは利益の2倍である2,000ドルであるため、投資を正当化することはできません。したがって、このオプションは、その推定と結論の両方において正しくありません。
D. 可用性の向上の価値を $9,000 と計算し、可用性の向上が投資に見合う価値があると判断します。
このオプションは、経済的利益を大幅に過大評価しています。計算すると、利益は約900ドルで、9,000ドルをはるかに下回っています。
利益が実際に9,000ドルであった場合、経済的利益は改善コスト(2,000ドル)よりも大幅に高くなるため、投資は正当化されます。ただし、このシナリオは、特定のデータや計算と一致しません。
正解：
A. 可用性の向上の価値を $900 と計算し、可用性の向上は投資に見合わないと判断します。
この結論は、投資コスト(2,000ドル)が、1年間の使用で計算された経済的利益(900ドル)よりも高いという事実に基づいています。ただし、この分析は単純化されており、顧客満足度、信頼性の向上による潜在的な成長、1 年以上の長期的なメリットなど、他の要因は考慮されていないことに注意することが重要です。
</div></details>

### Q. 問題35: 回答
これで、Compute Engine インスタンスのフリートが Google Cloud 内に正常にデプロイされました。ここでの目的は、Cloud Logging と Cloud Monitoring を通じて、会社の運用チームとサイバーセキュリティ チームがこれらのインスタンスのモニタリング指標とログにアクセスできるようにすることです。最小権限の原則を遵守しながらこれを実現するには、Identity and Access Management(IAM)を使用して、必要なロールを Compute Engine サービス アカウントに割り当てるにはどうすればよいでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. logging.admin と monitoring.editor のロールを Compute Engine サービス アカウントに付与します。
logging.admin は、ログを書き込むためだけに必要以上に、Cloud Logging へのフル アクセスを提供します。
monitoring.editor は、メトリクスの記述だけに必要以上に幅広い権限を提供します。
C. logging.editor ロールと monitoring.metricWriter ロールを Compute Engine サービス アカウントに付与します。
logging.editorには、ログと構成を管理するための権限が含まれており、単にログを書き込むのに必要な権限以上のものです。
monitoring.metricWriter は、監視データの書き込みに適しています。
D. logging.logWriter と monitoring.editor のロールを Compute Engine サービス アカウントに付与します。
logging.logWriter は、ログの書き込みに適しています。
monitoring.editor は、すべての監視ダッシュボードとアラートの作成と編集を含む幅広いアクセス許可を提供しますが、これは最小特権の原則を超える可能性があります。
正解：
A. logging.logWriter ロールと monitoring.metricWriter ロールを Compute Engine サービス アカウントに付与します。
logging.logWriter ロールを使用すると、サービス アカウントはログ エントリを書き込むことができるため、Cloud Logging との統合には十分です。
monitoring.metricWriter ロールは、サービス アカウントにモニタリング データを書き込むことを、Cloud Monitoring の統合に合わせて許可します。
これらのロールは、過度に広範なアクセス許可を付与することなく、ログ記録と監視に必要なアクセスを提供するため、最小特権の原則に準拠します。
リンクス：
https://cloud.google.com/logging/docs/access-control
https://cloud.google.com/monitoring/access-control
</div></details>

### Q. 問題36: 回答
組織で Cloud Run を使用してコンテナ化されたアプリケーションをデプロイしている。アプリケーションの新しいバージョンは内部テストに合格し、運用環境でテストする必要があります。
ベータ テスターがすべてのエンド ユーザーにロールアウトする前にアプリケーションをテストできるように、デプロイを構成するにはどうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Run アプリケーションを本番環境にデプロイし、トラフィックの 1% を新しいアプリケーション バージョンに転送します。
本番環境でトラフィックの 1% を新しいアプリケーションバージョンに誘導すると、ベータ機能が少数のエンドユーザーに公開され、制御されたテストには理想的ではなく、ユーザーエクスペリエンスに影響を与える可能性があります。
B. テストチーム外の IP アドレスからのリクエストを拒否するように Google Cloud Armor を設定します。
Google Cloud Armor を使用して IP アドレスでトラフィックをフィルタリングするのは面倒で、テスト チームのために正確で最新の IP アドレスのリストを維持することが困難な場合があるため、信頼性が低い可能性があります。
C. コンテナを Compute Engine のプライベート VM インスタンスにデプロイし、テスト チームのみにアクセス権を付与します。
Compute Engine 上のプライベート VM にコンテナをデプロイすると、本番環境とは異なる環境が作成されます。このアプローチでは、エンドユーザーへの信頼性と一貫性のあるロールアウトを確保するためにより多くの労力が必要であり、運用条件を正確に反映していない可能性があります。
正解：
D. リビジョンタグと「--no-traffic」オプションを使用して Cloud Run アプリケーションを本番環境にデプロイし、その URL をテストチームと共有します。
リビジョンタグと「--no-traffic」オプションを使用して Cloud Run アプリケーションを本番環境にデプロイすると、テストチームはエンドユーザーに影響を与えることなく、特定の URL で新しいバージョンにアクセスできます。この方法では、より広範なユーザー ベースに変更を公開することなく、テスト用の運用環境を正確に表現できます。
リンクス：
新しいタグ付きリビジョンをデプロイする
テスト、トラフィックの移行、ロールバックにタグを使用する
サーバーレス アプリで CI/CD を改善する 3 つの方法 |ビデオ
</div></details>

### Q. 問題37: 未回答
アプリケーションのデプロイにSpinnakerを利用し、パイプライン内にカナリアデプロイメントステージを含めました。アプリケーションは、起動時にオブジェクトを読み込むメモリ内キャッシュに依存しています。
カナリアバージョンと本番バージョンの比較を自動化するには、カナリア分析をどのように設定する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Canary を以前の製品バージョンの新しいデプロイと比較します。
Canaryを古いバージョンと比較し、本番環境の現在の状態を反映していないため、理想的ではありません。
C. Canary を現在の製品バージョンの既存のデプロイと比較します。
キャッシュのウォームアップ時間などの変数が発生し、比較の精度に影響を与える可能性があります。
D. カナリアを、以前の製品バージョンのスライディング ウィンドウの平均パフォーマンスと比較します。
より広範な比較を提供しますが、現在の運用環境のニュアンスを正確に反映していない可能性があり、分析を歪める可能性があります。
正解：
A. Canary を現在の製品バージョンの新しいデプロイと比較します。
カナリア テストでは、次の図に示すように、変更を部分的にロールアウトし、ベースライン デプロイに対してそのパフォーマンスを評価します。
この戦略は、カナリア テストのガイドラインに沿っており、カナリアは現在の運用環境を反映したベースライン デプロイと比較されます。このアプローチでは、バージョンと構成を制御し、カナリアに導入された変更のみに焦点を当てることで、その影響をより正確に分析できます。
リンクス：
自動デプロイのセットアップ
Canary を本番環境ではなくベースラインと比較する
カナリア テスト パターン
</div></details>

### Q. 問題38: 未回答
Compute Engine でホストされ、データの保存と取得のために Cloud SQL インスタンスとやり取りするアプリケーションを担当します。最近の更新後、同時アクティブユーザーの数は変更されていないにもかかわらず、データベースタイムアウトメッセージで示されるエラーが発生しました。
これらのデータベース タイムアウトの最も可能性の高い理由を特定するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Compute Engine インスタンスのシリアルポートのログを確認します。
シリアル ポート ログは、主に VM の状態とシステム レベルのイベントに関する情報を提供し、特定のアプリケーションやデータベース接続の問題は提供しません。
B. Cloud Profiler を使用して、アプリケーション全体のリソース使用率を視覚化します。
Cloud Profiler は、CPU やメモリの使用量など、アプリケーションのパフォーマンスを分析するための強力なツールですが、データベース接続の問題を直接特定できない場合があります。これは、アプリケーション コード内のボトルネックを特定するのに便利です。
D. Cloud Security Scanner を使用して、Cloud SQL が分散型サービス拒否(DDoS)攻撃を受けているかどうかを確認します。
Cloud Security Scanner は、ウェブ アプリケーション(特に App Engine と Compute Engine)の脆弱性を特定するために設計されています。Cloud SQL でのタイムアウトや DDoS 攻撃など、データベース固有の問題を検出または分析するようには設計されていません。
正解：
C. Cloud SQL インスタンスへの接続数が増加しているかどうかを確認します。
このオプションは、最近の更新によってデータベース接続が誤って増加したかどうかを特定することに重点を置いていますが、これはユーザー アクティビティだけではすぐにはわからない場合があります。バグや非効率的なクエリにより、必要以上の接続が開かれ、タイムアウトが発生する可能性があります。これは、特にユーザーの負荷が著しく変化していない場合に、更新がデータベースの接続処理能力に影響を与えたかどうかを確認するための直接的で効果的な方法です。
リンクス：
https://cloud.google.com/monitoring/docs/monitoring-overview#monitor-load
</div></details>

### Q. 問題39: 回答
Google Kubernetes Engine(GKE)クラスタのグループへの本番環境のデプロイを監督しています。
信頼できる CI/CD パイプラインによって正常にビルドされたイメージのみが運用環境にデプロイされるようにするには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. クラスタで Cloud Security Scanner を有効にします。
このツールは、App Engine、GKE、Compute Engine でホストされているウェブアプリケーションの脆弱性を特定するために設計されています。ただし、GKE クラスタにデプロイできるイメージに制限は適用されません。
b. コンテナー レジストリで脆弱性分析を有効にします。
これにより、コンテナー イメージの脆弱性がスキャンされますが、デプロイ ポリシーは適用されません。デプロイするイメージを制御するのではなく、イメージのセキュリティ問題を特定することが重要になります。
C. Kubernetes Engine クラスターをプライベート クラスターとして設定します。
クラスタをプライベートとして設定すると、パブリックインターネットから分離されるため、ネットワークセキュリティが強化されます。ただし、これは、CI/CD パイプラインの状態に基づいて特定のコンテナー イメージのデプロイを直接制御または制限するものではありません。
正解：
D. Kubernetes Engine クラスタをバイナリ認証でセットアップします。
Binary Authorization を使用すると、デプロイ時のセキュリティ制御を適用して、信頼できるコンテナ イメージのみが GKE にデプロイされるようにすることができます。CI/CD パイプラインと統合して、デプロイ前にイメージの整合性とソースを検証します。
リンクス：
Binary Authorization の概要
</div></details>

### Q. 問題40: 未回答
会社のソフトウェア配信パイプラインを作成しています。開発チームが提供する新しいアプリケーション機能は、テスト、ステージング、本番環境で Cloud Deploy を使用してデプロイする必要があります。対応するデプロイメント・ターゲットは既に定義されています。リリースを作成して配信パイプラインをインスタンス化しました。デプロイは、Google が推奨する方法に従って実行する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アーティファクト・レジストリで各ステージのイメージを作成し、対応するステージで各イメージを適用します。
デプロイごとに個別のイメージを作成することは、推奨される方法ではありません。
B. 各ステージの前に展開を承認するようにバイナリ承認を構成します。
バイナリ承認は、デプロイ前にセキュリティ チェックを提供します。ただし、デプロイ自体のソリューションではありません。
C. 配信ステージごとに独立した配信パイプラインを作成します。
このユースケースでは、Cloud Deploy で個別の配信パイプラインを作成することはおすすめしません。推奨されるアプローチは、デプロイを次のステージに昇格させることです。
正解：
D. ステージング段階と運用段階を通じて初期リリースをプロモートします。
リリースをインスタンス化すると、最初のターゲットにデプロイされます。ステージングに移行してから、再び本番環境に移行するようにプロモートできます。
リンクス：
https://cloud.google.com/deploy/docs/deploy-app-run
https://cloud.google.com/deploy/docs/promote-release
</div></details>

### Q. 問題41: 回答
組織の仮想マシン (VM) のコストを下げる必要があります。さまざまなオプションを検討した結果、プリエンプティブルVMインスタンスを利用することを選択しました。
プリエンプティブル VM はどの種類のアプリケーションに最も適していますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。スケーラブルなインメモリ キャッシュ システム。
一部のコミュニティメンバーは、そのスケーラビリティと重要でない性質のために適切であると主張していますが、インメモリシステムは一般的に安定性を必要とします。
B.組織の一般向け Web サイト。
ダウンタイムがユーザーエクスペリエンスに影響を与えるリスクがあるため、推奨されません。
C. 十分なクォーラムを持つ、分散された結果整合性のあるNoSQLデータベースクラスタ。
フォールトトレランスのためにこれが実行可能であると考える人もいますが、VMの中断によってデータベースの一貫性とパフォーマンスに大きな影響を与える可能性があります。
正解：
D. ビデオを取得してストレージバケットに保存するGPUアクセラレーションビデオレンダリングプラットフォーム。
注目すべきコミュニティメンバーがこのオプションを支持しており、そのバッチジョブの性質を強調しています。プリエンプティブル VM は、全体的な操作に深刻な影響を与えることなくタスクのチェックポイント設定と再開が可能な、ビデオ レンダリングなどのバッチ処理に適しています。
リンクス：
プリエンプティブルインスタンス上のGPU
</div></details>

### Q. 問題42: 未回答
コードは GitHub に保存され、Cloud Build で手動と Webhook の両方でビルドされます。コードをビルドするには、機密情報にアクセスする必要があります。Google が推奨する方法に従って、機密情報のセキュリティを確保したい。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 毎回コマンド ラインからビルドをトリガーし、必要な機密情報をパラメーターとして指定します。
この方法は、手動による介入が必要なため、Webhook によってトリガーされる自動ビルドには実行できません。さらに、コマンドラインパラメータを介して機密情報を公開しますが、これは安全ではありません。
B. GitHub リポジトリに機密情報のみを含む暗号化ファイルを作成し、Cloud Build ファイルで暗号化されたファイルを参照します。
機密情報を、たとえ暗号化された形式であっても、ソースコードリポジトリに保存することは、セキュリティのベストプラクティスに違反します。GitHub リポジトリにアクセスできる他のユーザーは、これらの暗号化された資格情報にアクセスする可能性があり、最小特権の原則に違反します。
D. コードを Github に保持します。機密情報専用の新しいリポジトリを Cloud Source リポジトリに作成し、Cloud Build ファイルで新しいリポジトリを参照します。
機密情報をソース リポジトリに格納することは、別のリポジトリに分離されている場合でも、お勧めしません。このクラウド ソース リポジトリ リポジトリにアクセスすると、機密情報への不正アクセスが発生し、最小権限の原則に違反する可能性があります。
正解：
C. Secret Manager でエントリを作成し、Cloud Build ファイルで機密シークレットの URI を指定して、ビルド ステップでシークレットをパラメータとして参照します。
Secret Manager の使用は、機密情報を安全に保存することで、ベスト プラクティスと一致します。Cloud Build はこれらのシークレットを安全に参照でき、IAM を使用すると、最小権限の原則に従うことができます。このアプローチにより、手動ビルドと Webhook トリガー ビルドの両方にシークレットを安全かつ便利に含めることができます。
リンクス：
シークレットマネージャーのシークレットを使用する
変数値の置換
</div></details>

### Q. 問題43: 回答
Google Kubernetes Engine(GKE)クラスタで一連のアプリケーションを実行しており、Cloud Kubernetes Engine モニタリングを利用している。次に、サードパーティによって開発された新しいコンテナー化されたアプリケーションを運用環境に導入します。このサードパーティ製アプリケーションは変更できず、再設定することもできません。その情報は /var/log/app_messages.log に記録され、目標はこれらのログエントリを Cloud Logging に転送することです。
これを達成するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. デフォルトの Cloud Kubernetes Engine Monitoring エージェント構成を使用します。
通常、コンテナーから stdout と stderr をキャプチャする既定の構成を利用します。ただし、コンテナー内の /var/log/app_messages.log などの特定のファイルからログが自動的に収集されるわけではありません。
B. Fluentd デーモンセットを GKE にデプロイします。次に、カスタマイズされた入力と出力の構成を作成して、アプリケーションのポッド内のログファイルを追跡し、Cloud Logging に書き込みます。
Fluentd は、特定のログ ファイルを追跡するように構成できますが、DaemonSet として、通常、共有ボリュームやその他の構成がないと、アプリケーション コンテナー内のログ ファイルにアクセスできません。
C. Kubernetes を Google Compute Engine (GCE) にインストールし、アプリケーションを再デプロイします。次に、組み込みの Cloud Logging 構成をカスタマイズして、アプリケーションのポッド内のログファイルを追跡し、Cloud Logging に書き込みます。
GCE に Kubernetes をインストールし、ログ収集用に Cloud をカスタマイズすることは、既存のインフラストラクチャに大幅な変更を加える複雑すぎるアプローチであり、特定の内部ログ ファイルへのアクセスの問題を直接解決するものではありません。
正解：
D. ポッド内のログファイルを追跡し、エントリを標準出力に書き込むスクリプトを記述します。スクリプトをアプリケーションのポッドでサイドカーコンテナとして実行します。コンテナ間の共有ボリュームを構成して、スクリプトがアプリケーションコンテナ内の /var/log への読み取りアクセスを持てるようにします。
アプリケーションコンテナとボリュームを共有するサイドカーコンテナは、特定のログファイル(/var/log/app_messages.log)に直接アクセスして追跡できます。次に、これらのログを独自の stdout または stderr に出力し、ノードのロギング エージェントによってキャプチャされて Cloud Logging に送信されます。このメソッドは、コンテナー内の特定のファイルからログにアクセスして転送する必要性に直接対処します。
リンクス：
ロギング アーキテクチャ
ロギング・エージェントでのサイドカー・コンテナーの使用
</div></details>

### Q. 問題44: 未回答
会社は、ヨーロッパのユーザーにサービスを提供するアプリケーションを実行しています。業界の規制では、特定のリージョンでのみリソースをインスタンス化する必要があります。Google Cloud 組織内のすべてのプロジェクトがこの要件に従っていることを確認する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ログベースのアラートを作成して、リージョンの許可リスト外で作成されたリソースを特定します。非準拠のリソースをすべて削除します。
このアプローチは事後対応型であり、アラートがトリガーされる前に未承認のリージョンにリソースを作成できるようにすることで、規制に違反します。重要なのは、非準拠のリソースの作成を事前に防止することです。
B. 1 時間ごとに資産インベントリ スキャンを実行して、すべてのリソースをカタログ化します。リージョンの許可リストに含まれていないインスタンスをすべて削除します。
オプション A と同様に、この方法は事後対応型であり、規制要件に違反する非準拠のリソースの作成後にのみ識別されます。作成後の検出ではなく、予防に重点を置く必要があります。
C. 新しいインスタンスを作成する権限を持つ限定されたユーザーグループを作成します。コンプライアンスを検証するプロセスを作成します。
制御されたユーザー グループのみに依存しても、人為的エラーや意図的なアクションによって承認されていないリージョンにリソースが作成される可能性があるため、コンプライアンスは保証されません。この方法では、必要な自動適用メカニズムが不足しています。
正解：
D. "in: allowed_values" リスト内の許可されたリージョンを使用して組織ポリシーを構成します。
許可されたリージョンの一覧を含む組織ポリシーを構成すると、これらのリージョンの外部にリソースが作成されるのを効果的に防止できます。このプロアクティブなアプローチは、リソース作成の時点でコンプライアンスが確実に実施され、偶発的または意図的なコンプライアンス違反の可能性を排除することで、規制要件と一致します。
リンクス：
リソースの場所の制限
組織ポリシー サービスの概要
</div></details>

### Q. 問題45: 回答
Terraform を使用して、CI/CD パイプライン内のコードとしてのインフラストラクチャを管理しています。Google Cloud プロジェクトにはインフラストラクチャ スタック全体のコピーが複数存在し、既存のインフラストラクチャに変更が加えられるたびに新しいコピーが作成されることを確認しました。目標は、インフラストラクチャスタックのインスタンスが常に1つだけ存在するようにすることで、クラウド支出を最適化することです。Google が推奨する方法に従う場合。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 新しいパイプラインを作成して、不要になった古いインフラストラクチャスタックを削除します。
削除用の新しいパイプラインの作成は事後対応型であり、複数のスタックが作成される根本原因には対処できません。これにより、複雑さが増す可能性があり、重複スタックの防止が保証されません。
C. パイプラインがソース管理から terraform.tfstate ファイルを格納および取得していることを確認します。
ソース管理に terraform.tfstate を格納することは、状態の競合につながる可能性があり、セキュリティで保護されておらず、状態ロックをサポートしていないため、お勧めしません。
D. パイプラインを更新して、最新の構成を適用する前に既存のインフラストラクチャを削除します。
最新の構成を適用する前にパイプラインを更新して既存のインフラストラクチャを削除すると、不要なダウンタイムとリスクが発生する可能性があります。また、インクリメンタルな変更や状態管理のための実用的なアプローチでもありません。
正解：
B. パイプラインが Terraform gcs バックエンドを使用して Cloud Storage から terraform.tfstate ファイルを保存および取得していることを確認します。
Terraformにgcsバックエンドを使用すると、状態が共有およびロックされ、同時実行によって重複するリソースが作成されるのを防ぎ、Terraformがインフラストラクチャを効果的に管理できるようになります。
これは、Google と Terraform が推奨する、Infrastructure as Code を大規模に管理するためのベスト プラクティスです。
リンクス：
https://developer.hashicorp.com/terraform/language/settings/backends/gcs
</div></details>

### Q. 問題46: 回答
お客様は、Compute Engine でホストされるウェブアプリケーションを担当します。このアプリケーションは、何千人ものユーザーに予約サービスを提供します。新機能をリリースした直後、監視ダッシュボードには、すべてのユーザーがログイン遅延が発生していることが示されます。当面の優先事項は、このインシデントがサービスのユーザーに与える影響を最小限に抑えることです。
最初に何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. クラウド監視を確認します。
これは問題の根本原因を理解するための重要な手順ですが、ユーザーへの影響を最小限に抑えるために必要な即時の解決策が提供されない可能性があります。
C. ログイン サービスを実行している仮想マシンをアップサイズします。
VM のサイズを増やすと、リソースの制約に関連している場合は、問題を軽減できる可能性があります。ただし、問題の原因が新しいリリース自体にある場合は、これが効果的でない可能性があります。
D. 新しいリリースを展開して、問題が解決するかどうかを確認します。
問題の根本原因を理解せずに別のリリースをデプロイすることは危険であり、問題を悪化させる可能性があります。
正解：
A. 最近のリリースをロールバックします。
これは、インシデント管理における一般的で推奨されるプラクティスです。新しいリリースが広範な問題を引き起こす可能性がある場合は、以前の安定したバージョンにロールバックすることで、問題をすばやく軽減できます。これにより、ユーザーはすぐに安心し、サービスに影響を与えることなく問題の原因を調査する時間を確保できます。サービスが安定したら、監視を確認したり、リソースを最適化したり、修正済みリリースのデプロイを検討したりできます。
リンクス：
インシデント管理計画とトレーニングによる軽減時間(TTM)の短縮
</div></details>

### Q. 問題47: 回答
あなたは、Google Kubernetes Engine(GKE)クラスタに複数の制約テンプレートを適用する必要があります。これらの制約には、Kubernetes APIの制限を含むポリシーパラメータが含まれます。目的は、これらのポリシー パラメーターを GitHub リポジトリに格納し、変更が加えられたときに自動的に適用されるようにすることです。
どうすればこれを実現できますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. パラメータが変更されたときに Cloud Build をトリガーするように GitHub アクションを設定します。Cloud Build で gcloud CLI コマンドを実行して変更を適用します。
これには、gcloud CLI コマンドを使用して Cloud Build をトリガーする、より手動のプロセスが含まれます。実現可能ですが、Kubernetes 固有の構成に Anthos を使用するほど合理的ではありません。
B.GitHub に変更があった場合。Webhook を使用して Anthos Service Mesh にリクエストを送信し、変更を適用します。
Anthos Service Mesh は、主にサービスレベルの運用に重点を置いており、Kubernetes API の制約を適用したり、構成管理のために GitHub リポジトリと同期したりするために直接使用されることはありません。
D. GitHub リポジトリで Config Connector を構成します。リポジトリに変更がある場合は、Config Connector を使用して変更を適用します。
Config Connector は、Kubernetes 構成を通じて GCP リソースを管理するために使用されます。リポジトリと同期することはできますが、主な用途は、Anthos Config Management で管理されるような Kubernetes API の制約を適用することではありません。
正解：
C. GitHub リポジトリを使用して Anthos Config Management を構成します。リポジトリに変更があった場合は、Anthos Config Management を使用して変更を適用します。
このツールは、GitHub リポジトリと同期するように構成できます。リポジトリ内の制約テンプレートに変更が加えられると、Anthos Config Management によってこれらの変更が GKE クラスタに自動的に適用されます。
このアプローチにより、クラスタ構成とポリシー適用の管理が合理化され、GKE 環境が GitHub リポジトリで定義されたポリシーと一貫性を維持できるようになります。
リンクス：
Policy Controller の概要
Anthos Config Management の概要
</div></details>

### Q. 問題48: 未回答
会社では、Compute Engine 仮想マシン(VM)にデプロイされたアプリケーションを使用して、顧客にサービスを提供しています。毎月 99% の可用性を提供するサービス レベル アグリーメント (SLA) があります。月の最初の 22 日間のテレメトリ データによると、99.28% の可用性で 10,000,000 件のリクエストを処理しました。開発チームは、アプリケーションに新しい機能を追加し、それらをデプロイしたいと考えています。サイト信頼性エンジニアリング (SRE) のプラクティスに従いながら、展開アプローチを決定する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 新機能をすぐにコンフィデンシャル VM にデプロイします。
コンフィデンシャル VM へのデプロイは、セキュリティの側面に重点を置き、エンド ユーザー要求の高可用性の確保には直接関係しません。このオプションでは、エラーバジェットの現在のステータスと、新機能のデプロイが可用性に及ぼす潜在的な影響は考慮されません。
C. SLO を変更し、新しい機能をすぐにデプロイするよう顧客に依頼します。
SLO は、SLA の達成を支援するために設定された内部目標であり、顧客と直接交渉されるものではありません。SLO を変更しても、SLA に基づく契約上の義務は変更されず、現在の可用性を危険にさらすことは正当化されません。
D. SLA を 98.5% に変更し、新機能をすぐにデプロイするように顧客に依頼します。
SLA の変更、特により低い標準への変更を提案することは、エラー バジェットが厳しい状況に対応するための一般的または推奨されるアプローチではありません。顧客の信頼と信頼性の認識に悪影響を与える可能性があります。焦点は、アプリケーションの更新を責任を持って管理しながら、合意されたSLAを維持することです。
正解：
B. 新機能の展開を翌月まで延期します。
既存の可用性が 99.28% で、残りのエラー バジェットが少ない場合、新機能を展開すると SLA に違反するリスクがあります。SRE のプラクティスでは、特にエラー バジェットが低い場合は、新機能の導入と安定性の維持のバランスを取ることが重要です。デプロイを延期すると、可用性がさらに低下するリスクが最小限に抑えられ、SLA の遵守に役立ちます。
リンクス：
エラー予算ポリシーの例
リスクの受け入れ
</div></details>

### Q. 問題49: 回答
組織は、Google Cloud プロジェクトのダッシュボードを Cloud Operations で作成するために、システムログを収集したいと考えています。既存および将来のすべての Compute Engine インスタンスを構成して、システムログを収集し、Ops Agent が最新の状態に保たれるようにする必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. gcloud CLI を使用して、Cloud Asset Inventory にリストされている各 VM に Ops Agent をインストールします。
この方法では、gcloud コマンドライン インターフェース(CLI)を使用して、Cloud Asset Inventory にリストされている各仮想マシン(VM)に Operations(Ops)Agent を手動でインストールする必要があります。このアプローチにより、各 VM のインストール プロセスを直接制御できるため、特定の要件に基づいてカスタマイズできます。ただし、特に多数の VM がある環境では、非常に時間がかかる可能性があります。 さらに、この方法では、将来作成される可能性のある新しい VM インスタンスに対する自動化されたソリューションが提供されないため、すべての新規および既存の VM に Ops エージェントがインストールされ、更新されていることを確認するために、手動による介入が継続的に必要になります。
B. [Cloud Operations VMs] ダッシュボードで [Agent status] が [Not detected] のすべての VM を選択します。次に、 [エージェントのインストール] を選択します。
このオプションでは、Cloud Operations VM ダッシュボードを使用して、現在 Ops Agent がインストールされていない VM を特定します。その後、ユーザーはこれらの VM へのエージェントのインストールを手動で開始できます。この方法はユーザーフレンドリーで、エージェントを簡単に識別して必要な VM にインストールするためのグラフィカル インターフェイスを提供します。特にエージェントのない VM を対象とし、監視機能のない VM が残らないようにします。ただし、前のオプションと同様に、このアプローチでは手動による介入が必要であり、新しく作成された VM にエージェントを自動的にインストールするためのソリューションは提供されません。 ユーザーはダッシュボードを定期的にチェックして、新しい VM にエージェントを特定してインストールする必要があります。
D. スタートアップ スクリプトを使用して Compute Engine イメージに Ops Agent をインストールする
スタートアップ スクリプトを使用して Compute Engine イメージに Ops Agent をインストールするということは、インストール プロセスを VM の初期化手順に埋め込むことを意味します。このアプローチにより、イメージからインスタンス化された新しい VM には、必要な監視機能が事前に装備されます。これは、VM が頻繁にスピンアップおよびスピンダウンされる環境で特に役立ちます。ただし、この方法では、スクリプトが実装される前に作成された既存の VM を更新する必要性には対応していません。また、起動スクリプトは、最新のエージェントのバージョンと構成に合わせて保守および更新する必要があります。
正解：
C. gcloud CLI を使用してエージェント ポリシーを作成します。
このアプローチにより、Google Cloud プロジェクト内の既存および将来のすべての Compute Engine インスタンスに Ops Agent を自動的にインストールして管理できます。エージェントポリシーを作成することで、インフラストラクチャ全体で一貫した監視を確保し、手作業と監視の可能性を大幅に削減します。この方法は、VM が頻繁に作成および使用停止される動的なクラウド環境で特に効果的です。
リンクス：
エージェント・ポリシーの管理
</div></details>

### Q. 問題50: 回答
週末のメンテナンス期間中、つまりユーザトラフィックの少ない時期に、内部アプリケーションの新しいバージョンを起動しました。しかし、メンテナンス後、本番環境で新機能が誤動作していることが判明しました。これにより、停止が長引くため、リリースのロールバックとその後の修正アップデートの展開が促されました。
リリース戦略を強化し、平均復旧時間を最小限に抑えて、将来の長時間の停止を防ぐには、どのような手順を実行する必要がありますか?(2 つのオプションを選択します。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。新しいコードをマージする前に、2 つの異なるピアにコードの変更をレビューしてもらう必要があります。
ピアレビューはコードの品質を向上させることができますが、本番環境のような条件下でのみ発生する問題を検出できない場合があります。
C. コード リンティング ツールを統合して、コードがリポジトリに受け入れられる前にコーディング標準を検証します。
Linting は、コーディング標準とスタイルのガイドラインが満たされていることを保証しますが、運用環境に固有の機能エラーや問題を特定できない場合があります。
D. 開発者に、リリース前にローカル開発環境で自動統合テストを実行するように要求します。
統合テストをローカルで実行する開発者は、いくつかの問題を早期に発見できます。ただし、ローカル環境では運用条件が完全に再現されない可能性があり、重大な問題を見逃す可能性があります。
正解：
B. CD サーバー経由で新しいコードをリリースする場合は、ブルー/グリーン デプロイ戦略を採用します。
これにより、2 つの運用環境 (青と緑) を作成し、1 つが現在のバージョンをホストし、もう 1 つが新しいリリースをホストできます。問題が発生した場合は、すぐに安定バージョンに戻すことができ、ダウンタイムを短縮できます。
E. CI サーバーを構成します。単体テストのスイートをコードに追加し、CI サーバーにコミット時に実行させ、変更を確認します。
一連の単体テストで継続的インテグレーション (CI) を実装すると、コミットのたびにコードの変更が自動的にテストされます。これにより、問題を早期に発見し、生産における重大な誤動作の可能性を減らすことができます。
</div></details>

## 3
### Q.  問題1: 未回答
アプリケーションが Google Cloud Platform(GCP)でホストされており、Jenkins を使用してアプリケーション リリースを GCP にデプロイする予定である。目標は、リリース プロセスを合理化し、運用上のオーバーヘッドを削減し、ユーザー データのセキュリティを維持することです。
どのようなステップを踏むべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Jenkins をローカル ワークステーションに実装します。
これは、スケーラブルな CI/CD セットアップには理想的ではありません。メンテナンスやスケーラビリティの面で課題があり、クラウドのメリットを活用していません。
B. オンプレミスの Kubernetes に Jenkins を実装します。
Kubernetes は優れたスケーラビリティを提供しますが、オンプレミスの設定では運用上のオーバーヘッドが高くなり、GCP サービスとシームレスに統合されない可能性があります。
C. Google Cloud Functions に Jenkins を実装します。
Jenkins は、Cloud Functions のようなサーバーレス プラットフォームで実行するようには設計されていません。Cloud Functions はイベントドリブンのステートレスなワークロードを対象としているため、このオプションは実現できません。
正解：
D. Compute Engine 仮想マシンに Jenkins を実装します。
GCP の Compute Engine で Jenkins を実行すると、スケーラブルで安全かつ効率的な CI / CD パイプラインを実現できます。必要なリソースとGCPサービスとの統合機能を提供し、運用効率とセキュリティコンプライアンスの両方を確保します。このアプローチは、リリースプロセスを合理化し、データセキュリティを維持するという目標とよく一致しています。
リンクス：
https://plugins.jenkins.io/google-compute-engine/
</div></details>

### Q.  問題2: 未回答
実稼働環境で Java アプリケーションを分析している。これらのアプリケーションにはすべて、Cloud Profiler と Cloud Trace がデフォルトでインストールされ、構成されています。目標は、パフォーマンスのチューニングが必要なアプリケーションを特定することです。
これを達成するために取るべき2つのステップは何ですか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーションの実時間と CPU 時間を調べます。差が大きい場合は、CPU リソースの割り当てを増やします。
これは、アプリケーションが待機に多くの時間を費やす場合 (CPU 時間と比較して実時間が多い場合)、より多くの CPU リソースの恩恵を受ける可能性があることを意味します。ただし、このアプローチでは、CPU リソースの制限が待機の原因であることを前提としており、常にそうであるとは限りません。
B. アプリケーションの実時間と CPU 時間を調べます。差が大きい場合は、メモリ リソースの割り当てを増やします。
A と似ていますが、メモリ割り当てを増やすことを提案します。これは、メモリの制約によってアプリケーションが待機している場合に関連している可能性がありますが、これはウォールクロック時間とCPU時間の違いによって直接示されるものではありません。
C. アプリケーションの実時間と CPU 時間を調べます。差が大きい場合は、ローカル・ディスク・ストレージの割り振りを増やします。
このオプションは、実時間と CPU 時間の間に大きな不一致がある場合に、ディスク I/O がボトルネックになる可能性があることを示唆しています。ただし、これはあまり一般的ではないシナリオであり、通常は特定の I/O ボトルネック インジケーターが必要になります。
正解：
D. アプリケーションの待機時間、実時間、および CPU 時間を調べます。レイテンシー時間がゆっくりとエラーバジェットを消費し、実時間とCPU時間の差が最小の場合は、アプリケーションに最適化のマークを付けます。
レイテンシー時間が徐々にエラーバジェットを消費し、実時間とCPU時間の差が最小の場合は、アプリケーションのパフォーマンスが非効率的であることを示しており、最適化の候補としてマークされています。
E. アプリケーションのヒープ使用量を調べます。使用率が低い場合は、アプリケーションに最適化のマークを付けます。
ヒープ使用率が一貫して低い場合は、メモリ リソースの割り当てが過剰である可能性があり、効率とリソース使用率を改善するための最適化領域です。
リンクス：
https://cloud.google.com/profiler/docs/concepts-profiling
Kubernetes Engine での Cloud Trace の使用
</div></details>

### Q.  問題3: 未回答
Compute Engine インスタンスでカスタム アプリケーションを管理すると、使用率が最大 90% まで急上昇することがありますが、通常は使用率は約 50% に戻ります。
現在、サーバーの使用率が 80% を超えた場合のアラートがありますが、受信する通知が多すぎます。
少なくとも 5 分間、一貫して高い使用率のみを通知するようにアラートを変更するにはどうすればよいですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 5 分間、使用率が 80% を超えることをフィルタリングする Cloud Run アプリケーションへの直接通知。
アラート分析用のカスタム Cloud Run アプリケーションを作成することは、この要件に対して不必要に複雑でリソースを大量に消費するソリューションです。
B. ログベースのメトリックからアラートを構成し、5 分間のローリング ウィンドウとメトリックの不在条件を設定します。
メトリックの不在に基づくアラートは、使用率が高いという問題には対処しません。むしろ、データがない期間や使用率が低い期間を示します。
D. 5 分のローリング ウィンドウと平均しきい値が 90% のログベースのメトリックからアラートを作成します。
しきい値を 90% に設定すると、通知の頻度は減りますが、80% から 90% の間で高い使用率が持続しているインスタンスは失われます。
正解：
C. 5 分間のローリング ウィンドウと平均しきい値が 80% のログベースのメトリックからアラートを設定します。
5 分間のローリング ウィンドウと平均しきい値を 80% に設定してアラートを構成すると、その期間にわたって使用率が一貫して高い場合にのみ通知されます。このアプローチは、頻繁で短命なスパイクの問題に効果的に対処し、持続的な高い使用率に焦点を当てています。
リンクス：
ログベースのメトリクスの概要
アラートポリシーの例の概要
Cloud Monitoring に関するアラートの作成 |ビデオ
</div></details>

### Q.  問題4: 未回答
Compute Engine でアプリケーション サーバーのグループを管理しており、デベロッパーがデバッグのためにアプリケーションログに簡単にアクセスできるように、構成の労力を最小限に抑えた安全な方法を探しています。
このソリューションをGoogle Cloud Platform(GCP)に実装するには、どのようなアプローチを使用しますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Cloud Logging Agent と Logs Private Viewer ロール:このオプションにはログ エージェントのデプロイも含まれますが、"ログ プライベート ビューアー" ロールはより制限が厳しく、機密データを含むログへのアクセスを必要とするシナリオに適しています。
C. Monitoring Viewer ロールを持つ Cloud Monitoring Agent:このセットアップは、ログではなくメトリックに重点を置いているため、デバッグ目的で詳細なアプリケーション ログにアクセスする必要がある開発者にはあまり適していません。
D. Cloud Storage にログをアップロードするための gsutil: この方法では、追加のスクリプトと手動設定が必要です。Google Cloud のオペレーション スイート(Stackdriver)を使用する場合に比べて効率が悪く、効果的なデバッグに不可欠なリアルタイムのログ アクセスも提供されません。
正解：
ある。
Cloud Logging エージェントをアプリケーション サーバーにデプロイします。
開発者に、ログにアクセスして表示するためのIAMログ閲覧者ロールを付与します。
この方法では、ログを一元管理するためのログエージェントのデプロイと適切な IAM ロールを組み合わせて、ログへの安全なロールベースのアクセスを確保します。効率的で、セキュリティを維持しながら構成の労力を最小限に抑えます。
リンクス：
ログエクスプローラーを使用してログを表示するView logs by using the Logs Explorer
IAMによるアクセス制御
</div></details>

### Q.  問題5: 回答
Cloud Run を使用してアプリケーションをホストし、ライブ プロダクション トラフィックを使用してアプリケーションの新しいバージョンをテストする予定である。同時に、品質保証チームが手動テストを実施します。目的は、テストフェーズ中の問題の潜在的な影響を最小限に抑え、必要に応じて以前のアプリケーションバージョンに戻す機能を維持することです。
新しいバージョンに推奨される 2 つの展開方法は何ですか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーションを新しい Cloud Run サービスとしてデプロイします。
これにより、新しいバージョンが完全に分離されますが、同じサービス内で段階的なロールアウトや簡単なロールバックの利点は活用されません。
C. タグなしで新しい Cloud Run リビジョンをデプロイし、--no-traffic オプションを使用します。
タグなしでデプロイすると、特にテスト用のトラフィックフローを管理する場合に、特定のリビジョンの管理と識別が困難になります。
D. 新しいアプリケーションバージョンをデプロイし、--no-traffic オプションを使用します。本番トラフィックをリビジョンの URL にルーティングします。
これはオプション B と似ていますが、タグ付けを行わないと、トラフィックのルーティングとテストのリビジョンの管理と識別の効率が低下する可能性があります。
正解：
B. タグを使用して新しい Cloud Run リビジョンをデプロイし、--no-traffic オプションを使用します。
これにより、ライブトラフィックをすぐに転送することなく新しいバージョンをデプロイできるため、制御されたテストと必要に応じて迅速なロールバックが可能になります。
E. 新しいアプリケーションバージョンを導入し、トラフィックを新しいバージョンに分割します。
カナリアデプロイと呼ばれるこのアプローチでは、パフォーマンスを綿密に監視しながら、トラフィックの一定割合を新しいバージョンに徐々にルーティングできます。
リンクス：
テスト、トラフィックの移行、ロールバックにタグを使用する
</div></details>

### Q.  問題6: 未回答
数時間にわたってすべてのユーザーに影響を与える重大なサービスの中断が発生しました。広範なインシデント管理作業の後、通常のサービス運用とユーザーアクセスが復元されました。サイト信頼性エンジニアリングのベストプラクティスに沿って、関連する利害関係者向けのインシデントレポートを準備する必要があります。
このプロセスで最初に取るべきステップは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 個々の関係者に電話して、何が起こったのかを説明します。
直接的なコミュニケーションは重要ですが、重大なインシデント発生後に即時かつ包括的に情報を広めることは現実的ではありません。事後分析は、詳細な書面による記録を提供します。
C. インシデント状態ドキュメントをすべての関係者に送信します。
このドキュメントは重要ですが、通常、事後分析で見つかった包括的な分析と将来のアクション プランを提供しません。
D. 担当エンジニアに、すべての関係者に謝罪メールを書くよう要求する。
SRE は、個人の責任よりも学習と改善を重視します。責任あるエンジニアに謝罪文を書くことを要求することは、これらの原則に沿うものではなく、組織の学習に必要な詳細な分析を提供しません。
正解：
B. 関係者に配布する事後分析を作成する。
重大なサービス中断後の事後分析レポートは不可欠です。何が起こったのか、なぜ起こったのか、影響、どのように解決されたのか、今後このようなインシデントを防ぐためにどのような手順を踏むのか、詳細な分析を含める必要があります。このアプローチにより、透明性が確保され、組織の学習が促進されます。
リンクス：
事後分析文化:失敗から学ぶ
</div></details>

### Q.  問題7: 未回答
Google Kubernetes Engine(GKE)でホストされている本番環境アプリケーション内の問題のトラブルシューティングを行っています。調査の結果、問題の原因は最近更新されたコンテナー イメージであることが判明しましたが、具体的なコード変更は特定されていません。現在、デプロイは latest タグを使用するように構成されています。問題を解決し、クラスターがコンテナーの機能バージョンを確実に実行するには、何をする必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 以前に動作していたコンテナーを指す stable という新しいタグを作成し、新しいタグを指すようにデプロイを変更します。
安定したタグを作成することは役に立ちますが、タグは変更可能であり、別の画像を指すように変更できるため、将来同様の問題が発生する可能性があります。
C. 以前の Git タグから新しいコンテナーをビルドし、新しいコンテナーへのデプロイでローリング更新を行います。
既知の良好な状態から新しいコンテナーをビルドしてデプロイすることは有効なアプローチですが、既存の作業イメージを使用する場合に比べて時間がかかる場合があります。
D. latest タグを以前のコンテナー イメージに適用し、デプロイでローリング更新を実行します。
以前の作業イメージに最新のタグを付け直すと、問題をすばやく解決できる場合があります。ただし、オプションAと同様に、タグの可変性は、後で混乱や同様の問題を引き起こす可能性があります。
正解：
B. 以前に動作していたコンテナーの sha256 ダイジェストを指すようにデプロイを変更します。
sha256 ダイジェストを指定することで、デプロイで以前に動作していたコンテナーの正確なバージョンが使用されるようにすることができます。これは、変更または更新できるタグを使用するよりも信頼性の高いアプローチです。
この方法は正確であり、機能することがわかっているコンテナイメージの正確なバージョンがデプロイされ、変更可能なタグに関連する問題を回避できます。
リンクス：
コンテナイメージダイジェストについて
</div></details>

### Q.  問題8: 回答
「europe-west2-a」ゾーンの 1 つの Compute Engine インスタンスにデプロイされたステートレスなウェブベースの API を維持する必要があります。サービス可用性のサービス レベル インジケーター (SLI) が、指定されたサービス レベル目標 (SLO) を下回っています。事後分析の結果、API への要求が頻繁にタイムアウトになることが確認されており、これは主に API で大量の要求が発生し、メモリが不足していることが原因です。目標は、サービスの可用性を高めることです。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. VPC フローログは、本番環境の VPC ネットワークのフロントエンドとバックエンドのサブネットで、サンプルボリュームスケールが 0.5 の場合にのみ有効にします。
サンプルボリュームスケール 0.5 の VPC フローログを有効にします。これにより、トラフィックの半分しかキャプチャされないため、API の問題の診断に必要な重要なデータが見落とされる可能性があります。完全なキャプチャに比べて包括的ではありません。
C. ボリュームスケール 0.5 のテストおよび本番環境の VPC ネットワークフロントエンドおよびバックエンドサブネットで VPC フローログを有効にします。運用前にテストで変更を適用します。
テストと本番環境の両方で VPC フローログを実装し、サンプル量スケールは 0.5 です。テストは含まれていますが、サンプルレートが低いと、徹底的な分析に十分な詳細が得られない可能性があります。
D. ボリュームスケール 1.0 のテストおよび本番 VPC ネットワークのフロントエンドおよびバックエンドサブネットで VPC フローログを有効にします。運用前にテストで変更を適用します。
オプションCと似ていますが、サンプル量スケールが1.0です。このオプションは、対象となる環境とキャプチャされるデータの点で包括的ですが、主な関心事が実稼働環境での現在のパフォーマンスである場合は必要ない場合があります。
正解：
B. VPC フローログは、サンプルボリュームスケールが 1.0 の本番環境 VPC ネットワークフロントエンドおよびバックエンドサブネットでのみ有効にします。
最適な選択肢になります。サンプルボリュームスケール 1.0 の本番環境 VPC ネットワークのフロントエンドとバックエンドの両方のサブネットで VPC フローログを有効にすると、すべてのネットワークトラフィックを包括的にキャプチャできます。このレベルの詳細は、タイムアウトやメモリ過負荷の潜在的な原因の特定など、問題の分析に不可欠です。すべてのネットワーク イベントをキャプチャすることで、サンプリング レートを低くしたり、範囲をテスト環境に制限したりするよりも、サービスの可用性の問題の根本原因を診断して対処できる可能性が高くなります。
リンクス：
ログのサンプリングと処理
</div></details>

### Q.  問題9: 未回答
Web アプリケーションの新機能に対するユーザーの反応を測定するための実験を実施しています。この機能をカナリアリリースとしてデプロイした後、ユーザーに送信される 500 件のエラーが急増し、監視データからレイテンシーの増加が示されています。
ユーザーへの悪影響を早急に軽減するために、まず取るべき行動は何でしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 遅延、トラフィック、エラー、飽和の監視を開始します。
監視は重要ですが、この場合、問題の即時軽減に従う必要があります。監視は継続的なアクティビティであり、カナリアのリリース前とリリース中にすでに実施されているはずですが、エラーの急増に対応するための最初のステップではありません。
C. インシデントの事後分析文書のデータを記録します。
事後分析のためにインシデントを文書化することは重要ですが、それは最初のステップではありません。最初に、エラーと遅延の急増の原因となっている問題に対処する必要があります。
D. 500 エラーの原因と遅延増加の根本原因を追跡します。
根本原因の調査は、長期的な解決と将来の発生を防ぐために重要です。ただし、当面の優先事項は、システムを安定させ、サービスの継続性を確保することであり、通常は問題を引き起こした変更をロールバックします。安定性が回復したら、徹底的な調査を行うことができます。
正解：
A. 実験的な Canary リリースをロールバックします。
新機能のデプロイ直後に 500 エラーが急激に増加し、待機時間が長くなることを考えると、最初に変更をロールバックするのが賢明です。このクイックアクションは、システムを安定させ、ユーザーエクスペリエンスへの悪影響を軽減することを目的としています。システムが安定したら、問題の原因の調査、事後分析のためのデータの収集、その他の監視アクティビティの検討に進むことができます。このアプローチは、検出された問題に対応してユーザーへの影響を最小限に抑えるという主な目的と一致しています。
リンクス：
https://sre.google/workbook/canarying-releases/
</div></details>

### Q.  問題10: 回答
Cloud Build を利用して新しい Docker イメージを作成し、Docker Hub にプッシュする CI / CD パイプラインがあります。Git はコードのバージョン管理に使用されます。Cloud Build の YAML 構成を変更すると、パイプラインが新しいアーティファクトを生成していないことがわかります。
サイト信頼性エンジニアリング (SRE) の原則に従ってこの問題に対処するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. CI パイプラインを無効にし、成果物を手動でビルドしてプッシュする方法に戻します。
これは効率的でも持続可能なソリューションでもありません。CIパイプラインを無効にして手動プロセスに戻すと、自動化と継続的インテグレーションの原則が損なわれ、エラーや非効率性のリスクが高まります。
B. 成果物をプッシュするように CI パイプラインを変更すると、Docker Hub ではなく Container Registry になります。
このオプションでは、パイプラインが新しい成果物を生成しないという問題は解決されません。成果物の宛先を変更しても、パイプライン構成の根本的な問題は解決されません。
C. 構成 YAML ファイルを Cloud Storage にアップロードし、エラー報告を使用して問題を特定して修正します。
このアプローチでは、Cloud Build の YAML 構成の問題に直接対処するものではありません。エラー報告は、アプリケーションのランタイム エラーを特定するための便利なツールですが、CI/CD パイプライン構成の問題をデバッグするための適切なツールではありません。
正解：
D. 以前の Cloud Build 構成ファイルと現在の Cloud Build 構成ファイルを Git で比較して、バグを見つけて修正します。
このオプションは、問題の根本原因を特定して解決することに重点を置いたサイト信頼性エンジニアリングの原則に沿ったものです。以前の構成ファイルと現在の構成ファイルを比較することで、新しい成果物の生成に失敗した可能性のある変更を特定し、的を絞った効果的な修正が可能になります。
イメージ レジストリの変更(オプション B)やパイプラインの無効化(オプション A)など、その他のオプションでは、Cloud Build の YAML 構成の変更後にパイプラインがアーティファクトの生成を停止した根本的な原因には直接対処できません。Cloud Storage とエラー報告を含むオプション C は、Cloud Build の YAML 構成の問題の修正には直接適用されません。
リンクス：
https://cloud.google.com/build/docs/troubleshooting
</div></details>

### Q.  問題11: 回答
サイト信頼性エンジニアとして、Google Kubernetes Engine(GKE)の本番環境で動作する Go ベースのアプリケーションのサポートを提供します。新しいバージョンのアプリケーションのリリース後、予期せず再起動する前に、約 15 分間スムーズに実行されることがわかります。これに対応して、Cloud Profiler をアプリケーションに組み込み、ヒープ使用量の継続的な増加を検出し、アプリケーションの定期的な再起動を実現します。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーション展開の CPU 制限を増やします。
CPU 制限を増やしても、直面しているメモリの問題に直接対処することはできません。CPU 関連のボトルネックには役立つかもしれませんが、アプリケーションのメモリ不足と再起動を防ぐことはできません。
B. ハイ メモリ コンピューティング ノードをクラスターに追加します。
高メモリのコンピューティング ノードをクラスターに追加すると、クラスターにより多くのメモリ リソースが提供される可能性がありますが、アプリケーション自体のメモリ増加の根本原因には対処できません。最初にアプリケーション内のメモリ使用量を最適化することをお勧めします。
D. Cloud Trace をアプリケーションに追加し、再デプロイします。
Cloud Trace を追加すると、パフォーマンスのモニタリングとトレースに役立ちますが、アプリケーションの再起動の原因となっているメモリ増加の問題が直接解決されるわけではありません。Cloud Trace を Cloud Profiler と組み合わせて使用すると、アプリケーション全体のモニタリングを改善できますが、メモリの問題に対する直接的な解決策にはなりません。
正解：
C. アプリケーション展開のメモリ制限を増やします。
アプリケーション展開のメモリ制限を増やすと、アプリケーションがメモリ要件をより効果的に処理するのに役立ちます。これにより、アプリケーションのメモリ不足や再起動を防ぐことができます。ただし、アプリケーションのメモリ使用量が継続的に増加している理由も調査する必要があります。アプリケーション内でメモリ リークや非効率的なメモリ管理が発生している可能性があり、対処する必要があります。
リンクス：
https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes
</div></details>

### Q.  問題12: 未回答
現在、Google Cloud での CI / CD パイプラインのデプロイとテストの戦略を開発中である。主な目標は、リリース展開の複雑さを軽減し、展開ロールバックの期間を最小限に抑えることです。さらに、実際の本番トラフィックでテストしながら、影響を受けるユーザーの数を徐々に増やす必要があります。
どのような展開とテストの戦略を選択する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. デプロイとカナリア テストを再作成する
デプロイを再作成します。この戦略では、アプリケーションの既存のバージョンを削除し、新しいバージョンに置き換えます。これはより単純な展開形式ですが、切り替え中に大幅なダウンタイムが発生する可能性があります。
カナリアテスト: オプション B と同様に、最初にユーザーの小さなサブセットに変更をロールアウトします。
この組み合わせにより、実際のトラフィックでテストできますが、再作成アプローチのダウンタイムは、特にデプロイの期間と複雑さを最小限に抑えることを目的とする場合、重大な欠点です。
C. ローリング更新プログラムのデプロイと A/B テスト
ローリング更新プログラムの展開:古いバージョンのアプリケーションのインスタンスを新しいバージョンに段階的に置き換えて、ダウンタイムを最小限に抑えます。
A/Bテスト:アプリケーションの 2 つ以上のバージョンを同時に実行し、それらの間でトラフィックを分割してパフォーマンスを比較します。
このアプローチにより、ダウンタイムが短縮され、実際のユーザーでさまざまなバージョンをテストできます。ただし、異なるバージョンを同時に管理する場合、より複雑になる場合があり、ロールバックは Blue/Green デプロイと比較して遅くなる可能性があります。
D. ローリング更新プログラムの展開とシャドウ テスト
ローリング更新プログラムの展開:オプションCで説明したように。
シャドウテスト:ユーザーへの実際の応答に影響を与えることなく、新しいバージョンのアプリケーションへの着信トラフィックを複製します。
この戦略では、ユーザーに影響を与えることなく実際の条件下でテストを行うことができますが、複雑でリソースを大量に消費する可能性があります。ブルー/グリーンデプロイほど単純なロールバックメカニズムは提供されません。
正解：
B. ブルー/グリーン デプロイとカナリア テスト
ブルー/グリーン展開:この方法では、2 つの同一の環境 (青と緑) を使用します。1 つの環境では現在の運用バージョン (青) が実行され、もう 1 つの環境は新しいバージョン (緑) をホストします。これらの環境は簡単に切り替えることができるため、デプロイ プロセスが簡素化され、必要に応じて青色の環境にすばやく戻すことができるため、ロールバックの期間が大幅に短縮されます。
カナリアテスト: ユーザー ベース全体に展開する前に、変更を少数のユーザーのサブセットに徐々にロールアウトします。これにより、新しいバージョンが実際の本番トラフィックでどのように機能するかを監視し、新しいバージョンの影響を受けるユーザーベースを段階的に増やすことができます。これは、本番環境のトラフィックでテストし、影響を受けるユーザーの数を徐々に増やすという目標と一致しています。
この組み合わせにより、堅牢なデプロイ手法と、実際のユーザー トラフィックを使用した効果的なテストのバランスが取れます。
リンクス：
アプリケーションのデプロイとテストの戦略
</div></details>

### Q.  問題13: 回答
ビジネスクリティカルなワークロードを、固定された Compute Engine インスタンスのセットで数か月間実行する必要がある。ワークロードは安定しており、正確な量のリソースが割り当てられています。パフォーマンスに影響を与えることなく、このワークロードのコストを削減する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. インスタンスをマネージド・インスタンス・グループに移行します。
インスタンスのスケーリングと管理のためのものであり、コスト削減ではありません。
C. インスタンスをプリエンプティブル仮想マシンに変換します。
その刹那的な性質のために危険です。重要なワークロードには適していません。
D. ワークロードの実行に使用するインスタンスのアンマネージドインスタンスグループを作成します。
主にインスタンスを整理するためのものであり、コストを削減するためのものではありません。
正解：
A. 確約利用割引の購入。
これは、一貫性のある長期的な使用のための大幅なコスト削減を提供するため、最適なオプションです。一定の期間 (通常は 1 年または 3 年) にコミットすると、VM の使用率が下がり、安定したワークロードのニーズに適切に適合します。
リンクス：
https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts
</div></details>

### Q.  問題14: 未回答
アプリケーションには、計算負荷の高いタスクを担当するモジュールが含まれています。開発チームは、さまざまなアルゴリズムをテストすることで、このモジュールを最適化することを目指しています。ステージング環境と本番環境の両方で各アルゴリズムのパフォーマンスを評価し、包括的なパフォーマンスデータを取得するには、どうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. コードをインストルメント化して、要求と応答のフローをキャプチャします。Cloud Trace でリクエストのレイテンシを確認します。
Cloud Trace は主に分散トレースに重点を置いており、複数のサービス間呼び出しがあるアプリケーションでのやり取りをモニタリングするのに適しています。アルゴリズムの最適化に必要な詳細なパフォーマンス指標を提供しないため、特定のコードセグメントやアルゴリズムの詳細なベンチマークには効果的ではありません。
C. アルゴリズムのさまざまな部分でログステートメントを使用してコードをインストルメント化します。ログベースの通知を送信して、アルゴリズムの実行に関するデータを収集します。
ログステートメントは実行フローに関する洞察を提供しますが、アルゴリズムのタイミングとパフォーマンスを変更し、結果に偏りが生じる可能性があります。さらに、ログステートメントによって収集されるデータは、通常、それほど詳細ではなく、視覚化の利便性が低いため、コード内の特定のパフォーマンスの問題やボトルネックを特定するのに効果的ではありません。
D. アルゴリズムのさまざまな部分でログステートメントを使用してコードをインストルメント化します。同じ制御構造が複数回呼び出されたときに、ログメトリックに基づいて通知を送信します。
オプション C と同様に、ログ ステートメントを使用すると、アルゴリズムの実際のパフォーマンスが妨げられる可能性があり、詳細または効率的な視覚化の手段は提供されません。ログメトリクスに基づいて通知を設定すると、繰り返しの呼び出しを特定するのに役立つ場合がありますが、効果的なアルゴリズムの最適化に必要な包括的なパフォーマンス分析は提供されません。
正解：
A. プロファイリングライブラリを使用してコードをインストルメント化します。Cloud Profiler でデータをフレームグラフとして視覚化します。
Cloud Profiler と統合されたプロファイリング ライブラリを利用すると、アプリケーションのパフォーマンスを詳細に把握できます。フレーム グラフは、呼び出し履歴を視覚的に表現し、パフォーマンスのボトルネックを特定して対処するのに役立ちます。この方法は、非効率なコードセグメントを視覚的にピンポイントで特定することにより、アルゴリズムを分析および最適化するのに効果的です。
リンクス：
プロファイリングの概念
アプリのパフォーマンスを測定する
Goでアプリのパフォーマンスを向上させるためのインストゥルメント(パート1:トレース)
Go でアプリのパフォーマンスを向上させるためのインストゥルメント (パート 2:プロファイラー)
</div></details>

### Q.  問題15: 回答
パブリック IP アドレスを持つ Compute Engine インスタンスで実行される新しいアプリケーション用に Cloud Logging を構成しています。ユーザー管理サービス アカウントがインスタンスにアタッチされます。必要なエージェントがインスタンスで実行されていることを確認しましたが、Cloud Logging でインスタンスのログエントリを表示できません。Google が推奨する方法に従って問題を解決したい。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. サービス アカウント キーをエクスポートし、そのキーを使用するようにエージェントを設定します。
サービスアカウントキーを使用したエージェントのエクスポートと設定は、サービスアカウントキーの管理に関連するセキュリティリスクのため、一般的には推奨されません。
B. デフォルトの Compute Engine サービス アカウントを使用するようにインスタンスを更新します。
既定のサービス アカウントには必要なアクセス許可がある場合がありますが、このアプローチでは最小特権の原則に従わず、必要以上のアクセス権が付与される可能性があります。
D. インスタンスが存在するサブネットで限定公開の Google アクセスを有効にします。
限定公開の Google アクセスでは、外部 IP アドレスを持たないインスタンスが Google サービスにアクセスできますが、Cloud Logging にログが表示されない問題とは直接関係ありません。この問題は、ネットワークアクセスではなく、権限に関連している可能性があります。
正解：
C. Logs Writer ロールをサービス アカウントに追加します。
Compute Engine インスタンスにアタッチされたサービス アカウントには、Cloud Logging にログを書き込むための適切な権限が必要です。サービス アカウントに Logs Writer ロールを付与すると、Cloud Logging にログを送信するために必要な権限が付与されます。
</div></details>

### Q.  問題16: 回答
お客様は、明確に定義されたサービスレベル目標(SLO)を持つ実質的なサービスに責任を負います。開発チームは、週を通してサービスの新しいリリースを頻繁にデプロイします。SLO の違反につながる重大なインシデントが発生した場合、開発チームの作業を機能開発からサービスの信頼性の向上に向け直すことを目指しています。
重大なインシデントが発生する前に、どのような対策を講じるべきでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 製品チームと交渉して、新機能のリリースよりもサービスの信頼性を常に優先します。
このアプローチは、サービスの現在のパフォーマンスやエラー予算の状態に関係なく、機能リリースよりも信頼性を全面的に優先するため、イノベーションと迅速な開発を妨げる可能性があります。
C. リリース頻度を週に 1 回以下に減らすよう開発チームと交渉します。
リリース頻度を制限すると、イノベーション プロセスが遅くなる可能性があり、SLO の遵守には直接関係しません。頻繁な展開は、適切に管理すれば有益です。
D. Jenkins パイプラインにプラグインを追加して、サービスが SLO から外れたときに新しいリリースを防止します。
これにより、SLO の遵守が厳格に実施されますが、開発ワークフローが中断される可能性があり、事前対応型ではなく事後対応型と見なされる可能性があります。
正解：
A. すべてのサービス関係者と協力して、適切なエラー予算ポリシーを策定します。
このアプローチでは、SLO に基づいて明確なガイドラインを設定し、どの程度のリスクが許容できるかを判断します。エラー バジェット ポリシーは、イノベーション (新機能) と信頼性のバランスを取るためのフレームワークを提供します。これにより、チームは、特にSLO違反の後、信頼性と機能開発のどちらに重点を置くべきかについて、十分な情報に基づいた決定を下すことができます。この戦略は、サイト信頼性エンジニアリング (SRE) のプラクティスに沿ったものです。
リンクス：
エラー予算ポリシーの例
エラーバジェットの動機14
</div></details>

### Q.  問題17: 回答
チームは、外部向けのアプリケーションに関連するインシデントの後、事後分析ポリシーを開発中です。このチームは、サイト信頼性エンジニアリング (SRE) のベスト プラクティスに沿って、インシデントが事後分析を必要とするタイミングを示すトリガーを特定することで、事後分析プロセスを強化することを目指しています。
事後分析ポリシーで定義する必要があるトリガーはどれですか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。外部の関係者が事後分析を依頼する
C. 社内の関係者が事後分析を要求した。
D. 監視システムは、アプリケーションのインスタンスの 1 つに障害が発生したことを検出します。
オプション A (外部の利害関係者が事後分析を要求する) とオプション C (内部の利害関係者が事後分析を要求する) も事後分析をトリガーする可能性がありますが、これらはより主観的であり、客観的なシステムベースのトリガーではなく、利害関係者の認識に依存します。オプション D (アプリケーションのインスタンスの 1 つに障害が発生したことを監視システムが検出する) は、サービスの中断やデータ損失などのより重大な結果につながらない限り、必ずしも事後分析を必要としない通常の運用上の問題です。
正解：
B. インシデントによりデータが失われた場合。
データの損失は、特に外部向けのアプリケーションにとって、どのシステムでも重大なイベントです。システムの整合性と信頼性に影響を与えるだけでなく、ユーザーの信頼と法令遵守に深刻な影響を与える可能性があります(特にGDPRなどのデータ保護法を考慮すると)。この場合の事後分析は、データ損失がどのように発生したか、将来それを防ぐために何ができるか、およびデータ回復戦略を改善する方法を理解するのに役立ちます。
E.CD パイプラインは問題を検出し、問題のあるリリースをロールバックします。
CD パイプラインのロールバックは、以前の状態への自動復帰を保証するのに十分な重大性を持つ、新しいリリースの重大な問題を示しています。この状況は、開発、テスト、またはデプロイのプロセスで問題が発生したことを示す明確な指標です。このコンテキストでの事後分析は、障害の根本原因を理解し、テストとデプロイの手順の有効性を評価し、リリース プロセス全体を改善して、将来同様の問題を防ぐために重要です。
リンクス：
事後分析文化:失敗から学ぶ
</div></details>

### Q.  問題18: 未回答
会社では、Pod の削除や再スケジュールの影響を受けやすいレガシー データベースを Google Kubernetes Engine(GKE)で運用しています。データベースのオペレーションが中断されないようにするには、適切な GKE メンテナンス戦略を選択する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. GKE メンテナンス チャネルを「Stable」に設定します。
GKE メンテナンス チャンネルを「Stable」に設定しても、更新は妨げられません。安定版のリリーススケジュールに基づいてスケジュールするだけです。これにより、更新中にPodが削除される可能性があります。
B. max-surge-upgrade を 0 に設定してノード プールのアップグレード戦略を構成します。
max-surge-upgrade が 0 に設定されたノード プールのアップグレード戦略では、メンテナンス更新は制限されません。メンテナンスは引き続き行われ、Podのエビクションにつながる可能性があります。
C. 「PodDisruptionBudget」オブジェクトを作成し、maxUnavailableを100%に指定します。
maxUnavailableを100%に設定した「PodDisruptionBudget」は、メンテナンスのアップグレードを妨げません。これは、自発的な中断中に利用できないPodの数を指定するだけで、メンテナンス中に立ち退きになる可能性があります。
正解：
D. 「マイナーまたはノードのアップグレードなし」のスコープでメンテナンス除外ウィンドウを設定します。
メンテナンス除外ウィンドウを "マイナーまたはノードのアップグレードなし" で構成すると、ノード プールの中断が効果的に防止されます。この設定により、ノードのアップグレードによるワークロードの削除や再スケジュールが回避され、GKE のレガシー データベースの安定性が確保されます。
リンクス：
メンテナンス期間と除外
リリース チャネルについて
GKE アップグレードのベスト プラクティス |ビデオ
クラスターのアップグレードに関するベスト・プラクティス
</div></details>

### Q.  問題19: 未回答
会社では Google Kubernetes Engine(GKE)でアプリケーションを運用しており、これらのアプリケーションの多くはエフェメラルボリュームに依存しています。ワーカー ノード上の DiskPressure ノードの状態によって引き起こされる特定のアプリケーションの不安定性を確認しました。ただし、どのPodが問題の原因であるかを特定するために必要なワークロードとノードへの実行アクセスがありません。
この問題に対処するには、どのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. メトリックス エクスプローラーを使用して、ノード/ephemeral_storage/used_bytes メトリックを確認します。
このオプションは、ノード全体のストレージ使用量の監視に重点を置いていますが、DiskPressure状態の原因となっているPodを直接特定するものではありません。
C. emptyDir ボリュームを持つすべての Pod を見つけます。df -h コマンドを使用して、ボリュームのディスク使用量を測定します。
このオプションは、df -hコマンドを使用して、ボリュームのディスク使用量を測定するために、emptyDirボリュームを持つPodを手動でチェックすることを提案します。ディスク使用率の高いPodを特定するのに役立ちますが、これは手動であり、時間がかかる可能性があります。
D. emptyDirボリュームを持つすべてのPodを見つけます。df -sh * コマンドを使用して、ボリューム ディスクの使用状況を測定します。
このオプションはオプション C と似ていますが、df -sh * コマンドの使用を指定します。オプションCと同様に、emptyDirボリュームで各Podをチェックするには手作業が必要です。
正解：
B. メトリックス エクスプローラーを使用して container/ephemeral_storage/used_bytes メトリックを確認します。
このオプションは、Google Kubernetes Engine(GKE)環境のエフェメラルボリュームによる DiskPressure ノードの状態の原因となる特定の Pod を特定するために推奨されます。
リンクス：
GKE システム指標
</div></details>

### Q.  問題20: 未回答
お客様は、オンプレミスと Google Cloud Platform の両方にデプロイされた、大規模な Google Kubernetes Engine(GKE)クラスタ上で動作する e コマース アプリケーションを担当します。このアプリケーションは、コンテナーで実行されるさまざまなマイクロサービスで構成されています。
CPU とメモリを最も多く消費しているコンテナーを特定するには、何をする必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Kubernetes Engine モニタリングを使用します。
Google Cloud 上の GKE クラスタには有効ですが、オンプレミスにデプロイされたクラスタ(特に Anthos セットアップ)を完全にはサポートしない可能性があります。この制限により、両方の環境での包括的な監視が妨げられる可能性があります。
C. Cloud Monitoring API を使用してカスタム指標を作成し、グループを使用してコンテナを整理します。
これには、カスタムメトリックを作成および整理するための追加の設定が必要です。柔軟なアプローチですが、より複雑になる可能性があり、オンプレミスの GKE クラスタとシームレスに統合できない場合があります。
D. Cloud Logging を使用して、アプリケーションログを BigQuery にエクスポートし、コンテナごとのログを集計して、CPU とメモリの消費量を分析します。
このオプションは、CPU とメモリの使用状況をリアルタイムで監視するのではなく、ログ分析に重点を置いています。これは、リソース使用率メトリックの監視ではなく、アプリケーションのログとイベントの分析に適しています。
正解：
B. Prometheus を使用してコンテナーごとにログを収集して集計し、その結果を Grafana で分析します。
オープンソースのモニタリング ソリューションである Prometheus は、Anthos や GKE などの環境に適しており、コンテナの指標に関する詳細な分析情報を提供します。これにより、オンプレミス環境とクラウド環境の両方からメトリックを収集できるため、デプロイ全体で包括的な監視が保証されます。その後、Grafana を使用して、これらのメトリックを視覚化および分析できます。このアプローチは、コミュニティの推奨事項と、複雑なハイブリッド環境に関する Google Cloud のガイダンスに沿ったものです。
リンクス：
メトリック収集の構成
ログ記録と監視
GKE ワークロード指標から Managed Service for Prometheus への移行
</div></details>

### Q.  問題21: 回答
本番環境の Google Cloud Platform(GCP)プロジェクトを Cloud Workspaces でモニタリングする戦略を策定しています。重要な要件は、開発プロジェクトやステージングプロジェクトからの不要なアラートを回避しながら、本番環境の問題を迅速に検出して対応する機能です。最小権限の原則に従って、関連するチーム メンバーに Cloud Workspaces へのアクセス権を付与することを目指します。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 関連するチームメンバーに、すべての GCP 本番環境プロジェクトへの読み取りアクセス権を付与します。各プロジェクト内に Cloud ワークスペースを作成します。
この方法では一元化ができず、複数のプロジェクトにわたる監視が複雑になります。開発環境、ステージング環境、運用環境を効率的に分離しません。
B. 関連するチームメンバーに、すべての GCP 本番プロジェクトに対する Project Viewer IAM ロールを付与します。各プロジェクト内に Cloud ワークスペースを作成します。
オプション A と同様に、このアプローチでは監視を一元化できません。また、プロジェクト閲覧者の役割は、監視の目的で必要以上に広範なアクセス権を提供する場合もあります。
C. 監視ワークスペースをホストする既存の GCP 運用プロジェクトを選択します。運用プロジェクトをこのワークスペースにアタッチします。関連するチームメンバーに Cloud Workspace への読み取りアクセス権を付与します。
ワークスペースに既存の運用プロジェクトを使用すると、監視と他の運用アクティビティが混在し、混乱やアクセス制御の複雑さにつながる可能性があります。より明確な組織とセキュリティプラクティスの監視に特化した別のプロジェクトを用意することをお勧めします。
正解：
D. 新しい GCP 監視プロジェクトを作成し、その中に Cloud Workspace を作成します。運用プロジェクトをこのワークスペースにアタッチします。関連するチームメンバーに Cloud Workspace への読み取りアクセス権を付与します。
このアプローチは、GCP でリソースを整理、監視するためのベスト プラクティスと一致しています。専用の監視プロジェクトを作成し、その中のすべての本番プロジェクトを Cloud Workspace にアタッチすることで、開発環境とステージング環境を明確に分離しながら、監視を一元化できます。チーム メンバーに監視ワークスペースへの読み取りアクセス権を付与すると、必要以上に広範なアクセス権を付与することなく、運用環境の問題を効果的に監視して対応できます。
リンクス：
複数のプロジェクトのメトリックスコープを構成する
プロジェクトのスコープ設定に関するベスト プラクティス
</div></details>

### Q.  問題22: 未回答
会社では、Google Kubernetes Engine(GKE)を利用してサービスを管理しています。開発環境内では、GKE クラスタは詳細ロギングを有効にしたアプリケーションを実行します。現在、デベロッパーは「kubectl logs」コマンドを使用してログにアクセスしており、Cloud Logging は利用していません。さらに、アプリケーションに対して定義された標準化されたログ構造はありません。目標は、アプリケーションのロギングに関連する費用を削減しながら、GKE オペレーション ログを確実に収集することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 開発クラスタに対して gcloud container clusters update --logging=SYSTEM コマンドを実行します。
このコマンドは、アプリケーションログではなく、システムレベルのログのみを収集するように GKE クラスタを構成します。これにより、ログの量は減少しますが、開発者が必要とする可能性のある重要なアプリケーションレベルのログが除外される可能性があります。
B. 開発クラスタに対して gcloud container clusters update --logging=WORKLOAD コマンドを実行します。
これにより、アプリケーションログを含むワークロードレベルのログを収集するようにクラスターが設定されます。この方法では、アプリケーション ログが冗長な場合、ログ記録の費用を大幅に削減できない可能性があります。
C. 開発環境に関連付けられたプロジェクトで gcloud logging sinks update _Default --disabled コマンドを実行します。
このコマンドは、デフォルトのロギングシンクを無効にします。これにより、すべてのログが Cloud Logging に送信されなくなりますが、運用ログがまだ必要な場合は極端すぎます。
正解：
D. 重大度 >= DEBUG resource.type = "k8s_container" 除外フィルターを、開発環境に関連付けられているプロジェクトの_Defaultログ シンクに追加します。
このフィルタは、「k8s_container」リソースの重大度レベルが DEBUG 以下のログを除外することで、Cloud Logging に取り込まれる詳細なアプリケーション ログの量を効果的に減らします。
コストの最適化と重要な運用ログの保持のバランスが取れているため、説明されているシナリオの効率的なソリューションになります。
このアプローチは、アプリケーションのログ記録が冗長で標準化されていない開発環境でのコスト効率の高いログ管理の目標と一致しています。
リンクス：
除外フィルター
</div></details>

### Q.  問題23: 回答
会社が HTTPS リクエストを使用して、URL からアクセスできる Cloud Run でホストされる一般公開サービスをトリガーしています。開発者は、サービスを顧客に公開する前に、サービスの最新リビジョンをテストできるようにする必要があります。https://booking-engine-abcdef.a.run.app
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 「gcloud run services update-traffic」を使用してすべてのトラフィックを最新のリビジョンにルーティングすると、本番環境のトラフィックがテストされていない可能性のある新しいリビジョンに転送され、テスト目的には適していません。
C. プライベートテスト用の認証トークンで「curl」を使用しても、パブリックURLに対してテストされ、特に構成されていない限り、最新の開発リビジョンではなく、本番環境に対応したリビジョンにアクセスします。
D. 'roles/run.invoker' ロールの付与はアクセス制御に関するものであり、特定のリビジョンをテストする必要性に直接対処するものではありません。このオプション (https://booking-engine-abcdef.private.run.app) で指定される URL は、プライベート アクセスを示唆していますが、製品バージョンとは異なる最新のリビジョンにアクセスする方法は指定されていません。
正解：
A. 「gcloud run deploy booking-engine --no-traffic --tag dev」コマンドを実行します。 テストには https://dev--booking-engine-abcdef.a.run.app URL を使用します。
このアプローチでは、特定のタグ (この場合は dev) を使用してサービスの新しいリビジョンをデプロイし、トラフィックを受信しないように設定します (--no-traffic)。これにより、本番環境のトラフィックに影響を与えることなく、新しいリビジョンをデプロイし、テストのためにアクセスすることができます。
特別にフォーマットされた URL (https://dev--booking-engine-abcdef.a.run.app) は、このタグ付きリビジョンへのアクセスを提供し、開発者が最新の変更をテストできるようにします。
リンクス：
gcloud run deploy を実行します
新しいタグ付きリビジョンをデプロイする
</div></details>

### Q.  問題24: 未回答
会社では 9 月に大規模な販売イベントを開催しており、この期間中の業務を支援する従業員のグループを選択しました。Google が推奨するプラクティスに合わせるために、これらの従業員に 9 月限定で Google Cloud リソースへのアクセス権を付与する最適なアプローチは何ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 識別された従業員のマシンからリソースへのトラフィックを許可するファイアウォール規則を追加します。9 月末にトラフィックを拒否するようにファイアウォール ルールを変更します。
ファイアウォールルールはネットワークトラフィックを規制できますが、リソースとの対話に必要なIAMアクセス許可は付与されません。この方法では、ネットワーク アクセスは制御されますが、リソース固有のアクセス許可は効果的に管理されません。
C. 識別された従業員を含むグループを作成し、リソースにアクセスするためのアクセス許可を持つカスタム Identity and Access Management (IAM) ロールをグループに割り当てます。9月末にカスタムロールを削除します。
9月末にカスタムロールを手動で削除すると、人為的ミスや見落としのリスクが生じます。さらに、即時のアクセス許可を付与する必要がない場合があり、慎重に管理しないとセキュリティ上のリスクにつながる可能性があります。
D. 識別された各従業員に、リソースにアクセスするアクセス許可を持つカスタム Identity and Access Management (IAM) ロールを割り当てます。Cloud Scheduler タスクを作成して、9 月末にカスタムロールを削除します。
グループではなく個々のユーザーにロールを割り当てることは、一般に効率が悪く、管理が困難です。Cloud Scheduler を使用してロールを削除することは、IAM 権限を管理するための標準的な方法ではなく、不必要な複雑さと潜在的なセキュリティ監視につながる可能性があります。
正解：
B. 識別された従業員を含むグループを作成します。リソースにアクセスするためのグループ権限を割り当て、9 月の日付/時刻式を使用して Identity and Access Management (IAM) 条件を構成します。
IAM 条件を利用して、指定した期間の権限を付与することは、Google が推奨する方法です。この方法では、必要な権限を自動的に割り当てたり取り消したりできるため、9 月中にのみアクセスできるようになります。これにより、管理が簡素化され、アクセス許可を手動で変更する必要がなくなるため、セキュリティが強化されます。
リンクス：
IAM条件の概要
</div></details>

### Q.  問題25: 回答
あなたは、複数の Google Cloud リージョンにまたがる Google Kubernetes Engine(GKE)にデプロイされた人気のあるモバイルゲームアプリケーションを担当しています。各リージョンには、複数の Kubernetes クラスターが含まれています。特定の地域のユーザーがアプリケーションに接続できないことを示すレポートを受け取りました。目的は、サイト信頼性エンジニアリングのプラクティスを遵守しながら、このインシデントを解決することです。
最初に何をすべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Cloud Monitoring を使用して、影響を受けるリージョンの CPU またはメモリ使用量の急増を確認します。
これは問題の診断には重要ですが、事後対応型のアプローチであり、ユーザーが経験したサービスの中断にすぐに対処できるわけではありません。
C. 高メモリと高 CPU のマシン タイプのインスタンスで構成される追加のノード プールをクラスターに追加します。
リソースを追加すると、問題がリソースに関連している場合に役立つ場合がありますが、根本原因を理解していないと、問題が解決せず、不要なコストが発生する可能性があります。
D. Cloud Logging を使用して、影響を受けるリージョンのクラスタをフィルタリングし、ログ内のエラー メッセージを検査します。
これは根本原因の特定に役立ちますが、ユーザーが現在影響を受けているインシデントの最初のステップではありません。インシデント後の分析に適しています。
正解：
A. 影響を受けるリージョンから、問題を報告していない他のリージョンにユーザー トラフィックを再ルーティングします。
この選択は、インシデント対応における当面の優先事項である、ユーザーへのサービスの復元と一致しています。トラフィックをリダイレクトすることで、ユーザーへの影響をすばやく軽減し、ログや監視ツールを使用して根本原因を調査できます。このアプローチは、サービスの可用性とユーザーエクスペリエンスを優先するというSREのプラクティスに準拠しており、その後、より詳細な分析とトラブルシューティングが行われます。
リンクス：
https://cloud.google.com/error-reporting/docs/viewing-errors
</div></details>

### Q.  問題26: 回答
コンテナ化されたアプリケーションの新しいバージョンはテスト済みで、Google Kubernetes Engine(GKE)の本番環境にデプロイする準備が整っています。新しいバージョンは、運用前の環境で完全な負荷テストを行うことはできませんでしたが、デプロイ後にアプリケーションでパフォーマンスの問題が発生しないことを保証することが不可欠です。デプロイ プロセスは自動化する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. ブルー/グリーン デプロイを使用して、継続的デリバリー パイプラインを介してアプリケーションをデプロイします。トラフィックを新しいバージョンのアプリケーションに移行し、Cloud Monitoring を使用してパフォーマンスの問題を探します。
ブルー/グリーンデプロイは、新しいバージョンをデプロイするための安全な方法でもありますが、通常、古いバージョン(ブルー)から新しいバージョン(グリーン)への完全な切り替えが必要です。これは、すべてのユーザーが一度に新しいバージョンに公開されるため、新しいバージョンが完全にロード テストされていない場合、カナリア デプロイよりもリスクが高くなる可能性があります。
C. kubectl を使用してアプリケーションをデプロイし、Config Connector を使用してバージョン間のトラフィックを徐々に増やします。Cloud Monitoring を使用して、パフォーマンスの問題を探します。
この方法はトラフィックの増加を管理するために使用できますが、カナリア デプロイと比較して継続的デリバリー パイプラインへの統合が不十分であり、同じレベルの自動化と監視の容易さを提供しない可能性があります。
D. kubectl を使用してアプリケーションをデプロイし、spec.updateStrategy.type フィールドを RollingUpdate に設定します。Cloud Monitoring を使用してパフォーマンスの問題を探し、問題がある場合は kubectl rollback コマンドを実行します。
ローリング アップデートは、Kubernetes でデプロイを更新するための一般的な戦略です。ただし、カナリア デプロイが提供するきめ細かな制御と即時のパフォーマンス フィードバックは提供されない可能性があり、負荷がかかった状態での新しいバージョンのパフォーマンスが不確実なシナリオでは特に重要です。
正解：
A. カナリア デプロイを使用して、継続的デリバリー パイプラインを介してアプリケーションをデプロイします。Cloud Monitoring を使用してパフォーマンスの問題を探し、指標でサポートされているトラフィックを増やします。
段階的なロールアウト:カナリア デプロイでは、最初に新しいバージョンを少数のユーザーにリリースできます。これにより、すべてのユーザーに影響を与えることなく、実際の条件下で新しいバージョンがどのように機能するかを監視できます。
パフォーマンス監視:Cloud Monitoring を使用すると、新しいバージョンのパフォーマンスを追跡し、問題を早期に特定できます。メトリックが良好な場合は、新しいバージョンへのトラフィックを徐々に増やすことができます。
自動および制御された展開:この方法は、自動化された継続的デリバリー パイプラインによく適合し、新しいバージョンの制御された段階的な公開を可能にします。
要約すると、オプション A はバランスの取れたアプローチを提供し、展開時の安全性と、パフォーマンスの問題を綿密に監視して対応する機能の両方を提供するため、新しいバージョンの全負荷パフォーマンスがまだ確立されていないシナリオに適しています。
リンクス：
https://www.cloudskillsboost.google/focuses/19122?parent=catalog#:~:text=URL%3A%20https%3A%2F%2Fwww
</div></details>

### Q.  問題27: 回答
Google Cloud でのアプリケーションのパフォーマンスが前回のリリース以降に低下しています。ダウンストリームの依存関係が原因で、一部の要求の完了に時間がかかる可能性があります。
アプリケーションでこの問題の原因を調査して特定するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. エラー報告は、パフォーマンス分析ではなく、ランタイム エラーのキャプチャと報告に重点を置いています。
B. Google Cloud Managed Service for Prometheus はモニタリングとアラートに使用されますが、より一般化されており、特定のリクエストのトレースに重点が置かれていません。
C. Cloud Profiler は CPU とメモリのプロファイリングを提供し、パフォーマンスの最適化に役立ちますが、リクエストフローのトレースやサービスインタラクションの特定のボトルネックの特定にはあまり適していません。
正解：
D. アプリケーションで Cloud Trace を構成します。
Cloud Trace は、アプリケーションからレイテンシ データを収集し、Google Cloud Console に表示する分散トレース システムです。パフォーマンスの問題を診断するために特別に設計されており、リクエストのトレースを表示し、アプリケーションスタックまたは相互接続されたサービスのネットワーク内のどこで遅延が発生しているかを特定できます。これにより、ダウンストリームの依存関係に関する問題を特定するのに理想的です。
リンクス：
クラウドトレース
</div></details>

### Q.  問題28: 未回答
Google Kubernetes Engine(GKE)で実行されているアプリケーションがあり、デプロイとサービスを使用して複数のマイクロサービスがデプロイされています。マイクロサービスの 1 つで、Pod が 5 時間以上実行した後に 403 エラーを返すという問題が発生しています。開発チームは、1か月間問題を解決できません。マイクロサービスが修正されるまで、Google が推奨するプラクティスに従い、可能な限り少ない手順でアプリケーションが円滑に動作し続けるようにする必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. cronジョブを作成して、5時間以上実行されているPodを終了します。
これにより、ポッドも再起動されますが、効率が悪くなります。組み込みのKubernetes機能を使用する代わりに、外部メカニズム(cronジョブ)を設定する必要があります。
C. Pod を監視し、5 時間以上実行されている Pod を終了します。
これは労働集約的であり、効率的ではありません。絶え間ない監視と手動による介入は、Kubernetes環境には理想的ではありません。
D. Podが403エラーを返すたびに通知するようにアラートを設定します。
これにより問題が通知されますが、問題は解決されません。この場合も、問題を修正するためのアクションを実行する必要があります。
正解：
B. マイクロサービスのデプロイに HTTP Liveness Probe を追加します。
HTTP Liveness Probes: Kubernetes では、Liveness Probe を使用してポッドの正常性を確認します。ポッドが異常な場合 (この場合、一定期間後に 403 エラーを返し始める場合)、liveness probe は失敗します。liveness probe が失敗すると、Kubernetes はポッドを自動的に再起動します。これにより、マイクロサービスの問題のあるインスタンスが、エラーを返す前にリサイクルされます。
自動化と効率性:このアプローチは自動化されており、継続的な監視や手動による介入を必要としないため、効率的でKubernetesのベストプラクティスに沿ったものになります。
問題に直接対処する: liveness probe は、目前の問題 (5 時間後に Pod が異常になる) を直接ターゲットとし、Pod を再起動することで解決します。
したがって、HTTP liveness probe を追加することは、Kubernetes のベスト プラクティスに準拠し、手動による介入を最小限に抑える最適なソリューションです。
リンクス：
Liveness Probe と Readiness Probe の設定
</div></details>

### Q.  問題29: 未回答
組織内で事後分析を実装することを計画しており、プロセスが十分に受け入れられるようにすることを目指しています。
そのために取るべき2つのステップとは?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 新入社員に、練習を通じてチームへの事後分析を行うことを奨励します。
新入社員を巻き込むことはトレーニングには有益ですが、経験豊富なガイダンスによって補完されなければ、強力な事後分析文化を確立するのに効果的ではない可能性があります。
B. すべての事後分析の実施を担当する指定チームを作成します。
専任のチームを持つことで、プロセスを合理化できるだけでなく、インシデントに直接関与するチームから責任を切り離すことができ、学習への影響を軽減できる可能性があります。
E. 以前の事後分析を批評するためのフォーラムを組織に提供します。
過去の事後分析を振り返ることは改善に役立ちますが、批評だけに焦点を当てると、学習や成長ではなく、プロセスに対する否定的な認識につながる可能性があります。
正解：
C. 上級管理職に事後分析を認め、参加するよう促す。
リーダーシップの参加は、事後分析の重要性を示し、透明性と学習の文化を確立します。
D. 効果的な事後分析を書くことは、報われ、称賛される習慣であることを確認する。
効果的な事後分析を認めて称賛することで、参加が促進され、組織の学習プロセスにおけるその価値が強調されます。
リンクス：
Lowe's SRE が平均復旧時間 (MTTR) を 80% 以上短縮した方法
事後分析文化:失敗から学ぶ
</div></details>

### Q.  問題30: 未回答
チームは、すべての CI / CD パイプラインに Cloud Build を採用しています。Cloud Build の kubectl ビルダーを利用して、新しいイメージを Google Kubernetes Engine(GKE)にデプロイする予定です。開発の手間を最小限に抑えながら GKE で認証するには、どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. cloudbuild.yaml ファイルで Cloud Build のコンテナ開発者ロールを指定します。
cloudbuild.yaml ファイル内で IAM ロールを割り当てることができないため、これは実現できません。IAM ロールは、ビルド設定ファイル内ではなく、IAM ポリシーレベルでサービスアカウントに割り当てられます。
C. コンテナ デベロッパー ロールを持つ新しいサービス アカウントを作成し、それを使用して Cloud Build を実行します。
これは有効なアプローチですが、既存のCloud Buildサービスアカウントにロールを割り当てる場合に比べて、開発の手間がかかります。新しいサービス アカウントを作成して管理する必要があります。
D. Cloud Build でサービス アカウントの認証情報を取得し、これらを kubectl に渡すための別のステップを作成します。
この方法はより複雑で、CI/CD パイプラインで追加の手順が必要です。これには、サービス アカウントの資格情報を手動で処理する必要があるため、開発の労力が増加し、潜在的なセキュリティ リスクが高まります。
正解：
A. コンテナ開発者ロールをCloud Buildサービスアカウントに割り当てます。
このロールは、GKE を操作するために必要な権限を Cloud Build に付与します。これは簡単なアプローチで、追加の設定やサービス アカウントを作成することなく、既存の Cloud Build サービス アカウントを活用します。この方法により、プロセスが簡素化され、既存の CI/CD パイプラインにシームレスに統合されます。
リンクス：
ユーザー指定のサービス アカウントを構成する
必要なIAM権限
</div></details>

### Q.  問題31: 未回答
開発チームがサービスの API の新しいバージョンを作成しました。目標は、サードパーティの開発者とサードパーティがインストールしたアプリケーションのエンドユーザーへの影響を最小限に抑えて、新しい API バージョンをデプロイすることです。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 新しいバージョンの API を導入し、古いバージョンの廃止を発表し、古い API の残りのユーザーに連絡し、古い API のユーザーにベスト エフォート サポートを提供し、古いバージョンを破棄します。
このアプローチでは、ユーザーの不意を突く可能性があります。古いバージョンの廃止を発表する前に新しいバージョンを導入すると、混乱を招き、ユーザーが適応するための十分な時間が与えられない可能性があります。
C. 古いバージョンの API の廃止を発表し、古い API の残りのユーザーに連絡し、新しいバージョンを導入し、古いバージョンを廃止し、古い API のユーザーにベストエフォート サポートを提供し、古いバージョンを廃止します。
このシーケンスは B と似ていますが、新しいバージョンを導入する前にユーザーに連絡するため、新しい API のロールアウトが遅れる可能性があります。
D. 新しいバージョンの API を導入し、古い API の残りのユーザーに連絡し、古いバージョンの廃止を発表し、古いバージョンを廃止し、古い API のユーザーにベストエフォート サポートを提供し、古いバージョンを廃止します。
このアプローチでは、廃止について通知される前に新しいバージョンでユーザーを驚かせるリスクがあり、混乱を伴う移行につながる可能性があります。
正解：
B. 古いバージョンの API の廃止を発表し、新しいバージョンを導入し、古い API の残りのユーザーに連絡し、古いバージョンを廃止し、古い API のユーザーにベストエフォート サポートを提供し、古いバージョンを廃止します。
このシーケンスにより、すべての利害関係者が今後の変更について事前に通知されます。新しいバージョンを導入する前に廃止を発表することで、ユーザーが準備と適応を行う時間を確保できます。個別のコミュニケーション、サポート、および古い API の体系的な段階的廃止をフォローアップすることで、中断を最小限に抑え、よりスムーズな移行を保証します。
リンクス：
古い API をオフにする正しい方法
</div></details>

### Q.  問題32: 回答
Google Cloud にデプロイされたアプリケーションのフロントエンド層を構成しています。フロントエンド層は NGINX でホストされ、Envoy ベースの外部 HTTP(S) ロードバランサーを前面に持つマネージド インスタンス グループを使用してデプロイされます。このアプリケーションは、europe-west2 リージョン内に完全に展開され、英国に拠点を置くユーザーのみにサービスを提供します。最もコスト効率の高いネットワーク層と負荷分散構成を選択する必要があります。
何を使うべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. グローバル ロード バランサーを備えた Premium レベル
このオプションは高いパフォーマンスと信頼性を提供しますが、地域固有のユーザー ベースにサービスを提供するアプリケーションにとっては最も費用対効果が高くありません。Premium レベルとグローバル ロード バランサーの両方のグローバル機能は、英国を拠点とするユーザーのみにサービスを提供するアプリケーションでは十分に活用されていません。
B. リージョン ロード バランサーを備えた Premium レベル
このオプションでは、Premium レベルが引き続き使用されますが、これはより高価であり、リージョン固有のアプリケーションには必要ありません。リージョン ロード バランサーは、アプリケーションの地理的範囲に合わせて調整されますが、Premium レベルの選択により、全体的なセットアップはコスト効率が悪くなります。
C. グローバル ロード バランサーを備えた Standard レベル
このオプションは、Standard Tierのため、より予算にやさしいです。ただし、グローバルロードバランサーの使用は、リージョン固有のオーディエンスのみにサービスを提供するアプリケーションには理想的ではありません。グローバル・ロード・バランサーの機能はほとんど不要であるため、この構成はコストとリソース使用率の点で最も効率的ではありません。
正解：
D. リージョン ロード バランサーを備えた Standard レベル
スタンダードレベル:Google Cloud のネットワーク サービスのスタンダード階層は、費用対効果が高いように設計されています。パブリック・インターネットを使用してユーザー・トラフィックをルーティングするため、地域のユーザー・ベースにサービスを提供するアプリケーションには十分です。アプリケーションは英国内のユーザーのみにサービスを提供し、英国内のリージョン (europe-west2) でホストされるため、より広範でグローバルに最適化されたネットワークを提供する Premium レベルと比較して、Standard レベルが適切で費用対効果が高くなります。
リージョン・ロード・バランサ:リージョンロードバランサーは、同じリージョンから発信され、同じリージョン内で提供されるトラフィックを持つアプリケーションに適しています。アプリケーションは europe-west2 にデプロイされ、英国のユーザーのみにサービスを提供するため、リージョン ロード バランサーはこの地理的範囲に合わせて調整されます。これにより、europe-west2リージョン内のNGINXインスタンス間でユーザートラフィックが効率的に分散されます。グローバルロードバランサーを選択することは不要であり、アプリケーションはグローバルトラフィック分散を必要としないため、費用対効果が低くなります。
要約すると、オプション D (リージョン ロード バランサーを備えた Standard レベル) は、リージョンの焦点とユーザー ベースを考慮すると、アプリケーションのニーズに最もコスト効率が高く、適切な選択肢です。
リンクス：
Cloud Load Balancing の概要
</div></details>

### Q.  問題33: 回答
さまざまなデバイスタイプでグローバルにアクセスされる App Engine で実行されるアプリケーションを管理しています。接続数をトラッキングするには、App Engine 専用の Cloud Monitoring を利用しています。
この情報を取得するには、Cloud Monitoring でどの指標に注目する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B.tcp_ssl_proxy/new_connections
この指標は、新しい TCP 接続と SSL 接続をトラッキングする Cloud Load Balancing とより整合しています。これは App Engine 環境に限ったことではありません。
C. tcp_ssl_proxy/open_connections
B と同様に、この指標は App Engine 内で直接ではなく、Cloud Load Balancing を通じてオープンな接続をトラッキングします。App Engine 接続のモニタリングには適していません。
D. flex/instance/connections/current です。
この指標は、App Engine フレキシブル環境インスタンスごとのアクティブな接続数を表します。便利ですが、より詳細で個々のインスタンスに焦点を当てており、flex/connections/current のようにアプリケーション全体の接続の全体像を提供するわけではありません。
正解：
A. flex/connections/current (フレックス/接続/カレント)
この指標は、App Engine フレキシブル環境のバージョンごとの現在アクティブな接続数を示します。インスタンスごとではなく、アプリケーションレベルで接続を包括的に把握できるため、App Engine にデプロイされたアプリケーションの全体的な負荷と接続ステータスを把握する上で非常に重要です。
リンクス：
Google Cloud の指標 |Appengine(アプリエンジン)
</div></details>

### Q.  問題34: 回答
キャッシュされたメモリに製品情報を格納するアプリケーションを担当します。キャッシュミスが発生するたびに、エントリが Cloud Logging に記録されます。目的は、キャッシュ ミスの頻度を経時的に追跡する視覚化を作成することです。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Logging を Google データポータルのソースとしてリンクします。キャッシュミスのログをフィルタリングします。
データポータルは視覚化には適していますが、複雑なデータ分析に適しており、キャッシュミスなどの特定のアプリケーションイベントのモニタリングには適していない場合があります。
B. Cloud Profiler を構成して、ログに基づいてキャッシュ ミスがいつ発生したかを特定して視覚化します。
Cloud Profiler はパフォーマンス プロファイリングに重点を置いており、キャッシュ ミスなどの特定のイベントのモニタリングには適していません。
D. BigQuery を Cloud Logging のシンクとして構成します。スケジュールされたクエリを作成して、キャッシュ ミス ログをフィルター処理し、別のテーブルに書き込みます
BigQuery をログのシンクとして使用し、スケジュールされたクエリを作成することは、ログを分析する効果的な方法ですが、ログベースの指標を作成するよりも複雑な設定であり、この特定の要件にはやり過ぎになる可能性があります。
正解：
C. Cloud Logging でログベースの指標を作成し、Cloud Monitoring でその指標のダッシュボードを作成します。
このオプションを使用すると、Cloud Logging の特定のログエントリ(この場合はキャッシュミス)に基づいてカスタム指標を作成できます。その後、Cloud Monitoring を使用して、この指標を経時的に視覚化するダッシュボードを作成できます。この方法は、Google Cloud のネイティブ ツール内でキャッシュ ミスを追跡し、視覚化するための簡単で統合された方法を提供します。
リンクス：
カウンター メトリック
</div></details>

### Q.  問題35: 未回答
Cloud のオペレーション スイートを利用して、Google Cloud Platform (GCP) でホストされているアプリケーションを監視しています。新しいアプリケーションを最近デプロイした後、そのログがクラウドのオペレーションスイートに表示されないという問題が発生しました
(Stackdriver)ダッシュボードをクリックします。
この問題をトラブルシューティングして解決する必要があります。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. アカウントにクラウドダッシュボードを使用するための適切な権限があることを確認します。
アカウントのアクセス許可を確認することは重要ですが、ログが表示されないことが問題であることを考えると、問題はダッシュボードのアクセス許可ではなく、ログ記録メカニズム自体に関連している可能性が高くなります。
C. ファイアウォールでポート 25 が開かれ、メッセージが Cloud のオペレーション スイートを通過できることを確認します。
ポート 25 を開くことは、通常、メール サービスに使用されるため、Cloud Logging とは関係ありません。
D. アプリケーションが必要なクライアント ライブラリを使用していること、およびサービス アカウント キーに適切なアクセス許可があることを確認します。
アプリケーションが正しいクライアントライブラリとサービスアカウントのアクセス許可を使用していることを確認することは、特にカスタムアプリケーションログの場合に重要です。ただし、Cloud Logging エージェントが正しくインストールまたは構成されていない場合、この手順では問題は解決しません。
正解：
A. クラウド エージェントがホスティング仮想マシンにインストールされていることを確認します。
Cloud Logging エージェントは VM から Cloud のオペレーション スイート(Stackdriver)にログを転送する役割を担うため、Cloud Logging エージェントのインストールを確認することは重要な最初のステップです。この手順は、アプリケーションが新規か既存かに関係なく基本的なものであり、Google Cloud のドキュメントで推奨されている一般的なトラブルシューティング手順です。
リンクス：
チェックリスト
エージェントを再インストールする
</div></details>

### Q.  問題36: 回答
本番環境サービスの一部は、eu-west-1 リージョン内の Google Kubernetes Engine(GKE)でホストされています。ビルドシステムは us-west-1 リージョンで動作します。目標は、ビルドシステムからスケーラブルなレジストリにコンテナーイメージをアップロードして、イメージをクラスターに転送するための帯域幅を最適化することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. gcr.io ホスト名を使用して、イメージを Google Container Registry(GCR)にプッシュします。
このデフォルトのホスト名は、特定のリージョン向けに最適化されません。グローバルに機能しますが、地域の設定に最適な帯域幅の最適化を提供しない場合があります。
B. us.gcr.io ホスト名を使用してイメージを Google Container Registry(GCR)にプッシュします。
これにより、米国を拠点とする場所に画像が保存され、eu-west-1 リージョンの GKE クラスタの帯域幅を最適化するには適していません。
D. eu-west-1 リージョンの Compute Engine インスタンスで実行されているプライベート イメージ レジストリにイメージをプッシュします。
これにより、リージョン制御が可能になりますが、GCR のスケーラビリティと統合機能が不足しており、GCR を管理すると複雑さとコストが増す可能性があります。
正解：
C. eu.gcr.io ホスト名を使用して、イメージを Google Container Registry(GCR)にプッシュします。
このアプローチにより、イメージが GKE クラスタに地理的に近い場所に保存され、クラスタがこれらのイメージを取得する際のダウンロード速度と帯域幅の使用が最適化されます。GCR は、コンテナ イメージ ストレージのためのスケーラブルで安全なソリューションを提供し、GKE と適切に統合されています。
リンクス：
アーティファクト・レジストリの場所
リポジトリ間でのイメージのコピー
レジストリーの追加
</div></details>

### Q.  問題37: 未回答
停止の事後分析でアクションアイテムを作成して割り当てています。停止は終了しましたが、その根本原因に対処する必要があります。目的は、チームがアクションアイテムに効率的に対処できるようにすることです。
これらのアクションアイテムに所有者とコラボレーターを割り当てるにはどうすればよいでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 各アイテムに複数の所有者を割り当てて、チームがアイテムに迅速に対処できるようにします。
これは、責任の分散につながる可能性があります。複数の人が責任を負っている場合、それぞれが相手が主導権を握っていると思い込んでしまうリスクがあります。
C. コラボレーターを割り当て、個々の所有者はアイテムに割り当てず、事後分析を非難できないようにします。
これにより、誰も責めることのない文化が維持されますが、明確な説明責任を欠く可能性があり、アクションアイテムが無視される可能性があります。
D. チームリーダーは SRE チームを担当しているため、すべてのアクションアイテムの所有者としてチーム リーダーを割り当てます。
すべての項目をチームリーダーに割り当てると、チームリーダーが圧倒される可能性があり、スケーラブルではありません。問題に最も近い個人が所有権を持つ方が効果的です。
正解：
B. アクションアイテムごとに 1 人の所有者と必要なコラボレーターを割り当てます。
これにより、明確な説明責任が確保されます。所有者はアイテムが対処されていることを確認する責任があり、共同編集者はサポートを提供します。この焦点を絞ったアプローチは、効率的な解決に役立ちます。
リンクス：
https://sre.google/sre-book/example-postmortem/
https://devops.com/when-it-disaster-strikes-part-3-conducting-a-blameless-post-mortem/
</div></details>

### Q.  問題38: 回答
あなたは、Google Kubernetes Engine(GKE)Autopilot クラスタにデプロイされたマイクロサービスのオンコール サイト信頼性エンジニアです。会社では、注文メッセージを Pub/Sub に公開するオンライン ストアを運営しており、マイクロサービスはこれらのメッセージを受信して、倉庫システムの在庫情報を更新します。販売イベントにより注文が増加し、在庫情報が十分に迅速に更新されていません。これにより、在庫切れの製品の注文が大量に受け付けられます。マイクロサービスのメトリックを確認し、一般的なレベルと比較します。
倉庫システムが注文時に製品在庫を正確に反映し、顧客への影響を最小限に抑える必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. サブスクリプションの確認期限を短くします。
これにより、メッセージが再配信される前にキューに確認されない時間が短縮されます。ただし、問題はメッセージの確認応答のタイミングではなく、処理能力にあるようです。期限を短くすると、すでに苦労しているシステムへの圧力が高まり、メッセージの再配信がより頻繁になり、問題が悪化する可能性があります。
B. 一般的なトラフィック レベルを許可する仮想キューをオンライン ストアに追加します。
仮想キューを実装すると、受信トラフィックをバッファリングして管理できる場合がありますが、この方法では、メッセージの既存のバックログやマイクロサービスの処理能力には対応できません。これは将来の問題の防止に役立つ可能性がありますが、マイクロサービスがメッセージの負荷に追いつかないという現在の問題は解決されません。
D. ポッドの CPU とメモリの制限を増やします。
現在のCPU使用率は通常よりも高くなっていますが、Podの制限には達していません。制限を増やしても、ボトルネックがメッセージを処理する並行プロセスの数である場合、それほど効果的ではない可能性があります。Podがリソース制限(メトリックには示されません)を超えたためにスロットルまたは終了されていない限り、このオプションは問題を軽減できない可能性があります
正解：
C. Pod レプリカの数を増やします。
このオプションは、マイクロサービスを直接スケールアウトし、メッセージの処理に使用できるPodの数を増やします。CPU 使用率が 30% に増加し、通常よりも高くなっていますが、制限に達していないため、配信されないメッセージが大幅に増加していることを考えると、スケールアウトは論理的な手順です。レプリカの数が多いほど、バックログの処理が速くなり、最も古い未確認メッセージの経過時間が短縮され、ストック更新の応答性が向上します。
これにより、マイクロサービスがスケールアウトされ、メッセージの同時処理が可能になり、Pub/Sub メッセージのバックログが削減され、ストック情報がより迅速に更新されます。
リンクス：
Kubernetes Engine を使用したデプロイの管理
</div></details>

### Q.  問題39: 回答
Google Kubernetes Engine(GKE)でアプリケーションの CI / CD パイプラインを構築し、デプロイに Kubernetes Deployment、Service、Ingress を利用しています。アプリケーション チームはブルー/グリーン手法によるデプロイを要求しており、ロールバック手順を実装する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. kubectl rollout undo コマンドを実行します。
ローリングアップデート戦略に適しています。ブルー/グリーンデプロイでは、両方のバージョンが同時に存在し、トラフィックは 1 つのデプロイを更新するのではなく、バージョン間で切り替えられます。
B. 新しいコンテナイメージを削除し、実行中のPodを削除します。
これは破壊的なアプローチであり、ブルー/グリーンデプロイの利点を活用していません。また、コンテナー イメージを削除しても、実行中のポッドには影響しません。
D. 新しい Kubernetes デプロイを 0 にスケーリングします。
新しいバージョンは停止しますが、トラフィックは以前のバージョンに自動的にリダイレクトされません。このアクションは、ロールバック プロセスを完了するためにサービスの更新で補完する必要があります。
正解：
C. 以前のKubernetesデプロイメントを指すようにKubernetes Serviceを更新します。
ブルー/グリーン デプロイでは、現在の (ブルー) バージョン用と新しい (グリーン) バージョンの 2 つの個別のデプロイがあります。通常、両方のバージョンが同時にデプロイされます。
ロールバックを実行するには、Kubernetes Serviceを更新して、トラフィックを以前の(青色の)デプロイに戻すだけです。これは、デプロイ自体に変更を加えることなく、最後の既知の良好な状態に戻すための迅速かつ効果的な方法です。
このアプローチは、サービスがルーターとして機能し、異なるデプロイ バージョン間でトラフィックを切り替えるブルー/グリーン手法と一致しています。
リンクス：
https://www.cloudskillsboost.google/focuses/639?parent=catalog#:~:text=If%20necessary%2C%20you%20can%20roll,back%20to%20the%20old%20version
</div></details>

### Q.  問題40: 回答
マイクロサービス・アーキテクチャーを使用するトラフィックの多い Web アプリケーションを管理しています。アプリケーションのホームページには、現在の天気、株価、ニュースの見出しなどのコンテンツを表示するさまざまなウィジェットがあります。各ウィジェットのコンテンツは、専用のマイクロサービスから供給されます。ホームページのレイアウトを編成するメインの配信スレッドでマイクロサービス障害が発生することがあり、その結果、ホームページが不完全なコンテンツで表示されることがあります。ユーザーは、何もしないよりも部分的なコンテンツを持つことを好みますが、この低下モードが頻繁に発生するのは不十分です。許容できるレベルのユーザーエクスペリエンスを維持するには、サービスレベル目標(SLO)を確立することを目指しています。この側面を効果的に測定するには、どのサービスレベル指標(SLI)を選択する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B.可用性 SLI: マイクロサービスの合計数に対する正常なマイクロサービスの比率。
マイクロサービス全体に対する正常なマイクロサービスの比率に注目しても、ユーザー エクスペリエンスへの影響を直接測定することはできません。これは重要な内部指標ですが、ホームページのコンテンツの完全性に関するユーザーの視点を反映していません。
C. 鮮度 SLI: 過去 10 分以内に更新されたウィジェットの割合。
ウィジェットが最近更新されたかを測定することは、コンテンツの関連性にとって重要ですが、このシナリオでユーザーの満足度を維持するための主な懸念事項である、マイクロサービスの障害による不完全なコンテンツの問題には対処しません。
D. レイテンシ SLI: マイクロサービス呼び出しの合計数に対する 100 ミリ秒未満で完了するマイクロサービス呼び出しの比率。
マイクロサービスの応答速度 (待機時間で測定) は重要ですが、必ずしもホームページ上のコンテンツの完全性と相関しているわけではありません。このSLIは、応答時間がコンテンツの完全性ではなく、ユーザーエクスペリエンスに直接影響するシナリオでより適切です。
正解：
ある。品質SLI:全回答数に対する劣化していない回答の比率。
このSLIは、目前の中核的な問題、つまり、ホームページが意図したすべてのコンテンツでレンダリングされる頻度と、マイクロサービスの障害によってパフォーマンスが低下した状態でレンダリングされる頻度を直接測定します。これは、完全なコンテンツ配信と不完全なコンテンツ配信の頻度を定量化し、ユーザーの満足度に直接影響を与えるため、ユーザーエクスペリエンスの焦点とよく一致します。
リンクス：
SLO の採用
</div></details>

### Q.  問題41: 回答
e コマース アプリケーションを Google Cloud Platform(GCP)に移行し、次の繁忙期に備えようとしています。
繁忙期に備えるための最初のステップは何でしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 前シーズンに使用したコンピューティング能力の 2 倍を事前プロビジョニングし、成長を期待しています。
これにより可用性は確保されますが、コスト効率が悪く、需要に正確に一致しない可能性があります。ロード テストは、プロビジョニングするリソースの量に関するデータ主導の決定を行うのに役立ちます。
B. 本番クラスターで AutoScaling を有効にします (増加がある場合)。
AutoScaling を有効にすると、システムが需要に基づいてリソースを自動的に調整できるため、お勧めします。ただし、これは、負荷テストを通じてアプリケーションのパフォーマンス特性を理解した後に行う必要があります。
D. ディザスター リカバリー (DR) 環境が拡大した場合に、その環境を水増しするための Runbook を作成します。
ディザスタリカバリ計画を立てることは重要ですが、これは繁忙期に増加する負荷を管理するというよりも、システム障害に備えるためのものです。これは、パフォーマンスのチューニングとスケーリングの戦略を補完するものですが、置き換えるものではありません。
正解：
C. アプリケーションをロードして、スケーリングのパフォーマンスをプロファイリングします。
このアプローチにより、負荷が増加した場合にアプリケーションがどのように動作するかを理解し、潜在的なボトルネックを特定し、スケーリングのニーズを判断できます。このデータは、スケーリング戦略を効果的に実装し、繁忙期に増加したトラフィックをアプリケーションが処理できるようにするために重要です。
リンクス：
ピーク時のトラフィックとローンチイベントを計画する
年末年始のショッピングのピークに備える:作戦指令室がバーチャル化
</div></details>

### Q.  問題42: 回答
Google Cloud でコンテナ化されたアプリケーションの CI / CD パイプラインを構築する必要があります。開発チームは、トランクベースの開発に中央の Git リポジトリを使用します。品質を向上させるために、アプリケーションの新しいバージョンに対してパイプライン内のすべてのテストを実行する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある：
このオプションは、開発者が単体テストを実行するために Git フックに依存しているため、開発者に責任が加わり、一貫性がなくなる可能性があります。また、受け入れテストは運用環境へのデプロイ後に実行されるため、重大な問題を検出するには遅すぎる可能性があります。
B:
A と同様に、初期単体テストを Git フックに依存しています。さらに、統合テストと受け入れテストを 1 つのステップにまとめるため、問題の切り分けの効率が低下する可能性があります。運用環境にデプロイする前にコードに運用準備完了のタグを付けると、パイプラインの自動テストの可能性を十分に活用できない可能性があります。
C:
このオプションでは、パイプライン内の単体テストが自動化されますが、受け入れテストは運用環境へのデプロイ後まで延期されるため、検証されていない機能がユーザーにリリースされる可能性があります。
正解：
D.
1. コードがプッシュされたときに Cloud Build をトリガーして単体テストを実行します。すべての単体テストが成功したら、アプリケーション コンテナーをビルドし、中央レジストリにプッシュします。
2. Cloud Build をトリガーしてコンテナをテスト環境にデプロイし、統合テストと受け入れテストを実行します。
3. すべてのテストが成功すると、パイプラインはアプリケーションを本番環境にデプロイし、スモークテストを実行します
このアプローチにより、コードがプッシュされたときに単体テストがすぐに実行され、問題を早期にキャッチできます。統合テストと受け入れテストのためにコンテナーをテスト環境にデプロイし、その後、スモーク テストを使用して運用環境にデプロイする次の手順では、アプリケーションの品質保証を最大化する包括的なテスト シーケンスが提供されます。
リンクス：
CI/CD パイプラインについて
</div></details>

### Q.  問題43: 未回答
同僚は、夜間のオンコール中にプロジェクトの仮想プライベートクラウド(VPC)に入る不審なトラフィックが増えていることに気付きました。同僚が Cloud Armor の設定を更新して、特定の IP アドレスから発信されるトラフィックを停止しました。この変更により、ネットワーク トラフィックは減少しましたが、影響を受けた顧客からのエスカレーションが発生し、会社にペナルティが課せられました。チームは、Google が推奨する方法を使用して再発を防止したいと考えています。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. すべての変更は、変更をレビューする追加の人がいる日中に行われることを要求するポリシーを確立します。
これは、構成をいつでも変更する必要があるため、実行可能なポリシーではありません。このケースでは、潜在的なサイバー攻撃が夜間に発生しました。チームには、いつでも適用できるプロセスが必要です。
B. この同僚に変更について第 2 レベルのレビューを要求するプロセスを作成します。
このアプローチは、すべての人のプロセスを改善するのではなく、個人に特化しているため、推奨されるプラクティスに従っていません。
D. 同僚のインシデント レポートを作成し、上級管理職にエスカレーションします。
このアプローチは、プロセスを改善するのではなく、個人を非難することに重点を置いているため、推奨されるプラクティスに従っていません。
正解：
C. 中核となる技術的問題を特定し、チーム全体で使用できるように学習内容を文書化します。
このソリューションは、個人を責めることなく、技術的な問題を客観的に特定するDevOps/SREアプローチを採用しています。
リンクス：
事後分析文化:失敗から学ぶ
大胆不敵な事後分析の共有 - CRE の人生の教訓
</div></details>

### Q.  問題44: 回答
組織は Google Cloud を使用したコンテナ化に着手しています。コンテナー イメージと Helm チャート用のフル マネージド ストレージ ソリューションが必要です。目標は、Google Kubernetes Engine(GKE)、Cloud Run、VPC Service Controls、Identity and Access Management(IAM)などの既存の Google Cloud サービスとシームレスに統合できるストレージ ソリューションを特定することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Docker を使用して、組織が所有するバケットをポイントする Cloud Storage ドライバを構成します。
Google Cloud Storage は堅牢なストレージ ソリューションですが、コンテナ イメージや Helm チャート向けに特別に最適化されているわけではありません。また、追加の構成が必要であり、Artifact Registry と同じレベルの GKE やその他の Google Cloud サービスとのシームレスな統合は提供されません。
B. 制限の厳しいロールベースのアクセス制御(RBAC)構成を使用して、オープンソースのコンテナ レジストリ サーバーを GKE で実行するように設定します。
このアプローチには、かなりのセットアップとメンテナンスが必要です。Google Cloudのネイティブソリューションのようなフルマネージドの側面が欠けており、他のGoogle Cloudサービスとシームレスに統合できない可能性があります。
D. コンテナ・レジストリをコンテナ・イメージのOCIベースのコンテナ・レジストリとして構成します。
Google Container Registry はコンテナ イメージの実行可能なオプションですが、Helm チャートはサポートされていません。また、Googleは、そのようなニーズに対応する、より機能豊富で統合されたソリューションとして、Artifact Registryに移行しています。
正解：
C. アーティファクト・レジストリを、Helmチャートとコンテナ・イメージの両方に対してOCIベースのコンテナ・レジストリとして構成します。
Artifact Registry は、Google Cloud が提供するフルマネージド サービスで、コンテナ イメージと Helm チャートの両方をサポートします。Google Kubernetes Engine(GKE)、Cloud Run、VPC Service Controls、IAM とうまく統合できます。これにより、Google Cloud でコンテナ化を採用している組織にとって理想的な選択肢となります。
リンクス：
https://cloud.google.com/artifact-registry/docs/helm
</div></details>

### Q.  問題45: 未回答
新しいリリースがサービスのすべてのメモリ リソースを消費したために停止が発生したサービスを担当します。ユーザーへの影響を最小限に抑えるために、リリースを正常にロールバックしました。現在、あなたは停止の事後分析を実施する任務を負っており、その開発においてサイト信頼性エンジニアリングのプラクティスを順守することを目指しています。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 停止の再発を回避するのではなく、新機能の開発に重点を置きます。
このアプローチでは、インシデントから学ぶことの重要性が見落とされています。停止の原因に対処せずに新機能のみに焦点を当てると、問題が再発する可能性があります。
C. 関係するすべてのエンジニアとの個別のミーティングを計画します。新しいリリースを承認し、運用環境にプッシュしたユーザーを特定します。
この方法は、非難の文化を生み出す可能性があり、オープンなコミュニケーションと学習に逆効果です。これにより、チーム メンバーが将来問題を報告するのを思いとどまらせる可能性があります。
D. Git 履歴を使用して、関連するコードのコミットを見つけます。そのコミットを行ったエンジニアが本番サービスで作業できないようにします。
これは、誰も責めない事後分析のアプローチと真っ向から矛盾します。コードコミットの責任を個人に負わせることは、恐ろしい環境を作り出し、イノベーションや正直なエラー報告を抑圧する可能性があります。
正解：
B. 原因の責任者ではなく、インシデントの原因を特定することに重点を置きます。
このアプローチは、SRE 文化における誰も責めない事後分析の原則に沿ったものです。個人の責任に焦点を当てるのではなく、インシデントにつながった体系的な問題とプロセスの崩壊を理解することに重点を置いています。これにより、正直な報告とエラーからの学習が促進され、将来の停止を防ぐためのより効果的なソリューションにつながります。
</div></details>

### Q.  問題46: 回答
Compute Engine マネージド インスタンス グループにデプロイされたウェブ アプリケーションを操作し、すべてのインスタンスに Ops Agent をインストールしています。特定の IP アドレスからの不審なアクティビティに気付いたら、その IP アドレスからのリクエスト数を最小限の運用オーバーヘッドで追跡するように Cloud Monitoring を構成します。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Webサーバーログをスクレイピングするスクリプトを作成します。IP アドレス リクエストの指標を Cloud Monitoring API にエクスポートします。
複雑さが増し、運用上のオーバーヘッドが発生します。スクリプトを保守し、Webサーバーのログ形式との互換性を確保する必要があります。このアプローチは、Ops Agent と Cloud Monitoring の組み込み機能を利用する場合に比べて、統合性が低く、リソースを大量に消費します。
C. アプリケーションを更新して、IP アドレス リクエストの指標を Cloud Monitoring API にエクスポートします。
効果的である可能性がありますが、アプリケーションコードの変更が必要です。このアプローチは、アプリケーション開発リソースと場合によっては新しいデプロイが必要になるため、より時間がかかり、煩わしいものになる可能性があります。
D. Ops エージェントをメトリック レシーバーで構成します。
は、標準メトリックまたはカスタムメトリックの収集に関するものですが、Webサーバーログから特定のIPアドレス情報を抽出するなど、ログデータの処理と分析を直接目的としたものではありません。
正解：
A. Ops エージェントをロギング レシーバで構成します。ログベースのメトリクスを作成します。
Compute Engine インスタンスの Ops Agent は、通常は IP アドレス情報を含むウェブサーバーログなどのログデータを収集するように設定できます。
Cloud Monitoring でログベースの指標を作成することで、不審な IP アドレスからのリクエスト数を具体的に追跡できます。このアプローチでは、既存のロギング メカニズムを活用し、Cloud Monitoring と直接統合することで、最小限の追加オーバーヘッドで特定の指標を効率的にモニタリングできます。
したがって、オプション A は、最小限の運用オーバーヘッドで目標を達成するための最も合理化された統合されたアプローチです。
リンクス：
ロギング受信側
</div></details>

### Q.  問題47: 回答
再利用可能なインフラストラクチャをコードとして開発する場合、各モジュールには、テスト プロジェクト内でモジュールを開始する統合テストが組み込まれます。ソース管理に GitHub を活用する場合の目標は、機能ブランチを継続的にテストし、変更が受け入れられる前にすべてのコードがテストを受けるようにすることです。
これを実現するには、統合テストをどのように自動化する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. CI/CD パイプラインには Jenkins サーバーを使用します。機能ブランチ内のすべてのテストを定期的に実行します。
Jenkins は定期的にテストを実行するように設定できますが、すべての機能ブランチの変更をテストするには、Cloud Build と GitHub の統合ほど効率的ではない可能性があります。
B. コードを承認する前に、プル要求のレビュー担当者に統合テストを実行するように依頼します。
この手動アプローチは、自動テストに比べて信頼性と拡張性が低くなります。それは、レビュアーの勤勉さと可用性に依存します。
C. Cloud Build を使用してテストを実行します。プル要求がマージされた後に実行するすべてのテストをトリガーします。
マージ後にテストを実行すると、開発サイクルの後半で問題が検出される可能性があります。メインブランチにエラーが発生しないように、マージする前にテストする方が効果的です。
正解：
D. Cloud Build を使用して、特定のフォルダでテストを実行します。GitHub のプルリクエストごとに Cloud Build をトリガーします。
これは、ビルドを自動化し、GitHub のプルリクエストによってトリガーされるテストを実行する Cloud Build の機能と一致しています。プルリクエストの Cloud Build トリガーを構成することで、コードの変更ごとに統合テストが自動的に実行されるようにし、変更をメインブランチにマージする前にコードの品質と一貫性を維持できます。
リンクス：
ビルド トリガーの作成と管理
</div></details>

### Q.  問題48: 未回答
1 つのゾーン内の自動スケーリングされたマネージド インスタンス グループでアプリケーションを実行しています。これは優先度の高いワークロードであるため、変更を確定する前に、マネージド・インスタンス・グループへの変更をテストする必要があります。これらの変更をテストしながら、要求を処理するのに十分な容量を確保するにはどうすればよいでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 複数のゾーンで実行するようにマネージド インスタンス グループを変更します。
マネージド・インスタンス・グループを複数のゾーンで実行すると、可用性とフォールト・トレランスを強化できますが、テストの変更がリクエストを処理するグループのキャパシティに影響を与えないことが直接保証されるわけではありません。
C. 変更中は、自動スケールを一時的に無効にします。
自動スケーリングを無効にすると、グループは現在の容量で固定されますが、変更の実装中に予期しない需要の急増が発生した場合は、これでは不十分になる可能性があります。
D. 予測自動スケーリングを有効にします。
予測自動スケーリングは、予想される負荷の変化を予測して対応するのに役立ちますが、変更時に十分な容量を保証するものではありません。予測モデルに影響を与えるリアルタイム要求や変更の予期しないスパイクは、容量不足につながる可能性があります。
正解：
A. 変更を行うときは、自動スケールを "スケールアウトのみ" に構成します。
自動スケールを "スケールアウトのみ" に構成すると、容量が減らなくなります。要求が増加した場合、マネージド インスタンス グループは需要を満たすためにスケールアウトされ、テスト フェーズ中に十分な容量が維持されます。
リンクス：
オートスケーラーの管理
</div></details>

### Q.  問題49: 回答
あなたのチームは最近、NGINX ベースのアプリケーションを Google Kubernetes Engine(GKE)にデプロイし、HTTP Google Cloud Load Balancer(GCLB)イングレスを介して一般公開しました。適切なサービスレベル指標(SLI)を定義することで、アプリケーションのフロントエンドのデプロイメントをスケーリングすることに関心があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Liveness プローブと Readiness プローブからの平均応答時間を使用するようにポッドの水平オートスケーラーを構成します。
Liveness プローブと Readiness プローブは、ポッドの健全性と準備状況を判断するために重要ですが、スケーリングの目的には最適な指標ではありません。これらのプローブからの平均応答時間は、スケーリングのより関連性の高いメトリックであるトラフィック負荷やアプリケーションに対する要求を直接反映していません。
B. GKE で垂直ポッド オートスケーラーを構成し、ポッドの拡張に合わせてクラスタ オートスケーラーがクラスタをスケーリングできるようにします。
Vertical Pod Autoscaler (VPA) は、ポッドの CPU とメモリの制限を調整しますが、トラフィックの負荷に基づいてポッドの数をスケーリングすることはありません。個々のポッドのリソース割り当てを最適化するには適していますが、ユーザーの需要やトラフィックに基づいてデプロイ全体をスケーリングするには適していません。
D. NGINX 統計エンドポイントを公開し、NGINX デプロイによって公開される要求メトリックを使用するように水平ポッド オートスケーラーを構成します。
NGINX 統計エンドポイントを公開し、そのリクエスト指標をスケーリングに使用することは効果的ですが、このアプローチは Cloud カスタム指標を使用する場合に比べて GKE や GCLB との統合が不十分です。追加の構成とメンテナンスの作業が必要であり、GCLB メトリックを活用するほど堅牢で単純ではない可能性があります。
正解：
C. Cloud カスタム メトリック アダプターをインストールし、GCLB によって提供されるリクエストの数を使用するように水平ポッド オートスケーラーを構成します。
Cloud カスタム指標を利用すると、Google Cloud Load Balancer(GCLB)から特定の関連指標(この場合はリクエスト数)をキャプチャできます。これらの指標を GKE の Horizontal Pod Autoscaler(HPA)で使用して、実際のトラフィック需要に基づいてポッドの数を自動的にスケーリングできます。このアプローチは、実際のユーザーの需要に基づくスケーリングとうまく連携し、リソースが効率的に利用されることを保証し、それによって望ましいパフォーマンスレベルを維持します。
リンクス：
ポッドの水平自動スケーリング
メトリクスに基づくPodの自動スケーリングの最適化
</div></details>

### Q.  問題50: 未回答
組織は、本番環境を含む複数のプロジェクトを Google Cloud に移行し、組織のポリシーを一元的に管理しています。
リソース階層のポリシー変更をロールアウトするために Google が推奨するプラクティスに合わせるには、どのようなアプローチを取る必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 運用環境用に 1 つの組織を作成し、階層全体にポリシーの変更を段階的に適用します。
ポリシーの変更を本番組織に直接実装すると、これらの変更が広範囲に及ぼし、意図しない影響が生じる可能性があるため、リスクが伴う可能性があります。最初に非運用環境でポリシーをテストすることが重要です。
C. 運用環境用の組織と、リリース前のチェック用に別のステージング環境を設定し、ステージング環境で実験を実行します。
ステージング環境はプレリリースチェックに不可欠ですが、本番環境の構成をミラーリングする必要があります。実験目的でステージング環境で構成を変更したり、セキュリティ設定を緩和したりすることはお勧めできません。
D. 運用環境の組織を形成し、運用プロジェクトの階層の下位から始めて、ポリシーの変更を段階的に導入します。
ポリシーの変更を本番組織に直接導入することは、たとえ段階的に行われたとしても、意図しない結果を招くリスクがあります。最初に、別の非運用環境でポリシーの変更を試すことをお勧めします。
正解：
A. ポリシーの変更を試すための組織を 1 つと、運用環境用に別の組織を設立します。
ポリシーの変更をテストするために別の組織を使用することは、賢明なアプローチです。これにより、本番環境に影響を与えるリスクを冒すことなく実験を行うことができます。この方法は、ポリシーがメイン組織に適用される前に、制御された設定で潜在的な問題と意図しない結果を特定するのに役立ちます。
リンクス：
https://cloud.google.com/architecture/identity/best-practices-for-planning#use_a_separate_organization_for_experimenting
</div></details>

## 4

### Q.  問題1: 未回答
チームは、世界中の何百万人もの顧客にサービスを提供している Compute Engine インスタンスに重要なセキュリティ パッチを適用する必要があります。
パッチを迅速に展開し、コストと顧客への影響を最小限に抑えるには、どのような戦略を採用する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. 世界中のすべての Compute Engine インスタンスを同時に更新する。
すべてのインスタンスを同時に更新するのは危険です。パッチによって問題が発生すると、すべてのユーザーに一度に影響を与え、サービスの大幅な中断につながる可能性があります。
C. 各リージョンの夜間にスケジュールされた 24 時間のローリング更新を実行します。
24 時間のローリング アップデートは、オフピーク時に更新を実行することで顧客への影響を最小限に抑えますが、重要なパッチには適していません。どのリージョンでも更新を遅らせると、システムが特定されたセキュリティの脅威に対して脆弱なままになる可能性があります。
D. A/B ロールアウトを実装し、Compute Engine ですべてのインスタンスのコピーを作成し、パッチを適用してから、ユーザーを切り替えます。
重複インスタンスの完全なセットの作成を伴う A/B ロールアウトは、費用対効果が高くなく、タイムリーでもありません。この方法では、かなりのリソースと時間を必要とするため、迅速なセキュリティパッチの展開は現実的ではありません。
正解：
A. セキュリティ パッチのカナリア ロールアウトを短期間で実行します。
Canary ロールアウトでは、最初に少数のインスタンスにパッチを適用できます。このアプローチにより、より広範なロールアウトの前にパッチの影響を監視し、問題を早期に特定して対処することで、リスクとコストを最小限に抑えることができます。
リンクス：
カナリアリリース
カナリアを放つことでベーコンを救う方法—CREの人生の教訓
</div></details>

### Q.  問題2: 回答
アプリケーション イメージが構築され、Google Container Registry(GCR)にアップロードされます。イメージが更新されるたびにアプリケーションをデプロイする自動パイプラインを作成し、開発作業を最小限に抑えることに重点を置きます。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Build を使用して Spinnaker パイプラインをトリガーします。
Cloud Build は Spinnaker パイプラインを直接トリガーしません。イメージの構築や CI プロセスに適しています。
C. Cloud Build のカスタム ビルダーを使用して Jenkins パイプラインをトリガーします。
これには、Cloud Build と Jenkins の統合を設定するための開発作業が多くなり、指定された要件に対して最も効率的ではありません。
D. Cloud Pub/Sub を使用して、Google Kubernetes Engine(GKE)で実行されているカスタム デプロイ サービスをトリガーします。
これは有効なアプローチですが、カスタム デプロイ サービスの開発と保守が必要になるため、Cloud Pub/Sub で Spinnaker を直接使用する場合と比較して、開発の労力が増加します。
正解：
B. Cloud Pub/Sub を使用して Spinnaker パイプラインをトリガーします。
このアプローチでは、Cloud Pub/Sub を活用して、新しいイメージが GCR にアップロードされるたびに Spinnaker でのデプロイをトリガーします。この方法は効率的で、GCP のネイティブ サービス(GCR と Cloud Pub/Sub)と継続的デリバリー プラットフォームである Spinnaker を統合するため、Google Cloud のベスト プラクティスに沿ったものです。これは、イメージの更新時に自動的にデプロイするための合理化されたソリューションであり、手動による介入を減らします。
リンクス：
Cloud Deploy の概要
</div></details>

### Q.  問題3: 回答
アプリケーション成果物は、現在、CI/CD パイプラインを介して構築およびデプロイされています。このパイプラインがアプリケーション シークレットに安全にアクセスできるようにし、セキュリティ侵害が発生した場合にこれらのシークレットのローテーションを容易にするために、
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ビルド時に開発者にシークレットの入力を求めます。シークレットを保存に保存しないように開発者に指示します。
このアプローチはスケーラブルでも安全でもありません。これには手動の介入が含まれ、人為的エラーのリスクが高まり、ビルドログにアクセスできるすべての人に秘密が公開されます。
B. シークレットを Git の別の構成ファイルに格納します。選択した開発者に構成ファイルへのアクセス権を付与します。
Git にシークレットを保存することは、たとえ別の構成ファイルであっても、安全ではありません。リポジトリが侵害されると、シークレットが公開されます。さらに、シークレットのローテーションも容易ではありません。
D. シークレットを暗号化し、ソース コード リポジトリに格納します。復号化キーを別のリポジトリに保存し、パイプラインにそのキーへのアクセス権を付与します。
シークレットを暗号化するとセキュリティのレイヤーが追加されますが、別のリポジトリの復号化キーと一緒にソースコードリポジトリに保存すると、リスクが生じる可能性があります。いずれかのリポジトリが侵害されると、シークレットが漏洩する可能性があります。この方法では、シークレットのローテーションと管理も複雑になります。
正解：
C. シークレットは、Cloud KMS の鍵で暗号化して Cloud Storage に保存します。CI / CD パイプラインに IAM 経由で Cloud KMS へのアクセスを提供します。
この方法では、シークレットを暗号化された形式で Cloud Storage に保存し、Cloud Key Management Service(KMS)によって管理される鍵で保護します。CI/CD パイプラインは、Identity and Access Management (IAM) を介してこれらのキーへのアクセスを許可でき、必要に応じてシークレットを復号化できます。このアプローチにより、シークレットが安全になるだけでなく、セキュリティ侵害が発生した場合にシークレットをローテーションすることが容易になります。
リンクス：
https://cloud.google.com/security-key-management
</div></details>

### Q.  問題4: 回答
アプリケーション・ログを 7 年間アーカイブする必要がある政府機関と共同作業を行っているとします。ここで行う作業は、ストレージ費用を最小限に抑えながらログをエクスポートして保持するように Cloud Logging を設定することです。
どのようなステップを踏むべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Storage バケットを作成し、バケットにログを直接送信するようにアプリケーションを変更します。
これにはアプリケーションの変更が必要であり、Cloud Logging のネイティブ エクスポート機能を使用するよりも複雑で信頼性が低くなる可能性があります。
B. Cloud Logging からログを取得して BigQuery に保存する App Engine アプリケーションを開発します。
App Engine アプリケーションの開発は、特に BigQuery が Cloud Storage に比べて長期保存に一般的にコストがかかることを考えると、不必要な複雑さとコストがかかります。
C. Cloud Logging でエクスポート設定を設定し、ログを永続ストレージに 7 年間保存するように Cloud Pub/Sub を設定します。
このセットアップは機能しますが、費用対効果の高い長期ストレージよりも、リアルタイムのログ分析とストリーミングに適しています。
正解：
D. クラウドシンクを作成し、名前を指定し、アーカイブされたログを保存するための Cloud Storage バケットを作成してから、ログのエクスポート先としてバケットを選択します。
この方法では、Cloud Logging の組み込みのエクスポート機能を使用して、ログを Cloud Storage バケットに効率的に転送するため、長期保存の費用対効果の高いソリューションです。合理化され、GCP のロギング サービスやストレージ サービスと直接統合されているため、追加の開発作業や複雑な構成が不要になります。
リンクス：
https://cloud.google.com/logging/docs/routing/overview#destinations
</div></details>

### Q.  問題5: 回答
現在、Google Cloud でのアプリケーションのデプロイ戦略を考案しているところです。デプロイ計画では、ライブ トラフィックを利用して、アプリケーションの新しいイテレーションのパフォーマンス メトリックを収集することを目的としています。目標は、アプリケーションを正式に起動する前に、完全な運用ワークロードでテストを実行することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ブルー/グリーンデプロイで A/B テストを使用します。
異なるバージョン間でユーザーの行動を比較するのに効果的です。ブルー/グリーンデプロイでは、問題が検出された場合に迅速なロールバックが可能になるため、ダウンタイムが最小限に抑えられます。
全負荷時の技術的なパフォーマンスよりも、ユーザーエクスペリエンスの指標に重点が置かれています。探している実際の条件下では、詳細なパフォーマンス メトリックが提供されない場合があります。
B. 継続的デプロイでカナリア テストを使用します。
少数のユーザーに対して段階的に変更を導入できるため、パフォーマンスと安定性の監視に最適です。継続的デプロイにより、更新プログラムが迅速に配信されます。
カナリア テストの規模が小さすぎて、完全な運用ワークロードを完全にシミュレートできない可能性があり、全負荷条件下でのテストの要件を満たさない可能性があります。
C. ローリング アップデートのデプロイでカナリア テストを使用します。
カナリア テストの制御されたテスト環境と、ローリング アップデートの段階的で安全なロールアウトを組み合わせます。これにより、実際のワークロードで、より制御された段階的な方法でアプリケーションをテストできます。
ローリング更新の段階的な性質により、新しいバージョンが完全な運用ワークロードに公開されるのが遅れる可能性があり、合計負荷条件下でのパフォーマンスに関するフィードバックがすぐに提供されない可能性があります。
正解：
D. 継続的デプロイでシャドウ テストを使用します。
シャドウ テストは、完全な運用ワークロードでテストするニーズと完全に一致します。ライブトラフィックをシャドウバージョンに複製し、エンドユーザーに影響を与えることなくパフォーマンスメトリックを収集できるようにします。継続的なデプロイにより、継続的なデプロイとテストが保証されます。
このアプローチにより、エンドユーザーのエクスペリエンスに影響を与えることなく、実際の運用条件下でアプリケーションの新しいイテレーションを広範囲にテストできると同時に、継続的デプロイの継続的なデプロイ プロセスの恩恵を受けることができます。この戦略により、新しいバージョンが最終的に動作する正確な条件下でどのように機能するかを包括的に理解し、目的と密接に連携します。
リンクス：
シャドウ・テスト・パターン
</div></details>

### Q.  問題6: 回答
ユーザーに大きな影響を与えたインシデントの事後分析レポートを作成しています。目標は、今後同様のインシデントが発生しないようにすることです。
次の 2 つのセクションのうち、事後分析に含めるべきセクションはどれですか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B.インシデントの原因となった従業員のリスト。
非難の文化を生み出し、学習や改善に逆効果になる可能性があるため、お勧めできません。
D. 過去のインシデントと比較したインシデントの重大度に対する評価。
コンテキストには役立ちますが、根本原因とアクションアイテムを理解することに比べると、将来の予防にとってはそれほど重要ではありません。
E. インシデントの影響を受けたすべてのサービスの設計文書のコピー。
これらを提供することは、技術的なコンテキストに役立ちますが、インシデント防止に直接関係しない情報でレポートが過負荷になる可能性があります。
正解：
ある。インシデントの根本原因の説明。
根本原因を理解することは、何が悪かったのか、そして同様の問題を防ぐ方法を特定するために不可欠です。
C. インシデントの再発防止のためのアクションアイテムのリスト。
このセクションでは、インシデントが再発しないようにするための具体的な手順と対策について説明します。
これらのセクションでは、インシデントを理解し、効果的なインシデント管理と防止の鍵となる改善のための明確な手順を提供することに重点を置いています。
リンクス：
https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons
</div></details>

### Q.  問題7: 回答
アプリケーション イメージは Cloud Build を使用して構築され、Google Container Registry(GCR)にアップロードされます。ソース管理でタグ付けされたリリース バージョンに基づいて、デプロイ用の特定のアプリケーション バージョンの指定を有効にするには、イメージをプッシュするときにどのようなアクションを実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ソース管理タグでイメージ ダイジェストを参照します。
このオプションは、イメージがビルドされてレジストリにプッシュされた後にイメージ ダイジェストが生成されるため、実用的ではありません。事前には知られておらず、ビルド プロセスの前にソース管理タグに直接リンクすることはできません。
B. イメージ名内のパラメーターとしてソース管理タグを指定します。
この方法では、ビルドまたはプッシュ プロセス中にソース管理タグを使用してイメージに手動でタグを付けます。機能しますが、人為的エラーが発生しやすく、手動による介入が必要なため、ビルドプロセスでの自動タグ付けと比較して効率が低下します。
D. GCR ダイジェストのバージョン管理を使用して、イメージをソース管理のタグと照合します。
この方法では、GCR で自動的に生成されたイメージのダイジェストをソース管理タグと照合します。ただし、ダイジェストはイメージがレジストリにプッシュされた後にのみ認識されるため、ソース管理タグに基づいて特定のアプリケーションバージョンを事前に指定するためにこれを使用することはできません。
正解：
C. Cloud Build を使用して、リリース バージョン タグをアプリケーション イメージに含めます。
Cloud Build には、ソース管理から派生したバージョン情報を使用して Docker イメージのタグ付けを自動化するメカニズムが組み込まれています。このアプローチにより、より堅牢でエラーに強いプロセスが可能になります。ビルドプロセス中にリリースバージョンタグを含めることを自動化することで、さまざまなアプリケーションバージョンとソースコードとの関連付けの追跡が簡素化され、より合理化され、管理しやすいデプロイにつながります。
リンクス：
ビルド構成ファイルを使用してイメージをビルドする
</div></details>

### Q.  問題8: 回答
チームは、Google Kubernetes Engine(GKE)にデプロイするための新しいアプリケーションを開発中です。当面のタスクは、さまざまなアプリケーションレベルのメトリックを 1 か所に集めて統合できる監視を確立することです。目的は、Google Cloud Platform サービスを活用しながら、モニタリングのためのセットアップ作業を最小限に抑えることです。
どのようなアプローチを取るべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. さまざまな指標をアプリケーションから Cloud Monitoring API に直接公開し、これらのカスタム指標を確認します。
この方法では、各指標を Cloud Monitoring API に公開するために手動で作業する必要があります。これは手間がかかる可能性があり、特にメトリックを頻繁に変更または更新する可能性のある新しいアプリケーションの場合、複数のメトリックを処理する最も効率的な方法ではありません。
B. Cloud Pub/Sub クライアント ライブラリをインストールし、アプリケーションからさまざまなトピックにさまざまな指標をプッシュしてから、集計された指標を確認します。
Cloud Pub/Sub はメッセージ キューイングと非同期通信のための強力なツールですが、アプリケーションの指標のみに使用すると、不必要な複雑さが増します。これは、直接的なメトリックの収集と監視よりも、イベント駆動型のアーキテクチャとメッセージの受け渡しに適しています。
D. すべての指標をアプリケーション固有のログメッセージの形式で出力し、これらのメッセージをコンテナから Cloud Logging コレクターに渡して、指標を確認します。
このアプローチでは、メトリックをログメッセージとして扱う必要がありますが、これは非効率的であり、適切なアプリケーション監視に必要な粒度や特異性を提供しない可能性があります。また、膨大な量のログデータが発生し、有意義なインサイトを抽出することが困難になることもあります。
正解：
C. OpenTelemetry クライアント ライブラリをアプリケーションにインストールし、メトリックのエクスポート先を構成してから、クラウド アプリケーションのメトリックを観察します。
このアプローチでは、クラウドネイティブソフトウェアのオブザーバビリティフレームワークであるOpenTelemetryを活用して、さまざまなアプリケーションレベルのメトリックをキャプチャします。OpenTelemetry は GCP と統合されているため、指標を Stackdriver(現在は Google Cloud Monitoring と呼ばれています)にシームレスにエクスポートできます。この方法では、OpenTelemetry は GKE などのクラウド環境でのモニタリングとオブザーバビリティのために特別に設計されたツールとライブラリの包括的なスイートを提供するため、セットアップの労力が最小限に抑えられます。
リンクス：
https://cloud.google.com/products/operations?hl=en
ユーザー定義メトリックの概要
</div></details>

### Q.  問題9: 回答
Compute Engine でアプリケーションを操作し、Cloud Clogging を利用してログを収集している。レビュー中に、特定のログエントリフィールドに個人を特定できる情報(PII)が存在することを確認します。これらの PII エントリは、一貫して "userinfo" というテキストで始まります。目的は、将来の調査のためにこれらのログエントリを安全にキャプチャし、Cloud Logging で公開されないようにすることです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. userinfo に一致する基本的なログフィルタを作成し、Cloud Storage をシンクとして Cloud Console でログのエクスポートを構成します。
ログが Cloud Logging に到達する前にログから PII が削除される場合は対応していません。
C. userinfo に一致する高度なログ フィルタを作成し、Cloud Console で Cloud Storage をシンクとしてログのエクスポートを構成してから、userinfo をフィルタとしてログの除外を設定します。
高度なログフィルタの作成と Cloud Storage へのログのエクスポートが含まれますが、PII データがエクスポート前に Cloud Logging に漏洩する可能性があります。
D. Ops Agent で Fluentd フィルタ プラグインを使用して、userinfo を含むログエントリを削除し、userinfo に一致する高度なログ フィルタを作成してから、Cloud Storage をシンクとして Cloud Console でログ エクスポートを構成します。
は、フィルタリングに Fluentd を使用し、エクスポートに高度なログ フィルターを使用するため、冗長で必要以上に複雑になる可能性があります。
正解：
B. Ops Agent で Fluentd フィルタ プラグインを使用して、userinfo を含むログエントリを削除し、そのエントリを Cloud Storage バケットにコピーします。
個人を特定できる情報が Cloud Logging で公開されるのを防ぐという目的に効果的に対処します。強力なログ コレクターである Fluentd を使用すると、PII を含むログエントリが Cloud Logging に到達する前に除外できます。これらのフィルタリングされたエントリは、将来の調査のために Cloud Storage バケットに安全に保存できます。
リンクス：
ルーティングとストレージの概要
除外フィルター
</div></details>

### Q.  問題10: 回答
今後の分析のために Cloud Logging から BigQuery にログエントリをエクスポートするための Cloud Logging シンクを作成しています。組織には、開発プロジェクトを含む「Dev」という名前の Google Cloud フォルダと、本番環境プロジェクトを含む「Prod」という名前のフォルダがあります。開発プロジェクトからのログエントリは「dev_dataset」にエクスポートする必要があり、本番プロジェクトからのログエントリは「prod_dataset」にエクスポートする必要があります。作成されるログ・シンクの数を最小限に抑え、ログ・シンクが将来のプロジェクトに適用されるようにします。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 組織レベルで 1 つの集約ログ シンクを作成します。
組織レベルで 1 つの集約されたログ シンクを作成することは広範であり、すべてのプロジェクトのログを含めることができるため、"開発" ログと "運用" ログを分離するための複雑なフィルター処理が必要です。
B. 各プロジェクトにログ シンクを作成します。
各プロジェクトでのログ シンクの設定は手間がかかり、新しいプロジェクトに自動的には適用されません。
C. 組織レベルで 2 つの集約ログ シンクを作成し、プロジェクト ID でフィルター処理します。
プロジェクト ID フィルターを使用して組織レベルで 2 つの集約されたログ シンクも、正確なフィルター処理とメンテナンスが必要です。
正解：
D. Dev フォルダーと Prod フォルダーに集約されたログ シンクを作成します。
ログエントリを効率的に管理し、今後の分析のために正しい BigQuery データセットにエクスポートするには、「Dev」フォルダと「Prod」フォルダの両方に集約されたログシンクをフォルダ単位で作成する必要があります。このセットアップにより、作成する必要があるログ シンクの数が最小限に抑えられ、これらのフォルダーに今後追加されるプロジェクトがログ シンクの構成を継承することが保証されます。これにより、開発プロジェクトのログは「dev_dataset」に、本番プロジェクトのログは「prod_dataset」にエクスポートされ、個々のプロジェクトシンクや追加の組織レベルのフィルターは必要ありません。このアプローチは合理化されており、Google Cloud でのリソース管理のベスト プラクティスと一致しています。
リンクス：
組織のログを集約して保存する
データガバナンスにログバケットを使用 (23 のリージョンでサポートされるようになりました)
集約されたシンクと一元化されたログ記録
</div></details>

### Q.  問題11: 回答
Terraform を使用して、Google Cloud 環境にデプロイされたアプリケーションを管理します。アプリケーションは、マネージド・インスタンス・グループによってデプロイされたインスタンスで実行されます。Terraform コードは、CI/CD パイプラインを使用してデプロイされます。マネージド・インスタンス・グループで使用されるインスタンス・テンプレートのマシン・タイプを変更すると、パイプラインはterraform適用ステージで失敗し、次のエラー・メッセージが表示されます:
Error waiting for Deleting Instance Template: The instance_template resource 'project/my-project/global/instanceTemplates/my-it-202201010101010000000001' is already being used by 'projects/my-project/regions/us-central1/instanceGroupManagers/my-mig
インスタンス テンプレートを更新し、アプリケーションの中断とパイプラインの実行回数を最小限に抑える必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 管理対象インスタンス・グループを削除し、インスタンス・テンプレートの更新後に再作成します。
すべてのインスタンスを削除してから再プロビジョニングする必要があるため、アプリケーションに大きな中断が発生します。
B. 新しいインスタンス・テンプレートを追加し、新しいインスタンス・テンプレートを使用するように管理対象インスタンス・グループを更新し、古いインスタンス・テンプレートを削除します。
有効なアプローチですが、より多くの手順が必要であり、Terraformのcreate_before_destroyによる自動処理と比較してエラーが発生しやすくなる可能性があります。
C. Terraform状態ファイルから管理対象インスタンス・グループを削除し、インスタンス・テンプレートを更新して、管理対象インスタンス・グループを再インポートします。
不必要で危険な操作です。これにより、Terraformの状態に不整合が生じる可能性があり、インスタンス・テンプレートの更新に関する問題に直接対処するものではありません。
正解：
D. インスタンステンプレートのライフサイクルブロックでcreate_before_destroyメタ引数をtrueに設定します。
Terraformでは、リソースのライフサイクル・ブロック内のcreate_before_destroyメタ引数を使用して、古いリソースが破棄される前に新しいリソースを作成する必要があることを指定します。
これをインスタンス・テンプレートに設定すると、Terraformは古いインスタンス・テンプレートを削除する前に新しいインスタンス・テンプレートを作成します。このアプローチにより、現在のインスタンス・テンプレートが管理対象インスタンス・グループによってまだ使用されているために、その削除がブロックされる問題が解決されます。
この方法では、マネージド インスタンス グループが新しいインスタンス テンプレートの使用にシームレスに移行できるため、中断が最小限に抑えられ、追加のパイプライン実行や手動による介入が回避されます。
リンクス：
ライフサイクルのメタ引数
https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_instance_template.html
</div></details>

### Q.  問題12: 回答
社内では、アプリケーションは Google Kubernetes Engine(GKE)でホストされ、そのデプロイは GitOps アプローチに従います。アプリケーション開発者は、多くの場合、アプリケーションをサポートするためにクラウドリソースを作成します。目標は、デベロッパーが Google が推奨するプラクティスを遵守しながら、インフラストラクチャをコードとして管理できるようにすることです。さらに、構成のずれを防ぐために、コードとしてのインフラストラクチャが定期的に調整されるようにするメカニズムを確立する必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. Terraform ビルダーを使用して Cloud Build を構成し、terraform plan コマンドと terraform apply コマンドを実行します。
これには、Cloud Build 内で Terraform を使用したインフラストラクチャ管理の自動化が含まれます。Infrastructure as Code の実行には効果的ですが、Kubernetes マニフェストや GKE で使用されている GitOps アプローチと自然に統合されるわけではありません。
C. Terraform Dockerイメージを使用してPodリソースを作成し、terraform planおよびterraform applyコマンドを実行します。
スタンドアロンのPodでTerraformを実行すると、必要なコマンドを実行できますが、クラウドリソースを管理するためのKubernetesとの統合がなく、継続的な調整は提供されません。
D. Terraform Dockerイメージを使用してジョブリソースを作成し、terraform planおよびterraform applyコマンドを実行します。
これにより、Kubernetesジョブを使用してTerraformコマンドが実行されます。Podのアプローチと同様に、タスクを実行しますが、クラウドリソース管理と継続的な調整のためのネイティブKubernetes統合がありません。
正解：
A. Google Kubernetes Engine(GKE)に Config Connector をインストールして構成します。
Config Connector は、Kubernetes マニフェストを使用して Google Cloud リソースを管理できるようにする Kubernetes アドオンです。GitOps アプローチと自然に統合され、Git で宣言された状態と GCP の実際の状態が継続的に調整され、構成のずれが防止されます。
リンクス：
Config Connector の概要
同期: Kubernetes と Anthos Config Management の新機能の整合性
</div></details>

### Q.  問題13: 回答
Cloud Logging にログを書き込むアプリケーションを管理します。一部のチーム メンバーにログをエクスポートする権限を与える必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. これらのメンバーのみがログをエクスポートできるように Access Context Manager を設定します。
Access Context Manager は、アクセス要求の属性に基づいてきめ細かなアクセス制御ポリシーを定義するために使用されます。Cloud Logging に関連する権限の管理やログのエクスポート機能には直接使用されません。
C. アクセス許可 logging.sinks.list と logging.sink.get を持つカスタム IAM ロールを作成して付与します。
カスタム ロールは特定のアクセス許可で調整できますが、アクセス許可 logging.sinks.list と logging.sinks.get では、それぞれログ シンクに関する情報の一覧表示と取得のみが許可されます。ログのエクスポートに必要な、これらのシンクを作成または管理するためのアクセス許可は含まれていません。
D. Cloud IAM で組織ポリシーを作成し、これらのメンバーのみがログのエクスポートを作成できるようにします。
Cloud IAM の組織ポリシーは、GCP 組織全体に広範なポリシーを適用するために使用されます。これらは、ログのエクスポートのアクセス許可を個々のチーム メンバーに付与するというこの特定のタスクには理想的なツールではありません。このレベルのアクセスコントロールは、IAM ロールによってより適切に処理されます。
正解：
A. チームメンバーに Cloud IAM の logging.configWriter の IAM ロールを付与します。
Cloud IAM の logging.configWriter ロールには、ログシンク(ログのエクスポートに使用)を作成、変更、削除するための権限が含まれています。このロールにより、チーム メンバーはログのエクスポート構成を管理し、ログのエクスポートを可能にする要件に合わせることができます。
リンクス：
始める前に
IAMによるアクセス制御
</div></details>

### Q.  問題14: 未回答
チームは、Google Cloud Platform(GCP)の内外へのデプロイを目的とした新しいアプリケーションを設計中です。目的は、システム・リソース使用率を含む包括的なメトリックを収集することです。一元化されたGCPサービスを利用して、データ収集システムを確立するために必要な労力を最小限に抑えながらこれを実現したいと考えています。
どのような手順を踏む必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Debugger パッケージをインポートし、タイミング情報を含むデバッグ メッセージを出力するようにアプリケーションを構成します。
継続的なメトリック収集にはあまり適していません。
C. タイミングライブラリを使用してコードをインストゥルメント化し、Cloud のオペレーションスイートによってスクレイピングされるヘルスチェックエンドポイントを介してメトリクスを公開します。
包括的なメトリクスの実行可能な方法であることに変わりはありませんが、コードのインストルメンテーションにはより多くの労力が必要です。
D. 両方の場所に Application Performance Monitoring (APM) ツールをインストールし、分析のために中央のデータ ストレージの場所へのエクスポートを構成します。
包括的なモニタリングには効果的ですが、設定の手間がかかり、Cloud Profiler ほど GCP サービスと統合されていません。
正解：
B. Cloud Profiler パッケージをインポートし、関数のタイミング データを Cloud のオペレーション スイートに中継してさらに分析するように構成します。
Cloud Profiler は GCP の外部で機能し、CPU 使用率のプロファイリングに重点を置いているため、当初検討されていたよりも適切な選択肢です。これはパフォーマンス分析に効果的なツールであり、セットアップの労力を最小限に抑えるという要件を満たします。
リンクス：
Cloud Profiler の概要
</div></details>

### Q.  問題15: 未回答
管理している運用システムで多数の停止が発生しており、これらの停止のたびにアラートを受信し、夜間に目が覚めます。これらのアラートは、異常なシステムによってトリガーされ、1分以内に自動的に再起動されます。サイト信頼性エンジニアリングのプラクティスを遵守しながら、スタッフの燃え尽き症候群を防ぐのに役立つプロセスを確立することに関心があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. アラートごとにインシデント レポートを作成します。
アラートごとにインシデント レポートを作成するのは大変な作業であり、アラートがアクション不可能な場合は実用的ではありません。
C. 異なるタイム ゾーンのエンジニアにアラートを配布します。
異なるタイムゾーンのエンジニアにアラートを配布することは、アラート疲れの管理に役立つ可能性がありますが、アクション不可能なアラートの根本的な問題には対処できません。
D. 関連するサービス レベル目標を再定義して、エラー バジェットが使い果たされないようにします。
サービスレベル目標の再定義は、停止が実際のサービスレベルアグリーメントやユーザーエクスペリエンスに影響を与えていない場合、関連性がない可能性があります。
正解：
A. 対処できないアラートを排除します。
実行できないアラートの排除は、広くサポートされているソリューションです。これは、実用的なアラートを重視する SRE の原則と一致しています。システムは迅速に回復し、即時のアクションを必要としないため、これらのアラートはノイズと見なすことができます。
リンクス：
SREの原則で信頼性の課題に対応
</div></details>

### Q.  問題16: 未回答
アプリケーションの構築には Cloud Build を使用します。コストと開発作業を最小限に抑えながら、ビルド時間を短縮することを目指しています。どのようなステップを踏むべきですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. machine-type オプションを使用して、より大きな Cloud Build 仮想マシン (VM) を使用します。
より大きな VM を使用すると、より多くのリソースを提供することでビルド時間を短縮できますが、コストが増加し、費用を最小限に抑えるという目的に反します。
B. 複数の Jenkins エージェントを実行して、ビルドを並列化します。
並列ビルド用に複数の Jenkins エージェントを実行すると、処理を高速化できますが、Cloud Build 環境での実装は費用対効果が高くなく、簡単ではない可能性があります。
C. 実行時間を最小化するために、複数の小さなビルド ステップを使用します。
複数の小さなビルド ステップを使用すると、理論的には実行時間を最小化できますが、ビルド時間を最小限に抑えるために重要な成果物のキャッシュや再利用の問題に必ずしも対処できるわけではありません。
正解：
D. Cloud Storage を使用して中間アーティファクトをキャッシュします。
コミュニティ メンバーの大多数は、Cloud Storage を使用して中間アーティファクトをキャッシュすることをサポートしています。このアプローチは、以前のビルドの結果を再利用することでビルド時間を短縮するのに効果的であり、コスト効率に優れています。これは、ビルドを高速化するための Google Cloud のベスト プラクティスとよく一致しています。
リンクス：
ビルドを高速化するためのベストプラクティスBest practices for faster up builds
Google Cloud Storage を使用したディレクトリのキャッシュ
</div></details>

### Q.  問題17: 回答
Google Kubernetes Engine(GKE)の JVM ベースのアプリケーションとマイクロサービス アーキテクチャで構築された e コマース ウェブサイトを運営しています。アプリケーションの負荷は日中に増加し、夜間に減少します。運用チームは、夕方のピーク負荷を処理するのに十分なPodを実行するようにアプリケーションを構成しています。負荷に対して十分なPodとノードのみを実行することで、スケーリングを自動化したい。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Vertical Pod Autoscalerを構成しますが、ノード・プール・サイズは静的に保ちます。
Vertical Pod Autoscaler (VPA) は、Pod の CPU とメモリの予約を調整しますが、より高い負荷を処理するために Pod の数を増やすことはありません。また、ノード プールのサイズを固定すると、需要の少ない期間にインフラストラクチャをスケールダウンできず、リソースが浪費される可能性があります。
B. Vertical Pod Autoscalerを構成し、クラスタAutoscalerを有効にします。
クラスター オートスケーラーはノード プールのサイズを調整しますが、VPA は、マイクロサービス アーキテクチャでさまざまな負荷を処理するためにより重要なカウントではなく、既存のポッドのサイズを変更するため、負荷の変動に対してはそれほど効果的ではありません。
C. Horizontal Pod Autoscalerを構成しますが、ノード・プール・サイズは静的に保ちます。
HPA はポッドの数を調整しますが、静的ノード プールでは、インフラストラクチャをスケールダウンできないため、ピーク時にリソースが不足したり、オフピーク時にリソースが浪費されたりする可能性があります。
正解：
D. Horizontal Pod Autoscalerを構成し、クラスタ・オートスケーラを有効にします。
ポッドの水平オートスケーラー(HPA):デプロイメントまたはReplicaSet内のPodの数は、観測されたCPU使用率やその他の選択されたメトリックに基づいて自動的に調整されます。これは、需要に応じてPodの数を増減できるため、さまざまなアプリケーションの負荷を処理するのに理想的です。
Cluster Autoscaler:ノード プールのサイズが自動的に調整されます。HPA によって Pod の数が増えると、クラスター オートスケーラーは追加の Pod に対応するためにノードを追加できます。逆に、需要が減少したときにノードを削除し、リソースを効率的に使用できます。
結論として、HPAとクラスターオートスケーラーを組み合わせたオプションDは、アプリケーションの負荷に合わせてPodの数と基盤となるインフラストラクチャの両方を調整し、常に最適なリソース使用率を確保する動的で効率的なスケーリングソリューションを提供します。
リンクス：
クラスターの自動スケーリングについて
メトリック API のサポート
</div></details>

### Q.  問題18: 不正解
チームは Google Kubernetes Engine(GKE)でマイクロサービスを運用しています。顧客を保護し、リリースポリシーを確立するために、エラーバジェットの消費を特定することを目的としています。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. メトリックから SLI を作成します。サービスが成功しない場合は、アラートポリシーを有効にします。
メトリックからサービスレベル指標(SLI)を作成することは、重要なステップです。ただし、サービス障害のアラートポリシーを有効にするだけでは、エラーバジェットの管理には直接つながりません。問題の特定には役立ちますが、SLO 違反にどの程度近づいているかを理解するための明確なフレームワークは提供されません。
B. Anthos Service Mesh の指標を使用して、マイクロサービスの健全性を測定します。
Anthos Service Mesh は、マイクロサービスの健全性に関する詳細な指標を提供し、モニタリングに役立ちます。ただし、このオプションでは、SLO とエラー予算管理の直接的な確立が欠けています。これは、リリースやエラー予算の消費に関する戦略的な意思決定よりも、監視に関するものです。
D. SLO を作成し、サービスの稼働時間チェックを設定します。サービスが成功しない場合は、アラートポリシーを有効にします。
SLO を作成し、稼働時間チェックを設定することをお勧めします。ただし、稼働時間チェックだけでは、包括的なエラー予算管理には不十分な場合があります。ダウンタイムを警告することはできますが、情報に基づいたリリースの決定に不可欠な、時間の経過に伴うエラーバジェットの消費率を必ずしも追跡するわけではありません。
正解：
C. SLO を作成します。select_slo_burn_rate でアラート ポリシーを作成します。
これには、サービスのパフォーマンス目標に基づいて SLO を定義し、select_slo_burn_rate を使用してエラーバジェットの消費を監視することが含まれます。エラーバジェットの消費量が多すぎる場合は、アラートポリシーをトリガーできます。このアプローチは、サービスの信頼性をプロアクティブに管理し、リリースと変更について十分な情報に基づいた決定を下すのに役立ちます。
この方法により、明確なパフォーマンス目標 (SLO) と、それらを効果的に監視するメカニズムが確保され、カスタマー エクスペリエンスを保護し、リリース ポリシーを導くために不可欠です。
リンクス：
SLO のアラートポリシーの作成
</div></details>

### Q.  問題19: 未回答
お客様は、明確に定義されたサービス レベル目標 (SLO) を持つサービスを担当します。過去 6 か月間、サービスは一貫して SLO を達成し、顧客満足度は一貫して高いままです。サービスの運用タスクのほとんどは自動化されており、頻繁に発生する反復タスクはほとんどありません。サイト信頼性エンジニアリング (SRE) のベスト プラクティスに準拠しながら、信頼性と展開速度のバランスを最適化することを目指しています。
どのような手順を踏む必要がありますか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. サービスの SLO をより厳格にすることを検討してください。
これは、特に顧客満足度がすでに高い場合、明確なメリットなしにチームへのプレッシャーを不必要に高める可能性があります。
D. 製品チームと協力して、新機能よりも信頼性の強化を優先します。
信頼性がすでにSLOを満たしており、顧客満足度が高い場合は、これは必要ないかもしれません。イノベーションや機能開発の妨げになることがあります。
E. サービスレベル指標(SLI)の実装を変更してカバレッジを拡大することを評価します。
SLIの適用範囲を拡大することで、より深い洞察を得ることができますが、SLOの一貫した達成によって証明されるように、現在のSLIがサービスのパフォーマンスと信頼性を効果的に捉えている場合は、その必要はないかもしれません。
正解：
B. サービスのデプロイ速度やリスクを高めることを検討します。
これにより、サービスの強力な実績を活用して、より多くのリスクを伴う可能性のある変更を革新および実装し、開発と改善を促進することができます。
C. 信頼性の向上を必要とする他のサービスにエンジニアリング リソースを再割り当てします。
サービスの効率性と安定性を利用して、信頼性を高め、システム全体のパフォーマンスを最適化するために、より注意が必要なサービスにリソースを向けることができます。
リンクス：
https://sre.google/workbook/implementing-slos/#slo-decision-matrix
</div></details>

### Q.  問題20: 回答
お客様は、大容量のエンタープライズ・アプリケーションの信頼性に責任を負います。多くのユーザーは、アプリケーションの重要な部分、特にデータ集約型のレポート機能で一貫した障害が発生し、HTTP 500エラーが発生したと報告しています。アプリケーションのダッシュボードを調査したところ、これらのエラーと、レポートの生成に使用される内部キューのサイズを表すメトリックとの間に有意な相関関係があることが分かりました。さらに分析を進めると、I/O待機時間が長いレポートバックエンドに障害が発生していることが判明しました。バックエンドの永続ディスク (PD) のサイズを変更することで、問題を迅速に解決しました。次に、レポート生成機能の可用性サービスレベル指標(SLI)を確立する必要があります。
どのように定義すればよいのでしょうか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。すべてのレポート生成バックエンドの I/O 待機時間の集計として。
この指標はバックエンドのパフォーマンスに関連していますが、ユーザー エクスペリエンスやレポート生成機能の実際の可用性を直接反映するものではありません。
C. 既知の正常なしきい値と比較したアプリケーションのレポート生成キュー サイズ。
このメトリックは、システムの負荷を示しますが、ユーザーが認識する可用性に直接変換されるわけではありません。キューが大きいからといって、必ずしもユーザーに障害が発生しているとは限りません。
D. 既知の正常なしきい値と比較したレポートバックエンドの PD スループット容量。
バックエンドのパフォーマンスにとって重要ですが、これはどちらかというとシステム容量の指標であり、ユーザーの観点から可用性や成功率を直接測定するものではありません。
正解：
B.成功した応答を生成するレポート生成要求の割合として。
SLIにとって実に効果的で適切な選択です。これは、ユーザーにとって最も重要な最終結果、つまりレポート生成要求の正常な完了に焦点を当てています。このメトリックは、ユーザーの期待を簡単に追跡、理解、調整できるため、サービスの正常性と応答性を監視するための貴重なツールになります。さらに、問題を迅速に特定して対処し、信頼性の高いユーザーエクスペリエンスを確保するのに役立ちます。
リンクス：
https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/sli-metrics/overview
SLO の実装
</div></details>

### Q.  問題21: 不正解
組織のために Cloud Run でマイクロサービスを構築してデプロイしている。サービスは、内部的に多くのアプリケーションによって使用されます。新しいリリースをデプロイし、ユーザーと開発者への影響を最小限に抑えながら、ステージング環境と運用環境で新しいバージョンを広範囲にテストする必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. ステージングで 1% から最新バージョンにトラフィックを分割します。このアプローチでは、ステージング環境でのトラフィックの即時分割が伴うため、ユーザーが露出する前に広範なテストを行うのには理想的ではない可能性があります。実際のユーザー トラフィック テストは可能ですが、最初のテスト フェーズでは十分な分離が得られない可能性があります。
B. ステージングと本番環境でトラフィックを 50% の最新バージョンに分割します。トラフィックの 50% をすぐに新しいバージョンに分割することは大きな変更であり、特に検出されていない問題がある場合はリスクが伴う可能性があります。このアプローチでは、最初の分離テストに十分な重点が置かれていません。
D. 新しいグリーン環境への展開: これはブルー/グリーンデプロイ戦略と似ていますが、ステージング用のまったく新しい環境(グリーンタグ)を管理するという追加のオーバーヘッドが伴います。これは、特に Cloud Run が本質的にバージョニングとトラフィック分割をサポートしているため、必要以上に複雑でリソースを大量に消費する可能性があります。
正解：
C. トラフィックを処理せずに、新しいバージョンのサービスを new-release タグを使用してステージング環境にデプロイします。新しいリリース バージョンをテストします。テストに合格した場合は、このタグ付けされたバージョンを徐々にロールアウトします。本番環境についても繰り返します。
ステージング環境のテスト:新しいバージョンを特定のタグを使用してステージング環境にデプロイし、最初はトラフィックを処理せずにすることで、通常の操作が中断されないようにします。これにより、エンドユーザーに影響を与えることなく、本番環境を厳密に反映した環境で徹底的なテストを行うことができます。
制御されたロールアウト:新しいバージョンがステージングでのテストに合格した後、段階的にロールアウトすることで、パフォーマンスを監視し、ユーザーのかなりの部分に影響を与える前に問題を早期にキャッチできます。この段階的なロールアウトは、リアルタイムのフィードバックに基づいて制御および調整できます。
生産への適用:運用環境でこのプロセスを繰り返すと、同じレベルの注意とテストが適用されるようになります。この方法では、環境間で展開プロセスの一貫性が維持されます。
結論として、オプションCは、分離して広範なテストを行い、その後に制御された段階的なロールアウトを可能にすることで、バランスの取れたアプローチを提供し、新しいバージョンの徹底的な検証を保証しながら、潜在的なユーザーと開発者への影響を最小限に抑えます。
リンクス：
https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration
</div></details>

### Q.  問題22: 未回答
Cloud Run を使用してサーバーレス アプリケーションを開発し、本番環境にデプロイしました。次に、アプリケーションのリソース使用率を評価して、コストを最適化します。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Trace とディストリビューティッド(分散)トレーシングを使用して、アプリケーションのリソース使用率をモニタリングします。
アプリケーションのさまざまなコンポーネントを通過する要求のパフォーマンスの監視と分析に重点を置いています。CPU やメモリなどのリソース使用率を監視するために特別に設計されたものではありません。
B. Cloud Profiler と Ops Agent を使用して、アプリケーションの CPU とメモリの使用率を監視します。
アプリケーション内の CPU やメモリなどのリソース使用量の継続的なプロファイリングに使用されます。ただし、Cloud Profiler は、Compute Engine や GKE など、環境をより詳細に制御できるアプリケーションに適しています。Cloud Run のサーバーレス コンテキストでは、基盤となるインフラストラクチャに対して同じレベルのアクセス権を持つことはできません。
D. Cloud Ops を使用してログベースのメトリックを作成し、アプリケーションのリソース使用率を監視します。
ログデータからカスタムメトリクスを作成するために使用できます。特定のログイベントに関するアラートのモニタリングと設定には便利ですが、コンテナの CPU とメモリの使用率のモニタリングは Cloud Monitoring ほど簡単ではありません。
正解：
C. Cloud Monitoring を使用して、アプリケーションのコンテナの CPU とメモリの使用率をモニタリングします。
Cloud Monitoring は、クラウドを利用したアプリケーションのパフォーマンス、アップタイム、全体的な健全性に関する詳細情報を提供します。Cloud Run から指標、イベント、メタデータをキャプチャできます。Cloud Monitoring を使用すると、コンテナの CPU とメモリの使用率を追跡できるため、サーバーレス環境でのリソースの使用状況を把握し、コスト最適化の領域を特定するために不可欠です。
リンクス：
Cloud Profiler の概要bookmark_border
正常性とパフォーマンスの監視
</div></details>

### Q.  問題23: 未回答
あなたの会社は、ホリデーショッピングシーズンに予定されているオンライン小売業者の大規模なマーケティングイベントを計画しているところです。短期間で Web アプリケーションのトラフィックが大幅に急増することが予想されます。
イベント中に発生する可能性のある問題に備えてアプリケーションを準備するには、何をする必要がありますか?(2つのオプションを選択)
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アプリケーションに Anthos Service Mesh を実装し、トポロジ マップで問題を特定します。
Anthos Service Mesh はサービス トポロジを可視化しますが、短期的なイベント専用に実装するのは現実的ではないかもしれません。長期的なオブザーバビリティとトラフィック管理に適しています。
C. 増加した容量要件を評価し、必要なクォータを管理するための戦略を考案します。
これは、インフラストラクチャが増加した負荷を処理できるようにするために重要です。これには、予想される需要を満たすためにリソースをスケーリングするための計画が含まれます。
D. サービスの待機時間を継続的に監視し、特に平均パーセンタイル待機時間に注目します。
これにより、負荷がかかった状態でのサービスのパフォーマンスと応答性を把握し、ボトルネックをすばやく特定して解決できます。
正解：
B. Cloud Monitoring を使用して関連するシステム指標が収集されていることを確認し、特定のしきい値でアラートを設定します。
これは、プロアクティブな監視に不可欠です。適切なアラートを設定することで、潜在的な問題がエスカレートする前に通知を受けることができます。
E. アプリケーションで発生する可能性のある一般的な障害について、Cloud Monitoring でアラートを設定します。
オプション B と同様に、Cloud Monitoring を使用して、特定の既知の障害シナリオのアラートを設定します。これにより、問題が発生したときに迅速に特定して対処できます。
オプション B と E は、突然のトラフィック急増に対処するために重要な即時の監視とアラートに最も直接的に関連しています。オプション C と D は、それぞれ容量計画とパフォーマンス監視にも重要ですが、B や E のように即時のアラートを提供できない場合があります。オプション A は貴重ですが、短期間で実装するにはより複雑になる可能性があります。
リンクス：
https://cloud.google.com/monitoring/alerts
</div></details>

### Q.  問題24: 回答
あなたは、トラフィックの多いマルチリージョンのWebアプリケーションのサービスレベル目標(SLO)を確立する任務を負っています。お客様は、アプリケーションが一貫して利用可能であり続け、迅速な応答時間を提供することを期待しています。現在、お客様はアプリケーションのパフォーマンスと可用性に満足しています。現在の測定値に基づいて、28 日間で 90 パーセンタイルのレイテンシーが 120 ミリ秒、95 パーセンタイルのレイテンシーが 275 ミリ秒であると判断しました。
チームが公開するレイテンシ SLO はどの程度ですか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 90パーセンタイル100ms;95パーセンタイル250ms
これにより、現在のパフォーマンスよりも厳しい目標が設定され、特にトラフィックの多い時期には、達成と維持が困難になる可能性があります。
B. 90パーセンタイル120ms;95パーセンタイル275ms
ユーザーが満足している現在のパフォーマンスと一致しますが、不必要に厳しい SLO をコミットする可能性があります。
D. 90パーセンタイル250ms;95パーセンタイル400ms
現在のパフォーマンスよりもはるかに緩い目標を設定するため、トラフィックの多い Web アプリケーションに対するユーザーの期待に沿わない可能性があります。
正解：
C. 90パーセンタイル150ms;95パーセンタイル300ms
現在のパフォーマンスよりも緩やかな目標を提供し、過剰にコミットすることなく、変動と改善の余地を残します。
リンクス：
サービス・レベル目標
SLI を使用したスターター SLO の計算
</div></details>

### Q.  問題25: 回答
Cloud Build は、アプリケーションの構築とデプロイに使用します。目標は、開発作業を最小限に抑えながら、データベース資格情報とその他のアプリケーション シークレットをビルド パイプラインに安全に統合することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. GCPコンソールでのコストの内訳:この機能では、コストの内訳を視覚的に把握できますが、特に請求データ内でシステムが明確に定義されていない場合は、同じプロジェクト内のシステムごとのコストを区別するために必要な粒度が得られない可能性があります。
B. BigQuery Billing Export のラベル:インスタンスにラベルを付け、請求分析に BigQuery を使用することが最も効果的なアプローチです。ラベルを使用してシステムごとに費用を分類でき、BigQuery では複雑なクエリと詳細な費用分析が可能です。この方法では、コスト属性の精度とスケーラビリティの両方が提供されます。
C. Cloud Logging を使用したインスタンス メタデータ:メタデータでインスタンスをエンリッチメントすると、インスタンスの識別に役立ちますが、Cloud Logging をコスト分析に使用するのは簡単ではありません。Cloud Logging はログデータ用に設計されているため、直接的な費用分析機能を簡単に提供できない場合があります。
正解：
D. Cloud Key Management Service(Cloud KMS)を使用してシークレットを暗号化し、Cloud Build のデプロイ構成に含めます。Cloud Build に KeyRing へのアクセス権を付与します。
このアプローチでは、暗号鍵を処理するための安全なマネージド サービスである Cloud KMS を利用して、シークレットが暗号化され、承認されたサービスのみがアクセスできるようにします。暗号化されたシークレットをデプロイ構成に含め、Cloud Build から復号のために KeyRing にアクセスすることで、セキュリティと最小限の開発作業のバランスを取ることができます。
リンクス：
暗号化されたデータを使用するためのビルドの構成
</div></details>

### Q.  問題26: 回答
組織は、変更諮問委員会 (CAB) を使用して、既存のサービスに対するすべての変更を承認します。このプロセスを刷新して、ソフトウェア配信のパフォーマンスへの悪影響を最小限に抑えたい。
どのような手順を踏む必要がありますか?(2つ選択してください。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. CAB をシニア マネージャーに置き換えて、開発から展開まで継続的に監視します。
これにより、意思決定が一元化される可能性がありますが、ボトルネックが発生するリスクがあり、ソフトウェア配信のパフォーマンスが必ずしも向上するとは限りません。上級管理職は単一障害点になり、効果的な意思決定に不可欠な各変更の詳細なコンテキストを欠く可能性があります。
B.開発者は独自の変更をマージできますが、問題が見つかった場合は、チームのデプロイ プラットフォームが変更をロールバックできるようにします。
開発者が独自の変更を個別にマージできるため、監視と品質保証が不十分になり、運用環境にエラーが発生する可能性があります。このアプローチはロールバック メカニズムに大きく依存しており、特に広範な変更の場合は複雑でリスクが伴う可能性があります。
D. バッチ変更は、より大規模で頻度の低いソフトウェア リリースに変更されます。
このアプローチは、現代のアジャイルプラクティスに反しています。リリースの規模が大きく、頻度が低いと、統合の課題が大きくなり、デプロイ時に障害が発生するリスクが高くなり、フィードバック ループが遅くなる可能性があります。また、各リリースの複雑さが増し、デバッグとメンテナンスが困難になる可能性があります。
正解：
C. 個々の変更に対するピア レビュー ベースのプロセスに移行し、コード チェックイン時に適用され、自動テストによってサポートされます。
このアプローチにより、チーム メンバー間のコラボレーションが促進されるだけでなく、変更がチェックインされる前に品質と信頼性が精査されます。ピアレビューと自動テストを組み合わせることで、コードの品質を大幅に向上させ、変更承認プロセスを合理化し、正式な変更諮問委員会(CAB)への依存を減らすことができます。
E. チームの開発プラットフォームで、開発者が変更の影響に関するフィードバックを迅速に得られるようにします。
変更に関するフィードバックを即座に提供するツールとプロセス (自動テストや継続的インテグレーションなど) を実装することが重要です。これにより、開発者は問題を迅速に特定して修正し、より俊敏で応答性の高い開発環境を育成できます。迅速なフィードバックループは、現代のソフトウェア開発プラクティスに不可欠であり、品質への積極的なアプローチを促進し、正式な取締役会による厳しい監視の必要性を減らします。
これらのステップは、最新のアジャイルとDevOpsのプラクティスとうまく連携しており、ソフトウェア開発ライフサイクルにおける効率性、コラボレーション、継続的な改善を促進します。
リンクス：
https://cloud.google.com/architecture/devops/devops-process-streamlining-change-approval
</div></details>

### Q.  問題27: 未回答
Google Kubernetes Engine(GKE)で実行され、ブルー / グリーン デプロイ手法を使用するアプリケーションを管理します。Kubernetes マニフェストの抜粋を以下に示します。
---
apiVersion: apps/v1
kind: Deployment
metadata:
name: app-green
labels:
app: my-app
version: green
<other fields snipped>
---
apiVersion: apps/v1
kind: Deployment
metadata:
name: app-blue
labels:
app: my-app
version: blue
<other fields snipped>
---
apiVersion: v1
kind: Service
metadata:
name: app-svc
spec:
selector:
app: my-app
version: green
<other fields snipped>
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: app-ingress
spec:
defaultBackend:
service:
name: app-svc
<other fields snipped>
Deployment app-green は、新しいバージョンのアプリケーションを使用するように更新されました。デプロイ後の監視中に、ユーザー要求の大部分が失敗していることに気付きます。テスト環境では、この動作は確認されませんでした。ユーザーに対するインシデントの影響を軽減し、開発者が問題のトラブルシューティングを行えるようにする必要があります。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. 新しいバージョンのアプリケーションを使用するように Deployment アプリ blue を更新します。
このオプションでは、app-blue デプロイを新しいバージョンに更新する必要がありますが、この状況ではお勧めできません。現在、app-blueは安定版であり、それを新しいバージョンに更新すると(app-greenで問題が発生しています)、安定した環境に同じ問題が発生する可能性があります。ブルー/グリーンデプロイの鍵は、新しいバージョンをテストまたはデプロイしている間、1つの安定したバージョンを維持することであるため、このアクションはその原則と矛盾します。
B. 以前のバージョンのアプリケーションを使用するように Deployment app-green を更新します。
アプリのグリーン展開を以前のバージョンにロールバックすることは、既知の良好な状態に戻すための可能な解決策です。ただし、ブルー/グリーンデプロイのコンテキストでは、このオプションは、単にトラフィックを他のデプロイ(app-blue)に切り替える場合に比べて効率が悪くなります。これには、アプリグリーンの再デプロイが含まれますが、これには時間がかかり、ブルー/グリーンデプロイ方法論に固有の即時切り替え機能を活用しません。
C. サービス app-svc のセレクタを app: my-app に変更します。
このオプションは、トラフィックを app-blue と app-green の両方にルーティングするようにサービスセレクタを変更します (どちらも app: my-app ラベルと一致するため)。ただし、ブルー/グリーンデプロイでは、安定した予測可能な動作を確保するために、一度に 1 つのバージョンにのみトラフィックをルーティングすることが重要です。両方のバージョンにルーティングすると、ユーザーに対する安定した応答と不安定な応答が混在する可能性がありますが、これは望ましくありません。
正解：
D. サービス app-svc のセレクタを app: my-app, version: blue に変更します。
これは、安定バージョンであると想定される app-blue デプロイにトラフィックを効果的に移行するため、推奨されるアプローチです。これにより、ユーザーへの影響を最小限に抑えて問題を即座に軽減し、分析とトラブルシューティングのためにアプリのグリーンデプロイを利用できるようにします。
要約すると、オプション D は最も効果的であり、テストとトラブルシューティングを可能にしながら安定性を維持するというブルー/グリーン デプロイ戦略の原則と一致しています。オプションA、B、Cは、異なる状況で実行可能である可能性はありますが、目前の問題に対処する上で同じレベルの即時性と制御を提供しません。
リンクス：
Kubernetesによるブルーグリーンデプロイメント
</div></details>

### Q.  問題28: 回答
Cloud Run で実行するアプリケーションを開発しており、API キーを使用してサードパーティの API にアクセスする必要がある。API キーを安全に保存して使用したいのは、Google が推奨する方法に従う場合。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. API キーをシークレット マネージャーにシークレット キーとして保存します。秘密鍵を /sys/api_key ディレクトリにマウントし、Cloud Run アプリケーションで鍵を復号します。
オプション A で説明するように、シークレットを環境変数として直接参照できる場合は、アプリケーションで秘密鍵をマウントして復号化する必要はありません。
C. Cloud Key Management Service(Cloud KMS)に API キーをキーとして保存します。Cloud Run アプリケーションでキーを環境変数として参照します。
D. Cloud Key Management Service(Cloud KMS)を使用して API キーを暗号化し、そのキーを環境変数として Cloud Run に渡します。鍵を復号して Cloud Run で使用します。
C. と D.Cloud Key Management Service(Cloud KMS)は暗号鍵の管理に使用されますが、API 鍵やその他の種類のシークレットを保存するための最適なソリューションではありません。Secret Manager は、この目的のために特別に設計されており、API キーの管理とアクセスをよりシンプルで直接的な方法を提供します。
正解：
A. Secret Manager の API キーをシークレットとして保存します。シークレットを Cloud Run アプリケーションの環境変数として参照します。
この方法は、API キーなどの機密データを管理するための Google のベスト プラクティスに沿ったものです。Secret Managerは、APIキーなどの機密情報を安全に保存、管理、アクセスするように設計されています。API キーを Secret Manager に保存することで、安全で一元的な方法で保存されます。
Cloud Run アプリケーションでシークレットを環境変数として参照することは、API キーにアクセスするための安全で便利な方法です。Cloud Run では、環境変数を Secret Manager に保存されているシークレットの値に自動的に設定できるため、アプリケーションはアプリケーションのコードや構成ファイルで公開することなく API キーにアクセスできます。
リンクス：
ライブラリから Cloud Run の Secret Manager 統合への移行
</div></details>

### Q.  問題29: 回答
グローバルな組織に所属し、現在 Compute Engine でモノリシック アプリケーションを実行している。目標は、アプリケーションに最も適したマシン・タイプ、つまり、最小限の複雑さで CPU 使用率を最適化するマシン・タイプを選択することです。十分な情報に基づいた決定を下すには、過去のシステム指標を活用します。目的は、Google が推奨する方法を遵守することです。
あなたは何をするべきか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
B. すべての VM に Ops エージェントを自動的にインストールするエージェント ポリシーを作成します。
Ops Agent は、詳細なシステム メトリックとログを提供しますが、マシン タイプを直接提案することはありません。
このオプションは、マシンタイプを選択するというよりも、監視機能を強化するためのものです。
C. gcloud CLI を使用して、VM のフリートに Ops Agent をインストールします。
オプション B と同様に、これには、強化された監視のために Ops エージェントを手動で設定することが含まれます。
貴重なデータを提供しますが、最適なマシンタイプを決定するには、手動による分析が必要です。
D. VM の Cloud Monitoring ダッシュボードを確認し、CPU 使用率が最も低いマシンタイプを選択します。
この手動のアプローチでは、CPU 使用率メトリックを分析して、効率的なマシンタイプを特定します。
これは、Recommender API からの自動レコメンデーションと比較して時間がかかり、精度が低くなる可能性があります。
正解：
A. Recommender API を使用して、提案されたレコメンデーションを適用します。
Recommender API は、使用状況の履歴データを分析し、最も効率的なマシンの種類と構成を提案します。
意思決定プロセスを自動化し、複雑さを軽減し、リソース最適化に関する Google のベスト プラクティスに合わせます。
このオプションは、データドリブンなインサイトを活用して CPU 使用率を最適化するのに最適です。
リンクス：
https://cloud.google.com/recommender/docs/overview
</div></details>

### Q.  問題30: 未回答
セキュリティのシフトレフトに向けた企業の取り組みの一環として、InfoSec チームはすべてのチームが Google Kubernetes Engine(GKE)クラスタにガードレールを実装することを要求しています。これらのガードレールは、信頼され承認されたイメージのみのデプロイを制限する必要があります。情報セキュリティ チームのセキュリティ目標を達成し、セキュリティのシフトレフトを促進するには、何をすべきでしょうか。
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. アーティファクト・レジストリでコンテナ分析を有効にし、コンテナ・イメージ内の共通脆弱性識別子(CVE)を確認します
このツールは、既知の脆弱性 (CVE) についてコンテナー イメージをスキャンします。イメージ内のセキュリティの問題を特定するために重要ですが、デプロイの制限を直接適用するものではありません。
C. Identity and Access Management(IAM)ポリシーを構成して、GKE クラスタに最小権限モデルを作成します。
最小権限の IAM ポリシーを実装すると、全体的なセキュリティが強化されますが、GKE クラスタにデプロイできるコンテナ イメージは特に管理されません。
D. GKEにFalcoまたはTwistlockをデプロイして、実行中のPodの脆弱性を監視します
FalcoやTwistlockなどのツールは、実行中のポッドの脆弱性や異常な動作を監視します。これらはランタイム セキュリティに役立ちますが、特定のコンテナー イメージのデプロイを制限するものではありません。
正解：
B. バイナリ承認を使用して、CI/CD パイプライン中にイメージを認証する
Binary Authorization は Google Cloud が提供するセキュリティ機能で、信頼できるコンテナ イメージのみが GKE にデプロイされるようにします。CI/CD パイプラインと統合して、デプロイ前にコンテナー イメージを自動的に証明および検証し、信頼できる承認されたイメージのみをデプロイするという目標に沿っています。
このアプローチは、デプロイを信頼できるイメージに制限する要件に直接対処し、開発プロセスの一部としてセキュリティを強化します。
リンクス：
Binary Authorization の概要
</div></details>

### Q.  問題31: 回答
あなたは、マネージド・インスタンス・グループを使用して自動スケーリングする機能を持つ必要がある新しい本番サービスをデプロイするタスクを負っています。このサービスは複数のリージョンにデプロイする必要があり、インスタンスごとに大量のリソースが必要です。
これらの要件を満たすには、展開計画をどのように進める必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Cloud Trace の結果をモニタリングして、最適なサイズを決定します。
Cloud Trace は、パフォーマンス分析とモニタリングのためのツールであり、主にアプリケーションのパフォーマンスのデバッグと最適化に使用されます。
微調整には役立ちますが、初期デプロイ計画、特にリージョン全体のリソースニーズを評価する場合にはあまり直接関係ありません。
B. マネージド・インスタンス・グループの構成でn2-highcpu-96マシン・タイプを使用します。
n2-highcpu-96 マシンタイプは、高い CPU パワーを提供し、計算負荷の高いアプリケーションに適しています。
ただし、特定のマシンの種類を選択する場合は、展開計画の出発点としてではなく、アプリケーションのリソースニーズに関する詳細な知識に基づいて行う必要があります。
C. サービスを複数のリージョンにデプロイし、内部ロード バランサーを使用してトラフィックをルーティングします。
このオプションでは、サービスをさまざまなリージョンに分散させることで、高可用性と回復性に重点を置いています。
複数リージョンのデプロイについては言及していますが、スケーラビリティの側面や、大規模なデプロイに必要なリソースクォータの考慮事項については直接説明していません。
正解：
D. リソース要件が、各リージョンの使用可能なプロジェクト クォータ制限内にあることを検証します。
この手順は、各リージョンのリソース制約内でデプロイが実行可能であることを確認するために重要です。サービスのリソース ニーズがプロジェクトのリージョン クォータ内にあることを確認することで、デプロイの失敗やスケーリングの制限の可能性を回避できます。これは、異なるリージョン間でのスケーリングと効率的なリソース割り当ての基盤を築くため、マルチリージョンのデプロイを目的とした高リソース サービスにとって特に重要です。
割り当て制限に重点を置くことは、Google Cloud でスケーラブルなマルチリージョン サービス デプロイを計画する上で重要な側面です。
リンクス：
ワークスペースの Google Cloud リソース割り当ての確認と設定
</div></details>

### Q.  問題32: 回答
あなたの会社は、CI / CD プロセスのために Google Cloud VM インスタンスでホストされている Jenkins に依存しています。次に、TerraformによるInfrastructure as Codeの自動化を組み込んで機能を強化する必要があります。主な目的は、Terraform 対応の Jenkins インスタンスに Google Cloud リソースを作成するために必要な承認があることを確認することです。
Google が推奨するプラクティスに従って、これを実現するにはどのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. Jenkins VM インスタンスに、適切な Identity and Access Management (IAM) アクセス許可を持つサービス アカウントがアタッチされていることを確認します。
通常、VM インスタンスはアタッチされたサービス アカウントを使用して、他の Google Cloud サービスとやり取りします。ただし、Jenkins で Terraform スクリプトを実行する場合、特に Terraform が複数のプロジェクトまたはサービス間でリソースを管理している場合、Terraform プロセスは本質的に VM のサービス アカウントを使用しない可能性があります。
B. Terraform モジュールを使用して、Secret Manager が資格情報を取得できるようにします。
このアプローチでは、Terraform を使用して Google Cloud の Secret Manager を操作し、認証情報を管理します。安全ですが、より複雑であり、リソースの作成に必要なアクセス許可を Jenkins に付与することに直接関係しない可能性があります。
D. Terraform コマンドを実行する前に、gcloud auth application-default login コマンドを Jenkins のステップとして追加します。
このコマンドはローカルの開発とテストに使用され、サーバーでのユーザー資格情報の管理に関連するセキュリティ リスクがあるため、運用環境では推奨されません。
正解：
C. Terraformインスタンス専用のサービス・アカウントを作成します。秘密鍵の値をダウンロードして、Jenkins サーバーの GOOGLE_CREDENTIALS 環境変数にコピーします。
このアプローチにより、Terraform は Google Cloud リソースとやり取りするために必要な認証情報を利用できます。
サービス・アカウント・キーの使用は、管理およびセキュリティ上の懸念から一般的にはあまり好まれませんが、このコンテキストでは、Terraformが必要なアクセス権を確実に取得するための直接的で明確な方法を提供します。
サービス アカウント キーを安全に管理し、キーのストレージとアクセス制御に関するベスト プラクティスに従うことが重要です。
リンクス：
https://registry.terraform.io/providers/hashicorp/google/2.20.3/docs/resources/google_service_account_key
</div></details>

### Q.  問題33: 回答
最小権限の原則に従い、Google が推奨するプラクティスに従うには、会社のセキュリティ チームに、指定された「_Required」バケット内のデータ アクセス監査ログへの読み取り専用アクセス権を付与する必要があります。
必要なアクセス許可を付与するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. roles/logging.viewer ロールをセキュリティ チームの各メンバーに割り当てます。
このロールは、データアクセス監査ログ以外のログを含む可能性のあるすべてのログエントリを表示するためのアクセス権を付与します。
この役割を各チーム メンバーに個別に割り当てると、拡張性が低く、特にセキュリティ チームの規模が頻繁に変わる場合は、管理の面で効率的ではありません。
データ アクセス監査ログに機密データが含まれている場合、最小特権の原則に厳密には従いません。
B. roles/logging.viewer ロールを、すべてのセキュリティ チーム メンバーを含むグループに割り当てます。
このロールは、データ アクセス監査ログのみが必要な場合でも、必要以上に広範なアクセス権を付与します。
グループへのロールの割り当ては、アクセス管理が簡素化されるため、個々の割り当てよりも効率的で管理しやすくなります。
ただし、機密性の高い監査ログを含む「_Required」バケットの特定のケースでは、最小権限の原則に完全に準拠していない可能性があります。
C. roles/logging.privateLogViewer ロールをセキュリティ チームの各メンバーに割り当てます。
このロールはより具体的で、プライベート ログへのアクセスを許可し、最小特権の原則により整合しています。
オプション A と同様に、各個人に役割を割り当てることは、特にチームが変更する場合、効率が悪く、管理が難しくなります。
正解：
D. roles/logging.privateLogViewer ロールを、すべてのセキュリティ チーム メンバーを含むグループに割り当てます。
このロールは、プライベートログを表示するために設計されており、データアクセス監査ログにアクセスする必要があるセキュリティチームに適しています。
このロールをグループに割り当てることは、権限管理を一元化するため、最も管理しやすくスケーラブルなオプションです。
必要なログのみへのアクセスを提供することで、最小特権の原則に厳密に準拠し、アクセス権を持つユーザーの監査を容易にします。
リンクス：
監査関連の職務のIAMロール
</div></details>

### Q.  問題34: 回答
本番環境の Google Kubernetes Engine(GKE)で実行されているNode.jsアプリケーションを管理しています。このアプリケーションは、さまざまな依存アプリケーションに対して HTTP 要求を頻繁に行います。これらの依存アプリケーション間のパフォーマンス問題の潜在的な原因をプロアクティブに特定するには、どのような手順を実行する必要がありますか?
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
A. すべてのアプリケーションを Cloud Profiler でインストゥルメント化します。
これは、継続的なCPUとメモリのプロファイリングのための強力なツールであり、アプリケーションのパフォーマンスを最適化するのに役立ちます。ただし、マイクロサービス全体の HTTP 要求のパフォーマンスを追跡するのではなく、リソース使用状況の分析に重点を置いています。
C. Cloud Debugger を使用して、各アプリケーション内のロジックの実行を確認し、すべてのアプリケーションをインストゥルメント化します。
パフォーマンスに影響を与えずに本番環境でアプリケーションをデバッグすることを目的としています。これにより、特定のポイントでコードを検査できます。ロジックエラーの診断やアプリケーションの状態の検査には役立ちますが、HTTPリクエストのパフォーマンスを直接測定または分析するものではありません。
D. 依存アプリケーションへの HTTP 要求と応答時間をログに記録するようにNode.jsアプリケーションを変更します。Cloud Logging を使用して、パフォーマンスの低い依存アプリケーションを見つけます。
HTTP リクエストとレスポンス時間をログに記録するようにアプリケーションを変更し、Cloud Logging を使用してこれらのログを分析します。このアプローチでは応答の遅さを特定できますが、特に複雑なマイクロサービス環境では、手間がかかり、Cloud Trace の自動化された包括的な分析機能が不足しています。
正解：
B. すべてのアプリケーションを Cloud Trace でインストゥルメント化し、サービス間 HTTP リクエストを確認します。
HTTPリクエストのレイテンシーを測定し、分散システム全体のパスを追跡するために特別に設計されています。これにより、要求がアプリケーション スタックをどのように流れるかについて詳細な分析情報が提供されるため、サービス間通信のパフォーマンスのボトルネックを特定するのに非常に効果的です。
リンクス：
Cloud Trace の概要
</div></details>

### Q.  問題35: 未回答
組織内のDevOpsプロジェクトのリーダーとして、サービスインフラストラクチャの管理とインシデント対応を担当するDevOpsチームを監督します。同時に、ソフトウェア開発チームは、コードの記述、提出、およびレビューを担当します。現在、どちらのチームも公開されているサービス レベル目標 (SLO) を確立していません。目的は、サービスの新しい共同所有モデルを作成することであり、この新しいモデル内で各チームに割り当てる特定の責任を決定する必要があります。
ある。
B.
C.
D.
1. 
2. 
3. 
4. 
<details><div>
    答え：

不正解:
ある。 このモデルでは、DevOpsチームは、インフラストラクチャ管理、オンコール業務、コードレビューの実行など、大きな責任を負っています。ソフトウェア開発チームは、主にコードを提出し、DevOpsチームが満たすべきSLOを定義しています。これにより、DevOpsチームに過度の負担がかかる可能性があり、不均衡が生じる可能性があります。
イ.オプションAと似ていますが、コードレビューが共同責任となります。DevOps チームは引き続きインフラストラクチャを管理し、ソフトウェア開発チームはインシデントのオンコールを担当します。ただし、SLO を公開する責任は明確に定義されていません。
D.このオプションは C と似ていますが、コード レビューを実行するユーザーを指定しません。これは、両方のチームが SLO とインシデント対応に関与することを前提としています。このモデルは、コードレビューが共有責任であると暗黙のうちに理解されている場合に機能します。
正解：
ウ. このオプションは、責任をより均等に分散しているようです。DevOps チームはインフラストラクチャを管理し、両方のチームがオンコールの職務とコード レビューの責任を共有し、どちらも SLO の採用と公開に関与します。これにより、チームワークが促進され、サービスに対する理解が共有される可能性があります。
オプションCは、最もバランスの取れた、明確な責任分担により、コラボレーション環境が促進されているようです。これにより、両方のチームが、共同所有モデルの成功の鍵となるオンコール業務やSLOなど、サービスの重要な側面に従事できるようになります。このモデルは、チーム間での所有権の共有と理解を促進するため、サービスの信頼性とパフォーマンスの向上につながる可能性があります。
リンクス：
ソフトウェア開発チームのアウトソーシングにおける役割と責任
https://sre.google/workbook/on-call/
</div></details>
