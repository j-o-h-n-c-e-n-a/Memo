## 1
### Q. 問題1: 不正解
あなたの会社では一元化された分析プラットフォームとしてBigQueryを使用しています。毎日新しいデータが読み込まれ、ETLパイプラインが元のデータを修正し、最終的なユーザーに提供する準備をしています。このETLパイプラインは定期的に変更されるため、エラーが発生することがありますが、そのエラーが2週間後に発見されることもあります。これらのエラーから回復する方法を提供する必要があり、バックアップはストレージコストを最適化する必要があります。
BigQueryでのデータ整理やバックアップの保存はどのようにすればよいですか？
1. 
2. データを月ごとに別々のテーブルに整理し、データをエクスポートして圧縮し、Cloud Storageに保存する
3. 
4. 
<details><div>
    答え：2
説明
BigQueryでのコスト最適なデータ整理やバックアップの保存方法を選択する必要があります。
BigQueryにおいてデータをアーカイブしながら破損を復元するためのプラクティスはたとえば以下のような方法があります。
- 破損の検出が 7 日以内の場合、過去の時点のテーブルに対してクエリを行い、スナップショット デコレータを利用して、破損する前のテーブルを復元します。
- BigQuery からデータをエクスポートし、エクスポートしたデータを含む（ただし破損したデータは含まない）新しいテーブルを作成します。
- 特定の期間ごとに、データを異なるテーブルに格納します。この方法では、データセット全体ではなく、データの一部のみを新しいテーブルに復元するだけで済みます。
- 特定の期間にデータセットのコピーを作成します。データ破損のイベントが、ポイントインタイム クエリでキャプチャできる期間（7 日以上前など）を超えた場合に、そのコピーを使用できます。また、データセットをあるリージョンから別のリージョンへコピーして、リージョンのエラーの際にデータの可用性を確保することもできます。
- 元のデータを Cloud Storage に保存します。これにより、新しいテーブルを作成して、破損していないデータを再読み込みできます。そこから、新しいテーブルを指すようにアプリケーションを調整できます。
今回の場合、2週間前の破損も復元しなければならないため、スナップショットデコレーターの使用は不適切です。
したがって、正解は「データを月ごとに別々のテーブルに整理し、データをエクスポートして圧縮し、Cloud Storageに保存する」です。
参照：
https://cloud.google.com/architecture/dr-scenarios-for-data#managed-database-services-on-gcp
</div></details>

### Q. 問題2: 正解
2 PBの履歴データをオンプレミスのストレージ・アプライアンスからCloud Storageに6ヶ月以内に移行する必要があります。一方で、アウトバウンド・ネットワークの容量は20 Mb/secに制限されています。
このデータをCloud Storageに移行するにはどうすればよいですか？
1. Transfer Applianceを使用してデータをCloud Storageにコピーする
2. 
3. 
4. 
<details><div>
    答え：1
説明
2PBのデータを20Mbpsの帯域幅で転送すると、数ヶ月以上の時間が必要になるため、別の方法でデータを転送する必要があります。
Transfer Appliance は大容量のストレージ デバイスであり、データを Google アップロード施設にデータの転送と安全な配送をし、そこからデータは Cloud Storage にアップロードされます。
迅速かつ効率的にデータを移動できるように、Transfer Appliance には次のようなパフォーマンス機能があります。
- すべての SSD ドライブ：ハードディスク ドライブよりも高い信頼性があり、移行をスムーズにします。
- 複数のネットワーク接続オプション ：10 Gbps の RJ45 インターフェースまたは 40 Gbps QSFP+ インターフェースを使用して、デバイスから Transfer Appliance にデータを素早く移動できます。
- 複数の Transfer Appliance でのスケーラビリティ：複数の Transfer Appliance をオーダーして転送速度を上げることで、転送をスケーリングできます。
- グローバル分散処理：Google との間の配送時間を短縮することで、Cloud Storage へのデータ転送を迅速化できます。
- 追加のソフトウェアがない：すでに保有している一般的ソフトウェアを使用して、Transfer Appliance に公開された NFS 共有をワークステーションにマウントすることで、Transfer Appliance に直接コピーします。
したがって、正解は「Transfer Applianceを使用してデータをCloud Storageにコピーする」です。
参照：
https://cloud.google.com/storage-transfer/docs/overview
</div></details>

### Q. 問題3: 正解
ある日に雨が降るかどうかを予測するモデルを構築しています。何千もの入力特徴がありますが、モデルの精度への影響を最小限に抑えながら、いくつかの特徴を取り除くことで、学習速度を向上させることができるかどうかを調べています。
要件を達成するためにするべきことは何ですか？
1. 
2. 共依存性の高い特徴を1つの代表的な特徴にまとめる
3. 
4. 
<details><div>
    答え：2
説明
数千の特徴量に対して次元圧縮を行うことが有効です。
これによって、学習速度の向上だけでなく、過学習の防止が実現できます。
主成分分析（Principal Component Analysis: PCA）は、教師なしの機械学習アルゴリズムで、データセット内の次元（特徴の数）を減らしながらも、できるだけ多くの情報を保持しようとするものです。
これは、元の特徴の合成物である、成分と呼ばれる新しい特徴のセットを見つけることによって行われます。
このアルゴリズムを使用すると、特徴量の中で相関が高いもの同士が新たな次元にまとめられます。
特徴量が多くあり、特徴量を減らしたい場合や、特徴量同士の相関があることでモデルの精度に影響が出ている場合などで使用されます。
したがって、正解は「共依存性の高い特徴を1つの代表的な特徴にまとめる」です。
参照：
https://aiacademy.jp/media/?p=254
https://qiita.com/shuva/items/bcf700bd32ae2bbb63c7
</div></details>

### Q. 問題4: 不正解
Google BigQueryにソーシャルメディアの投稿を毎分10,000件のペースでほぼリアルタイムに保存・分析する必要があります。当初は、個々の投稿に対してストリーミングインサートを使用するようにアプリケーションを設計していました。このアプリケーションでは、ストリーミングインサートの直後にデータの集約も行います。しかし後に、ストリーミングインサート後のクエリは強い一貫性を示さず、クエリからのレポートはストリームの中のデータを見逃す可能性があることがわかりました。
アプリケーションの設計をどのように調整すればよいですか？
1. 
2. 
3. 
4. ストリーミングインサート後のデータ可用性の平均レイテンシーを推定し、常に2倍の時間を待ってからクエリを実行する
<details><div>
    答え：4
説明
BigQueryは、マネージドストレージにデータを取り込む方法として、ロードジョブや外部ソースに対するクエリなど、いくつかの方法をサポートしています。
BigQueryは、ストリーミングと呼ばれる取り込み方法もサポートしており、より自由で継続的な取り込みスタイルを必要とするユーザーのニーズに応えることを目的としています。
ストリーミングによる個々の挿入の高いスループットをサポートするために、BigQueryのストリーミングシステムはメタデータ情報、特にテーブルの存在とスキーマ情報を積極的にキャッシュしています。
実際には、いくつかのインタラクションは最終的に一貫した動作を示すことを意味する。
たとえば、テーブルが存在する前にデータのストリーミングを試み、テーブルを作成してストリーミングを再開した場合、テーブル作成後しばらくの間は、ストリーミングシステムが「テーブルが見つかりません」というエラーでその後の挿入を拒否する可能性があります。
同様に、テーブルのスキーマを拡張するような変更も、ストリーミング挿入が新しいカラムを参照できるようになるまで、ある程度の期間を必要とする場合があります。
したがって、クエリ実行はストリーミングインサートの後、レイテンシーを考慮してそれよりも長い時間待った後に行う必要があります。
したがって、正解は「ストリーミングインサート後のデータ可用性の平均レイテンシーを推定し、常に2倍の時間を待ってからクエリを実行する」です。
参照：
https://cloud.google.com/blog/products/bigquery/life-of-a-bigquery-streaming-insert
</div></details>

### Q. 問題5: 不正解
あなたは3つのデータ処理ジョブを開発しました。1つは、Cloud Dataflowパイプラインを実行し、クラウドストレージにアップロードされたデータを変換し、結果をBigQueryに書き込みます。2つ目は、オンプレミスサーバーからデータを取り込み、クラウドストレージにアップロードします。3つ目は、サードパーティのデータプロバイダーから情報を取得し、Cloud Storageに情報をアップロードするCloud Dataflowパイプラインです。これら3つのワークフローの実行をスケジュールして監視し、必要なときに手動で実行できるようにする必要があります。
要件を達成するためにするべきことは何ですか？
1. Cloud ComposerでDirect Acyclic Graphを作成し、ジョブのスケジュールと監視を行う
2. 
3. 
4. 
<details><div>
    答え：1
説明
複数のパイプラインの実行を自動化するマネージドサービスを選択する必要があります。
Cloud ComposerはApache Airflow で構築された、フルマネージドのワークフロー オーケストレーション サービスです。
DAG（Direct Acyclic Graph）と呼ばれるデータ形式でジョブ同士の関係性を定義し、実行します。
Cloud Composer はフルマネージド サービスであり、Apache Airflow は互換性に優れているため、リソースのプロビジョニングに気をとられず、ワークフローの作成、スケジューリング、モニタリングに専念できます。
BigQuery、Dataflow、Dataproc、Datastore、Cloud Storage、Pub/Sub、AI Platform などの Google Cloud プロダクトとのエンドツーエンドの統合により、ユーザーはパイプラインを自由かつ完全にオーケストレートできます。
これによって、複数の依存関係のあるジョブを、少ないオーバーヘッドで実行することが可能になります。
したがって、正解は「Cloud ComposerでDirect Acyclic Graphを作成し、ジョブのスケジュールと監視を行う」です。
参照：
https://cloud.google.com/composer/docs/how-to/using/writing-dags
https://cloud.google.com/composer/?hl=en
</div></details>

### Q. 問題6: 正解
あなたは、Google Homeなどの様々な家庭用アシスタントとオンライン販売機能を統合したいと考えている小売業者です。顧客の音声コマンドを解釈して、バックエンドシステムに注文を出す必要があります。
どのソリューションを選ぶべきでしょうか？
1. 
2. 
3. Dialogflow Enterprise Edition
4. 
<details><div>
    答え：3
説明
今回の要件では、会話型アプリケーションが必要です。
Dialogflowを使用すると、カスタマーサービス、コマース、生産性、IoTデバイスなどのための会話型アプリを作成することができます。
Dialogflow Enterprise Editionでは、Dialogflowのすべての利点を拡張し、大規模なビジネスのニーズに対応するため、より高い柔軟性とサポートを提供します。
さらに、Dialogflowにおける音声統合を発表し、開発者が音声ベースのアプリケーションを構築できるようにします。
以下は、Dialogflowが提供する機能の詳細です。
- 機械学習による会話型インタラクション：
Dialogflowは自然言語処理により、会話体験をより速く構築し、より速くイテレーションすることができます。
ユーザーの発言例をいくつか提示すると、Dialogflowが独自のモデルを構築し、どのようなアクションを起こすべきか、どのようなデータを抽出すべきかを学習し、ユーザーに最も適切で的確な応答を提供することができます。
- 一度構築すれば、どこにでもデプロイ可能：
Dialogflowで会話型アプリを構築し、ウェブサイト、アプリ、Google Assistantやその他の人気メッセージングサービスを含む32の異なるプラットフォームで展開できます。
また、Dialogflowは多言語や多言語体験をサポートしているので、世界中のユーザーにアプローチすることができます。
- 高度なフルフィルメントオプション：
フルフィルメントとは、ピザの注文を処理したり、ユーザーの質問に対して正しい答えを導き出すなど、ユーザーの発言に対応したアクションを定義することです。
Dialogflowでは、パブリッククラウドやオンプレミスでホストされているかにかかわらず、フルフィルメント用の任意のWebhookに接続することができます。
Dialogflowの統合コードエディタにより、Dialogflowのコンソール内で直接これらのアクションをコーディング、テスト、実装することができます。
- 音声認識によるボイスコントロール ：
Dialogflowは会話型アプリが音声コマンドや音声会話に応答することを可能にします。
音声認識と自然言語理解を組み合わせ、1つのAPIコールで利用できます。
したがって、正解は「Dialogflow Enterprise Edition」です。
参照：
https://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps
https://dialogflow.com/v2-faq
</div></details>

### Q. 問題7: 正解
Google Cloud Platform上で動作するPOSアプリケーションで、決済処理を行いたいと考えています。ユーザー数は爆発的に増える可能性がありますが、インフラのスケーリングは管理したくありません。
どのGoogleデータベースサービスを使うべきでしょうか？
1. 
2. 
3. 
4. Cloud Datastore
<details><div>
    答え：4
説明
Datastore は、自動スケーリングと高性能を実現し、アプリケーション開発を簡素化するように構築された NoSQL ドキュメント データベースです。Datastore の特長は次のとおりです。
- アトミック トランザクション：
Datastore は、結果が「すべて成功」と「何も起こらない」のどちらかになる一連のオペレーションを実行できます。
- 可用性の高い読み取り / 書き込み処理：
Datastore は、障害発生点からの影響を最小限に抑える冗長性を備えた Google データセンターで実行されています。
- 優れたスケーラビリティと高いパフォーマンス：
Datastore は分散アーキテクチャを使用して自動的にスケーリングを管理します。インデックスとクエリの制約を組み合わせることで、データセットのサイズではなく結果セットのサイズに応じてクエリをスケールします。
- 柔軟なストレージとデータのクエリ：Datastore はオブジェクト指向の言語やスクリプト言語に自然に対応させることが可能で、複数のクライアントを通じてアプリケーションに公開されています。また SQL ライクなクエリ言語も提供しています。
- 強整合性と結果整合性のバランス：
Datastore ではキーによるエンティティの検索が可能なため、祖先クエリでは常に強整合性が保たれたデータを受け取ります。また、その他のすべてのクエリでは結果整合性が保たれます。このような整合性モデルにより、大量のデータとユーザーを処理しながら優れたユーザー エクスペリエンスを提供するアプリケーションを実現できます。
Datastore は、大規模な構造化データに対して可用性の高いアクセスを必要とするアプリケーションに最適です。Datastore は、次のようなタイプのすべてのデータを保存、クエリする目的で使用できます。
- 小売店向けにリアルタイムな在庫と商品の詳細を提供する商品カタログ
- ユーザーの過去の行動と好みに応じてカスタマイズされたエクスペリエンスを提供するユーザープロフィール
- ある銀行口座から別の口座への送金など、ACID プロパティに基づくトランザクション
したがって、正解は「Cloud Datastore」です。
参照：
https://cloud.google.com/datastore/docs/concepts/overview
https://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for
</div></details>

### Q. 問題8: 不正解
ある組織では、ユーザーレベルのデータが格納されたテーブルを含む Google BigQuery データセットを管理しています。このデータの集計を他のGoogle Cloudプロジェクトに公開したいと考えていますが、ユーザーレベルのデータへのアクセスは制御され続ける必要があります。さらに、全体のストレージコストを最小限に抑える必要があります。
要件を達成するためにするべきことは何ですか？
1. 集計結果を提供するオーソライズドビューを作成し、共有する
2. 
3. 
4. 
<details><div>
    答え：1
説明
BigQueryにおいてユーザーレベルでのアクセスコントロールを容易に実現する方法を選択する必要があります。
データセットに表示アクセス権を設定する場合、BigQuery ではオーソライズドビューを作成する。
オーソライズドビューを使用すると、元のテーブルへのアクセス権がないユーザーでも、クエリの結果を特定のユーザーやグループと共有できます。
ビューの SQL クエリを使用して、ユーザーがクエリを実行できる列（フィールド）を制限することもできます。
したがって、正解は「集計結果を提供するオーソライズドビューを作成し、共有する」です。
参照：
https://cloud.google.com/bigquery/docs/share-access-views
https://cloud.google.com/bigquery/docs/authorized-views
</div></details>

### Q. 問題9: 正解
あなたは、Cloud Pub/Subから来るデータをBigQueryの静的参照データとしてエンリッチするためのApache Beamパイプラインを設計しています。参照データは、ワーカー1台のメモリに収まる程度のサイズです。また、このパイプラインは、エンリッチされた結果を分析するためにBigQueryに書き込む必要があります。
このパイプラインはどのジョブタイプとトランスフォームを使用すべきでしょうか？
1. 
2. 
3. ストリーミングジョブ、Pub/SubIO、BigQueryIO、サイドインプット
4. 
<details><div>
    答え：3
説明
Cloud Pub/Sub、BigQueryを用いたパイプラインに適したジョブとトランスフォームを選択する必要があります。
Cloud Pub/Subを用いる場合は、パイプラインはリアルタイムで流れてくるデータを処理するためにストリーミングジョブを選択する必要があります。
これによって、メッセージの重複排除、1 回限りの処理、タイムスタンプ付きイベントからのデータ ウォーターマークの生成により、Pub/Sub のスケーラブルな「最低 1 回」配信モデルを補完します。
また、Pub/Sub IO、BigQueryIOを使用することで、Pub/Sub サブスクリプションから JSON 形式のメッセージを読み取り、それらを BigQuery テーブルに書き込むストリーミング パイプラインを構築することができます。
このテンプレートは、Cloud Pub/Sub データを BigQuery に移動する簡単なソリューションとして使用できます。
このテンプレートは Pub/Sub から JSON 形式のメッセージを読み込み、BigQuery 要素に変換します。
Apache Beamでは、データ分析のためのエンリッチメントを行う際には、サイドインプットパターンが推奨されています。
これによって、さらなる分析に役立つ可能性のある追加情報を指定してデータを拡充することが可能になります。
したがって、正解は「ストリーミングジョブ、Pub/SubIO、BigQueryIO、サイドインプット」です。
参照：
https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub
https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#cloudpubsubsubscriptiontobigquery
https://beam.apache.org/documentation/patterns/side-inputs/
</div></details>

### Q. 問題10: 正解
毎日何十万ものソーシャルメディアの投稿を、最小のコストと少ないステップで分析したいと考えています。
以下のような要件があります。
- 1日1回、投稿をバッチロードして、Cloud Natural Language APIで実行します。
- 投稿からトピックとセンチメントを抽出します。
- アーカイブや再処理のために、生の投稿を保存する必要があります。
- ダッシュボードを作成し、組織内外の人々と共有します。
分析を行うためにAPIから抽出したデータと、ソーシャルメディアの生の投稿の両方を保存して、履歴を保存する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 生のソーシャルメディアの投稿をCloud Storageに保存し、APIから抽出したデータをBigQueryに書き込む
4. 
<details><div>
    答え：3
説明
生データの保存先と、抽出したデータの保存先を適切に選択する必要があります。
生データの保存はCloud Storageが最適です。
生の投稿データは構造化されていない場合も多く、またソーシャルメディアの仕様の変更によっては構造が変化する可能性があります。
オブジェクトストレージを使用することで、これらの問題に対応しつつ低コストでデータを保存することができます。
抽出されたデータの保存先はBigQueryが最適です。
これは、今回の要件が、抽出されたデータをもとにダッシュボードを作成するためです。
BigQueryはBIツールとの連携が容易におこうなうことができるため、少ないステップで分析ソリューションを実装することが可能です。
したがって、正解は「生のソーシャルメディアの投稿をCloud Storageに保存し、APIから抽出したデータをBigQueryに書き込む」です。
参照：
https://cloud.google.com/storage
https://cloud.google.com/bigquery/docs/visualize-data-studio
</div></details>

### Q. 問題11: 正解
Cloud Dataflowで、Cloud Pub/Subトピックからメッセージを受信し、その結果をEUのBigQueryデータセットに書き込むパイプラインを実行しています。
現在、パイプラインはeurope-west4に配置されており、インスタンスタイプn1-standard-1のワーカーを最大で3つ保持しています。あなたは、3つのワーカーのCPU使用率が最大となるピーク時には、パイプラインがタイムリーにレコードを処理することができなくなることに気付きました。
パイプラインのパフォーマンスを向上させるために、どのアクションを取ることができますか？（2つ選択）
1. 最大ワーカー数の増加
2. Cloud Dataflowのワーカーに、より大きなインスタンスタイプを使用する
3. 
4. 
5. 
<details><div>
    答え：1,2
説明
現在のコンピューティングリソースではリアルタイムの処理ができないため、コンピューティングリソースの増強が必要です。
Cloud Dataflowでは、ワーカーという単位でデータの処理を行います。
このワーカーは、スケールアップとスケールアウトに対応しているため、ワーカーインスタンスのサイズを大きくすること、ワーカー数を増やすことで、処理をスムーズに行うことができます。
したがって、正解は以下の通りです。
- 最大ワーカー数の増加
- Cloud Dataflowのワーカーに、より大きなインスタンスタイプを使用する
参照：
https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#autoscaling
</div></details>

### Q. 問題12: 不正解
Google Cloud Monitoring Logging を使用して Google BigQuery の使用状況を監視したいと考えています。挿入ジョブを使用して特定のテーブルに新しいデータが追加されたときに、監視ツールにインスタント通知を送信する必要がありますが、他のテーブルの通知は受信したくありません。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. Cloud Monitoring API を使用して、Pub/Sub にエクスポートするための高度なログフィルタを備えたプロジェクトシンクを作成し、監視ツールからトピックをサブスクライブする
<details><div>
    答え：4
説明
特定のテーブルで通知を受けることが重要な要件です。
これは、高度なログフィルターを使用してテーブルログのみをフィルタリングし、通知のためにCloud Pub/Subにプロジェクトシンクを作成することで実現できます。
高度なログクエリとは、プロジェクトのログエントリ全体のうちの特定部分を指定するブール式のことです。これは、次のことに使用できます。
- 特定のログやログサービスからログエントリを選択する。
- 特定の時間範囲内のログエントリを選択する。
- メタデータやユーザー定義フィールドに対する条件を満たすログエントリを選択する。
- すべてのログエントリのサンプリング率を決める。
したがって、正解は「Cloud Monitoring API を使用して、Pub/Sub にエクスポートするための高度なログフィルタを備えたプロジェクトシンクを作成し、監視ツールからトピックをサブスクライブする」です。
参照：
https://cloud.google.com/logging/docs/routing/overview
https://cloud.google.com/logging/docs/view/advanced-queries
</div></details>

### Q. 問題13: 正解
お客様は物流会社を経営しており、車両ベースのセンサーのイベント配信の信頼性を向上させたいと考えています。しかし、イベント収集インフラからイベント処理インフラへの接続を提供する専用線は、信頼性が低く、予測不可能な遅延が発生します。この問題を、最もコスト効率の良い方法で解決したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. データ収集デバイスがCloud Pub/Subにデータをパブリッシュする
3. 
4. 
<details><div>
    答え：2
説明
コスト効率を意識しながら、リアルタイムで生成されるイベントの処理インフラを構築する必要があります。
Cloud Pub/Subを使用するとサービスが 100 ミリ秒程度のレイテンシで非同期にデータを転送することができます。
Pub/Sub は、データを取り込んで配布するためのストリーミング分析とデータ統合パイプラインに使用されます。
これは、サービスの統合を目的としたメッセージング指向のミドルウェア、または、タスクを同時に読み込むキューとして使用されます。
イベントは、未加工か処理された状態かを問わず、チームや組織の複数のアプリケーションでリアルタイム処理のために使用できます。
これにより、「エンタープライズ イベントバス」とイベント ドリブンなアプリケーション設計パターンがサポートされます。Pub/Sub を使用すると、イベントを Pub/Sub にエクスポートする多くの Google システムと連携できます。
したがって、正解は「データ収集デバイスがCloud Pub/Subにデータをパブリッシュする」です。
参照：
https://cloud.google.com/pubsub/docs/overview
https://cloud.google.com/interconnect/docs/concepts/overview
</div></details>

### Q. 問題14: 不正解
ある会社では、顧客管理や注文管理を行うデータベースは、しばしば高負荷にさらされています。そのため、業務に支障をきたさないようにデータベースに対して分析クエリを実行することが難しくなっています。データベースは MySQL クラスタに入っており、毎晩 mysqldump でバックアップを取っています。あなたは業務への影響を最小限に抑えて分析を行いたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. バックアップを Google Cloud SQL にマウントし、Google Cloud Dataproc を使用してデータを処理する
<details><div>
    答え：4
説明
今回は業務への影響を最小限に抑えることを優先しながらデータを処理する必要があります。
状況を整理すると、毎晩バックアップを取っているので、このバックアップを活用することで、業務への影響を最小化しながらデータ分析を行うことができます。
Dataprocはバックアップのmysqldumpを利用してデータ分析をおこなうために最適なサービスです。
Dataproc は、Apache Spark、Apache Flink、Presto をはじめ、30 以上のオープンソース ツールやフレームワークを実行するための、フルマネージドでスケーラビリティの高いサービスです。
Dataproc を使用すれば、データレイクのモダナイゼーション、ETL、安全なデータ サイエンスを、Google Cloud と完全に統合された極めてスケーラブルな環境で、低コストで実現できます。
ダンプしたファイルをCloud Storageに保存することで、Dataprocにインポートをすることが可能になります。
したがって、正解は「バックアップを Google Cloud SQL にマウントし、Google Cloud Dataproc を使用してデータを処理する」です。
参照：
https://cloud.google.com/blog/products/data-analytics/genomics-data-analytics-with-cloud-pt2
</div></details>

### Q. 問題15: 不正解
何百万台ものコンピュータのCPUとメモリの使用量を時系列で保存するためのデータベースを選択する必要があります。このデータは1秒間隔のサンプルで保存する必要があります。アナリストは、このデータベースに対してリアルタイムでアドホックな分析を行います。クエリを実行するたびに課金されるのは避けたいと考えています。また、データセットを将来的に拡張できるようなスキーマデザインにしたいと考えています。
どのデータベースとデータモデルを選ぶべきでしょうか？
1. 
2. 
3. Cloud Bigtableにナロー・テーブルを作成し、Computer Engineのコンピュータ識別子と各秒のサンプル時間を組み合わせた行キーを設定する
4. 
<details><div>
    答え：3
説明
アドホック分析が行えるデータベースを選択し、拡張性に優れたデータスキーマの設計を行う必要があります。
Cloud Bigtableは、最大 99.999% の可用性で大規模な分析ワークロードにも運用ワークロードにも対応できる、フルマネージドでスケーラブルな NoSQL データベース サービスです。
Cloud Bittableでは、センサーデータなどの時系列データの保存にも適しています。
行数が多く（背が高く）カラム数の少ない（幅の狭い）テーブルでは、1行あたりのイベント数が少なく、1つのイベントだけということもあり得ますが、背が低く幅の広いテーブルでは、1行あたりのイベント数が多くなります。
時系列データには、一般的に背の高いテーブルと幅の狭いテーブルを使用する必要があります。
これは、1 行に 1 つのイベントを格納することで、データに対するクエリの実行が容易になるためです。
行ごとに多くのイベントを格納すると、行のサイズの合計が推奨される最大値を超える可能性が高くなります。
したがって、正解は「Cloud Bigtableにナロー・テーブルを作成し、Computer Engineのコンピュータ識別子と各秒のサンプル時間を組み合わせた行キーを設定する」です。
参照：
https://cloud.google.com/bigtable/docs/schema-design-time-series#patterns_for_row_key_design
</div></details>

### Q. 問題16: 正解
あなたは、配送ラベルの読み取りに携帯型スキャナを使用している運送会社で働いています。この会社には厳格なデータ・プライバシー基準があり、現在スキャナーは受信者の個人識別情報（PII）のみを分析システムに送信するようになっていますが、これはユーザーのプライバシー規則に違反しています。あなたは、分析システムへのPIIの漏洩を防ぐために、クラウドネイティブなマネージドサービスを使って、スケーラブルなソリューションを迅速に構築したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. トピックを読み取り、Cloud Data Loss Prevention API へのコールを行う Cloud Function を構築する。タグ付けと信頼度を使用して、レビュー用のバケットにデータを渡すか隔離する
<details><div>
    答え：4
説明
PIIの漏洩を防止するためのサービスを選択する必要があります。
Cloud Data Loss Preventionは、事実上どこからでもデータをスキャン、発見、分類、レポートする力を提供します。
Cloud DLPは、Cloud Storage、BigQuery、Cloud Datastoreの機密データのスキャンと分類をネイティブにサポートし、ストリーミングコンテンツAPIにより、追加のデータソース、カスタムワークロード、アプリケーションへのサポートを可能にします。
クラウドデータロスプリベンション（DLP）は、データセキュリティとプライバシーのレイヤーを追加してデータワークロードに組み込むことにより、機密データを保護することができます。
また、Cloud StorageやBigQueryなどのストレージリポジトリにあるデータの大規模な検査、発見、分類のためのネイティブサービスも提供します。
当初はAPIとしてリリースされたCloud DLPですが、現在はユーザーインターフェース（UI）が含まれており、これらの機能をセキュリティ、プライバシー、コンプライアンスチームに拡張するのに役立っています。
Google Cloud Consoleで一般的に利用できるようになったCloud DLP UIを使用すると、ジョブ、ジョブトリガー、設定テンプレートを作成することで、わずか数クリックで機密データを発見、検査、分類することができます。
また、Cloud DLPでは、スキャンしたバイト数に基づくストレージ検査の価格設定が簡素化されており、コストをより予測しやすくなっています。
したがって、正解は「トピックを読み取り、Cloud Data Loss Prevention API へのコールを行う Cloud Function を構築する。タグ付けと信頼度を使用して、レビュー用のバケットにデータを渡すか隔離する」です。
参照：
https://cloud.google.com/dlp/
https://cloud.google.com/blog/products/identity-security/take-charge-of-your-data-scan-for-sensitive-data-in-just-a-few-clicks
</div></details>

### Q. 問題17: 不正解
あなたのソフトウェアは、すべてのメッセージングにシンプルなJSONフォーマットを使用します。これらのメッセージは、Google Cloud Pub/Subに公開され、Google Cloud Dataflowで処理されます。
Dataflowで処理し、CFOのためのリアルタイム・ダッシュボードを作成します。テスト中に、ダッシュボードにいくつかのメッセージが欠けていることに気づきました。ログを確認すると、すべてのメッセージがCloud Pub/Subに正常にパブリッシュされています。
問題を解決するために次に何をすべきでしょうか？
1. 
2. 固定のデータセットをCloud Dataflowパイプラインで実行し、その出力を分析する
3. 
4. 
<details><div>
    答え：2
説明
全てのデータが正常にパブリッシュされている一方で、メッセージの欠落が発生しているため、Dataflowパイプラインのトラブルシューティングが必要です。
Dataflowでは、Apache Beam SDK の組み込みロギング インフラストラクチャを使用して、パイプラインの実行中に情報をログに記録できます。
Google Cloud Console を使用して、パイプラインの実行中と実行後のロギング情報をモニタリングできます。
今回の例では、特定のデータセットを実行するとエラーが発生することが判明しているため、再度同じデータセットをDataflowパイプラインで実行すると、問題を再現することができます。
問題が再現された後に、ロギング情報を確認することで、Dataflow中の処理のエラーを判別できます。
したがって、正解は「固定のデータセットをCloud Dataflowパイプラインで実行し、その出力を分析する」です。
参照：
https://cloud.google.com/dataflow/docs/guides/logging
</div></details>

### Q. 問題18: 正解
Google Cloud BigtableのストレージとしてHDD（ハードディスクドライブ）を選択する有効なユースケースはどれですか？
1. 最低でも10TBのデータを保存するバッチワークロード
2. 
3. 
4. 
<details><div>
    答え：1
説明
Bigtableでは、永続ストレージとして、ソリッド ステート ドライブ（SSD）とハードディスク ドライブ（HDD）のどちらにするかを指定します。
SSD ストレージは、ほとんどのユースケースで最も効率的でコスト効果の高い選択肢です。
HDD ストレージは、非常に大きいデータセット（10 TB 超）で、レイテンシがあまり重要でない場合やアクセス頻度が低い場合に適切であることがあります。
どちらのタイプのストレージを選択した場合でも、多数の物理ドライブにわたって分散してレプリケーションされたファイル システムにデータが保存されます。
HDD ストレージは、以下の条件を満たしているユースケースに適しています。
- 10 TB 以上のデータを保存する予定である。
- ユーザー向けやレイテンシの影響を受けやすいアプリケーションを支援するデータを使用しない。
ワークロードは次のいずれかのカテゴリに分類されます。
- バッチ ワークロード：スキャンと書き込みが行われます。また、少数の行の読み込みがまれにランダムに行われます。
- データ アーカイブ：大量のデータの書き込みが行われます。そのデータの読み取りはほとんど行われません。
たとえば、多数のリモート センシング装置の詳細な履歴データを保存し、そのデータを使用して日次レポートを生成する予定がある場合には、パフォーマンスが低下しても HDD ストレージのコスト削減が優先されることがあります。それに対して、データを使用してリアルタイム ダッシュボードを表示する場合は、HDD ストレージの使用は適切とはいえません。この場合、読み取りが頻繁に行われ、またスキャンではない読み取りははるかに遅いからです。
今回のケースでは長時間にわたるバッチジョブであるためHDDが最適です。
したがって、正解は「最低でも10TBのデータを保存するバッチワークロード」です。
参照：
https://cloud.google.com/bigtable/docs/choosing-ssd-hdd
</div></details>

### Q. 問題19: 正解
あなたの会社では、ハイブリッドクラウドを導入しています。クラウドプロバイダーのサービス間でデータを移動させ、各クラウドプロバイダーのサービスを利用する複雑なデータパイプラインを持っています。
パイプライン全体をオーケストレーションするには、どのクラウドネイティブサービスを使うべきでしょうか？
1. 
2. Cloud Composer
3. 
4. 
<details><div>
    答え：2
説明
パイプラインの実行を自動化するマネージドサービスを選択する必要があります。
Cloud ComposerはApache Airflow で構築された、フルマネージドのワークフロー オーケストレーション サービスです。
DAG（Direct Acyclic Graph）と呼ばれるデータ形式でジョブ同士の関係性を定義し、実行します。
Cloud Composer はフルマネージド サービスであり、Apache Airflow は互換性に優れているため、リソースのプロビジョニングに気をとられず、ワークフローの作成、スケジューリング、モニタリングに専念できます。
BigQuery、Dataflow、Dataproc、Datastore、Cloud Storage、Pub/Sub、AI Platform などの Google Cloud プロダクトとのエンドツーエンドの統合により、ユーザーはパイプラインを自由かつ完全にオーケストレートできます。
これによって、複数の依存関係のあるジョブを、少ないオーバーヘッドで実行することが可能になります。
したがって、正解は「Cloud Composer」です。
参照：
https://cloud.google.com/composer/docs/how-to/using/writing-dags
https://cloud.google.com/composer/?hl=en
</div></details>

### Q. 問題20: 不正解
あなたの会社は約3年前にゲームアプリをリリースしました。毎日行われる処理として、前日のログファイルを、テーブル名がLOGS_yyyymmddの別のGoogle BigQueryテーブルにアップロードしていました。テーブルのワイルドカード関数を使用して、すべての時間範囲の日次および月次レポートを生成していました。最近、長い日付範囲をカバーする一部のクエリで、テーブル数が1,000の制限を超えて失敗することがわかりました。
この問題を解決するにはどうしたらよいでしょうか。
1. 
2. シャード化されたテーブルを1つのパーティショニングされたテーブルに変換する
3. 
4. 
<details><div>
    答え：2
説明
3年間毎日テーブルが生成され続けた結果、テーブル数の上限に到達しました。
これら日付別テーブルを単一のテーブルに変換することで今回の問題は解決できます。
日付別テーブルを以前に作成している場合は、bq コマンドライン ツールで partition コマンドを使用して、関連する一連のテーブル全体を単一の取り込み時間パーティション分割テーブルに変換できます。
partition コマンドを実行すると、BigQuery によって日付別テーブルからパーティションを生成するコピージョブが作成されます。
したがって、正解は「シャード化されたテーブルを1つのパーティショニングされたテーブルに変換する」です。
参照：
https://cloud.google.com/bigquery/docs/creating-partitioned-tables#converting_date-sharded_tables_into_ingestion-time_partitioned_tables
https://cloud.google.com/bigquery/docs/creating-partitioned-tables#convert-date-sharded-tables
</div></details>

### Q. 問題21: 正解
あなたは、以下の条件を満たすクラウドネイティブな履歴データ処理システムを設計しています。
- 分析対象のデータはCSV、Avro、PDF形式で、Cloud Dataproc、BigQuery、Compute Engineなどの複数の分析ツールからアクセスされます。
- ストリーミングデータパイプラインは、毎日新しいデータを保存します。
- パフォーマンスはソリューションの要素ではありません。
- ソリューションの設計では、可用性を最大限に高める必要があります。
このソリューションでは、データストレージをどのように設計しますか？
1. 
2. 
3. 
4. データをマルチリージョナル Cloud Storage バケットに保存する。Cloud Dataproc、BigQuery、およびCompute Engineを使用してデータに直接アクセスする
<details><div>
    答え：4
説明
CSV、Avroといったリレーショナルデータベースに格納できる形式だけでなく、PDFというリレーショナルデータベースに格納できないデータを保存する必要があります。
Cloud Storageは、あらゆるデータを格納できるオブジェクトストレージです。
今回の全てのデータについて格納をすることができます。
また、マルチリージョンでデータを保存することで、可用性を高めることが可能になります。
Cloud Storageに保存されたデータは、BigQuery、Dataproc、Compute Engineから直接アクセスすることが可能です。
したがって、正解は「データをマルチリージョナル Cloud Storage バケットに保存する。Cloud Dataproc、BigQuery、およびCompute Engineを使用してデータに直接アクセスする」です。
参照：
https://cloud.google.com/
https://cloud.google.com/storage/docs/locations
</div></details>

### Q. 問題22: 正解
あなたの会社のアプリケーションではFirebase AnalyticsとGoogle BigQueryの統合が有効になっています。Firebase は BigQuery に app_events_YYYYMMDD というフォーマットで毎日自動的に新しいテーブルを作成するようになりました。あなたは過去 30 日間のすべてのテーブルをレガシー SQL でクエリしたいと考えています。
要件を達成するためにするべきことは何ですか？
1. TABLE_DATE_RANGE関数を使用する
2. 
3. 
4. 
<details><div>
    答え：1
説明
今回はレガシーSQLで有効な関数を選択する必要があります。
テーブル ワイルドカード関数である TABLE_DATE_RANGE（） 関数は、期間に対応する複数の日次テーブルにクエリを実行する。
この関数によって、今回要件になっている過去30日間の全てのテーブルをクエリすることが可能になります。
したがって、正解は「TABLE_DATE_RANGE関数を使用する」です。
参照：
https://cloud.google.com/bigquery/docs/reference/legacy-sql
</div></details>

### Q. 問題23: 正解
住宅価格を予測するモデルを作成しています。予算の都合上、リソースに制約のある1台の仮想マシンで実行しなければなりません。
どの学習アルゴリズムを使うべきでしょうか？
1. 線形回帰
2. 
3. 
4. 
<details><div>
    答え：1
説明
住宅価格という連続値を予測するため、今回の問題は分類ではなく回帰の問題になります。
また、データ構造は複数の説明変数から構成されており、住宅価格という目的変数を予測する。
今回のようなケースの場合であれば、線形回帰が最適なアルゴリズムになります。
ロジスティック回帰は回帰という名前ですが、実際は2値分類に使用されます。
したがって、正解は「線形回帰」です。
参照：
https://qiita.com/kwi0303/items/989b3b9ee6eabb067892
</div></details>

### Q. 問題24: 正解
社内のさまざまな部署にBigQueryへのアクセスを設定する必要があります。あなたのソリューションは、以下の要件に準拠している必要があります。
- 各部門は自分のデータにのみアクセスできる必要があります。
- 各部門には、テーブルを作成・更新し、チームに提供することができる必要のある1人以上のリードがいます。
- 各部門にはデータアナリストがいて、データの照会はできても修正はできないようにする必要があります。
BigQuery のデータへのアクセスをどのように設定するべきでしょうか？
1. 
2. 各部門のデータセットを作成する。部門のリーダーにはWRITERのロールを割り当て、データアナリストにはそのデータセットのREADERのロールを割り当てる
3. 
4. 
<details><div>
    答え：2
説明
複数の部門にまたがり、複数の権限階層を持った組織に対してアクセスコントロールを提供する必要があります。
BigQuery は、プロジェクト レベルのアクセスを許可するため、IAM の基本ロールをサポートしています。
基本ロールでは、データセットに対する操作の細かな制御がなされています。
READERロールはデータセットのテーブルの読み取り、クエリ、コピー、エクスポートを行えます。
データセット内のルーティンを読み取ることも可能です。
WRITERロールはREADERロールで可能な操作に加えて、データセットのデータを編集または追加できます。
今回の場合であれば、リーダーにWRITERのロールを、データアナリストにはREADERのロールを割り当てることで要件を達成できます。
したがって、正解は「各部門のデータセットを作成する。部門のリーダーにはWRITERのロールを割り当て、データアナリストにはそのデータセットのREADERのロールを割り当てる」です。
参照：
https://cloud.google.com/bigquery/docs/access-control-primitive-roles
https://cloud.google.com/bigquery/docs/table-access-controls-intro
</div></details>

### Q. 問題25: 正解
あなたの分析チームは、いくつかの異なる指標に基づいて、どの顧客があなたの会社と再び仕事をする可能性が最も高いかを判断するための簡単な統計モデルを構築したいと考えています。彼らは、Google Cloud Storageに格納されたデータを使用して、Apache Spark上でモデルを実行したいと考えています。このジョブの実行には、Dataprocを使用します。テストの結果、このワークロードは15ノードのクラスタで約30分で実行でき、結果を Google BigQueryに出力します。このワークロードは、毎週実行する計画です。
コスト面ではどのようにクラスターを最適化すべきでしょうか？
1. 
2. クラスタにプリエンプト可能な仮想マシン（VM）を使用する
3. 
4. 
<details><div>
    答え：2
説明
クラスタのコンピューティングリソースをコスト最適なものに置き換えることが有効です。
今回は、クラスタにプリエンプティブな仮想マシン（VM）を使用することで、dataprocクラスタのコストを削減することができます。
プリエンプティブル VM インスタンスは、標準 VM の料金よりもはるかに低価格（60～91% 割引）で利用できます。
ただし、他のタスクがリソースを再利用する必要がある場合、Compute Engine がこのインスタンスを停止（プリエンプト）する可能性があります。
プリエンプティブル インスタンスは Compute Engine の余剰のキャパシティを利用する機能であり、使用できるかどうかは利用状況に応じて異なります。
Dataproc の高度な柔軟性モード（EFM）は、シャッフル データを管理して、動作中のクラスタからのノードの削除に起因するジョブ進行の遅延を最小限に抑えます。
EFM は、ユーザーが選択できる 2 つのモードのいずれかでシャッフル データの負荷を軽減する。
どちらの EFM モードでもセカンダリ ワーカーに中間シャッフル データが保存されることはないため、EFM はプリエンプティブル VM を使用するクラスタ、またはセカンダリ ワーカー グループの自動スケーリングのみを行うクラスタに適しています。
したがって、正解は「クラスタにプリエンプト可能な仮想マシン（VM）を使用する」です。
参照：
https://cloud.google.com/dataproc/docs/concepts/compute/preemptible-vms
https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/flex
</div></details>

### Q. 問題26: 正解
あなたは、Cloud Dataflowのストリーミング・パイプラインを操作しています。このパイプラインは、Pub/Subサブスクリプションソースからのイベントをウィンドウ内で集約し、その結果得られた集約物をCloud Storageバケットにシンクします。ソースは一貫したスループットを持っています。パイプラインの動作に関するアラートをCloud Monitoringで監視し、処理が確実に行われるようにしたいと考えています。また、Cloud Monitoringでパイプラインの動作に関するアラートを監視し、データが処理されていることを確認したいとも考えています。
どのCloud Monitoringアラートを作成すればよいでしょうか。
1. 
2. 送信元のSubscription/num_undelivered_messagesの増加、および送信先のインスタンス/ストレージ/使用済みバイトの変化率の減少に基づくアラート
3. 
4. 
<details><div>
    答え：2
説明
処理が確実に行われていることを確認するためのアラートを実装する必要があります。
Pub/Subでは、一貫したスループットが維持されているのであるならば、ある期間中の処理されていないメッセージや、サブスクリプションされたメッセージ数は一定になります。
それらの値が増加することは、パフォーマンスに関する問題が発生しているということになるため、アラートを設定することが有効です。
また、一定の処理をし続けているコンピューティングリソースは、インスタンス / ストレージ / 使用済みバイトも定常的になると考えられます。
それらの値が減少するということは、コンピューティングリソースがメッセージの処理を十分に行えていないということになるため、減少を検知するアラートを設定することが有効です。
したがって、正解は「送信元のSubscription/num_undelivered_messagesの増加、および送信先のインスタンス/ストレージ/使用済みバイトの変化率の減少に基づくアラート」です。
参照：
https://cloud.google.com/pubsub/docs/monitoring#monitoring_forwarded_undeliverable_messages
</div></details>

### Q. 問題27: 正解
あなたは，自然言語処理の分野で回帰問題に取り組んでおり，データセットには1億枚のラベル付きサンプルが含まれています．データをランダムにシャッフルし、データセットを訓練サンプルとテストサンプルに分けました（割合は9:1）。ニューラルネットワークを訓練し，テストセットでモデルを評価したところ，モデルの平均平方根誤差（RMSE）が訓練セットではテストセットの2倍になっていることがわかりました。
モデルの性能を向上させるにはどうすればよいですか？
1. 
2. 
3. 正則化手法 (ドロップアウトやバッチ正規化など) を試して、過学習を回避します。
4. 追加のレイヤーを導入したり、使用する語彙やNグラムのサイズを大きくしたりするなどして、モデルの複雑さを増す
<details><div>
    答え：３
説明
モデルの二乗平均平方根誤差 (RMSE) が、トレーニング セットではテスト セットの 2 倍であるという観察結果は、過学習を示しています。過学習は、モデルが学習データに近づきすぎて、一般的なパターンではなく学習データにノイズや特異性をキャプチャすることを学習した場合に発生します。モデルのパフォーマンスを向上させるには、過学習を減らすことに重点を置く必要があります。正しいアプローチは次のとおりです。
C. 
正則化手法:ドロップアウト、バッチ正規化、L2 正則化などの手法は、過学習を軽減するように設計されています。トレーニング中にモデルのパラメーターに制約を導入し、トレーニング データに近づきすぎないようにし、目に見えないデータへの一般化を促進します。
テストセットサイズの拡大(オプションA):トレーニングとテストの分割でテスト サンプルのシェアを増やしても、過学習には直接対処できません。重要なのは、既存のテストセットの相対的なパフォーマンスであり、そのサイズではありません。
より多くのデータを収集する(オプションB):より多くのデータを収集すると、特定のケースでは役立ちますが、必ずしも過学習が解決されるとは限りません。一般に、データセットのサイズを増やすことを検討する前に、まずモデルを最適化し、正則化手法を適用することをお勧めします。
モデルの複雑性を増す(オプションD):モデルの複雑さが増すと、過学習が解決されるどころか悪化する可能性があります。複雑なモデルほどパフォーマンスが向上するというのは、よくある誤解です。適切な正則化を備えた単純なモデルは、多くの場合、過度に複雑なモデルよりも優れた性能を発揮します。
正しくないオプション -
A. トレーニングとテストの分割でテスト サンプルのシェアを増やします。
トレーニングとテストの分割でテスト サンプルの割合を増やしても、過学習の問題に直接対処できるわけではありません。トレーニングとテストの間でデータの割り当てが変更されるだけで、モデルの動作には影響しません。
問題は、テストセットのサイズではなく、モデルがトレーニングセットから未知のデータにうまく一般化できないことです。このオプションでは、過学習の根本原因には対処できません。
B. より多くのデータを収集し、データセットのサイズを大きくしてみてください。
より多くのデータを収集することは、特に意味のあるパターンを学習するための十分なデータがモデルにない場合に、場合によっては有用な戦略となる可能性があります。ただし、このシナリオでは、既にかなりのデータセット (100M の例) があります。
データセットのサイズを大きくしても、過学習の問題に直接対処できない場合があります。過学習は、多くの場合、モデルが複雑すぎるか、使用可能なデータの量に対してパラメーターが多すぎることが原因です。一般に、大規模なデータセットがある場合は、モデルの正則化に重点を置く方が効果的です。
D. 追加のレイヤーを導入したり、使用する語彙や n-gram のサイズを増やしたりするなどして、モデルの複雑さを増します。
モデルの複雑さが増すと、過学習は解決されるどころか悪化する可能性があります。より複雑なモデルでは、トレーニング データにノイズや特異性が当てはめられやすく、一般化が不十分になる可能性があります。
過学習は、通常、モデルが既に複雑すぎて使用可能なデータがない場合に発生します。複雑さを増すことは、推奨されるアプローチではありません。代わりに、モデルを単純化し、正則化手法を適用して過学習を防ぐことをお勧めします。

誤り説明
今回のケースは適合不足（Underfitting）の状態であるため、モデルの複雑さが不足しています。
この場合は、使用する語彙の数やNグラムのサイズを大きくするなどすることによって、モデルの複雑さを増加させることができます。
したがって、正解は「追加のレイヤーを導入したり、使用する語彙やNグラムのサイズを大きくしたりするなどして、モデルの複雑さを増す」です。
参照：
https://nozma.github.io/ml_with_python_note/2-2-%E6%B1%8E%E5%8C%96%E9%81%8E%E5%89%B0%E9%81%A9%E5%90%88%E9%81%A9%E5%90%88%E4%B8%8D%E8%B6%B3.html
</div></details>

### Q. 問題28: 正解
Google Cloud上のデータパイプラインのために、Cloud Pub/SubからBigQueryへのJSONメッセージの書き込みと変換を行うサービスを選択しています。サービスのコストは最小限に抑えたいと考えています。また、サイズが変化する入力データ量を監視し、最小限の手動操作で対応したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. Cloud Dataflowを使用して変換を実行する。Cloud Monitoringでジョブシステムの遅延を監視する。ワーカーインスタンスにデフォルトのオートスケーリング設定を使用する
4. 
<details><div>
    答え：3
説明
Pub/Subのデータを処理するスケーラブルなサービスを選択する必要があります。
また、データ量の監視も自動的に行えるようにする必要があります。
Pub/Sub は、スケーラブルで耐久性のあるイベントの取り込みおよび配信システムです。
Dataflow は、メッセージの重複排除、1 回限りの処理、タイムスタンプ付きイベントからのデータ ウォーターマークの生成により、Pub/Sub のスケーラブルな「最低 1 回」配信モデルを補完する。
Dataflow を使用するには、Apache Beam SDK でパイプラインを記述し、Dataflow サービスでパイプライン コードを実行する。
データ量監視についてはCloud Monitoring（Cloud Monitoring）が有効です。
Cloud Monitoring は、Google Cloud、Amazon Web Services（AWS）、ホストされた稼働時間プローブ、アプリケーション インストゥルメンテーションから指標、イベント、メタデータを収集する。
BindPlane サービスを使用して、150 以上の共通のアプリケーション コンポーネント、オンプレミス システム、ハイブリッド クラウド システムからデータを収集することもできます。
データを取り込むと、Google Cloud のオペレーション スイートはダッシュボード、グラフ、アラートを介して分析情報を提供する。
したがって、正解は「Cloud Dataflowを使用して変換を実行する。Cloud Monitoringでジョブシステムの遅延を監視する。ワーカーインスタンスにデフォルトのオートスケーリング設定を使用する」です。
参照：
https://cloud.google.com/Pub/Sub/docs/Pub/Sub-dataflow
https://cloud.google.com/migrate/compute-engine/docs/4.8/how-to/monitoring/using-Cloud Monitoring-monitoring
</div></details>

### Q. 問題29: 不正解
MySQL を使用した Cloud SQL の導入を計画しています。ゾーンに障害が発生した場合、高可用性を確保する必要があります。
要件を達成するためにするべきことは何ですか？
1. あるゾーンにCloud SQLインスタンスを作成し、同じリージョン内の別のゾーンにフェイルオーバーレプリカを作成する
2. 
3. 
4. 
<details><div>
    答え：1
説明
Cloud SQLはフルマネージド リレーショナル データベース サービスです。
豊富な拡張機能コレクション、構成フラグ、デベロッパー エコシステムを備えた従来と同じリレーショナル データベースを運用しつつ、自己管理の煩わしさから解放されます。
Cloud SQL インスタンスは高可用性（HA）構成をとることができます。
HA 構成の目的は、ゾーンまたはインスタンスが利用できなくなったときのダウンタイムの削減です。
これは、ゾーンでサービスが停止した場合や、インスタンスが破損した場合に発生することがあります。
HA を使用すれば、クライアント アプリケーションで引き続きデータを使用できるようになります。
HA 構成は「クラスタ」とも呼ばれ、データの冗長性を確保します。
HA 向けに構成された Cloud SQL インスタンスは「リージョン インスタンス」とも呼ばれ、構成されたリージョン内のプライマリ ゾーンとセカンダリ ゾーンに配置されます。
リージョン インスタンスはプライマリ インスタンスとスタンバイ インスタンスで構成されます。
各ゾーンの永続ディスクへの同期レプリケーションにより、トランザクションが commit されたとしてレポートされる前に、プライマリ インスタンスへの書き込みのすべてが両方のゾーンのディスクに複製されます。
インスタンスまたはゾーンで障害が発生した場合、永続ディスクはスタンバイ インスタンスにアタッチされ、新しいプライマリ インスタンスになります。
ユーザーは新しいプライマリに再転送されます。
このプロセスは、フェイルオーバーと呼ばれます。
したがって、正解は「あるゾーンにCloud SQLインスタンスを作成し、同じリージョン内の別のゾーンにフェイルオーバーレプリカを作成する」です。
参照：
https://cloud.google.com/sql/docs/mysql/high-availability
</div></details>

### Q. 問題30: 正解
BigQueryで非正規化されたデータ構造を使用するメリットとして、どのようなものがありますか？
1. 
2. クエリの速度が向上し、クエリがシンプルになる
3. 
4. 
<details><div>
    答え：2
説明
高度に構造化された膨大なデータを高いパフォーマンスでクエリする際は、非正規化をすることが有効です。
これによって、クエリを行うレコードの範囲を絞ることができるため、クエリ時間の短縮が可能になります。
また、クエリ自体もシンプルになります。
したがって、正解は「クエリの速度が向上し、クエリがシンプルになる」です。
参照：
https://cloud.google.com/solutions/bigquery-data-warehouse
</div></details>

### Q. 問題31: 正解
あなたは中堅企業に勤務しており、業務システムのトランザクションデータをオンプレミスのデータベースからGCPに移行する必要があります。そのデータベースのサイズは約20TBのサイズがあります。
どのデータベースを選択すべきでしょうか？
1. Cloud SQL
2. 
3. 
4. 
<details><div>
    答え：1
説明
20TBのデータを保持できるリレーショナルデータベースサービスを選ぶ必要があります。
Cloud SQL は、Google Cloud Platform 上のリレーショナル データベースの設定、維持、運用、管理を支援するフルマネージドのデータベース サービスです。
Cloud SQL は、MySQL、PostgreSQL、または SQL Server で使用できます。
Cloud SQLは、最大64個のプロセッサーコアと400GB以上のRAMを追加することで簡単にスケールアップでき、最大30TBのストレージをサポートします。
さらに、Cloud SQLは、ストレージの容量が限界に近づくと、自動的にスケールアップすることができます。
今回の要件の全てをCloud SQLは満たすことができます。
したがって、正解は「Cloud SQL」です。
参照：
https://cloud.google.com/sql/docs
</div></details>

### Q. 問題32: 正解
あなたの会社は現在、キャンペーンのためのデータパイプラインを設定しています。すべてのGoogle Cloud Pub/Subストリーミング・データについて、重要なビジネス要件の1つは、キャンペーン中の入力とそのタイミングを定期的に識別できることです。エンジニアは、この目的のためにGoogle Cloud Dataflowのウィンドウ化と変換を使用することにしました。しかし、この機能をテストしたところ、すべてのストリーミング・インサートに対してCloud Dataflowジョブが失敗することがわかりました。
この問題の最も可能性の高い原因は何でしょうか？
1. 
2. 
3. 
4. 非グローバルウィンドウ関数を適用していないため、パイプラインの作成時にジョブが失敗する
<details><div>
    答え：4
説明
今回は、ウィンドウ関数に関するトラブルシューティングを行う必要があります。
Dataflowにおいて、非グローバルウィンドウ関数もしくは非デフォルト関数を使用すると、エラーが発生する。
今回のケースであればグローバルウィンドウ関数を使用する必要があります。
したがって、正解は「非グローバルウィンドウ関数を適用していないため、パイプラインの作成時にジョブが失敗する」です。
参照：
https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines
https://beam.apache.org/documentation/programming-guide/#windowing
</div></details>

### Q. 問題33: 不正解
Google Cloud Pub/Sub サブスクリプションをソースとして、Google Cloud Dataflow ストリーミングパイプラインが動作しています。新しいCloud Dataflowパイプラインを現在のバージョンと互換性のないものにするために、コードの更新を行う必要があります。この更新を行う際に、データを失うことはしたくありません。
要件を達成するためにするべきことは何ですか？
1. 現在のパイプラインを更新し、ドレインフラグを使用する
2. 
3. 
4. 
<details><div>
    答え：1
説明
実行中のパイプラインを、データの整合性を確保しながら更新する必要があります。
Dataflowジョブは、二つの方法で停止することが可能です。
- ジョブをキャンセルする：
この方法は、ストリーミング パイプラインとバッチ パイプラインの両方に適用されます。ジョブをキャンセルすると、Dataflow サービスはバッファデータなどのデータの処理を停止します。
- ジョブをドレインする：
この方法は、ストリーミング パイプラインにのみ適用されます。ジョブをドレインすると、Dataflow サービスはバッファ内のデータの処理を完了すると同時に、新しいデータの取り込みを中止できます。
今回の場合は、Drainオプションを使用することで、バッファ内のデータ処理を確実に終了し、新しいパイプラインに切り替えることが可能です。
なお、パイプラインのウィンドウ処理に対する変更は、固定時間ウィンドウやスライド時間ウィンドウの長さの変更など、小規模にとどめることが推奨されています。
ウィンドウ処理アルゴリズムの変更など、ウィンドウ処理やトリガーに大きな変更を加えると、パイプライン出力に予期しない結果が生じることがあります。
したがって、正解は「現在のパイプラインを更新し、ドレインフラグを使用する」です。
参照：
https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline
https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Mapping
https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing
</div></details>

### Q. 問題34: 正解
あなたの会社は、データの取り込みと配信を集中的に行うシステムを選択しています。要件を満たすために、メッセージングシステムとデータ統合システムを検討しています。主な要件は以下の通りです。
- トピック内の特定のオフセットを検索する機能。場合によっては30日前のデータのにさかのぼることができること
- 何百ものトピックに対するパブリッシュ/サブスクライブのサポートをすること
- キーごとの順序付けを維持すること
- インフラストラクチャの管理を最小化すること
要件を満たすためにどのシステムを選ぶべきですか？
1. 
2. 
3. Cloud Pub/Sub
4. 
<details><div>
    答え：3
説明
整合性を保ったパブリッシュ/サブスクライブを行うことのできるサービスを選択する必要があります。
Pub/Sub は、非同期のメッセージング サービスです。
Pub/Sub は、イベントを生成するサービスと、イベントを処理するサービスを分離します。
Pub/Sub は、メッセージ指向のミドルウェア、またはストリーミング分析用のイベントの取り込みと配信のパイプラインとして使用できます。
いずれの場合も、パブリッシャー アプリケーションがメッセージを作成してトピックへ送信します。
サブスクライバー アプリケーションでは、トピックに対するサブスクリプションが作成されてメッセージが受信されます。
サブスクリプションとは、特定のトピックに関するメッセージの受信に関心があることを示す名前付きエンティティです。
Apache KafkaもPub/Subと同様のメッセージングソリューションで、トピックベースで過去のデータに遡ることができます。
しかし、Pub/Subは最大で30日間のデータ保持が可能であり、メッセージの順序指定も可能であります。
更に、Pub/Subはマネジメントサービスのため、インフラストラクチャの管理が不要です。
一方で、KafkaはGoogle Cloud上でカスタムVMによるホスティングが必要です。
したがって、正解は「Cloud Pub/Sub」です。
参照：
https://cloud.google.com/architecture/migrating-from-kafka-to-pubsub#comparing_features
https://cloud.google.com/pubsub/docs/replay-overview
https://cloud.google.com/pubsub/docs/ordering
</div></details>

### Q. 問題35: 不正解
適切に設計された行キーを使ってCloud Bigtableにデータを書き込むデータパイプラインがあります。あなたはパイプラインを監視して、Cloud Bigtableクラスタのサイズを増やすタイミングを判断できるようにしたいと考えています。
これを実現するためには、どのアクションを取ることができますか？（2つ選択）
1. 
2. 
3. 書き込み操作のレイテンシーを監視する。書き込みのレイテンシーが持続的に増加した場合、Bigtableクラスターのサイズを増やす
4. ストレージの使用率を監視する。使用率が最大容量の70%を超えたら、Bigtableクラスターのサイズを増やす
5. 
<details><div>
    答え：3,4
説明
BigtableはNoSQL型のデータベースです。
Bigtableのパフォーマンスに影響を与える可能性がある要因を正確に監視することで、スケールアップを適切に行うことができます。
書き込み操作レイテンシーは、Bigtableのパフォーマンスを表すメトリクスになります。
このメトリクスが継続的に増加しているということは、既存のキャパシティーでは書き込み処理を維持することができないということです。
ストレージ使用率も重要なメトリクスです。
Bigtableのストレージ容量は、ストレージ タイプとクラスタ内のノード数によって決まります。
クラスタに保存されるデータ量が増加すると、Bigtable はクラスタ内のすべてのノードにデータを分散してストレージを最適化します。
データを継続的に保存し続けるためには、使用率が増大していることはいち早く検知する必要があります。
したがって、正解は以下の通りです。
- 書き込み操作のレイテンシーを監視する。書き込みのレイテンシーが持続的に増加した場合、Bigtableクラスターのサイズを増やす
- ストレージの使用率を監視する。使用率が最大容量の70%を超えたら、Bigtableクラスターのサイズを増やす
参照：
https://cloud.google.com/bigtable/docs/performance#typical-workloads
</div></details>

### Q. 問題36: 正解
あなたはセキュリティ会社でデータ分析エンジニアとして働いています。あなたの会社は、人間の顔が写っているかどうかのラベルが付けられた画像のデータセットを用いて、データセットを使って、画像中の人間の顔の表情を認識するニューラルネットワークを作りたいと考えています。
どのようなアプローチが最も効果的でしょうか？
1. 
2. 
3. 深層学習を用いて、複数の隠れ層を持つニューラルネットワークを構築し、顔の特徴を自動的に検出する
4. 
<details><div>
    答え：3
説明
データセットを使用した教師あり学習による顔認識はニューラルネットワークの一種である畳み込みネットワーク (CNN：Convolutional Neural Network)によって実現可能です。
CNNは画像認識に適した手法です。
ディープラーニングの研究の中で最も進められている画像認識、物体検出、領域推定などの画像分野で活用されています。
したがって、正解は「深層学習を用いて、複数の隠れ層を持つニューラルネットワークを構築し、顔の特徴を自動的に検出する」です。
参照：
https://tokai-kaoninsho.com/%E3%82%B3%E3%83%A9%E3%83%A0/face01-%E3%81%AE%E9%A1%94%E3%82%92%E8%AD%98%E5%88%A5%E3%81%99%E3%82%8B%E4%BB%95%E7%B5%84%E3%81%BF/
https://ainow.ai/2019/08/06/174245/
</div></details>

### Q. 問題37: 不正解
あなたの会社では、1時間に20,000個のファイルが作成されています。各データファイルは、4KB以下のカンマ区切り値（CSV）ファイルとしてフォーマットされています。すべてのファイルを処理するには、Google Cloud Platformにインジェストする必要があります。会社のサイトからGoogle Cloudへのレイテンシーは200msで、インターネット接続の帯域幅は50Mbpsに制限されています。現在、データ取り込みポイントとして、Google Compute Engineの仮想マシン上にセキュアFTP（SFTP）サーバーを導入しています。ローカルのSFTPクライアントは、専用のマシン上で動作し、CSVファイルをそのまま送信しています。目標は、前日のデータを含むレポートを、毎日午前10時までにエグゼクティブに提供することです。この設計では、帯域利用率がかなり低いにもかかわらず、現在のボリュームに追いつくのがやっとの状態です。あなたは、季節性を考慮すると、今後3ヶ月間はファイル数が2倍になると考えています。
あなたがパフォーマンスを改善するためにとるべき行動はどれですか？（2つ選択）
1. 
2. 
3. gsutilツールを使用してCSVファイルをストレージバケットに並行して送信するようにデータ取り込みプロセスを再設計する
4. 1,000個のファイルをテープアーカイブ（TAR）ファイルにまとめる。代わりにTARファイルを送信し、CSVファイルを受信したらクラウド上で分解する
5. 
<details><div>
    答え：3,4
説明
帯域幅の一部しか利用されていないため、ファイルが大量にあることに起因するオーバーヘッドが問題となっています。
高速のネットワーク接続を使用している場合は、gsutil -m（マルチスレッド / マルチ処理）オプションを利用することで、大量のファイルを迅速に転送できます。
ただし、一部のファイルのダウンロードが失敗していても、gsutil ではどのファイルが正常にダウンロードされたかを追跡しません。
たとえば、マルチスレッド転送を利用して 100 ファイルをダウンロードし、そのうち 3 ファイルのダウンロードが失敗した場合、どの転送が失敗したかを判別して転送を再試行する処理は、スクリプト側で行う必要があります。
こうしたケースは、前述のように定期的にチェックと実行を行うことで対応できます。
もう一つの方法は、複数のファイルをアーカイブし一つのファイルにまとめることです。
確かにgsutilで直接tarを使うことはできませんが、Cloud Storageにtarファイルをロードし、LinuxでCompute Engineインスタンスにファイルを移動し、tarでファイルを分割してCloud Storageにコピーし直すことが可能です。
多くのファイルを大きなtarでバッチ処理することで、Cloud Storageのスループットが向上する。
したがって、正解は以下の通りです。
- gsutilツールを使用してCSVファイルをストレージバケットに並行して送信するようにデータ取り込みプロセスを再設計する
- 1,000個のファイルをテープアーカイブ（TAR）ファイルにまとめる。代わりにTARファイルを送信し、CSVファイルを受信したらクラウド上で分解する
参照：
https://cloud.google.com/storage/docs/gsutil/addlhelp/ScriptingProductionTransfers
https://cloud.google.com/storage/docs/gsutil/addlhelp/TopLevelCommandLineOptions
</div></details>

### Q. 問題38: 不正解
お客様の会社では、30ノードのApache Hadoopクラスターをクラウドに移行しています。すでに作成したHadoopジョブを再利用し、クラスターの管理をできる限り少なくしたいと考えています。また、クラスターのライフサイクルを超えてデータを保存できるようにしたいと考えています。
要件を達成するために
1. 
2. 
3. 
4. Google Cloud Storage コネクタを使用する Cloud Dataproc クラスタを作成する
<details><div>
    答え：4
説明
Hadoop クラスタで利用可能なストレージを選択する必要があります。
Dataproc は、Apache Hadoop および Hadoop 分散ファイル システム（HDFS）と統合されています。
DataprocとGCSをGoogle Cloud Storageのコネクタで接続すると、クラスタの寿命が来た後もデータを保存することができます。
Dataproc はストレージに Hadoop 分散ファイル システム（HDFS）を使用する。
また、HDFS 互換の Cloud Storage コネクタが自動的にインストールされるため、HDFS と並行して Cloud Storage も使用できます。
クラスタに対してデータの移動を行うには、HDFS や Cloud Storage へのアップロードとダウンロードを使用する。
したがって、正解は「Google Cloud Storage コネクタを使用する Cloud Dataproc クラスタを作成する」です。
参照：
https://cloud.google.com/dataproc/docs/concepts/dataproc-hdfs
</div></details>

### Q. 問題39: 正解
あなたの会社が運営するグローバルに分散したオークションアプリケーションでは、ユーザーがアイテムに入札することができます。時には、ユーザーがほぼ同じ時間に同じ入札を行い、異なるアプリケーションサーバがその入札を処理します。各入札イベントには、アイテム、金額、ユーザー、タイムスタンプが含まれています。どのユーザーが最初に入札したかを判断するために、これらの入札イベントをリアルタイムで1つの場所に集約したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. 各アプリケーション・サーバーは、入札イベントが発生するとGoogle Cloud Pub/Subに書き込む。プルサブスクリプションを使用して、Google Cloud Dataflowを使用して入札イベントを引き出す。タイムスタンプで最初に処理された入札イベントを受け入れる
<details><div>
    答え：4
説明
グローバルで発生する入札イベントをスケーラブルな方法で集約する方法が必要です。
Cloud Pub/Subでメッセージを発行し、Dataflowで消費することで、イベント発生時のタイムスタンプに基づいて、メッセージをデキューすることが可能になります。
また、他のソリューションと比べてもスケーラブルでコスト効率が良い方法です。
したがって、正解は「各アプリケーション・サーバーは、入札イベントが発生するとGoogle Cloud Pub/Subに書き込む。プルサブスクリプションを使用して、Google Cloud Dataflowを使用して入札イベントを引き出す。タイムスタンプで最初に処理された入札イベントを受け入れる」です。
参照：
https://cloud.google.com/Pub/Sub
https://cloud.google.com/Pub/Sub/docs/Pub/Sub-dataflow
</div></details>

### Q. 問題40: 正解
会社のために新しいリアルタイムデータウェアハウスを構築しており、Google BigQueryのストリーミングによるインジェスチョンを使用する予定です。データが一度しか送られてこないという保証はありません。一方で、データの各行には一意のIDとイベントのタイムスタンプがあります。そのためデータをインタラクティブに照会する際には、重複したデータが含まれないようにしたいと考えています。
どのクエリタイプを使うべきでしょうか？
1. 
2. 
3. 
4. ROW_NUMBERウィンドウ関数を使い、ユニークIDでPARTITIONし、WHERE row equals 1を指定する
<details><div>
    答え：4
説明
重複排除は一般に、システムと BigQuery 間のネットワーク エラーや BigQuery の内部エラーといった特定のエラー状態でストリーミング挿入の状態を判断する方法がない分散システムでの再試行シナリオで必要です。
一方で、BigQuery の重複排除はベスト エフォート型であり、データの重複がないことを保証するメカニズムとしての使用には適していません。
さらに、データの高い信頼性と可用性を保証するために、BigQuery はベスト エフォート型の重複排除の品質を低下させる可能性があります。
ストリーミングの実行後に重複行が残らないようにするには、次の手動プロセスを使用する。
- テーブル スキーマ内の列として insertId を追加し、各行のデータに insertId 値を含めます。
- ストリーミングが停止した後に、クエリを実行して重複をチェックする。
- 重複を排除するには、次のクエリを実行する。宛先テーブルを指定し、サイズの大きい結果を許容し、結果のフラット化を無効にする。
#standardSQL
SELECT
* EXCEPT（row_number）
FROM （
SELECT
*,
ROW_NUMBER（）
OVER （PARTITION BY ID_COLUMN） row_number
FROM
`TABLE_NAME`）
WHERE
row_number = 1
したがって、正解は「ROW_NUMBERウィンドウ関数を使い、ユニークIDでPARTITIONし、WHERE row equals 1を指定する」です。
参照：
https://cloud.google.com/bigquery/streaming-data-into-bigquery#manually_removing_duplicates
</div></details>

### Q. 問題41: 正解
50,000個のセンサーから分単位で送られるデータをBigQueryテーブルに挿入する必要があります。データ量は今後、大幅な増加が予想されています。また、集約されたトレンドをリアルタイムで分析するために、データを取り込んでから1分以内に利用できるようにする必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. Cloud Dataflow パイプラインを使用して、データを BigQuery テーブルにストリームする
3. 
4. 
<details><div>
    答え：2
説明
大量のストリーミングデータを取り込み、1分以内にデータ処理を行う必要があります。
この場合、バッチ処理ではなくリアルタイムでのストリーミング処理が最適です。
Cloud Dataflow を使用すると、データ転送のレイテンシを抑えた、高速で簡素化されたストリーミング データ パイプライン開発が可能になります。
Cloud Dataflowを使ってテキストファイルのセットから処理され、結果としてシミュレーションされたリアルタイムデータはBigQueryに保存することができます。
したがって、正解は「Cloud Dataflow パイプラインを使用して、データを BigQuery テーブルにストリームする」です。
参照：
https://cloud.google.com/dataflow/docs/samples/join-streaming-data-with-sql
https://cloud.google.com/dataflow
</div></details>

### Q. 問題42: 正解
Web アプリケーションのログを含むトピックを持つ Apache Kafka クラスタがオンプレミスにあります。このデータをGoogle Cloudにレプリケートして、BigQueryやCloud Storageで分析する必要があります。あなたはのチームでは、レプリケーション方法としてKafka Connectプラグインの導入を避けるためにミラーリングが望ましいと考えています。
要件を達成するためにするべきことは何ですか？
1. GCE の VM インスタンス上に Kafka クラスタを展開する。オンプレミスのクラスターを構成して、GCEで稼働しているクラスターにトピックをミラーリングする。DataprocクラスタまたはDataflowジョブを使用して、Kafkaからの読み取りとGCSへの書き込みを行う
2. 
3. 
4. 
<details><div>
    答え：1
説明
要件にあるように、今回はコネクタではなくミラーリングを使用したトピックのマイグレーションを行う必要があります。
Apache Kafka はオープンソースの分散型イベント ストリーミング プラットフォームで、これを使用すると、アプリケーションはイベント ストリームのパブリッシュ、受信登録、保存、処理を行えます。
Kafka サーバーは、クライアント アプリケーションがやり取りするマシンのクラスタとして実行され、イベントの読み取り、書き込み、処理を行います。
Kafka は、アプリケーションの分離、メッセージの送受信、アクティビティのトラッキング、ログデータの集計、ストリームの処理に使用できます。
Google Cloud上でKafkaクラスタを展開する際は、GCEのVMインスタンスを使用うすることが最適です。
これによって、オンプレミスのクラスターからトピックをミラーリングし、後続のGoogle Cloudサービスへの連携が容易になります。
したがって、正解は「GCE の VM インスタンス上に Kafka クラスタを展開する。オンプレミスのクラスターを構成して、GCEで稼働しているクラスターにトピックをミラーリングする。DataprocクラスタまたはDataflowジョブを使用して、Kafkaからの読み取りとGCSへの書き込みを行う」です。
参照：
https://cloud.google.com/architecture/migrating-from-kafka-to-pubsub
https://cloud.google.com/blog/products/data-analytics/apache-kafka-for-gcp-users-connectors-for-pubsub-dataflow-and-bigquery
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330
</div></details>

### Q. 問題43: 正解
あなたは、株式取引を保存するデータベースと、ある企業の調整可能な期間中の平均株価を取得するアプリケーションを運営しています。データはCloud Bigtableに格納され、株式取引の日付が行のキーの始まりとなるテーブル構造を持っています。このアプリケーションには数千人の同時利用者がいますが、銘柄を追加するにつれてパフォーマンスが低下していることに気づきました。
アプリケーションのパフォーマンスを向上させるために何をすべきでしょうか？
1. Cloud Bigtableテーブルの行キーの構文を、銘柄のシンボルで始まるように変更する
2. 
3. 
4. 
<details><div>
    答え：1
説明
拡張性に優れたデータスキーマの設計を行う必要があります。
行数が多く（背が高く）カラム数の少ない（幅の狭い）テーブルでは、1行あたりのイベント数が少なく、1つのイベントだけということもあり得ますが、背が低く幅の広いテーブルでは、1行あたりのイベント数が多くなります。
時系列データには、一般的に背の高いテーブルと幅の狭いテーブルを使用する必要があります。
これは、1 行に 1 つのイベントを格納することで、データに対するクエリの実行が容易になるためです。
行ごとに多くのイベントを格納すると、行のサイズの合計が推奨される最大値を超える可能性が高くなります。
今回のケースでは行キーが日付であり、構成銘柄が列で保持されているため、銘柄が追加されるたびに幅が広くなってしまいます。
行キーを銘柄のシンボルにすることで、レコード数は増加しますが幅が増加しなくなり、パフォーマンスを維持することができます。
したがって、正解は「Cloud Bigtableテーブルの行キーの構文を、銘柄のシンボルで始まるように変更する」です。
参照：
https://cloud.google.com/bigtable/docs/schema-design-time-series#patterns_for_row_key_design
</div></details>

### Q. 問題44: 不正解
ストリーミングのCloud Dataflowパイプラインを運用しています。あなたは、ウィンドウ化アルゴリズムとトリガー戦略が異なる新バージョンのパイプラインを持っています。また、実行中のパイプラインを新バージョンに更新したいと考えています。アップデートの際には、データが失われないようにしたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. Drain オプションを使用して Cloud Dataflow パイプラインを停止する。更新されたコードで新しいCloud Dataflowジョブを作成する
<details><div>
    答え：4
説明
実行中のパイプラインを、データの整合性を確保しながら更新する必要があります。
Dataflowジョブは、二つの方法で停止することが可能です。
- ジョブをキャンセルする：
この方法は、ストリーミング パイプラインとバッチ パイプラインの両方に適用されます。ジョブをキャンセルすると、Dataflow サービスはバッファデータなどのデータの処理を停止します。
- ジョブをドレインする：
この方法は、ストリーミング パイプラインにのみ適用されます。ジョブをドレインすると、Dataflow サービスはバッファ内のデータの処理を完了すると同時に、新しいデータの取り込みを中止できます。
今回の場合は、Drainオプションを使用することで、バッファ内のデータ処理を確実に終了し、新しいパイプラインに切り替えることが可能です。
なお、パイプラインのウィンドウ処理に対する変更は、固定時間ウィンドウやスライド時間ウィンドウの長さの変更など、小規模にとどめることが推奨されています。
ウィンドウ処理アルゴリズムの変更など、ウィンドウ処理やトリガーに大きな変更を加えると、パイプライン出力に予期しない結果が生じることがあります。
したがって、正解は「Drain オプションを使用して Cloud Dataflow パイプラインを停止する。更新されたコードで新しいCloud Dataflowジョブを作成する」です。
参照：
https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline
https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Mapping
https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing
</div></details>

### Q. 問題45: 正解
あなたの会社では、独自のシステムを使用して、6時間ごとに在庫データをクラウド上のデータ取り込みサービスに送信しています。送信されるデータには、いくつかのフィールドからなるペイロードと送信時のタイムスタンプが含まれます。送信に失敗した可能性がある場合、システムはデータを再送信する。
どのようにデータを重複排除するのが最も効率が良いのでしょうか？
1. 各データエントリにグローバル一意識別子（GUID）を割り当てる
2. 
3. 
4. 
<details><div>
    答え：1
説明
今回の場合は、クラウド上のデータ取り込みサービス側で二重送信を検知する効率的な方法が必要です。
各データエントリにGUIDを割り当てることは、最も効率的です。
クラウドでは既知のGUIDを見るたびに、新しく送信されたエントリを破棄することで、複数回同じレコードが送信されたとしても重複することはありません。
ハッシュ値を用いる方法は、送信時のタイムスタンプが含まれてしまい値が変わり、取り込み側で重複を認識できなくなる可能性があります。
したがって、正解は「各データエントリにグローバル一意識別子（GUID）を割り当てる」です。
参照：
https://cloud.google.com/pubsub/docs/faq#duplicates
</div></details>

### Q. 問題46: 正解
あなたの会社のオンプレミスのApache Hadoopサーバーが寿命に近づいているため、IT部門はクラスターをGoogle Cloud Dataprocに移行することを決定しました。クラスタを同じように移行するには、ノードあたり50TBのGoogle Persistent Diskが必要になります。CIOは、それだけのブロック・ストレージを使用することによるコストを懸念しています。あなたは、移行にかかるストレージコストを最小限に抑えたいと考えています。
要件を達成するためにするべきことは何ですか？
1. Google Cloud Storageにデータを入れる
2. 
3. 
4. 
<details><div>
    答え：1
説明
コスト最適なApache Hadoopのストレージを選択する必要があります。
Dataproc はストレージに Hadoop 分散ファイル システム（HDFS）を使用する。
また、HDFS 互換の Cloud Storage コネクタが自動的にインストールされるため、HDFS と並行して Cloud Storage も使用できます。
クラスタに対してデータの移動を行うには、HDFS や Cloud Storage へのアップロードとダウンロードを使用する。
したがって、正解は「Google Cloud Storageにデータを入れる」です。
参照：
https://cloud.google.com/dataproc/docs/concepts/dataproc-hdfs
</div></details>

### Q. 問題47: 正解
Google Cloud上の10TBのデータベースの一部である2つのリレーショナルテーブルのストレージを設計しています。水平方向にスケールするトランザクションをサポートする必要があります。また、非キーカラムに対するレンジクエリのためにデータを最適化したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. ストレージにはCloud Spannerを使用する。セカンダリ・インデックスを追加して、クエリ・パターンをサポートする
4. 
<details><div>
    答え：3
説明
Cloud Spannerは無制限のスケーリング、強い整合性、最大 99.999% の可用性を備えたフルマネージド リレーショナル データベースです。
無制限のスケーリングによって、リレーショナル セマンティクスと SQL のすべてのメリットを享受することができます。
任意のサイズで開始し、ニーズの拡大に応じて制限なしでスケーリングすることが可能です。
また、計画的ダウンタイムのない、オンラインによるスキーマ変更で高可用性を実現することができます。
更に、リージョンや大陸全体にわたる強い整合性で高性能のトランザクションを提供する。
Cloud Spannerは、他の列のセカンダリ インデックスを作成することもできます。
セカンダリ インデックスは、ルックアップを使用することで得られるメリットに加えて、Cloud Spanner でより効率的にスキャンを実行し、全テーブル スキャンではなくインデックス スキャンを行うこともできます。
セカンダリ インデックスを列に追加すると、その列のデータをより効率的に検索できるようになります。
たとえば、特定の範囲の LastName 値に対して SingerId のセットをすばやく検索する必要がある場合は、Cloud Spanner でテーブル全体をスキャンしなくてもいいように、LastName にセカンダリ インデックスを作成する必要があります。
したがって、正解は「ストレージにはCloud Spannerを使用する。セカンダリ・インデックスを追加して、クエリ・パターンをサポートする」です。
参照：
https://cloud.google.com/spanner
https://cloud.google.com/blog/products/databases/spanner-relational-database-for-all-size-applications-faqs
https://cloud.google.com/spanner/docs/secondary-indexes
</div></details>

### Q. 問題48: 不正解
あなたは、地震データを分析するシステムを構築しています。ETLプロセスは、Apache Hadoopクラスタ上で一連のMapReduceジョブとして実行されます。ETLプロセスは、いくつかのステップが計算コストを要するため、データセットの処理に何日もかかります。そんな中、センサーのキャリブレーションのステップが省略されていることがわかりました。
今後、センサーのキャリブレーションを体系的に行うために、ETLプロセスをどのように変更すべきでしょうか？
1. 
2. 生データにセンサーキャリブレーションを適用する新しいMapReduceジョブを導入し、他のすべてのMapReduceジョブがこの後に処理されるようにする
3. 
4. 
<details><div>
    答え：2
説明
一般的にキャリブレーションは処理の前に行われるべきものです。
また、このプロセスは自動化する必要があります。
今回はMapReduceジョブのプラクティスに則った修正が必要です。
ベストプラクティスとしてMapReduceジョブは、一つの目的に対して一つ作成されるべきで、モノリシック化されるべきではありません。
今回の例であれば、既存のジョブは数日かかるため、仮にこのジョブにキャリブレーションを追加すると、テスト自体に多くの工数がかかりますし、メンテナンス容易性が低下する。
つまり、キャリブレーションの処理を追加する際は、新たにMapReduceジョブを追加する必要があります。
したがって、正解は「生データにセンサーキャリブレーションを適用する新しいMapReduceジョブを導入し、他のすべてのMapReduceジョブがこの後に処理されるようにする」です。
参照：
https://cloud.google.com/architecture/hybrid-and-multi-cloud-architecture-patterns
</div></details>

### Q. 問題49: 不正解
あなたの会社は、規制の厳しい業界に属しています。要件の1つとして、個々のユーザーが業務に必要な最小限の情報にしかアクセスできないようにすることが挙げられます。この要件をGoogle BigQueryで実現したいと考えています。
どのような3つのアプローチが考えられますか？（3つ選択）
1. 
2. ロールによってテーブルへのアクセスを制限する
3. 
4. BigQuery API へのアクセスを承認されたユーザーに制限する
5. 
6. Google Cloud Monitoring 監査ログ を使用して、ポリシー違反を判断する
<details><div>
    答え：2,4,6
説明
適切なアクセス制限及び、違反状況の監視が必要です。
BigQuery テーブル ACL では、テーブルやビューなどのリソースにテーブルレベルの権限を設定できます。
テーブルレベルの権限により、データまたはビューにアクセスできるユーザー、グループ、サービス アカウントが決まります。
ユーザーに完全なデータセットへのアクセス権を与えることなく、特定のテーブルまたはビューへのアクセス権を付与できます。
たとえば、ユーザーに BigQuery データ閲覧者（roles/bigquery.dataViewer）のロールを付与すると、対象のユーザーはデータセット全体に対するアクセス権がなくてもテーブルまたはビューのみにクエリを実行できます。
監視にはCloud Monitoring 監査ログ が有効です。
監査ログは、Google Cloud リソース内でオンプレミス環境と同じレベルの透明性を確保しながら「いつ誰がどこで何をしたか」という問いに答えるために役立ちます。監査ログを有効にすると、セキュリティ、監査、コンプライアンス エンティティが Google Cloud のデータとシステムをモニタリングして、脆弱性や外部データの不正使用の可能性を確認できます。
したがって、正解は以下の通りです。
- ロールによってテーブルへのアクセスを制限する
- BigQuery API へのアクセスを承認されたユーザーに制限する
- Google Cloud Monitoring 監査ログ を使用して、ポリシー違反を判断する
参照：
https://cloud.google.com/bigquery/docs/table-access-controls-intro
</div></details>

### Q. 問題50: 不正解
あなたのチームは、社内でETLの開発・保守を担当しています。Dataflowジョブの1つが、入力データにエラーがあるために失敗しており、パイプラインの信頼性を向上させる必要があります（すべての失敗データを再処理できるようにすることも含む）。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. データを変換する DoFn に try... catch ブロックを追加し、sideOutput を使用して PCollection を作成し、後で Pub/Sub に保存できるようにする
<details><div>
    答え：4
説明
Dataflowジョブで発生した入力データの問題に対する適切なトラブルシューティングを行う必要があります。
Dataflowでは、パイプラインコードのDoFnに try … catchブロックを追加することで、例外をキャッチすることが可能になります。
たとえば、ParDo で実行されたいくつかのカスタム入力検証が失敗する要素を削除する場合は、DoFn 内で例外を処理し、要素をドロップします。
いくつかの異なる方法で、失敗した要素を追跡することもできます。
- 失敗した要素をログに記録し、Cloud Logging を使用して出力をチェックできます。
- ログの表示の手順に沿って、Dataflow ワーカーログとワーカー起動ログで警告やエラーを確認できます。
- 失敗した要素を ParDo で追加出力に書き込み、後で調査できます。
例外がキャッチされた際は、詳細なエラー分析が必要になるため、sideOutputを用いてPCollectionを作成することが有効です。
PCollection は、パイプライン データとして機能する複数要素のデータセットを表します。
PCollection は、固定サイズのデータセット、または継続的に更新されるデータソースの制限なしデータセットを保持できます。
また、PCollectionに保存した後は、Pub/Subに改めて保存することも可能なため、失敗データの再処理も可能になります。
したがって、正解は「データを変換する DoFn に try... catch ブロックを追加し、sideOutput を使用して PCollection を作成し、後で Pub/Sub に保存できるようにする」です。
参照：
https://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline#detecting_an_exception_in_worker_code
</div></details>

## 2
### Q. 問題1: 正解
あなたの勤務している工場では、1日1回午前2時にアプリケーションのログファイルを1つのログファイルにまとめています。あなたは、そのログファイルを処理するためにGoogle Cloud Dataflowジョブを書きました。あなたは、ログファイルが1日に1回、できるだけ安価に処理されることを確認する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. Google App Engine Cron Service で cron ジョブを作成し、Cloud Dataflow ジョブを実行する
4. 
<details><div>
    答え：3
説明
Google Cloud Dataflowジョブを定時実行する最適な方法を選択する必要があります。
App Engine Cron サービスを使用すると、指定した時刻または一定間隔で動作する定期スケジュール タスクを構成できます。
このようなタスクは、一般的に cron ジョブと呼ばれています。cron ジョブは App Engine Cron サービスによって自動的にトリガーされます。
これを使用して、レポートメールを毎日送信する、キャッシュ データを 10 分ごとに更新する、概要情報を 1 時間に 1 回更新するといったことを実現できます。
したがって、正解は「Google App Engine Cron Service で cron ジョブを作成し、Cloud Dataflow ジョブを実行する」です。
参照：
https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml
</div></details>

### Q. 問題2: 不正解
あなたの組織では、6ヶ月前からGoogle BigQueryでデータを収集・分析しています。分析したデータの大部分は、events_partitioned という名前のタイムスタンプで分割されたテーブルに置かれています。クエリのコストを削減するために、過去14日間のデータのみをクエリするeventsというビューを作成しました。このビューはレガシー SQL で記述されています。来月、既存のアプリケーションがODBC接続を介してBigQueryに接続し、イベントデータを読み取る予定です。その際あなたはアプリケーションが接続できることを確認する必要があります。
どのアクションを取るべきでしょうか？（2つ選択）
1. 
2. 
3. 標準SQLを使用してevents_partitionedに対する新しいビューを作成する
4. 認証に使用するODBC接続用のサービスアカウントを作成する
5. 
<details><div>
    答え：3,4
説明
標準SQLクエリではレガシーSQLで定義されたビューを参照することができません。
したがって今回は、ODBC接続で接続するために、標準SQLを使用してevents_partitionedテーブル上に新しいビューを作成する必要があります。
また、ODBCドライバはこの際にBigQueryのロールが必要になります。
Google Cloudのベストプラクティスとして、サービスアカウントを用いたロールの付与が推奨されています。
したがって、正解は以下の通りです。
- 標準SQLを使用してevents_partitionedに対する新しいビューを作成する
- 認証に使用するODBC接続用のサービスアカウントを作成する
参照：
https://cloud.google.com/bigquery/docs/views
https://cloud.google.com/community/tutorials/bigquery-from-excel
</div></details>

### Q. 問題3: 正解
あなたは自動車メーカーに勤めており、Google Cloud Pub/Subを使用してデータパイプラインを設定し、異常なセンサーイベントを捕捉しています。Cloud Pub/Sub のプッシュ購読を使用しており、作成したカスタム HTTPS エンドポイントを呼び出して、これらの異常なイベントが発生したときに対処しています。カスタム HTTPS エンドポイントは、非常に多くの重複したメッセージを受け取り続けています。
これらの重複メッセージの最も可能性の高い原因は何ですか？
1. 
2. 
3. 
4. カスタムエンドポイントが、確認応答期限内にメッセージを確認していない
<details><div>
    答え：4
説明
Pub/Sub の使用中に発生した問題のトラブルシューティングを行う必要があります。
今回の場合で最も可能性が高いのが、確認応答期限内のメッセージ確認ができていないことです。
確認応答期限が切れる前にメッセージの確認応答を行わないと、Pub/Sub によってメッセージが再送信されます。
その結果、Pub/Sub によって重複するメッセージが送信されることがあります。
Google Cloud のオペレーション スイートを使用して、expiredレスポンス コードで確認応答オペレーションをモニタリングし、この状態を検出する。
このデータを取得するには、確認応答メッセージ オペレーション指標を選択し、response_code ラベルでグループ化するかフィルタする。
なお、response_code は指標のシステムラベルであり、指標ではありません。
したがって、正解は「カスタムエンドポイントが、確認応答期限内にメッセージを確認していない」です。
参照：
https://cloud.google.com/Pub/Sub/docs/troubleshooting#dupes
</div></details>

### Q. 問題4: 正解
あなたの会社では、複数の異なるチームがデータ処理を行っています。チームはそれぞれ独自の分析ツールを使用していますが、中にはGoogle BigQueryを使って直接クエリにアクセスできるものもあります。チーム同士がお互いのデータを見ることができないようにデータを保護する必要があります。また、データへの適切なアクセスを確保したいと考えています。
あなたが取るべき3つのステップはどれですか？（3つ選択）
1. 
2. チームごとに異なるデータセットにデータをロードする
3. 
4. チームのデータセットを承認されたユーザーに制限する
5. 
6. 各チームのユーザーに適切なIAMロールを使用する
<details><div>
    答え：2,4,6
説明
複数のチームが存在する場合の適切なデータウェアハウスのアクセスコントロールを選択する必要があります。
データセット レベルの権限により、特定のデータセット内のテーブル、ビュー、テーブルデータにアクセスできるユーザー、グループ、サービス アカウントが決まります。
たとえば、あるユーザーに特定のデータセットに対する bigquery.dataOwner Identity and Access Management（IAM）ロールを付与した場合、そのユーザーはそのデータセット内のテーブルとビューを作成、更新、削除できます。
ID で Google Cloud API を呼び出す場合、BigQuery ではその ID がリソースを使用するための適切な権限を持っている必要があります。
権限を付与するには、ユーザー、グループ、またはサービス アカウントにロールを付与する。
したがって、正解は以下の通りです。
- チームごとに異なるデータセットにデータをロードする
- チームのデータセットを承認されたユーザーに制限する
- 各チームのユーザーに適切なIAMロールを使用する
参照：
https://cloud.google.com/bigquery/docs/access-control
https://cloud.google.com/bigquery/docs/dataset-access-controls
</div></details>

### Q. 問題5: 不正解
ペタバイト規模の分析データがあり、そのためのストレージと処理プラットフォームを設計する必要があります。Google Cloudでデータウェアハウス形式の分析を行い、他のクラウドプロバイダーのバッチ分析ツール用にデータセットをファイルとして公開することができなければなりません。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. データセット全体をBigQueryに保存し、データの圧縮コピーをCloud Storageのバケットに保存する
4. 
<details><div>
    答え：3
説明
データウェアハウスによる分析と、圧縮データの保存を、それぞれ適切なサービスを利用して行う必要があります。
BigQueryはビジネスのアジリティに対応して設計された、サーバーレスでスケーラビリティと費用対効果に優れたマルチクラウド データ ウェアハウスです。
機械学習機能が組み込まれた、安全でスケーラブルなプラットフォームで分析情報へのアクセスを提供します。
また、柔軟性の高いマルチクラウド分析ソリューションで、クラウド全体のデータを活用しビジネス上の意思決定を強化できます。
BigQuery にデータを読み込んだ後、Cloud Storageなどに、さまざまな形式でデータをエクスポートできます。
BigQuery は最大 1 GB のデータを 1 つのファイルにエクスポートできます。
1 GB を超えるデータをエクスポートする場合は、データを複数のファイルにエクスポートする必要があります。
データを複数のファイルにエクスポートすると、さまざまなサイズのファイルになります。
したがって、正解は「データセット全体をBigQueryに保存し、データの圧縮コピーをCloud Storageのバケットに保存する」です。
参照：
https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-omni
</div></details>

### Q. 問題6: 正解
ある運送会社では、荷物追跡のライブデータをリアルタイムでApache Kafkaストリームに送信しています。このデータはBigQueryにロードされます。会社のアナリストは、BigQuery でトラッキング データを照会して、パッケージのライフサイクルにおける地理的な傾向を分析したいと考えています。このテーブルは元々、日付ごとのパーティショニングで作成されました。時間の経過とともに、クエリの処理時間が長くなりました。あなたは、BigQuery のクエリパフォーマンスを向上させる変更を実装する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. BigQuery でパッケージ・トラッキング ID カラムにクラスタリングを実装する
3. 
4. 
<details><div>
    答え：2
説明
クエリパフォーマンスを向上させるために、テーブルを再編成する必要があります。
BigQuery でクラスタ化テーブルを作成すると、テーブルのスキーマ内の 1 つ以上の列の内容に基づいてテーブルのデータが自動的に編成されます。
指定した列は、関連するデータを同じ場所に配置するために使用されます。
複数の列を使用してテーブルをクラスタ化する場合は、指定する列の順序が重要です。
指定した列の順序によって、データの並べ替え順序が決まります。
クラスタリングは、フィルタ句を使用するクエリやデータを集計するクエリなど、特定のタイプのクエリのパフォーマンスを向上させることができます。
クエリジョブまたは読み込みジョブによってデータがクラスタ化テーブルに書き込まれると、BigQuery はクラスタリング列の値を使用してデータを並べ替えます。
これらの値は、BigQuery ストレージ内の複数のブロックにデータを整理するために使用されます。
クラスタリング列に基づいてデータをフィルタする句を含むクエリを送信すると、BigQuery は並べ替えられたブロックを使用して不要なデータのスキャンを省略します。
テーブルまたはパーティションが 1 GB 未満の場合、クラスタ化テーブルと非クラスタ化テーブルとの間のクエリ パフォーマンスに大きな違いはない可能性があります。
同様に、クラスタリング列の値に基づいてデータを集計するクエリを送信すると、ブロックの並べ替えによって類似の値を持つ行が同じ場所に配置されるため、パフォーマンスが向上します。
したがって、正解は「BigQuery でパッケージ・トラッキング ID カラムにクラスタリングを実装する」です。
参照：
https://cloud.google.com/bigquery/docs/clustered-tables
</div></details>

### Q. 問題7: 正解
既存の初期化アクションを使用して、起動時にCloud Dataprocクラスタのすべてに追加の依存関係を展開する必要があります。会社のセキュリティポリシーでは、Cloud Dataprocノードがインターネットにアクセスできないようにする必要があるため、パブリック初期化アクションはリソースを取得できません。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. すべての依存関係を、VPCセキュリティ境界内のCloud Storageバケットにコピーする
4. 
<details><div>
    答え：3
説明
Dataproc クラスタを作成するときは、クラスタを設定した直後に Dataproc が Dataproc クラスタ内のすべてのノードで実行する初期化アクションとして実行可能ファイルまたはスクリプトを指定できます。
初期化アクションは、ジョブの実行時に依存関係をインストールしなくてもジョブをクラスタに送信できるよう、Python パッケージのインストールなど、ジョブの依存関係を設定するために多く用いられます。
初期化アクション スクリプトのサンプルは、次の場所にあります。
- GitHub リポジトリ
- Cloud Storage - リージョン gs://goog-dataproc-initialization-actions-<REGION> バケット内
今回はインターネットへのアクセスができないため、Cloud Storageに依存関係をコピーすることで初期化アクションを行うことができます。
したがって、正解は「すべての依存関係を、VPCセキュリティ境界内のCloud Storageバケットにコピーする」です。
参照：
https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions
</div></details>

### Q. 問題8: 不正解
あなたの会社のデータアナリストは、プロジェクトで複数のGCPサービスを使用できるようにするために、プロジェクトでCloud IAM Ownerロールを割り当てられています。会社のルールとして、すべてのBigQueryデータアクセスログを6ヶ月間保持する必要があります。あなたは、会社の監査担当者のみがすべてのプロジェクトのデータアクセスログにアクセスできるようにする必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. データアクセスログを、監査ログ用に新規作成されたプロジェクトのCloud Storageバケットに、集約されたエクスポートシンクを介してエクスポートする。エクスポートされたログを含むプロジェクトへのアクセスを制限する
<details><div>
    答え：4
説明
監査人が全てのプロジェクトのアクセスログに正しくアクセスできるような設定を行う必要があります。
集約シンクは、組織またはフォルダに含まれる Google Cloud リソースからのログエントリを結合してルーティングします。
たとえば、組織に含まれるすべてのフォルダの監査ログエントリを集約し、Cloud Storage バケットに転送できます。
集約シンク機能がないと、シンクは、シンクが作成された正確なリソース（Google Cloud プロジェクト、組織、フォルダ、請求先アカウント）からのログエントリのルーティングに限定されます。
Google Cloud のフォルダと組織に対して集約シンクを作成できます。
Cloud プロジェクトと請求先アカウントには子リソースが含まれていないため、これらのリソースの集約シンクは作成できません。
したがって、正解は「データアクセスログを、監査ログ用に新規作成されたプロジェクトのCloud Storageバケットに、集約されたエクスポートシンクを介してエクスポートする。エクスポートされたログを含むプロジェクトへのアクセスを制限する」です。
参照：
https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors
https://cloud.google.com/iam/docs/job-functions/auditing#scenario_operational_monitoring
</div></details>

### Q. 問題9: 正解
サービスの販売データを保存する新しいトランザクション・テーブルを Cloud Spanner に作成する必要があります。あなたは主キーとして何を使用するかを決定する必要があります。
パフォーマンスの観点から、どの戦略を選択すべきでしょうか？
1. 
2. 
3. ランダムな一意の識別子番号（バージョン4 UUID）
4. 
<details><div>
    答え：3
説明
Cloud Spannerのデータモデルに即した主キーを選択する必要があります。
Cloud Spanner データベースは、1 つ以上のテーブルを含むことができます。
テーブルは行、列、値という構造を持ち、主キーを備えている点で、リレーショナル データベース テーブルに似ています。
Cloud Spanner のデータは、厳格に型指定されています。
データベースごとにスキーマを定義する必要があり、そのスキーマでは各テーブルの各列のデータ型を指定する必要があります。
可能なデータ型には、スカラー型と配列型が含まれます。
また、1 つ以上のセカンダリ インデックスをテーブルに定義できます。
主キーを選択する上で重要な点は、ホットスポットを回避するという点です。
単調増加する値やエポックタイムなどは、ホットスポットを発生させる可能性があるため不適切です。
代わりに、キーのハッシュ値やUUIDなどのランダムな値を用いることは、ベストプラクティスです。
したがって、正解は「ランダムな一意の識別子番号（バージョン4 UUID）」です。
参照：
https://cloud.google.com/spanner/docs/schema-and-data-model
https://cloud.google.com/spanner/docs/schema-and-data-model#choosing_a_primary_key
https://cloud.google.com/spanner/docs/schema-and-data-model
</div></details>

### Q. 問題10: 不正解
BigQueryデータウェアハウスの主要な在庫テーブルを読み込む、ほぼリアルタイムの在庫ダッシュボードを作成する必要があります。過去の在庫データは、品目と場所ごとの在庫残高として保存されています。1時間ごとに数千件の在庫更新があります。ダッシュボードのパフォーマンスを最大化し、データの正確性を確保したいと思います。
要件を達成するためにするべきことは何ですか？
1. BigQuery の UPDATE 文を活用して、在庫残高が変化しているときに更新する
2. 
3. BigQuery ストリーミングを使用して、日次在庫移動テーブルに変更をストリーミングします。履歴在庫残高テーブルに結合するビューで残高を計算します。在庫残高表を毎晩更新します。
4. 
<details><div>
    答え：３
説明
C. 
オプション C が推奨される理由は次のとおりです。
ほぼリアルタイムのデータ:BigQuery ストリーミングを使用すると、在庫の変更が発生したときにキャプチャできるため、在庫移動表をほぼリアルタイムで最新の状態に保つことができます。これは、インベントリ ダッシュボードにとって非常に重要です。
パフォーマンス：履歴在庫残高テーブルを在庫移動テーブルから分離することで、移動データを照会するときにスキャンされるデータの量を減らすことができます。これにより、クエリのパフォーマンスが大幅に向上します。
精度：ビューでの残高の日次計算により、在庫残高テーブルに最も正確なデータが反映されます。このプロセス中に、必要なデータクレンジングと検証を実行する機会があります。
効率：在庫残高テーブルを毎晩更新することで、データを統合して最適化し、クエリのパフォーマンスにより適したものにすることができます。これは、ダッシュボードの速度を維持するのに役立ちます。
キーポイント -
ほぼリアルタイムのインベントリ ダッシュボードを作成する必要があります。
ダッシュボードは、BigQuery データ ウェアハウスのメイン インベントリ テーブルを読み取ります。
履歴在庫データは、品目および場所ごとの在庫残高として保存されます。
毎時間、数千件のインベントリーの更新があります。
ダッシュボードのパフォーマンスを可能な限り向上させ、データが正確であることを確認する必要があります。
正しくないオプション -
オプション A(BigQuery UPDATE ステートメントを使用)は、同時実行の問題が発生する可能性があるため、リアルタイム更新に適しておらず、クエリのパフォーマンスに影響を与える可能性があります。オプション B (在庫残高テーブルのパーティション分割) では、クエリのパフォーマンスは向上しますが、リアルタイムの更新やデータの正確性はオプション C ほど効果的には対応できません。
オプション D(BigQuery バルクローダーを使用)は、バッチ読み込みのシナリオに適しており、インベントリ ダッシュボードに必要なほぼリアルタイムのデータ更新を提供できない場合があります。したがって、このコンテキストでは最適な選択ではありません。

誤り説明
BigQueryのINSERT、UPDATE、DELETE、MERGEといったデータ操作言語（DML）文により、Google CloudのエンタープライズデータウェアハウスであるBigQueryに保存されているデータの追加、変更、削除を行うことができるようになります。
BigQueryのDMLは、1つのジョブでテーブル内の任意の数の行を挿入、更新、削除することをサポートしています。
2020年3月までは、DMLの１日あたりの上限がありましたが、それ以降は上限が撤廃されました。
これによって、今回のような1時間に数千回データ更新がある場合でもDMLを用いてパフォーマンスを最大化することが可能です。
したがって、正解は「BigQuery の UPDATE 文を活用して、在庫残高が変化しているときに更新する」です。
参照：
https://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery
</div></details>

### Q. 問題11: 不正解
あなたの会社では、ホリデーシーズンにリアルタイムのデータを分析してさまざまなオファーを提供する、初のダイナミックキャンペーンを実施しています。データサイエンティストは、30日間のキャンペーン期間中、毎時急速に増加するテラバイトのデータを収集しています。Google Cloud Dataflowを使用してデータを前処理し、Google Cloud Bigtableの機械学習モデルに必要な特徴量データを収集しています。チームは、初期負荷である10TBのデータの読み取りと書き込みで、パフォーマンスの低下を観察しています。コストを最小限に抑えながら、このパフォーマンスを改善したいと考えています。
要件を達成するためにするべきことは何ですか？
1. スキーマを再定義し、テーブルの多数の行に読み取りと書き込みを均等に分散させる
2. 
3. 
4. 
<details><div>
    答え：1
説明
Bigtableのパフォーマンスの問題に対するトラブルシューティングを行う必要があります。
パフォーマンスのボトルネックが発生していると思われる場合は、いくつかの項目をチェックする必要があります。
代表的なものを以下に列挙します（その他は公式ドキュメントを参照してください）。
- Key Visualizer でテーブルのスキャン結果を確認する：
Bigtable 用 Key Visualizer ツールは、クラスタ内の各テーブルを毎日スキャンし、その使用パターンを表示する。
Key Visualizer を使用すると、使用パターンが問題の原因かどうかを確認できます。
たとえば、ホットスポットになっている行や CPU の過剰使用を確認できます。
- テーブル内で多数の異なる行の読み取りと書き込みが行われていることを確認します：
Bigtable では、読み取りオペレーションと書き込みオペレーションがテーブル全体に均等に分散され、結果として、ワークロードがクラスタ内のすべてのノードに分散されるときに、最高のパフォーマンスが得られます。
読み取りオペレーションと書き込みオペレーションをすべての Bigtable ノードに分散させることができない場合は、パフォーマンスが低下します
- Bigtable の読み取りと書き込みを実行するコードをコメントアウトしてみる：
パフォーマンスの問題が解消した場合は、次善のパフォーマンスしか得られないような方法で Bigtable を使用している可能性があります。
パフォーマンスの問題が解決しない場合は、おそらく Bigtable とは無関係の問題が発生しています。
今回であれば、テーブルの多数の行で読み取りと書き込みが分散されているかを確認する必要があります。
もしも特定の行に読み取りと書き込みが集中しているとするならば、スキーマの再設計が必要になります。
したがって、正解は「スキーマを再定義し、テーブルの多数の行に読み取りと書き込みを均等に分散させる」です。
参照：
https://cloud.google.com/bigtable/docs/performance#troubleshooting
</div></details>

### Q. 問題12: 不正解
GCEのVMインスタンスにMariaDBのSQLデータベースを導入しており、モニタリングとアラートの設定が必要です。ネットワーク接続、ディスクIO、レプリケーションの状態などのメトリクスを最小限の開発労力でMariaDBから収集し、ダッシュボードやアラートにCloud Monitoringを使用したいと考えています。
要件を達成するためにするべきことは何ですか？
1. OpenCensusエージェントをインストールし、Cloud Monitoringエクスポーターでカスタムメトリック収集アプリケーションを作成する
2. 
3. 
4. 
<details><div>
    答え：1
説明
Cloud Monitoringでは、ネットワーク接続、ディスクID、レプリケーションの状態などのカスタムメトリクスはデフォルトで収集することができません。
従って、オーバーヘッドの少ない方法で、VMからカスタムメトリクスを収集するためのツールをインストールする必要があります。
Google Cloudでは、OpenCensusを使ったカスタムメトリクスの収集が推奨されています。
OpenCensus は無料のオープンソース プロジェクトで、次のことが可能になります。
- メトリクスおよびトレースデータをさまざまな言語で収集するための、ベンダーに依存しないサポートを提供できます。
- 収集したデータを、Cloud Monitoring を含むさまざまなバックエンド アプリケーションにエクスポートできます。
したがって、正解は「OpenCensusエージェントをインストールし、Cloud Monitoringエクスポーターでカスタムメトリック収集アプリケーションを作成する」です。
参照：
https://cloud.google.com/monitoring/custom-metrics/open-census
</div></details>

### Q. 問題13: 不正解
Google Cloudで新しいパイプラインを作成し、IoTデータをCloud Pub/SubからCloud Dataflowを経由してBigQueryにストリーミングしています。データをプレビューしていると、データの約2％が破損していることに気付きました。この破損データをフィルタリングするために、Cloud Dataflowパイプラインを変更する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. Cloud DataflowにParDoトランスフォームを追加し、破損した要素を破棄する
3. 
4. 
<details><div>
    答え：2
説明
破損データに対して新たに処理を追加するための効率的な方法を選択する必要があります。
Dataflowで新たに処理を追加する際は、ParDoトランスフォームの追加が有効です。
ParDo は、Apache Beam SDK のコア並列処理オペレーションです。入力 PCollection の各要素に対してユーザー指定の関数を呼び出します。
ParDo は、0 個以上の出力要素を 1 つの出力 PCollection に収集します。ParDo 変換は、要素を個別に、場合によっては並行して処理します。
これによって、データ破損が確認された場合のカスタム処理を追加することができます。
したがって、正解は「Cloud DataflowにParDoトランスフォームを追加し、破損した要素を破棄する」です。
参照：
https://cloud.google.com/dataflow/docs/concepts/beam-programming-model
https://cloud.google.com/dataflow/docs/apis?authuser=2
</div></details>

### Q. 問題14: 不正解
あなたはGoogle Cloud上で、ユーザーのブログ投稿に題名ラベルを自動生成するアプリケーションを開発しています。この機能を迅速に追加しなければならないという競争上のプレッシャーがあり、追加の開発リソースもありません。また、チーム内に機械学習の経験者はいません。
要件を達成するためにするべきことは何ですか？
1. アプリケーションからCloud Natural Language APIを呼び出する。生成されたEntity Analysisをラベルとして処理する
2. 
3. 
4. 
<details><div>
    答え：1
説明
機械学習の実装経験を持ったエンジニアがいないため、マネージドサービスを用いたラベル付けを行う必要があります。
Natural Language APIは、エンティティ分析を使用し、ドキュメント（メール、チャット、ソーシャル メディアなど）の中でフィールドを検索してラベルを付けることができます。
次に、感情分析を使用してお客様の意見を把握し、プロダクトとユーザー エクスペリエンスに関する実用的な情報を得ることができます。
したがって、正解は「アプリケーションからCloud Natural Language APIを呼び出する。生成されたEntity Analysisをラベルとして処理する」です。
参照：
https://cloud.google.com/natural-language
</div></details>

### Q. 問題15: 正解
数百万台のIoTデバイスから送信される遠隔測定データを処理するために、NoSQLデータベースを選択することになりました。データ量は年間100TBで増加しており、各データ項目には約100の属性があります。データ処理パイプラインには、ACID（atomicity, consistency, isolation, and durability）は必要ありません。一方で、高可用性と低レイテンシーが要求されます。あなたは、個々のフィールドに対してクエリを実行することによってデータを分析する必要があります。
どのデータベースがあなたの要件を満たしていますか？（3つ選択）
1. 
2. HBase
3. 
4. MongoDB
5. Cassandra
6. 
<details><div>
    答え：2,4,5
説明
数百TBにものぼる大量のデータを処理するためのデータベースを選択する必要があります。
また、IoTデバイスから送信されるデータは非構造データであるため、NoSQLデータベースを選択する必要があります。
この要件を満たすのは、HBase, MongoDB, Cassandraです。
HBaseは、Googleのビッグテーブルに似たNoSQLデータモデルで、膨大な量の構造化データへの迅速なランダムアクセスを実現するために設計されました。
Hadoop File System （HDFS）が提供するフォールトトレランスを利用しています。
HDFSはHadoopエコシステムの一部であり、Hadoop File System内のデータに対してランダムなリアルタイムリード/ライトアクセスを提供する。
HDFSには、直接またはHBaseを介してデータを格納することができます。
データ消費者は、HBaseを使用してHDFSのデータをランダムに読み取り、アクセスする。HBaseはHadoop File Systemの上に置かれ、読み取りと書き込みのアクセスを提供する。
MongoDB Atlas は、Google のグローバルにスケーラブルで信頼性の高いインフラストラクチャ上で、フルマネージド サービスを提供する。
Atlas を使用すると、UI を数回クリックするか API 呼び出しを実行するだけでデータベースを簡単に管理できます。
移行は簡単で、グローバル クラスタなどの高度な機能を備え、世界中のどこにいても低レイテンシの読み取りおよび書き込みアクセスが可能です。
Apache Cassandraは、オープンソースのNoSQL分散型データベースで、パフォーマンスを損なうことなく拡張性と高可用性を実現できます。
コモディティ・ハードウェアやクラウド・インフラ上で直線的なスケーラビリティと実証済みのフォールトトレランスを実現し、ミッションクリティカルなデータに最適なプラットフォームとなります。
したがって、正解は以下の通りです。
- HBase
- MongoDB
- Cassandra
参照：
https://cloud.google.com/dataproc/docs/concepts/components/hbase
https://cloud.google.com/mongodb
https://cloud.google.com/blog/products/databases/open-source-cassandra-now-managed-on-google-cloud
</div></details>

### Q. 問題16: 正解
あなたは、Google Cloud上でレコメンデーション・エンジンを使用するアプリケーションを開発しています。このソリューションでは、過去の視聴履歴に基づいて新しい動画を顧客に表示する必要があります。このソリューションでは、顧客が視聴したことのある動画のエンティティに対するラベルを生成する必要があります。あなたのデザインは、数TBのデータ上の他の顧客の好みからのデータに基づいて、非常に高速なフィルタリング提案を提供することができなければなりません。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. Cloud Video Intelligence APIを呼び出してラベルを生成するアプリケーションを構築する。データをCloud Bigtableに格納し、予測されたラベルをユーザーの視聴履歴に合わせてフィルタリングして好みを生成する
4. 
<details><div>
    答え：3
説明
膨大な動画データに対するラベル付けを自動的に行うソリューションが必要です。
Video Intelligence API は、LABEL_DETECTION 機能を使用して動画映像に表示されるエンティティを識別し、それらのエンティティにラベル（タグ）でアノテーションを付けることができます。
この機能は、物体、場所、活動、動物の種類、商品などを識別できます。
ラベル検出はオブジェクト トラッキングとは異なります。
オブジェクト トラッキングとは異なり、ラベル検出ではフレーム全体（境界ボックスなし）にラベルを付けます。
たとえば、踏切を通過する列車の動画では、「train」、「transportation」、「railroad crossing」などのラベルが返されます。
各ラベルには時間セグメントがあり、エンティティが検出された時点を、動画の先頭からの時間オフセット（タイムスタンプ）として示する。
各アノテーションには、その他の追加情報も含まれます。
たとえば、この中のエンティティ ID を使用すると、Google Knowledge Graph Search API でエンティティの詳細を確認できます。
したがって、正解は「Cloud Video Intelligence APIを呼び出してラベルを生成するアプリケーションを構築する。データをCloud Bigtableに格納し、予測されたラベルをユーザーの視聴履歴に合わせてフィルタリングして好みを生成する」です。
参照：
https://cloud.google.com/video-intelligence/docs/feature-label-detection
</div></details>

### Q. 問題17: 不正解
あなたは、ECサイトでユーザーに衣服を推薦するためのモデルを構築しています。ユーザーのファッションの好みは時間とともに変化することがわかっているので、新しいデータが利用可能になったときにモデルにストリーミングするためのデータパイプラインを構築します。
このデータをどのようにモデルのトレーニングに利用すればよいですか？
1. 
2. 既存のデータと新しいデータの組み合わせでモデルを継続的に再学習する
3. 
4. 
<details><div>
    答え：2
説明
データドリフトを防止するための適切な継続的な学習プロセスを選択する必要があります。
一般的に、ML ワークフローでは、特定のモデルを 1 回トレーニングしデプロイするだけでは不十分なことがよくあります。
最初はモデルの精度が望ましいレベルであっても、予測リクエストに使用されるデータが（おそらく、時間の経過に応じて）最初にモデルのトレーニングに使用したデータとまったく違うものになっている場合、精度が変わってくることがあります。
例えば、ある顧客が自分に合ったオファーを受けたときに、そのサービスを購入する可能性はどの程度あるのか、といった顧客の行動を予測しようとするモデルがあるとする。
明らかに、市場は時間とともに変化し、顧客の嗜好は変化し、競合は新たな施策を投入する。
そのため、定期的に再トレーニングを行う必要があります。このような場合、新しいデータを追加するだけでなく、以前学習に使用されていたデータも利用することが一般的に有効とされています。
したがって、正解は「既存のデータと新しいデータの組み合わせでモデルを継続的に再学習する」です。
参照：
https://cloud.google.com/blog/ja/topics/developers-practitioners/event-triggered-detection-data-drift-ml-workflows
https://qiita.com/tshowis/items/f4c1f5579079e4d264d3
</div></details>

### Q. 問題18: 不正解
あなたは、企業が経済動向を把握することを支援する経済コンサルティング会社に勤務しています。分析の一環として、Google BigQuery を使用して、パン、ガソリン、牛乳など、最も一般的に販売されている 100 種類の商品の平均価格と顧客データを関連付けています。これらの商品の平均価格は30分ごとに更新されます。このデータが常に最新であることを確認し、BigQueryの他のデータとできるだけ低コストで組み合わせられるようにしたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. データをリージョンの Google Cloud Storage バケットに保存して更新し、BigQuery で連携データソースを作成する
3. 
4. 
<details><div>
    答え：2
説明
BigQueryは巨大なデータセットのためのフルマネージドクエリサービスですが、データをBigQueryのデータセットに移動しなくても、BigQueryのデータアクセスレイヤーを使用してデータ連携をすることが可能です。
これは、BigQueryは連携型のデータアクセスモデルを備えており、Bigtable、Cloud Storage、Google Driveから直接、永続テーブルと一時テーブルを使ってデータを照会することができるからです。
複数のGoogle Cloud Platformサービスにデータがあり、データレイクやデータウェアハウス戦略を構築している場合、この機能がコスト最適化の観点からも有用になる場合があります。
今回の例であれば、BigQueryにインポート処理をする場合よりも、Cloud Storageのストレージコストの方が安くなるためコスト最適なソリューションを実現できます。
したがって、正解は「データをリージョンの Google Cloud Storage バケットに保存して更新し、BigQuery で連携データソースを作成する」です。
参照：
https://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer
</div></details>

### Q. 問題19: 正解
あなたの会社では、Bigtableを用いたデータ分析を行なっています。現在は永続ディスクにHDDを使用しています。一方で、データは日々増大し、より多くの書き込み/読み取りスループットが必要になったため、HDDからSSDに切り替えたいと考えています。
BigtableインスタンスのストレージをHDDからSSDに切り替えるには、どのような操作が望ましいでしょうか。
1. 
2. 既存のインスタンスからデータをエクスポートし、そのデータを新しいインスタンスにインポートする
3. 
4. 
<details><div>
    答え：2
説明
Bigtable インスタンスを作成した後は、そのインスタンスでの SSD ストレージまたは HDD ストレージの選択は変更できません。
Google Cloud Console を使用して、インスタンスで使用しているストレージのタイプは変更できません。
テーブルの保存場所であるストレージ タイプを変更する場合は、バックアップ機能を使用します。
バックアップの手順は以下の通りになります。
- 希望するストレージ タイプを使用するインスタンスを作成するか、インスタンスを使用するように計画します。
- テーブルのバックアップを作成します。
- バックアップから、別のインスタンスの新しいテーブルに復元します。
したがって、正解は「既存のインスタンスからデータをエクスポートし、そのデータを新しいインスタンスにインポートする」です。
参照：
https://cloud.google.com/bigtable/docs/choosing-ssd-hdd#switching
</div></details>

### Q. 問題20: 不正解
あなたはデータ処理のパイプラインを設計しています。パイプラインは、負荷の増加に応じて自動的にスケールアップできなければなりません。メッセージは少なくとも一度は処理され、それは1時間のウィンドウ内で完結されなければなりません。
どのように設計すればよいでしょうか。
1. 
2. 
3. 
4. メッセージの取り込みにはCloud Pub/Subを、ストリーミング解析にはCloud Dataflowを使用する
<details><div>
    答え：4
説明
スケーラブルでありながら1時間内に確実に処理されるようパイプラインを構築する必要があります。
Cloud Pub/SubとCloud Dataflowを使用することで、リアルタイムのイベント ストリームを取り込んで処理、分析し、価値の高い分析情報を得るためのパイプラインを構築することができます。
いずれのサービスもマネージドサービスであり、スケーラビリティが確保されているため、負荷の増加に対しても高い可用性を担保し続けます。
したがって、正解は「メッセージの取り込みにはCloud Pub/Subを、ストリーミング解析にはCloud Dataflowを使用する」です。
参照：
https://cloud.google.com/solutions/stream-analytics/
https://cloud.google.com/blog/products/data-analytics/streaming-analytics-now-simpler-more-cost-effective-cloud-dataflow
</div></details>

### Q. 問題21: 不正解
あなたは、荷物を適切に配送するために配送ライン上を移動する物流センターを持つ運送会社で働いています。この会社は、配送ラインにカメラを追加して、輸送中の荷物の視覚的な損傷を検出して追跡したいと考えています。あなたは、ダメージを受けた荷物の検出を自動化し、輸送中の荷物にリアルタイムで人間が確認できるようフラグを立てる方法を作る必要があります。
どのソリューションを選ぶべきでしょうか？
1. 
2. 画像のコーパスで AutoML モデルを学習し、そのモデルを中心に API を構築してパッケージ追跡アプリケーションと統合する
3. 
4. 
<details><div>
    答え：2
説明
運送会社が扱う荷物は人の顔などとは異なり、一般的な学習済みモデルを使用することができません。
そのため今回は、新たに固有データを教師データとしたモデルを作成する必要があります。
AutoML Vision を使用すると、ラベル付きデータからパターンを認識するようコンピュータをトレーニングする「教師あり学習」を実行できます。
教師あり学習を使用することで、関心のあるパターンやコンテンツを画像で認識するようモデルをトレーニングできます。
AutoML Vision を使用してカスタムモデルをトレーニングするには、分類する画像のラベル付きサンプル（入力）と、ML システムに予測させるカテゴリまたはラベル（回答）を提供する必要があります。
AutoML Vision のトレーニングでは、カテゴリ / ラベルごとに最低でも 100 枚の画像が必要です。
各ラベル用の高品質のサンプルが増えるにつれて、ラベルをうまく認識する可能性が高まります。
一般的には、トレーニング プロセスに取り入れるラベル付きデータが多いほど、モデル品質は向上します。
したがって、正解は「画像のコーパスで AutoML モデルを学習し、そのモデルを中心に API を構築してパッケージ追跡アプリケーションと統合する」です。
参照：
https://cloud.google.com/vision/automl/docs/beginners-guide
</div></details>

### Q. 問題22: 正解
工場からのリアルタイムセンサーデータをBigtableに流していますが、パフォーマンスが極端に低下しています。
リアルタイムのダッシュボードを作成するクエリでBigtableのパフォーマンスを向上させるには、行キーをどのように再設計すればよいでしょうか。
1. 
2. 
3. 
4. <sensorid>#<timestamp>という形式の行キーを使用する
<details><div>
    答え：4
説明
Google Cloudの推奨する行キー設計のベストプラクティスに則る必要があります。
Google Cloudは、行キーはタイムスタンプのみ、または先頭にタイムスタンプを持つことを推奨していません。
代わりに適切に定義された行の範囲を取得できる行キーを作成することを推奨しています。
今回の選択肢の中でその要件を満たす選択肢は<sensorid>#<timestamp>という行キーのみです。
したがって、正解は「<sensorid>#<timestamp>という形式の行キーを使用する」です。
参照：
https://cloud.google.com/bigtable/docs/schema-design
</div></details>

### Q. 問題23: 正解
下の図に示すようなデータがあります。2つの次元はXとYで、それぞれのドットの色はクラスを表しています。このデータを、線形アルゴリズムを使って正確に分類したいとします。そのためには、合成の特徴を加える必要があります。
特徴量の値はどのようにすればよいですか？
1. X^2+Y^2
2. 
3. 
4. 
<details><div>
    答え：1
説明
原点を中心に同心円状に同じクラスが分布しています。
つまり、XとYで円形を表現できる関数 X^2 + Y^2であれば、原点からの距離という形で線形分類が可能になります。
したがって、正解は「X^2+Y^2」です。
参照：
https://qiita.com/rennnosuke/items/fab837825b64bf50be56
http://playground.tensorflow.org
</div></details>

### Q. 問題24: 不正解
あなたは、eコマース企業のためのカート自動リセットシステムを設計しています。このシステムは、以下のルールに基づいてユーザーにメッセージを送信します。
- 60分の間、ユーザーがサイト上で何も操作していない
- 30ドル以上の商品をバスケットに入れた場合
- トランザクションを完了していない
Google Cloud Dataflow を使用してデータを処理し、メッセージを送信すべきかどうかを判断します。
パイプラインはどのように設計すればよいですか？
1. 
2. 
3. ギャップタイムを60分に設定したセッションウィンドウを使用する
4. 
<details><div>
    答え：3
説明
ランダムな時間にアクションをする各々のユーザーに対して60分の放置を検知するためのウィンドウ設計をする必要があります。
ウィンドウ関数は、個々の要素のタイムスタンプで制限なしコレクションをグループ化する。
各ウィンドウには一定数の要素が入ります。
Apache Beam SDK または Dataflow SQL ストリーミング拡張機能で次のウィンドウを設定する。
- タンブリング ウィンドウ（Apache Beam では固定ウィンドウ）
- ホッピング ウィンドウ（Apache Beam ではスライディング ウィンドウ）
- セッション ウィンドウ
タンブリング ウィンドウとは、データ ストリームを重なりなく分ける一定の時間間隔を表する。
たとえば、30 秒のタンブリング ウィンドウに設定すると、タイムスタンプ値が [0:00:00-0:00:30] の要素が最初のウィンドウに表示されます。
2 番目のウィンドウには、[0:00:30-0:01:00] のタイムスタンプ値を持つ要素が表示されます。
ホッピング ウィンドウとは、データ ストリーム内の一定の時間間隔を表する。
タンブリング ウィンドウは重なりませんが、ホッピング ウィンドウは重なることがあります。
たとえば、ホッピング ウィンドウが 30 秒ごとに開始し、1 分間のデータとウィンドウを持つ場合があります。
ホッピング ウィンドウの開始間隔はピリオドといいます。
セッション ウィンドウには、別の要素とのギャップ期間に存在する複数の要素が含まれます。
ギャップ期間とは、データ ストリームの新しいデータの間隔を表する。
ギャップ期間の後にデータを取得すると、そのデータには新しいウィンドウが割り当てられます。
たとえば、セッション ウィンドウでは、ユーザーのマウスの操作を表すデータ ストリームを分割できます。
このデータ ストリームでは、長時間アイドル状態が続き、クリックが多い期間が点在する。
セッション ウィンドウには、クリックで生成されたデータを含めることができます。
したがって、正解は「ギャップタイムを60分に設定したセッションウィンドウを使用する」です。
参照：
https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines
</div></details>

### Q. 問題25: 正解
あなたは現在、スパム分類器を学習しています。学習の結果を見たところ、トレーニングデータに対してオーバーフィッティングしていることに気がつきました。
この問題を解決するために、あなたはどのようなアクションを取ることができますか？（3つ選択）
1. 
2. 正則化の制約を増やす
3. ドロップアウトを増やす
4. 
5. 
6. 特徴量の数を減らす
<details><div>
    答え：2,3,6
説明
機械学習モデルを正しく評価するためには、学習データとは異なるデータを用いたテストでの性能を評価する必要があります。
テストでの性能のことを汎化性能といい、この性能が実問題での性能であると考えることができます。
トレーニングデータセットに対する性能とテストデータセットに対する性能に応じて、モデルの学習状況の問題は大きく２つに分類されます。
- 過剰適合（overfitting）
トレーニングデータセットに対する性能が良いが、テストデータセットに対する性能が低い状態です。
実際の問題にモデルを使用した際に、思ったような性能が得られない可能性があるため、モデルに以下のようなチューニングを加える必要があります。
-- 正則化の制約を増やす
-- ドロップアウトを増やす
-- 次元圧縮（特徴量の数を減らす）
-- （ニューラルネットワークの場合）レイヤーを減らして単純なモデルにする
-- 学習率を小さくする
- 適合不足（underfitting）
トレーニングデータセットに対する性能も、テストデータセットに対する性能も低い状態です。
モデルのチューニング等を行ってもこの状態が改善されない場合は、データセット自体を増やす必要があります。
データセットはマニュアルでのデータセットの追加や、データ拡張による追加が有効です。
また、特徴量が少なすぎることにより、現象を正しくとらえられていない可能性もあります。
この場合は、新たな特徴量を追加する、特徴量エンジニアリングによって特徴量を合成する、といった対処が必要です。
今回であれば、過剰適合を防止するために必要なアクションを取る必要があります。
したがって、正解は以下の通りです。
- 正則化を増やす
- ドロップアウトを増やす
- 特徴量の組み合わせを減らす
参照：
https://www.ydc.co.jp/column/mi/mieruka03.html
https://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6
</div></details>

### Q. 問題26: 不正解
あなたの会社はGCPとのハイブリッド展開を維持しており、匿名化された顧客データに対して分析が行われています。データはクラウドにインポートされますが、データセンターからGCP上のデータ転送サーバーに並行してアップロードすることで、ストレージを構築しています。経営陣から、毎日の転送に時間がかかりすぎるという連絡があり、問題の解決を依頼されています。あなたは、転送速度を最大化したいと考えています。
あなたはどのようなアクションを取るべきでしょうか？
1. 
2. 
3. データセンターからGCPへのネットワーク帯域幅を増やす
4. 
<details><div>
    答え：3
説明
ボトルネックを特定した上で適切なアクションを取る必要があります。
今回のケースであれば、並列アップロードを利用した転送を行っているにも関わらず、データ転送に時間がかかってしまっています。
並列アップロードはオンプレミスからクラウドへデータをアップロードする際に、既存の帯域幅を最大限利用するオプションです。
つまり、今回の場合であればそもそもネットワークの帯域幅が不十分であるということが言えます。
パブリックインターネットを利用する場合と、ダイレクトピアリングを利用する場合で、対応は異なりますが、いずれにせよネットワークの帯域幅を増やす必要があります。
したがって、正解は「データセンターからGCPへのネットワーク帯域幅を増やす」です。
参照：
https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#increasing_network_bandwidth
</div></details>

### Q. 問題27: 正解
現在、米国東部のデータセンターにオンプレミスのKafkaクラスターを1つ設置し、世界中のIoTデバイスからメッセージを取り込む役割を担っています。世界の大部分はインターネット接続が不十分なため、メッセージがエッジでバッチ処理され、一度に受信され、負荷が急増することがあります。このKafkaクラスタの状態は、管理が難しく、莫大なコストがかかるようになっています。
このシナリオに対して、Googleが推奨するクラウドネイティブアーキテクチャは何でしょうか？
1. 
2. 
3. Cloud Pub/Subに接続されたIoTゲートウェイと、Cloud Pub/Subからのメッセージを読み込んで処理するCloud Dataflow
4. 
<details><div>
    答え：3
説明
IoTデバイスから効率よくデータを取り込むためのサービスの組み合わせを選択する必要があります。
Pub/Sub は、スケーラブルで耐久性のあるイベントの取り込みおよび配信システムです。
Pub/Sub を使用すると、パブリッシャーとサブスクライバーと呼ばれるイベント プロデューサーとコンシューマーのシステムを作成できます。
パブリッシャーは、同期リモート プロシージャ コール（RPC）ではなく、イベントをブロードキャストすることによってサブスクライバーと非同期に通信します。
パブリッシャーは、イベントが処理される方法やタイミングとは無関係に、Pub/Sub サービスにイベントを送信します。
その後、Pub/Sub によって、イベントに反応する必要のあるすべてのサービスにイベントが配信されます。
IoTデバイスのデータストリーミングのようなケースでは、サブスクライバーがデータを受信するのをパブリッシャーが待たなければならない RPC を介して通信するシステムと比べて、このような非同期統合のほうがシステム全体の柔軟性と堅牢性が向上します。
Dataflow は、メッセージの重複排除、1 回限りの処理、タイムスタンプ付きイベントからのデータ ウォーターマークの生成により、Pub/Sub のスケーラブルな「最低 1 回」配信モデルを補完します。
Dataflow を使用するには、Apache Beam SDK でパイプラインを記述し、Dataflow サービスでパイプライン コードを実行します。
したがって、正解は「Cloud Pub/Subに接続されたIoTゲートウェイと、Cloud Pub/Subからのメッセージを読み込んで処理するCloud Dataflow」です。
参照：
https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub
https://cloud.google.com/pubsub
</div></details>

### Q. 問題28: 不正解
あなたが管理するインフラストラクチャには、YouTubeのチャンネルが含まれています。あなたは、YouTube チャンネル データを Google Cloud に送信して分析するプロセスを作成するよう命じられました。あなたは、世界中のマーケティングチームが、最新の YouTube チャンネル ログ データに対して ANSI SQL およびその他の種類の分析を実行できるようなソリューションを設計したいと考えています。
Google Cloud へのログ データ転送をどのように設定すればよいでしょうか。
1. Storage Transfer Serviceを利用して、オフサイトバックアップファイルを宛先としてCloud Storage Multi-Regional storage bucketに転送する
2. 
3. 
4. 
<details><div>
    答え：1
説明
Google Cloudへの適切なデータ転送方法を選択し、Cloud Storageのバケットロケーションを選択する必要があります。
Cloud Storageへのデータ転送の場合は、Storage Transfer Serviceが適切な選択肢です。
Storage Transfer Serviceは、クラウドまたはオンプレミスのソースからデータを転送する安全で低コストのサービスです。
Storage Transfre Serviceは、コードを 1 行も書かずに転送を完了することができるだけでなく、転送ステータスをモニタリングする一元化されたジョブ管理を利用できます。
Cloud Storageのバケットロケーションは、今回の要件を踏まえるとマルチリージョンが最適です。
マルチリージョンは、米国などの、2 つ以上の地理的な場所を含む広い地理的なエリアとなります。
マルチリージョンまたはデュアルリージョンに保存されるオブジェクトは、地理的に冗長になります。
これによって、Cloud Storageに保存されているデータの処理に対してグローバルで高いパフォーマンスを実現することができます。
したがって、正解は「Storage Transfer Serviceを利用して、オフサイトバックアップファイルを宛先としてCloud Storage Multi-Regional storage bucketに転送する」です。
参照：
https://cloud.google.com/storage-transfer-service
https://cloud.google.com/bigquery-transfer/docs/introduction
https://cloud.google.com/storage/docs/locations
</div></details>

### Q. 問題29: 正解
倉庫内の温度データを収集するために、新たに1万台のIoTデバイスを世界中に配備しています。これらの大規模なデータをリアルタイムで処理、保存、分析する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. データをGoogle Cloud Pub/Subに送信し、Cloud Pub/SubをGoogle Cloud Dataflowにストリーミングして、Google BigQueryにデータを保存する
3. 
4. 
<details><div>
    答え：2
説明
デバイスが分散し、常時接続されている今日の世界では、多くのビジネス業務において、バッチ処理（通常は過去のデータに対する一括処理）とリアルタイムの両方で大量のメッセージを受信し、処理することが必要とされています。
Google Cloud Platform（GCP）は、このような種類のワークロードに適した総合的なサービス・エコシステムを提供する。
今回のケースでは、データ収集はメッセージングミドルウェアのCloud Pub/Subで行い、データ蓄積をBigQueryで行うことが有効です。
その際、Pub/SubのトピックはDataflowに連携することで、シームレスにBigQueryに蓄積することができます。
したがって、正解は「データをGoogle Cloud Pub/Subに送信し、Cloud Pub/SubをGoogle Cloud Dataflowにストリーミングして、Google BigQueryにデータを保存する」です。
参照：
https://cloud.google.com/blog/products/iot-devices/quick-and-easy-way-set-end-end-iot-solution-google-cloud-platform
</div></details>

### Q. 問題30: 正解
あなたはデータ分析アプリケーションのためのストリーミングパイプラインを構築しています。現在、上流のデータの仕様変更のために、ジョブを再構築する必要性が出てきました。あなたはジョブをキャンセルし、新しいパイプラインでデータを処理したいと考えています。この際、現在インフライトのデータが処理され、出力に書き込まれることを確実にしたいと考えています。
Dataflow監視コンソールで、パイプラインジョブを停止するために使用できるコマンドは次のうちどれですか？
1. 
2. ドレイン
3. 
4. 
<details><div>
    答え：2
説明
実行中のパイプラインの切り替えにあたり、すでにインフライトのデータについては確実に処理する必要があります。
Dataflowジョブは、二つの方法で停止することが可能です。
- ジョブをキャンセルする：
この方法は、ストリーミング パイプラインとバッチ パイプラインの両方に適用されます。ジョブをキャンセルすると、Dataflow サービスはバッファデータなどのデータの処理を停止します。
- ジョブをドレインする：
この方法は、ストリーミング パイプラインにのみ適用されます。ジョブをドレインすると、Dataflow サービスはバッファ内のデータの処理を完了すると同時に、新しいデータの取り込みを中止できます。
今回の場合は、Drainオプションを使用することで、バッファ内のデータ処理を確実に終了し、新しいパイプラインに切り替えることが可能です。
なお、パイプラインのウィンドウ処理に対する変更は、固定時間ウィンドウやスライド時間ウィンドウの長さの変更など、小規模にとどめることが推奨されています。
ウィンドウ処理アルゴリズムの変更など、ウィンドウ処理やトリガーに大きな変更を加えると、パイプライン出力に予期しない結果が生じることがあります。
したがって、正解は「ドレイン」です。
参照：
https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline
https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Mapping
https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing
</div></details>

### Q. 問題31: 正解
数百万件の機密性の高い患者記録をオンプレミスのリレーショナルデータベースからBigQueryにコピーする必要があります。データベースの合計サイズは10 TBです。あなたは安全で時間効率の良いソリューションを設計する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. データベースからレコードを Avro ファイルとしてエクスポートする。このファイルを Transfer Appliance にコピーして Google に送信し、GCP コンソールの BigQuery Web UI を使用して Avro ファイルを BigQuery にロードする
3. 
4. 
<details><div>
    答え：2
説明
大量の機密性の高い構造データをオンプレミスからBigQueryにロードする必要があります。
ポイントは、エクスポートするファイル形式と、転送方法です。
10TBの構造データはAvroファイルとしてエクスポートすることが最適です。
Apache Avro はデータがバイナリエンコードされる、軽量で柔軟なデータフォーマットです。
BigQuery や Apache Kafka などのビッグデータ処理基盤でしばしば使用されるフォーマットで、データ構造をリッチに表現することができ、スキーマ付きのファイルのサポートもされています。
大容量データのセキュアな転送は、Transfer Applianceが最適です。
Transfer Appliance は大容量のストレージ デバイスであり、データを Google アップロード施設にデータの転送と安全な配送をし、そこからデータは Cloud Storage またはBigQueryに移行することができます。
データは、アップロード中、Google のデータセンターへの転送中、および Cloud Storage へのアップロード後に暗号化されます。
したがって、正解は「データベースからレコードを Avro ファイルとしてエクスポートする。このファイルを Transfer Appliance にコピーして Google に送信し、GCP コンソールの BigQuery Web UI を使用して Avro ファイルを BigQuery にロードする」です。
参照：
https://cloud.google.com/transfer-appliance/docs/2.0/overview
https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets
</div></details>

### Q. 問題32: 不正解
Cloud Dataprep を使用して、BigQuery テーブル内のデータのサンプルにレシピを作成しました。このレシピを、実行時間が変動するロードジョブが完了した後、同じスキーマのデータを毎日アップロードする際に再利用したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. Cloud Dataprep ジョブを Cloud Dataflow テンプレートとしてエクスポートし、Cloud Composer ジョブに組み込む
<details><div>
    答え：4
説明
Dataprepの機能を利用して、スキーマのデータを再利用する方法を選択する必要があります。
Dataprepは、オーケストレーションAPIを公開しているため、スケジューラーやCloud Composerなどのオーケストレーションソリューション内でCloud Dataprepを統合することができます。
これにより、他のツールに直接統合することで、Cloud Dataflowテンプレート以外の自動化を拡大し、分析やAI/MLイニシアティブのための反復可能なデータパイプラインを作成し、時間の節約と信頼性を高めることができます。
さらに、このAPIでは、Cloud Dataflowテンプレートでは利用できないCloud Dataprep変数またはパラメータを使用して、動的な入出力を使用できます。
その結果、1つのCloud Dataprepフローを再利用して、実行時に評価されるさまざまな入出力値で実行することができます。
したがって、正解は「Cloud Dataprep ジョブを Cloud Dataflow テンプレートとしてエクスポートし、Cloud Composer ジョブに組み込む」です。
参照：
https://cloud.google.com/blog/products/data-analytics/how-to-orchestrate-cloud-dataprep-jobs-using-cloud-composer
</div></details>

### Q. 問題33: 不正解
社内の IT アプリケーションの 1 つと Google BigQuery を統合し、ユーザーがアプリケーションのインターフェイスから BigQuery にクエリを実行できるようにします。個々のユーザーが BigQuery を認証することはなく、データセットへのアクセス権も与えたくありません。IT アプリケーションから BigQuery に安全にアクセスする必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. サービスアカウントを作成し、そのアカウントにデータセットへのアクセス権を付与する。データセットへのアクセスには、そのサービスアカウントの秘密鍵を使用する
4. 
<details><div>
    答え：3
説明
アクセスコントロールをGoogle Cloudのベストプラクティスに則り実装する必要があります。
Google Cloudは、アプリケーションなどが別のサービスを利用する際にサービスアカウントの利用を推奨しています。
サービス アカウントは特別なタイプの Google アカウントで、Google API のデータにアクセスして認証を受ける必要がある人間以外のユーザーを表する。
通常、サービス アカウントは次のような場合に使用されます。
- 仮想マシン（VM）でのワークロードの実行。
- Google API を呼び出すオンプレミスのワークステーションまたはデータセンターでのワークロードの実行。
- 1 人のユーザーのライフサイクルに結び付けられていないワークロードの実行。
アプリケーションは Google API の呼び出しにサービス アカウントの ID を使用するため、ユーザーは直接関与しません。
したがって、正解は「サービスアカウントを作成し、そのアカウントにデータセットへのアクセス権を付与する。データセットへのアクセスには、そのサービスアカウントの秘密鍵を使用する」です。
参照：
https://cloud.google.com/iam/docs/understanding-service-accounts
</div></details>

### Q. 問題34: 不正解
あなたは、40万人以上の従業員を抱える大手ファーストフードレストランチェーンで働いています。Google BigQuery で従業員情報を FirstName フィールドと LastName フィールドで構成される Users テーブルに保存しています。IT部門のメンバーがアプリケーションを構築しており、BigQueryのスキーマとデータを変更して、アプリケーションが各従業員のFirstNameフィールドの値にスペースを連結し、その後にLastNameフィールドの値を加えたFullNameフィールドを照会できるようにしてほしいと依頼されました。
コストを最小限に抑えながら、そのデータを利用できるようにするにはどうすればよいでしょうか。
1. 
2. Users テーブルに FullName という新しい列を追加する。UPDATE ステートメントを実行して、各ユーザーの FullName カラムを FirstName と LastName の値の連結で更新する
3. 
4. 
<details><div>
    答え：2
説明
コスト最適という観点で、正しい選択をする必要があります。
注意が必要な点は、ビュー機能は都度都度クエリを実行するためコストがかかるという点です。
これを回避するためにはマテリアライズドビューといった機能を利用すべきです（選択肢には含まれていません）。
今回の例であれば、DMLにおいてUPDATEステートメントを実行しすることが最適です。
BigQueryのINSERT、UPDATE、DELETE、MERGEといったデータ操作言語（DML）文により、Google CloudのエンタープライズデータウェアハウスであるBigQueryに保存されているデータの追加、変更、削除を行うことができるようになります。
BigQueryのDMLは、1つのジョブでテーブル内の任意の数の行を挿入、更新、削除することをサポートしています。
DMLはコストがかからないため、常にフルネームの問い合わせを行うことができるコスト最適な方法です。
したがって、正解は「Users テーブルに FullName という新しい列を追加する。UPDATE ステートメントを実行して、各ユーザーの FullName カラムを FirstName と LastName の値の連結で更新する」です。
参照：
https://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery
</div></details>

### Q. 問題35: 不正解
ETL ジョブを BigQuery 上で実行するように移行した後、移行したジョブの出力が元のジョブの出力と同じであることを検証する必要があります。あなたは元のジョブの出力を含むテーブルをロードし、その内容を移行したジョブの出力と比較して、両者が同一であることを確認したいと考えています。一方で、このテーブルには、比較のために結合できるような主キー列がありません。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. Dataproc クラスタと BigQuery Hadoop コネクタを使用して、各テーブルからデータを読み取り、ソート後のテーブルの非タイムスタンプ列からハッシュを計算する。各テーブルのハッシュを比較する
4. 
<details><div>
    答え：3
説明
両者が同一であることを確かめるためには、全てのデータを比較することが必要で、サンプリングによる比較では不十分です。
今回は主キー列がないため、各テーブルのレコードのハッシュを比較することが有効です。
また、データの読み取りにはBigQuery コネクタの利用が有効です。
BigQuery コネクタは、Spark アプリケーションと Hadoop アプリケーションが BigQuery から取得したデータを処理し、ネイティブの用語を使用してデータを BigQuery に書き込むことができるようにするライブラリです。
BigQuery コネクタを使用することで、BigQuery へのプログラマティックな読み取り / 書き込みアクセスが可能になります。
したがって、正解は「Dataproc クラスタと BigQuery Hadoop コネクタを使用して、各テーブルからデータを読み取り、ソート後のテーブルの非タイムスタンプ列からハッシュを計算する。各テーブルのハッシュを比較する」です。
参照：
https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery
</div></details>

### Q. 問題36: 不正解
数日かけてカンマ区切りの値（CSV）ファイルからGoogle BigQueryテーブルCLICK_STREAMにデータをロードしました。 DT列 には、クリック イベントのエポック タイムが格納されています。便宜上、すべてのフィールドがSTRING型として扱われるシンプルなスキーマを選択しました。ここで、サイトを訪れたユーザーのウェブセッションの継続時間を計算したいので、データタイプをTIMESTAMPに変更したいとします。将来のクエリの計算コストを高くすることなく、移行作業を最小限に抑えたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. テーブルCLICK_STREAMのすべての行を返すクエリを構築し、組み込み関数を使用して列DTからの文字列をTIMESTAMP値にキャストする。このクエリを、列TSがTIMESTAMP型である宛先テーブルNEW_CLICK_STREAMに実行する。今後はCLICK_STREAMテーブルではなく、NEW_CLICK_STREAMテーブルを参照する。今後、新しいデータはテーブルNEW_CLICK_STREAMに読み込まれる
4. 
<details><div>
    答え：3
説明
列のデータ型の変更は、Cloud Console、bq コマンドライン ツール、API で行うことができません。
列の新しいデータ型を指定するスキーマを適用してテーブルを更新しようとすると、次のようなエラーが返されます。
BigQuery error in update operation: Provided Schema does not match Table project_id:dataset.table.
そのため、手動でのデータ型変更を行う必要があります。
方法としては、SQLクエリを使用する方法と、テーブルの再作成をする方法があり、後者の方がより少ないコストで実行できます。
新しいテーブルにデータをエクスポートして読み込むためのエクスポート ジョブまたは読み込みジョブには料金がかかりません。
現在、BigQuery の読み込みジョブとエクスポート ジョブは無料です。
読み込みジョブを使用して元のテーブルを上書きする場合、発生するストレージ費用は 2 つではなく 1 つのテーブル分ですが、元のデータは失われます。
したがって、正解は「テーブルCLICK_STREAMのすべての行を返すクエリを構築し、組み込み関数を使用して列DTからの文字列をTIMESTAMP値にキャストする。このクエリを、列TSがTIMESTAMP型である宛先テーブルNEW_CLICK_STREAMに実行する。今後はCLICK_STREAMテーブルではなく、NEW_CLICK_STREAMテーブルを参照する。今後、新しいデータはテーブルNEW_CLICK_STREAMに読み込まれる」です。
参照：
https://cloud.google.com/bigquery/docs/manually-changing-schemas#changing_a_columns_data_type
</div></details>

### Q. 問題37: 正解
あなたは銀行に勤務しており、融資申請者のデフォルト率を予測するモデルを学習するように頼まれました。データセットは、すでに許可された融資申請と、その申請が不履行になったかどうかの情報が含まれたものが含まれています。
あなたは何をすべきでしょうか？
1. 
2. クレジット・デフォルト・リスク・スコアを予測するために線形回帰を学習する
3. 
4. 
<details><div>
    答え：2
説明
今回作成するべきモデルは、融資申請者がデフォルトになりうる可能性を出力するモデルです。
つまり、分類モデルではなく回帰モデルです。
今回与えられているデータは、融資申請を通過したデータで、その中にデフォルトになった申請も含まれているため、教師データとしては既に充足しています。
つまり、与えられたデータを基に回帰モデルを構築することが今回すべきことになります。
したがって、正解は「クレジット・デフォルト・リスク・スコアを予測するために線形回帰を学習する」です。
参照：
https://aiacademy.jp/texts/show/?id=140
</div></details>

### Q. 問題38: 不正解
BigQueryに保存されているデータがあります。BigQuery データセット内のデータは、高可用性が求められます。コストを最小限に抑えたこのデータのストレージ、バックアップ、リカバリー戦略を定義する必要があります。
BigQuery テーブルをどのように構成するべきでしょうか。
1. 
2. 
3. BigQuery データセットをマルチリージョナルに設定する。緊急時には、ポイントインタイムのスナップショットを使用してデータを復元する
4. 
<details><div>
    答え：3
説明
BigQueryの高可用性を実現するための適切な方法を選択する必要があります。
データセットの作成時に BigQuery データを保存するロケーションを指定します。
データセットを作成した後、ロケーションを変更することはできませんが、データセットを別のロケーションにコピーするか、手動でデータセットを別のロケーションに移動（再作成）することができます。
これによって、あるリージョンで障害が発生した際も、高可用性と耐久性の両方を実現します。
また、BigQuery は、バックアップと障害復旧をサービスレベルで行います。
テーブルに対する変更について、7 日間の完全な履歴を保持することで、BigQuery では、FROM 句でテーブル デコレータまたは SYSTEM_TIME AS OF を使用して、データの特定の時点のスナップショットをクエリできます。
これによって、コストを最小限に抑えつつ、バックアップからの復元をリクエストせずに、変更を簡単に元に戻すことができます（テーブルが明示的に削除されると、その履歴は 7 日後にフラッシュされます）。
したがって、正解は「BigQuery データセットをマルチリージョナルに設定する。緊急時には、ポイントインタイムのスナップショットを使用してデータを復元する」です。
参照：
https://cloud.google.com/bigquery/docs/locations
https://cloud.google.com/solutions/bigquery-data-warehouse#backup-and-recovery
</div></details>

### Q. 問題39: 不正解
あなたは現在ニューラルネットワークモデルの学習を行なっていますが、完了するまでに何日もかかっています。そのため、あなたは学習速度を上げたいと思っています。
要件を達成するためにするべきことは何ですか？
1. 
2. トレーニングデータセットのサブサンプルを作成する
3. 
4. 
<details><div>
    答え：2
説明
学習にはトレーニングデータが使用されているため、トレーニングデータに関する処理を行う必要があります。
サブサンプリングとは主に不均衡データに対して用いられるデータ処理手法で、特定のクラスのデータから新しいサブセットを生成する処理です。
これによって、特定のクラスのデータセットが減少し、学習パフォーマンスが向上する。
したがって、正解は「トレーニングデータセットのサブサンプルを作成する」です。
参照：
https://qiita.com/msekino/items/390069cad04595107ee4
https://ichi.pro/tokeiteki-gakushu-de-ta-no-sanpuringu-to-risanpuringu-162315950189496
</div></details>

### Q. 問題40: 正解
あなたは、金融市場のデータを消費者と共有するアプリケーションを構築しており、消費者はデータフィードを受信します。データは市場からリアルタイムで収集されます。
消費者は以下の方法でデータを受け取ります。
- リアルタイムのイベントストリーム
- リアルタイムストリームおよび履歴データへの ANSI SQL アクセス
- バッチ式の履歴エクスポート
あなたは要件を満たすためにどのソリューションを使うべきか？
1. 
2. Cloud Pub/Sub、Cloud Storage、BigQuery
3. 
4. 
<details><div>
    答え：2
説明
リアルタイムのイベントを処理し、蓄積されたデータはSQLアクセスやバッチによるエクスポートに対応する必要があります。
今回のようなストリーミング分析においては、ストリーミングデータの取り込みにCloud Pub/Subを使用することで、柔軟性の高いスケーラブルなパイプラインを構築することができます。
また、Cloud Pub/Subで収集したデータについては、BigQuery のストリーミング API を使用すれば、SQL ベースの分析用に、毎秒数百万ものイベントをデータ ウェアハウスに直接ストリーミングできます。
履歴データのエクスポート先についてはCloud Storageがコスト最適なソリューションになります。
Cloud StorageのデータはBigQueryによってSQLクエリをかけることも可能です。
したがって、正解は「Cloud Pub/Sub、Cloud Storage、BigQuery」です。
参照：
https://cloud.google.com/solutions/stream-analytics/
</div></details>

### Q. 問題41: 正解
あなたは、GoogleのDataflow SDKを使用してソフトウェアアプリケーションを開発しています。条件付きループやforループなどの複雑なプログラミング構造を使用して、分岐パイプラインを作成したいと考えています。
データ処理作業にはどのコンポーネントを使用するべきですか？
1. 
2. トランスフォーム
3. 
4. 
<details><div>
    答え：2
説明
Dataflowは、サーバーレスかつ高速で、費用対効果の高い、統合されたストリーム データ処理とバッチデータ処理サービスです。
Dataflow を使用すると、データ転送のレイテンシを抑えた、高速で簡素化されたストリーミング データ パイプライン開発が可能になります。
SDKではApache Beam SDK を使用することで、OSS コミュニティドリブンの開発を行うことができます。
Apache Beam は、バッチとストリーミングの両方のデータの並列処理パイプラインを定義するオープンソースの統合モデルです。
Apache Beam プログラミング モデルは、大規模なデータ処理の構造を単純化します。
Apache Beam SDK の 1 つを使用して、パイプラインを定義するプログラムを構築します。
次いで、Dataflow などの Apache Beam がサポートする分散処理バックエンドの 1 つがパイプラインを実行します。
このモデルのおかげで、並列処理の物理的なオーケストレーションではなく、データ処理ジョブの論理的な構成に集中できます。
トランスフォームは、データを変換する処理オペレーションを表します。
トランスフォームは、1 つ以上の PCollection を入力として受け取り、そのコレクションの各要素に対して指定したオペレーションを実行して、1 つ以上の PCollection を出力として生成します。
トランスフォームでは、ほとんどすべての処理オペレーションを実行できます。
これには、データに対する算術演算の実行、データの書式の変換、データのグループ化、データの読み取りと書き込み、必要な要素のみを出力するためのデータのフィルタ処理、1 つの値へのデータ要素の結合が含まれます。
したがって、正解は「トランスフォーム」です。
参照：
https://cloud.google.com/dataflow/model/programming-model#transforms
</div></details>

### Q. 問題42: 不正解
あなたは、それぞれが異なるサプライヤーから最大750種類の異なる部品を調達する製造会社で働いています。あなたは、それぞれの部品について平均1000個のラベル付きデータセットを収集しました。あなたのチームは、倉庫作業員が部品の写真に基づいて入荷した部品を認識できるようなアプリを導入したいと考えています。このアプリの最初の実用版を（PoCとして）数営業日以内に実装したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. Cloud Vision AutoMLを使用するが、データセットを減らす
3. 
4. 
<details><div>
    答え：2
説明
数日間という短い期間で実用に耐えうるベースラインモデルを作成する必要があります。
AutoML を使用すると、機械学習の専門知識が限られていても、ビジネスニーズに合った高品質のモデルをトレーニングできます。
また、数分で独自のカスタム機械学習モデルを構築することができます。
Cloud Vision AutoML は、機械学習を使用して画像データのコンテンツを分析します。
Cloud Vision AutoML を使用すると、画像データを分類する ML モデルをトレーニングできます。
また、画像データ内のオブジェクトを検索することもできます。
分類モデルは、画像データを分析し、画像に適用されるコンテンツ カテゴリのリストを返します。
たとえば、猫を含むかどうかで画像を分類するモデルをトレーニングすることも、犬の品種で犬の画像を分類するようにモデルをトレーニングすることもできます。
AutoMLでは、最低でもラベルあたり100枚の画像が必要で、学習させる画像に応じてコストとトレーニング時間が増加します。
今回はPoCであるため、1000枚のデータを学習させる必要はなく、必要最低限の枚数までデータを減らすことが適切です。
したがって、正解は「Cloud Vision AutoMLを使用するが、データセットを減らす」です。
参照：
https://cloud.google.com/automl/
https://cloud.google.com/vertex-ai/docs/start/automl-model-types#image
https://www.youtube.com/watch?v=GbLQE2C181U
</div></details>

### Q. 問題43: 不正解
スタートアップ企業は、正式なセキュリティポリシーを導入していません。現在、社内の誰もがGoogle BigQueryに保存されているデータセットにアクセスできます。各チームはサービスを自由に利用しており、ユースケースを文書化していません。あなたはこの状況を改善するために、データウェアハウスのセキュリティ確保を依頼されました。そのために、各チームの利用状況を把握する必要があります。
あなたはまず何をすべきでしょうか？
1. Google Cloud Monitoring 監査ログ を使用してデータアクセスを確認する
2. 
3. 
4. 
<details><div>
    答え：1
説明
各チームの利用状況を確認するためには、監査ログを利用することが有効です。
Cloud 監査ログ は Google Cloud が提供するログの集まりで、Google Cloud サービスの使用に関連する運用上の懸念事項を把握することができます。
したがって、正解は「Google Cloud Monitoring 監査ログ を使用してデータアクセスを確認する」です。
参照：
https://cloud.google.com/bigquery/docs/reference/auditlogs/#mapping-audit-entries-to-log-streams
https://cloud.google.com/bigquery/docs/monitoring#slots-available
</div></details>

### Q. 問題44: 不正解
あなたの会社ではCloud Storageに、アプリケーションの過去データを保存しています。あなたはこれらのデータに対して分析を行う必要があります。プログラミングやSQLの知識を必要とせずに、無効なデータエントリを検出し、データ変換を実行するソリューションを使用したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. レシピ付きのCloud Dataprepを使用して、エラーを検出し、変換を実行する
3. 
4. 
<details><div>
    答え：2
説明
プログラミングやSQLのコーディングを行わない分析を実現するサービスを選択する必要があります。
Cloud Dataprep by Trifacta は、分析、レポート、機械学習に使用する構造化データと非構造化データを視覚的に探索、クリーニング、準備できるインテリジェント データ サービスです。
Dataprep はサーバーレスで、規模に関係なく稼働します。
デプロイや管理が必要なインフラストラクチャはありません。
また、理想的なデータ変換操作が UI 入力のたびに提案、予測されるため、コードを書く必要がありません。
したがって、正解は「レシピ付きのCloud Dataprepを使用して、エラーを検出し、変換を実行する」です。
参照：
https://cloud.google.com/dataprep/
</div></details>

### Q. 問題45: 不正解
タイムスタンプとIDカラムのWHERE句を使用してBigQueryテーブルをフィルタリングするクエリがあります。bq query "" -dry_runを使用すると、timestampとIDのフィルタが全体のデータのごく一部を選択しているにもかかわらず、このクエリがテーブルのフルスキャンをトリガーすることが分かっています。あなたは、既存のSQLクエリに最小限の変更を加えるだけで、BigQueryによってスキャンされるデータ量を減らしたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. パーティショニング・カラムとクラスタリング・カラムを持つテーブルを再作成する
4. 
<details><div>
    答え：3
説明
BigQueryのベストプラクティスに則りコスト最適化を行う必要があります。
クラスタリングとパーティショニングにより、クエリで処理されるデータの量を削減できます。
クラスタ化テーブルまたはパーティション分割テーブルをクエリする際にスキャンされるパーティション数を制限するには、述部フィルタを使用します。
クラスタ化テーブルにクエリを実行する場合、そのクエリにクラスタ化された列のフィルタが含まれていると、BigQuery はフィルタ式とブロック メタデータを使用して、クエリでスキャンされるブロックをプルーニングします。
パーティション分割テーブルをクエリすると、パーティショニング列のフィルタがパーティションのプルーニングに使用され、クエリの費用を抑えることができます。
したがって、正解は「パーティショニング・カラムとクラスタリング・カラムを持つテーブルを再作成する」です。
参照：
https://cloud.google.com/bigquery/docs/best-practices-costs</div></details>

### Q. 問題46: 不正解
あなたは、3つのクリニックの数百人の患者をカバーするパイロットプロジェクトとして、患者記録用のデータベースを設計しました。あなたの設計では、すべての患者とその診察を表すために単一のデータベーステーブルを使用し、レポートを生成するために自己結合を使用しました。サーバーのリソース使用率は50%でした。その後、プロジェクトの範囲が拡大し、データベースには100倍の患者レコードを保存しなければならなくなりました。この時点で、レポートの実行には時間がかかりすぎたり、計算リソースが不足してエラーが発生したりするため、実行ができなくなってしまいました。
データベースの設計をどのように調整すればよいでしょうか。
1. 
2. 
3. マスターの患者記録テーブルを患者テーブルと訪問者テーブルに正規化し、自己結合を避けるために他の必要なテーブルを作成する
4. 
<details><div>
    答え：3
説明
今回のケースでは、自己結合を使ったことでDBに負荷がかかっています。
通常、自己結合は、行依存の関係をコンピューティングするために使用する。
自己結合を使用すると、出力行の数が 2 乗倍になる可能性があります。
このように出力データが増加した場合は、パフォーマンスが低下することがあります。
必要に応じて別のテーブルを作成することで、必要最低限の労力で、負荷を軽減することができます。
したがって、正解は「マスターの患者記録テーブルを患者テーブルと訪問者テーブルに正規化し、自己結合を避けるために他の必要なテーブルを作成する」です。
参照：
https://cloud.google.com/bigquery/docs/best-practices-performance-patterns
https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#explicit-alias-visibility
</div></details>

### Q. 問題47: 不正解
Google Data Studio 360 で、大規模なチームのための重要なレポートを作成します。このレポートでは、データ ソースとして Google BigQuery を使用しています。ビジュアライゼーションで、1時間以内のデータが表示されないことに気づきました。
どうすればこの問題を解決できますか？
1. レポート設定を編集してキャッシュを無効にする
2. 
3. 
4. 
<details><div>
    答え：1
説明
最新のデータが更新されないという点で、キャッシュの影響だと推察できます。
キャッシュとは、データの一時的な保存システムのことである。
キャッシュされたデータの取得は、基礎となるデータセットから直接取得するよりもはるかに速く、送信されるクエリー数を減らすのに役立ち、有料のデータアクセスにかかるコストを最小限に抑えることができます。
BigQueryはクエリをかけた回数に応じて課金されるため、キャッシュを活用することはコスト最適化につながります。
一方で、最新のデータを確認したいという要望が出た場合はこのキャッシュを手動でクリアする必要があります。
したがって、正解は「レポート設定を編集してキャッシュを無効にする」です。
参照：
https://support.google.com/datastudio/answer/7020039?hl=en
</div></details>

### Q. 問題48: 不正解
Cloud Storageにデータをアーカイブしたいと考えています。一部のデータは非常に機密性が高いため、「TNO（Trust No One）」アプローチを用いてデータを暗号化し、クラウド事業者のスタッフがデータを解読できないようにしたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. boto構成ファイルに顧客提供の暗号化キー（CSEK）を指定する。gsutil cp を使用して、各アーカイブファイルを Cloud Storage バケットにアップロードする。セキュリティチームのみがアクセスできる別のプロジェクトにCSEKを保存する
<details><div>
    答え：4
説明
Googleが推奨する方法で、TNOアプローチに基づくデータの暗号化を行う必要があります。
Trust no oneの設計思想は、暗号化のための鍵は常に、それを適用するユーザーの手の中にあるべきであり、またあり続けることを必要としています。
これは、暗号化されたデータに外部の人間がアクセスできないことを意味します（暗号化が十分に強力であることが前提です）。
boto 構成ファイルには、gsutil の動作を制御する値が含まれています。
たとえば、gsutil が優先的に使用する API を決定する prefer_api 変数などです。boto 構成ファイルの変数は、構成ファイルを直接編集することで変更できます。
大半のユーザーはこれらの変数を編集する必要はありませんが、編集の必要が生じる主な理由には以下のようなものがあります。
- プロキシ経由で使用できるように gsutil を設定する。
- 顧客管理または顧客指定の暗号鍵を使用する。
- 独自の運用に合わせて gsutil の動作を全体的にカスタマイズする。
したがって、正解は「boto構成ファイルに顧客提供の暗号化キー（CSEK）を指定する。gsutil cp を使用して、各アーカイブファイルを Cloud Storage バケットにアップロードする。セキュリティチームのみがアクセスできる別のプロジェクトにCSEKを保存する」です。
参照：
https://cloud.google.com/storage/docs/boto-gsutil
</div></details>

### Q. 問題49: 正解
次のような要件を持つ新規プロジェクト用のデータベースを選択する必要があります。
- フルマネージド
- 自動的にスケールアップすることができる
- トランザクションに一貫性がある
- 6 TB までのスケールアップが可能であること
- SQL を使ってクエリを実行できる
どのデータベースを選びますか？
1. Cloud SQL
2. 
3. 
4. 
<details><div>
    答え：1
説明
フルマネージド型で自動スケールアップが可能なリレーショナルデータベースサービスを選ぶ必要があります。
Cloud SQL は、Google Cloud Platform 上のリレーショナル データベースの設定、維持、運用、管理を支援するフルマネージドのデータベース サービスです。
Cloud SQL は、MySQL、PostgreSQL、または SQL Server で使用できます。
Cloud SQLは、最大64個のプロセッサーコアと400GB以上のRAMを追加することで簡単にスケールアップでき、最大30TBのストレージをサポートします。
さらに、Cloud SQLは、ストレージの容量が限界に近づくと、自動的にスケールアップすることができます。
今回の要件の全てをCloud SQLは満たすことができます。
したがって、正解は「Cloud SQL」です。
参照：
https://cloud.google.com/sql/docs
</div></details>

### Q. 問題50: 正解
2TBのリレーショナル・データベースをGoogle Cloud Platformに移行する必要があります。このデータベースを使用しているアプリケーションを大幅にリファクタリングするためのリソースはなく、運用コストを第一に考えています。
データの保存と提供のために、どのサービスを選択しますか？
1. 
2. 
3. 
4. Cloud SQL
<details><div>
    答え：4
説明
リファクタリングを行うリソースがなく、運用コストを最小化する必要があるため、フルマネージドのリレーショナルデータベースサービスを使用する必要があります。
Cloud SQL は、Google Cloud Platform 上のリレーショナル データベースの設定、維持、運用、管理を支援するフルマネージドのデータベース サービスです。
Cloud SQL は、MySQL、PostgreSQL、または SQL Server で使用できます。
Cloud SQL は、ローカル MySQL、PostgreSQL、SQL Server データベースの代替となるクラウドベースのサービスです。
Cloud SQL を使用することで、データベースの管理にかかる時間を減らし、データベースを使用する時間を増やすことができます。
Compute Engine、App Engine、Google Cloud の他のサービスで実行されている多くのアプリケーションは、データベース ストレージに Cloud SQL を使用する。
したがって、正解は「Cloud SQL」です。
参照：
https://cloud.google.com/sql/docs/features
https://cloud.google.com/sql/docs
</div></details>

## 3
### Q. 問題1: 未回答
現在あなたが構築しているシステムでは、Kafkaクラスターを経由してRedisクラスターにストリーミングデータを挿入するように設定しています。どちらのクラスターもCompute Engineインスタンス上で動作しています。必要に応じて作成、ローテーション、破棄できる暗号化キーを使って、静的データを暗号化する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. Cloud Key Management Serviceで暗号化キーを作成する。これらのキーを使用して、すべてのCompute Engineクラスター・インスタンスのデータを暗号化する
3. 
4. 
<details><div>
    答え：2
説明
適切な暗号化キー管理と、それを用いた暗号化を行う必要があります。
Cloud Key Management Service を使用すると、単一の集中型クラウド サービスで暗号鍵の作成、インポート、管理を行い、暗号化オペレーションを実行できます。
これらの鍵を使用してこれらのオペレーションを実行するには、Cloud KMS を直接使用するか、Cloud HSM または Cloud External Key Manager を使用するか、他の Google Cloud サービス内の顧客管理の暗号鍵（CMEK）を使用する。
Cloud KMS を使用すると、データの究極の管理者となり、オンプレミスと同じ方法で暗号鍵を管理でき、データ全体の信頼性の証明と監視が可能になります。
したがって、正解は「Cloud Key Management Serviceで暗号化キーを作成する。これらのキーを使用して、すべてのCompute Engineクラスター・インスタンスのデータを暗号化する」です。
参照：
https://cloud.google.com/kms/docs
</div></details>

### Q. 問題2: 未回答
お客様の金融サービス会社では、クラウド技術への移行を進めており、50TBの金融時系列データをクラウドに保存したいと考えています。このデータは頻繁に更新され、常に新しいデータが流れ込んできます。また、既存のApache Hadoopのジョブをクラウドに移行して、このデータに対するインサイトを得たいと考えています。
データの保存にはどのサービスを使うべきでしょうか？
1. Cloud Bigtable
2. 
3. 
4. 
<details><div>
    答え：1
説明
常に生成されて書き込まれる時系列データを保存する適切なストレージを選択する必要があります。
Cloud Bigtable は、Google のフルマネージド NoSQL ビッグデータのデータベース サービスです。
Cloud BigtableをCloud Dataflowと組み合わせることで、ストリーミングのデータのオペレーションを容易に行うことができます。
したがって、正解は「Cloud Bigtable」です。
参照：
https://cloud.google.com/bigtable/docs
https://cloud.google.com/blog/products/databases/getting-started-with-time-series-trend-predictions-using-gcp
https://cloud.google.com/bigtable/docs/hbase-dataflow-java
</div></details>

### Q. 問題3: 未回答
あなたは証券会社でデータ分析エンジニアとして働いています。特定の銘柄の最近の価格履歴に基づいてその銘柄の価格を予測する機械学習モデルを作成したいと考えています。
この場合、どのようなタイプの推論器を使用すればよいのでしょうか。
1. 
2. 教師あり学習の回帰モデル
3. 
4. 
<details><div>
    答え：2
説明
過去の価格履歴に基づいて銘柄の価格を予測するタスクは、過去データを教師データとして用いた教師あり学習です。
また、入力データを特定のクラスに分類するのではなく、価格という連続値を予測するため、回帰モデルを使用する必要があります。
したがって、正解は「教師あり学習の回帰モデル」です。
参照：
https://aiacademy.jp/texts/show/?id=140
</div></details>

### Q. 問題4: 未回答
あなたの会社では、カンマ区切りの値（CSV）ファイルを Google BigQuery に読み込んでいます。データは完全に正常にインポートされますが、インポートされたデータはソースファイルとバイト単位で一致しません。
この問題の最も可能性の高い原因は何ですか？
1. 
2. 
3. BigQuery に読み込まれた CSV データは、BigQuery のデフォルトのエンコーディングを使用していない
4. 
<details><div>
    答え：3
説明
SV データを読み込む際は、データを新しいテーブルまたはパーティションに読み込む、データを既存のテーブルまたはパーティションに追加する、または既存のテーブルまたはパーティションを上書きすることが可能です。
BigQuery に読み込まれたデータは Capacitor の列型（BigQuery のストレージ形式）に変換されます。
CSV ファイルを BigQuery に読み込む場合は、次の点に注意してください。
- CSV ファイルはネストされたデータや繰り返しデータに対応していません。
- バイト オーダー マーク（BOM）文字を削除する。予期しない問題が発生する可能性があります。
- gzip 圧縮を使用した場合、BigQuery はデータを並列で読み取ることができません。圧縮された CSV データを BigQuery に読み込む場合は、圧縮されていないデータを読み込むよりも時間がかかります。圧縮データと非圧縮データを読み込むをご覧ください。
- 同じ読み込みジョブに圧縮ファイルと非圧縮ファイルの両方を含めることはできません。
- gzip ファイルの最大サイズは 4 GB です。
- CSV データまたは JSON データを読み込む場合、DATE 列の値に区切りとしてダッシュ（-）を使用し、YYYY-MM-DD（年-月-日）の形式にする必要があります。
- JSON または CSV データを読み込む場合、TIMESTAMP 列のタイムスタンプ値の日付部分の区切りにはダッシュ（-）を使用し、日付は YYYY-MM-DD（年-月-日）の形式にする必要があります。タイムスタンプの時間部分 hh:mm:ss（時-分-秒）には、区切りとしてコロン（:）を使用する。
今回の例であれば、エンコーディング方法が異なっていたためファイルサイズが異なっていたのだと推察することができます。
したがって、正解は「BigQuery に読み込まれた CSV データは、BigQuery のデフォルトのエンコーディングを使用していない」です。
参照：
https://cloud.google.com/bigquery/docs/loading-data#loading_encoded_data
</div></details>

### Q. 問題5: 未回答
あなたの会社では、Google Cloud Dataflowで学習アルゴリズムのデータ前処理を行っています。このステップでは多数のデータログが生成されており、チームはそれらを分析したいと考えています。このキャンペーンは動的な性質を持っているため、データは毎時間指数関数的に増加しています。
データサイエンティストは、ログに含まれる新しい重要な特徴を探すために、次のようなコードを書いてデータを読みました。
BigQueryIO.Read -
.named（"ReadLogData"）
.from（"clouddataflow-readonly:samples.log_data"）
このデータ読み取りのパフォーマンスを向上させたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. .fromQuery 操作を使用して、テーブルから特定のフィールドを読み取る
3. 
4. 
<details><div>
    答え：2
説明
問題文中のコードでパフォーマンスボトルネックを特定する必要があります。
BigQueryIO.read.from（）は、BigQueryからテーブル全体を直接読み取ります。
この関数は、テーブル全体をGoogle Cloud Storageの一時ファイルにエクスポートし、後でそこから読み込むことになります。
これはエクスポートジョブを実行するだけで、後のDataflowは（BigQueryからではなく）GCSから読み込むため、ほとんど計算を必要としません。
一方で、全てのデータを読み込むという性質上、時間がかかります。
BigQueryIO.read.fromQuery（） は、クエリを実行し、クエリ実行後に受け取った結果を読み込みます。
そのため、この関数は最初にクエリを実行する必要があり、その分経済的・計算的コストがかかります。
しかし、一部のデータのみを指定しているため、読み取りパフォーマンスは向上する。
今回はパフォーマンスを向上させるという目的があるため、fromQuery（）を使用することが適切です。
したがって、正解は「.fromQuery 操作を使用して、テーブルから特定のフィールドを読み取る」です。
参照：
https://cloud.google.com/bigquery/docs/best-practices-costs#avoid_select_
https://stackoverflow.com/questions/49898462/bigqueryio-read-fromquery-performance-slow
</div></details>

### Q. 問題6: 未回答
あなたは、外部の顧客から、データベースからのデータを毎日ダンプしてもらい、それを受け取っています。データはカンマ区切り値（CSV）ファイルとしてGoogle Cloud Storageにロードします。あなたはこのデータをGoogle BigQueryで分析したいが、データには間違ったフォーマットや破損している行がある可能性があります。
あなたはこのパイプラインはどのように構築する必要がありますか？
1. 
2. 
3. 
4. Google Cloud Dataflow バッチパイプラインを実行してデータを BigQuery にインポートし、エラーを別のデッドレターテーブルにプッシュして分析する
<details><div>
    答え：4
説明
パイプラインの構築をする際に、正しく不正のデータの情報を受け取れるように設定する必要があります。
パイプラインで、要素を処理できない際の原因はさまざまですが、一般的な原因はデータの問題です。
このような状況におけるアプローチの 1 つは、DoFn.ProcessElement メソッドで例外をキャッチする方法です。
例外ブロックでは、エラーをログに記録して要素を廃棄する。
ただし、これによりデータが失われ、手動処理またはトラブルシューティングのためにデータが検査されなくなります。
Google Cloudが推奨するより良い方法としては、デッドレター キュー（またはデッドレター ファイル）と呼ばれるパターンを使用することです。
DoFn.ProcessElement メソッドで例外をキャッチして、エラーを通常どおりにロギングする。
ただし、失敗した要素をドロップする代わりに、分岐出力を使用して、失敗した要素を個別の PCollection オブジェクトに書き込みます。
その後、これらの要素はデータシンクに書き込まれ、別の変換を使用して、後で検査や処理を行うことができます。
したがって、正解は「Google Cloud Dataflow バッチパイプラインを実行してデータを BigQuery にインポートし、エラーを別のデッドレターテーブルにプッシュして分析する」です。
参照：
https://cloud.google.com/blog/products/gcp/handling-invalid-inputs-in-dataflow
https://cloud.google.com/Pub/Sub/docs/handling-failures
</div></details>

### Q. 問題7: 未回答
スケーラブルな方法でデータを収集する必要のある新しいアプリケーションを構築しています。データは1日中アプリケーションから継続的に届き、年末には1日あたり約150GBのJSONデータが生成される予定です。お客様の要件は以下の通りです。
- プロデューサとコンシューマのデカップリング
- インジェストされた生データを無期限に保存するためのスペースとコストの効率化
- ほぼリアルタイムの SQL クエリ
- SQLで照会するために、少なくとも2年間の履歴データを保持すること
これらの要件を満たすためには、どのパイプラインを使用すべきでしょうか？
1. 
2. 
3. 
4. Cloud Pub/Subにイベントを発行するアプリケーションを作成し、JSONイベントのペイロードをAvroに変換するCloud Dataflowパイプラインを作成して、データをCloud StorageとBigQueryに書き込む
<details><div>
    答え：4
説明
リアルタイムのイベントを処理し、蓄積されたデータはSQLアクセスが行える必要があり、履歴データはエクスポートに対応する必要があります。
今回のようなストリーミング分析においては、ストリーミングデータの取り込みにCloud Pub/Subを使用し、Cloud Dataflowを用いたETL処理を実装することで、柔軟性の高いスケーラブルなパイプラインを構築することができます。
また、Cloud Dataflowで変換したデータについては、BigQuery のストリーミング API を使用すれば、SQL ベースの分析用に、毎秒数百万ものイベントをデータ ウェアハウスに直接ストリーミングできます。
履歴データのエクスポート先についてはCloud Storageがコスト最適なソリューションになります。
Cloud StorageのデータはBigQueryによってSQLクエリをかけることも可能です。
したがって、正解は「Cloud Pub/Subにイベントを発行するアプリケーションを作成し、JSONイベントのペイロードをAvroに変換するCloud Dataflowパイプラインを作成して、データをCloud StorageとBigQueryに書き込む」です。
参照：
https://cloud.google.com/solutions/stream-analytics/
</div></details>

### Q. 問題8: 未回答
データウェアハウスを BigQuery に移行しています。すべてのデータをデータセットのテーブルに移行しました。このテーブルは組織の複数のユーザーが使用します。彼らは、チーム メンバーシップに基づいて特定のテーブルのみを見る必要があります。
ユーザーの権限はどのように設定しますか？
1. 各テーブルのテーブルレベルで、データビューアのアクセス権をユーザー／グループに割り当てる
2. 
3. 
4. 
<details><div>
    答え：1
説明
BigQueryのテーブルの中で、他のユーザーと共有してはならないテーブルについて、適切なアクセスコントロールを行う必要があります。
BigQueryでは、各テーブルに対してテーブルレベルのアクセスコントロールを行うことができます。
この場合、データビューアの権限などを、ユーザーもしくはグループに割り当てることが可能です。
例えばBob はフランチャイズ店舗のオーナーであり、Aliceは会社のデータオーナーであるケースを考えます。
この場合、Alice は bq コマンドライン ツールを使用して、Bob を含むフランチャイズ店舗のオーナーに inventory テーブルの BigQuery データ閲覧者（roles/bigquery.dataViewer）のロールを付与します。
これで、Bob はデータセット全体にアクセスせずに inventory テーブルを直接クエリできるようになります。
したがって、正解は「各テーブルのテーブルレベルで、データビューアのアクセス権をユーザー／グループに割り当てる」です。
参照：
https://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case
https://cloud.google.com/bigquery/docs/table-access-controls-intro#permissions
</div></details>

### Q. 問題9: 未回答
リアルタイムのアプリケーションにBigtableを使っていますが、読み込みと書き込みが混在した重い負荷がかかっています。最近、新たなユースケースを発見し、データベース全体で特定の統計を計算する分析ジョブを1時間ごとに実行する必要があります。本番アプリケーションの信頼性と、分析作業の信頼性の両方を確保する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 通常のワークロードにはlive-trafficアプリ・プロファイルを使用し、解析ワークロードにはbatch-analyticsプロファイルを使用する
4. 
<details><div>
    答え：3
説明
今回のケースでは、リアルタイムでの処理と、バッチでの処理が混在しています。
このような場合には、クラスタのレプリケーションによる処理の分離が適切です。
単一のクラスタ上で、多数の大規模読み取りを実行するバッチ分析ジョブを、読み取りや書き込みを実行するアプリケーションと並行して実行すると、大規模バッチジョブがアプリケーションのユーザーにとって処理を遅くする可能性があります。
レプリケーションでは、単一クラスタ ルーティングのアプリ プロファイルを使用してバッチ分析ジョブとアプリケーション トラフィックを異なるクラスタにルーティングできるため、バッチジョブがアプリケーションのユーザーに影響を与えることはありません。
したがって、正解は「通常のワークロードにはlive-trafficアプリ・プロファイルを使用し、解析ワークロードにはbatch-analyticsプロファイルを使用する」です。
参照：
https://cloud.google.com/bigtable/docs/replication-overview#use-cases
</div></details>

### Q. 問題10: 未回答
あなたはGoogle Cloud上で実行されるマルチステップのデータパイプラインの実行を自動化したいと考えています。パイプラインにはCloud DataprocとCloud Dataflowのジョブが含まれ、互いに複数の依存関係があります。可能な限りマネージドサービスを使用したいと考えており、パイプラインは毎日実行されます。
どのツールを使うべきでしょうか？
1. 
2. Cloud Composer
3. 
4. 
<details><div>
    答え：2
説明
パイプラインの実行を自動化するマネージドサービスを選択する必要があります。
Cloud ComposerはApache Airflow で構築された、フルマネージドのワークフロー オーケストレーション サービスです。
DAG（Direct Acyclic Graph）と呼ばれるデータ形式でジョブ同士の関係性を定義し、実行します。
Cloud Composer はフルマネージド サービスであり、Apache Airflow は互換性に優れているため、リソースのプロビジョニングに気をとられず、ワークフローの作成、スケジューリング、モニタリングに専念できます。
BigQuery、Dataflow、Dataproc、Datastore、Cloud Storage、Pub/Sub、AI Platform などの Google Cloud プロダクトとのエンドツーエンドの統合により、ユーザーはパイプラインを自由かつ完全にオーケストレートできます。
これによって、複数の依存関係のあるジョブを、少ないオーバーヘッドで実行することが可能になります。
したがって、正解は「Cloud Composer」です。
参照：
https://cloud.google.com/composer
https://cloud.google.com/composer/docs/how-to/using/using-dataflow-template-operator
</div></details>

### Q. 問題11: 未回答
現在オンプレミスで運用しているApache Hadoopをクラウドに移行することを計画しています。長時間のバッチジョブを処理するために、可能な限りフォールト・トレラントでコスト効率の高いデプロイメントを実現する必要があります。また、移行にあたってはマネージドサービスを利用したいと考えています。
要件を達成するためにするべきことは何ですか？
1. Cloud Dataprocクラスターをデプロイする。標準的なHDDパーシステントディスクと50%のプリエンプト可能なワーカーを使用する。データをCloud Storageに保存し、スクリプトの参照先をhdfs://からgs://に変更する
2. 
3. 
4. 
<details><div>
    答え：1
説明
オンプレミス環境から Google Cloud に Apache Spark ワークロードを移動する場合は、Dataproc を使用して Apache Spark / Apache Hadoop クラスタを実行することが推奨されています。
Dataproc は、Google Cloud が提供する、完全にサポートされているフルマネージド サービスです。ストレージとコンピューティングを分離できるため、費用を管理し、ワークロードのスケーリングを柔軟に行うことができます。
Dataprocでは、永続ストレージとして、ソリッド ステート ドライブ（SSD）とハードディスク ドライブ（HDD）のどちらにするかを指定します。
SSD ストレージは、ほとんどのユースケースで最も効率的でコスト効果の高い選択肢です。
HDD ストレージは、非常に大きいデータセット（10 TB 超）で、レイテンシがあまり重要でない場合やアクセス頻度が低い場合に適切であることがあります。
どちらのタイプのストレージを選択した場合でも、多数の物理ドライブにわたって分散してレプリケーションされたファイル システムにデータが保存されます。
HDD ストレージは、以下の条件を満たしているユースケースに適しています。
- 10 TB 以上のデータを保存する予定である。
- ユーザー向けやレイテンシの影響を受けやすいアプリケーションを支援するデータを使用しない。
ワークロードは次のいずれかのカテゴリに分類されます。
- バッチ ワークロード：スキャンと書き込みが行われます。また、少数の行の読み込みがまれにランダムに行われます。
- データ アーカイブ：大量のデータの書き込みが行われます。そのデータの読み取りはほとんど行われません。
たとえば、多数のリモート センシング装置の詳細な履歴データを保存し、そのデータを使用して日次レポートを生成する予定がある場合には、パフォーマンスが低下しても HDD ストレージのコスト削減が優先されることがあります。それに対して、データを使用してリアルタイム ダッシュボードを表示する場合は、HDD ストレージの使用は適切とはいえません。この場合、読み取りが頻繁に行われ、またスキャンではない読み取りははるかに遅いからです。
今回のケースでは長時間にわたるバッチジョブであるためHDDが最適です。
したがって、正解は「Cloud Dataprocクラスターをデプロイする。標準的なHDDパーシステントディスクと50%のプリエンプト可能なワーカーを使用する。データをCloud Storageに保存し、スクリプトの参照先をhdfs://からgs://に変更する」です。
参照：
https://cloud.google.com/bigtable/docs/choosing-ssd-hdd
</div></details>

### Q. 問題12: 未回答
政府の規制により、ある種のデータへのアクセスを監査可能な形で記録することが求められています。
期限切れのログがすべて正しくアーカイブされると仮定した場合、その対象となるデータはどこに保管すべきでしょうか。
1. 
2. 許可された担当者のみが閲覧可能なBigQueryデータセット内で、データアクセスログを使用して監査可能な状態にする
3. 
4. 
<details><div>
    答え：2
説明
監査可能な形でログを記録し、適切なサービスに保存する必要があります。
Cloud Audit Logs は Google Cloud が提供するログの集まりで、Google Cloud サービスの使用に関連する運用上のアクセス等の記録を把握することができます。
BigQueryを用いることで、Cloud Audit Logsを収集して解析することが可能になります。
したがって、正解は「許可された担当者のみが閲覧可能なBigQueryデータセット内で、データアクセスログを使用して監査可能な状態にする」です。
参照：
https://cloud.google.com/bigquery/docs/reference/auditlogs/
https://cloud.google.com/architecture/exporting-Cloud Monitoring-logging-for-security-and-access-analytics
</div></details>

### Q. 問題13: 未回答
あなたの会社のビジネスオーナーから、銀行取引のデータベースを渡されました。各行には、ユーザーID、取引タイプ、取引場所、取引額が含まれています。彼らはあなたに、このデータにどのような種類の機械学習を適用できるかを調査するよう依頼しました。
適用できる機械学習の応用例は次のうちどれですか？（3つ選択）
1. 
2. 教師なしの学習で、どの取引が不正である可能性が最も高いかを判断する
3. 特徴の類似性に基づいて、取引をN個のカテゴリに分けるクラスタリング
4. 教師あり学習で、取引の場所を予測する
5. 
<details><div>
    答え：2,3,4
説明
データに存在するラベルに対しては教師あり学習を利用することができます。
一方で、存在しないラベリング（不正検知、特定のセグメンテーション）を推測するためには教師なし学習が必要です。
今回の例では、取引場所はラベルが存在するため、他の特徴量を用いた教師あり学習が可能です、
一方で、不正検知や特定のセグメンテーションに関する情報はないため、教師なし学習による探索的なラベリングを行う必要があります。
したがって、正解は以下の通りです。
- 教師なしの学習で、どの取引が不正である可能性が最も高いかを判断する
- 特徴の類似性に基づいて、取引をN個のカテゴリに分けるクラスタリング
- 教師あり学習で、取引の場所を予測する
参照：
https://ledge.ai/unsupervised/
https://avinton.com/blog/2017/11/supervised-and-unsupervised-machine-learning/
</div></details>

### Q. 問題14: 未回答
組織内の各分析チームは、それぞれのプロジェクトで BigQuery ジョブを実行しています。各チームがプロジェクト内のスロット使用状況を監視できるようにしたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. BigQuery メトリック slots/allocated_for_projectに基づいて Cloud Monitoring ダッシュボードを作成する
3. 
4. 
<details><div>
    答え：2
説明
BigQuery スロットは、BigQuery で SQL クエリを実行するために使用される仮想 CPU です。
BigQuery では、クエリのサイズと複雑さに応じて、クエリに必要なスロット数が自動的に計算されます。
BigQueryスロットの使用状況はCloud Monitoring (Cloud Monitoring) によってメトリクスとして確認することができます。
また、メトリクスに基づくダッシュボードを作成することで、使用状況を直感的に理解することが可能です。
Slots/allocated_for_project は、プロジェクト内のクエリジョブに現在割り当てられているBigQueryスロットの数を確認することができます。
このメトリクスは60秒ごとにサンプリングされます。
また、サンプリング後、最大420秒間はデータが表示されません。
したがって、正解は「BigQuery メトリック slots/allocated_for_projectに基づいて Cloud Monitoring ダッシュボードを作成する」です。
参照：
https://cloud.google.com/monitoring/api/metrics_gcp
</div></details>

### Q. 問題15: 未回答
あなたは、それぞれ異なる優先順位と予算を持つ複数のビジネスユニットを持つ大企業でBIの責任者をしています。あなたは、BigQueryのオンデマンド価格を使用しており、プロジェクトごとに2,000の同時オンデマンドスロットのクォータを設定しています。ユーザがクエリを実行するためのスロットを取得できないことがあり、これを修正する必要があります。一方で、アカウントに新しいプロジェクトを導入することは避けたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 定額制に切り替え、プロジェクトの優先順位を階層的に設定する
4. 
<details><div>
    答え：3
説明
プロジェクトを新しく作らないという条件で、最適なクエリスロットルを確保する必要があります。
今回のケースであれば、定額制の料金モデルを採用することが最適です。
このモデルでは、一定数のスロットがお客様のプロジェクトに割り当てられ、プロジェクト間で階層的な優先順位モデルを確立することができます。
定額制は、複数の事業部門があり、優先順位や予算が異なるワークロードを抱える大企業に特に適しています。
したがって、正解は「定額制に切り替え、プロジェクトの優先順位を階層的に設定する」です。
参照：
https://cloud.google.com/bigquery/pricing
</div></details>

### Q. 問題16: 未回答
あなたのチームは，二値分類問題に取り組んでいます。デフォルトのパラメータでサポートベクターマシン（SVM）分類器を学習し、検証セットで曲線下面積（AUC）が0.87になりました。このモデルのAUCを向上させたいと考えています。
要件を達成するためにするべきことは何ですか？
1. ハイパーパラメータ・チューニングの実行
2. 
3. 
4. 
<details><div>
    答え：1
説明
SVMの更なる性能向上に必要なチューニングを行う必要があります。
問題文を読むと、デフォルトのパラメータで学習を行なっている記述があります。
SVMは、いくつかのハイパーパラメータをチューニングすることで、性能が向上します。
今回はハイパーパラメータチューニングによって性能向上が見込まれます。
したがって、正解は「ハイパーパラメータ・チューニングの実行」です。
参照：
https://developers.google.com/machine-learning/crash-course/classification/check-your-understanding-roc-and-auc
</div></details>

### Q. 問題17: 未回答
過去3年分の履歴データをBigQueryに保存し、毎日新しいデータをBigQueryに配信するデータパイプラインを構築しています。データサイエンスチームが日付カラムでフィルタリングし、30・90日分のデータに限定してクエリを実行すると、クエリがテーブル全体をスキャンしていることに気づきました。また、請求額が予想以上に早く増えていることにも気がつきました。あなたは、SQLクエリを実行する能力を維持しながら、できるだけコスト効率よくこの問題を解決したいと考えています。
要件を達成するためにするべきことは何ですか？
1. DDLを使用してテーブルを再作成する。TIMESTAMP型またはDATE型を含む列でテーブルを分割する
2. 
3. 
4. 
<details><div>
    答え：1
説明
BigQueryはスキャンしたデータ量に応じてコストがかかるため、スキャンする範囲を目的に応じて少なくする必要があります。
パーティション分割テーブルはパーティションと呼ばれるセグメントに分割された特殊なテーブルで、データの管理や照会をより簡単に行うことができます。
大きいテーブルを小さいパーティションに分割することでクエリのパフォーマンスを向上させることができ、クエリで読み取られるバイト数を減らすことによってコストを管理できます。
BigQuery テーブルを分割する方法は次のとおりです。
- 時間単位の列: テーブルの TIMESTAMP、DATE、または DATETIME 列に基づいてテーブルが分割されます。
- 取り込み時間: BigQuery がデータを取り込む際のタイムスタンプに基づいてテーブルが分割されます。
- 整数範囲: テーブルは整数列に基づいて分割されます。
クエリで、パーティショニング列の値に対する限定フィルタを使用すると、BigQuery では、フィルタに一致するパーティションがスキャンされ、残りのパーティションはスキップされます。
このプロセスは「プルーニング」と呼ばれます。
今回の例であれば、30日、90日のデータをTIMESTAMP型もしくはDATE型でパーティショニングすることで、クエリ範囲を絞ることができます。
したがって、正解は「DDLを使用してテーブルを再作成する。TIMESTAMP型またはDATE型を含む列でテーブルを分割する」です。
参照：
https://cloud.google.com/bigquery/docs/partitioned-tables
</div></details>

### Q. 問題18: 未回答
米国を拠点とするあなたの会社は、ユーザーの行動を評価し、それに対応するためのアプリケーションを作成しました。プライマリテーブルのデータ量は、毎秒250,000レコードずつ増加します。多くのサードパーティがアプリケーションのAPIを使用して、自分たちのフロントエンド・アプリケーションに機能を組み込んでいます。アプリケーションのAPIは、以下の要件に準拠している必要があります。
- 単一のグローバルエンドポイント
- ANSI SQL のサポート
- 最新のデータへの一貫したアクセス
要件を達成するためにするべきことは何ですか？
1. 
2. 北米にリーダー、アジアとヨーロッパにリードオンリーのレプリカを持つCloud Spannerを導入する
3. 
4. 
<details><div>
    答え：2
説明
グローバルスケールで強力な一貫性を持つデータベースサービスを利用する必要があります。
Cloud Spannerは無制限のスケーリング、強い整合性、最大 99.999% の可用性を備えたフルマネージド リレーショナル データベースです。
グローバルなトランザクション整合性、高可用性のための自動の同期レプリケーション、2 つの SQL 言語（Google 標準 SQL（拡張機能を含む ANSI 2011）と PostgreSQL）が含まれています。
したがって、正解は「北米にリーダー、アジアとヨーロッパにリードオンリーのレプリカを持つCloud Spannerを導入する」です。
参照：
https://cloud.google.com/spanner/docs/
https://cloud.google.com/spanner/docs/instances#available-configurations-multi-region
</div></details>

### Q. 問題19: 未回答
HadoopジョブをオンプレミスのクラスターからdataprocとGCSに移行しました。Sparkジョブは複雑な分析ワークロードで、多くのシャッフル操作で構成されており、初期データはParquetファイル（1つのサイズが平均200～400MB）です。Dataprocへの移行後にパフォーマンスの低下が見られたため、最適化を行いたいと考えています。しかし、この組織はコストに非常に敏感であることを念頭に置く必要があるため、このワークロードでは、Dataprocをプリエンプティブで使用し続けたいと考えています（プリエンプティブでないワーカーは2台のみ）。
要件を達成するためにするべきことは何ですか？
1. Parquetファイルのサイズを大きくして、最小1GBになるようにする
2. 
3. 
4. 
<details><div>
    答え：1
説明
既存にプロセスをコスト増に繋がらない形でチューニングする必要があります。
従って、プロセスで使用しているコンピューティングリソース自体の最適化は避けるべきです。
Parquet は効率的なカラム型ファイル形式であり、Spark でアプリケーションの実行に必要なデータのみを読み取ることができます。
SparkジョブでParquetファイルを使用する場合、ファイルサイズの目安は1GBと言われています。
今回のケースでは、Parquetサイズが200~400MBと小さいため、この点は改善の余地がある点になります。
したがって、正解は「Parquetファイルのサイズを大きくして、最小1GBになるようにする」です。
参照：
https://cloud.google.com/dataproc/docs/support/spark-job-tuning
https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs
https://community.databricks.com/s/question/0D53f00001HKHWDCA5/what-is-an-optimal-size-for-file-partitions-using-parquet
</div></details>

### Q. 問題20: 未回答
あなたは、画像認識領域のニッチサービスに取り組んでいます。あなたのチームが開発したモデルは、あなたのチームが実装したカスタムC++ TensorFlow Opsに組み込まれています。これらのOpsは、メインのトレーニングループ内で使用され、大規模な行列の乗算を実行しています。現在、モデルのトレーニングには最大で数日かかります。Google Cloud上のアクセラレータを使用することで、この時間を大幅に短縮し、コストを抑えたいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. CPUのままで、モデルをトレーニングするクラスタのサイズを大きくする
<details><div>
    答え：4
説明
カスタマイズされたC++ TensorFlow Opsに対応可能な選択肢を選ぶ必要があります。
状況によっては、Compute Engine インスタンスで GPU または CPU を使用して機械学習ワークロードを実行する場合があります。
一般的に、次のガイドラインに基づいてワークロードに最適なハードウェアを決定できます。
CPU
- 最大限の柔軟性を必要とする迅速なプロトタイピング
- トレーニングに時間がかからない単純なモデル
- 実際のバッチサイズが小さい小規模なモデル
- C++ で記述されたカスタム TensorFlow 演算が多くを占めるモデル
- ホストシステムの使用可能な I/O またはネットワーク帯域幅によって制限が課せられるモデル
GPU
- ソースが存在しないモデルまたはソースを変更するのが煩雑すぎるモデル
- CPU 上で少なくとも部分的に実行しなければならない多数のカスタム TensorFlow 演算を使用するモデル
- Cloud TPU で利用できない TensorFlow 演算を使用するモデル（利用可能な TensorFlow 演算のリストをご覧ください）
- 実際のバッチサイズが大きい中～大規模なモデル
TPU
- 行列計算が多くを占めるモデル
- メインのトレーニング ループ内にカスタム TensorFlow 演算がないモデル
- トレーニングに数週間または数か月かかるモデル
- 実際のバッチサイズが非常に大きい非常に大規模なモデル
今回のケースでは、C++で記述されたカスタムTensorFlow演算を多く行なっているため、CPUの利用が最適です。
したがって、正解は「CPUのままで、モデルをトレーニングするクラスタのサイズを大きくする」です。
参照：
https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus
https://cloud.google.com/tpu/docs/tpus
</div></details>

### Q. 問題21: 未回答
お客様の組織のマーケティングチームは、お客様のデータセットのセグメントを定期的に更新しています。あなたはマーケティングチームから、BigQuery で更新する必要のある 100 万件のレコードを含む CSV を受け取りました。一方で、BigQuery で UPDATE ステートメントを使用すると、quotaExceeded エラーが発生します。
どうしたらこのエラーを解決できますか？
1. 
2. 
3. 
4. CSV ファイルから新しいレコードを新しい BigQuery テーブルにインポートする。新しいレコードを既存のレコードにマージし、その結果を新しいBigQueryテーブルに書き込むBigQueryジョブを作成する
<details><div>
    答え：4
説明
100万件のレコード更新によってquotaExceededエラーが引き起こされています。
クォータとは、クラウドプロジェクトが使用できる特定の共有 Google Cloud リソースの量を制限するもので、ハードウェア、ソフトウェア、ネットワークコンポーネントなどが含まれます。
今回のケースでは、UPDATE文によってこの問題が発生しているため、その他の方法を用いてこの上限に到達しないようにデータをインポートする必要があります。
Google Cloudが推奨するBigQueryへのインポートの方法は、次のBigQueryのMERGEステートメントを使用して、別のテーブル（新着情報が保管されている）の内容に基づいて既存テーブルのバッチ更新を実行する方法です。
MERGE dataset.Inventory T
USING dataset.NewArrivals S
ON T.ProductID = S.ProductID
WHEN MATCHED THEN
UPDATE SET quantity = T.quantity + S.quantity
WHEN NOT MATCHED THEN
INSERT (ProductID, quantity) VALUES (ProductID, quantity)
この方法によって、クォータを超過することなく大規模なレコードの更新を実行することができます。
したがって、正解は「CSV ファイルから新しいレコードを新しい BigQuery テーブルにインポートする。新しいレコードを既存のレコードにマージし、その結果を新しいBigQueryテーブルに書き込むBigQueryジョブを作成する」です。
参照：
https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement
https://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery
</div></details>

### Q. 問題22: 未回答
広告会社では、広告ブロックのクリック率を予測するSpark MLモデルを開発しています。これまではオンプレミスのデータセンターで開発していましたが、現在はGoogle Cloudに移行しています。データセンターは間もなく閉鎖されるため、迅速なリフト＆シフトの移行が必要です。しかし、今まで使っていたデータはBigQueryに移行されてしまいます。Spark MLモデルの再学習を定期的に行っているので、既存のトレーニングパイプラインをGoogle Cloudに移行する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 既存のSpark MLモデルのトレーニングにCloud Dataprocを使用し、BigQueryから直接データを読み取る
4. 
<details><div>
    答え：3
説明
Spark MLモデルをホストするためのサービスを選びつつ、BigQueryとの連携を行う必要があります。
Dataproc は、Apache Spark、Apache Flink、Presto をはじめ、30 以上のオープンソース ツールやフレームワークを実行するための、フルマネージドでスケーラビリティの高いサービスです。
Dataproc を使用すれば、データレイクのモダナイゼーション、ETL、安全なデータ サイエンスを、Google Cloud と完全に統合された極めてスケーラブルな環境で、低コストで実現できます。
Dataprocでは、Apache Spark 用の BigQuery コネクタを使用すると、データ サイエンティストは、BigQuery のシームレスでスケーラブルな SQL エンジンの能力と Apache Spark の機械学習機能を融合できます。
今回のケースではBigQueryに学習データが移行されてしまったため、コネクタによって連携をすることで、Dataproc上で引き続きモデル開発を行うことができます。
したがって、正解は「既存のSpark MLモデルのトレーニングにCloud Dataprocを使用し、BigQueryから直接データを読み取る」です。
参照：
https://cloud.google.com/dataproc
https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml
</div></details>

### Q. 問題23: 未回答
組織がGCPの利用を拡大するにつれ、多くのチームが独自のプロジェクトを作成するようになりました。プロジェクトは、展開の段階や対象者の違いに応じてさらに増えていきます。各プロジェクトには、独自のアクセス制御設定が必要です。中央のITチームは、すべてのプロジェクトにアクセスする必要があります。
さらに、Cloud StorageのバケットやBigQueryのデータセットのデータを共有して、アドホックに他のプロジェクトで使用する必要があります。ポリシーの数を最小限にすることで、アクセスコントロール管理を簡素化したいと考えています。
要件を達成するためにどのようなステップを取る必要がありますか？（2つ選択）
1. 
2. リソース階層を導入し、アクセスコントロールポリシーの継承を活用する
3. チームごとに異なるグループを作成し、クラウドIAMポリシーでグループを指定する
4. 
<details><div>
    答え：2,3
説明
チームの複雑さに対応するための適切な組織構造を定義する必要があります。
Google Cloud のリソースは階層的に構成されており、組織ノードは階層のルートノード、プロジェクトは組織の子、その他のリソースはプロジェクトの子孫にあたります。
リソース階層の異なるレベルでIAM（Identity and Access Management）ポリシーを設定することができます。
リソースは、親リソースのポリシーを継承します。リソースの有効なポリシーは、そのリソースに設定されているポリシーと、親から継承したポリシーの和になります。
リソース階層を導入することで、チームごとに異なるグループを作成し、各グループで独自のアクセスコントロールを設定しつつ、中央のITチームが全てのプロジェクトにアクセスすることが可能になります。
したがって、正解は以下の通りです。
- リソース階層を導入し、アクセスコントロールポリシーの継承を活用する
- チームごとに異なるグループを作成し、クラウドIAMポリシーでグループを指定する
参照：
https://cloud.google.com/iam/docs/resource-hierarchy-access-control
https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
</div></details>

### Q. 問題24: 未回答
あなたの会社は最近急速に成長し、以前よりもかなり高い割合でデータを取り込むようになりました。あなたは、Apache Hadoopで毎日のバッチMapReduce分析ジョブを管理しています。しかし、最近のデータ量の増加により、バッチジョブの処理が遅れています。あなたは、開発チームがコストを増やさずに分析の応答性を高める方法を提案するよう求められました。
どのような方法を提案すべきでしょうか？
1. 
2. Apache Sparkでジョブを書き換える
3. 
4. 
<details><div>
    答え：2
説明
データの増加を踏まえて、スケーラブルな分散コンピューティング環境を整備する必要があります。
Apache Spark は、SQL、ストリーミング、機械学習、グラフ処理用の組み込みモジュールを備えた大規模なデータ処理のための統合分析エンジンです。
Spark は、Apache Hadoop、Apache Mesos、Kubernetes で実行できます。単独でもクラウドでも実行可能であり、さまざまなデータソースに対して実行できます。
Google Cloud 上では、Dataproc を使用して Apache Spark クラスタを、シンプルで統合され、費用対効果に優れた方法で実行することが可能です。
したがって、正解は「Apache Sparkでジョブを書き換える」です。
参照：
https://cloud.google.com/learn/what-is-apache-spark
</div></details>

### Q. 問題25: 未回答
あるオンライン小売業者は、現在のアプリケーションを Google App Engine で構築しています。同社の新たな取り組みとして、アプリケーションを拡張し、顧客がアプリケーションを介して直接取引できるようにすることが求められていました。また、ショッピングのトランザクションを管理し、ビジネスインテリジェンス（BI）ツールを使用して複数のデータセットからの結合データを分析する必要があります。この目的のためには、単一のデータベースのみを使用したいと考えています。
どのGoogle Cloudデータベースを選ぶべきでしょうか？
1. 
2. Cloud SQL
3. 
4. 
<details><div>
    答え：2
説明
トランザクションへの対応と、とBIツールを使用した統合データの分析を行うことのできる、データベースを採用する必要があります。
Cloud SQL は、Google Cloud Platform 上のリレーショナル データベースの設定、維持、運用、管理を支援するフルマネージドのデータベース サービスです。
BigQueryとは異なりトランザクションを記録するためのSQLデータベースとして使用が可能です。
また、Cloud SQL インスタンスには、あらゆるアプリケーションからアクセスできます。
App Engine、Compute Engine、Google Kubernetes Engine、自社のワークステーションのいずれからでも、簡単に接続でき、もちろんBIツールとの接続も可能です。
したがって、正解は「Cloud SQL」です。
参照：
https://cloud.google.com/sql/
</div></details>

### Q. 問題26: 未回答
あなたは、サードパーティから毎月CSV形式のデータファイルを受け取っています。このデータをクレンジングする必要がありますが、3ヶ月に一度、ファイルのスキーマが変更されます。これらの変換を実行するための要件は以下の通りです。
- 変換をスケジュール通りに実行すること
- 開発者ではないアナリストが変換を修正できるようにすること
- トランスフォームを設計するためのグラフィカルなツールを提供すること
要件を達成するためにするべきことは何ですか？
1. Cloud Dataprep を使用して変換レシピを構築・維持し、スケジュールに基づいて実行する
2. 
3. 
4. 
<details><div>
    答え：1
説明
変換レシピの定期的な変更が可能でありかつ、定期実行可能なサービスを選択する必要があります。
また、開発者ではないアナリストのためにノーコードによるデータ変換をサポートする必要があります。
Cloud Dataprep by Trifacta は、分析、レポート、機械学習に使用する構造化データと非構造化データを視覚的に探索、クリーニング、準備できるインテリジェント データ サービスです。
Dataprep はサーバーレスで、規模に関係なく稼働します。
デプロイや管理が必要なインフラストラクチャはなく、素早くデータ変換ロジックを構築し定期実行も行うことが可能です。
Dataprep は、最適なデータ変換操作を UI で操作を行うたびに自動で提案、予測します。
変換のシーケンスを定義しておけば、Dataprep は内部的に Dataflow または BigQuery を使用し、あらゆるサイズの構造化データセットまたは非構造化データセットをわずか数回のクリックで処理できるようにします。
そのため、コードを記述する必要がなくなります。
したがって、正解は「Cloud Dataprep を使用して変換レシピを構築・維持し、スケジュールに基づいて実行する」です。
参照：
https://cloud.google.com/dataprep/
https://cloud.google.com/dataprep/docs/html/Overview-of-RapidTarget_136155049
</div></details>

### Q. 問題27: 未回答
あなたは、ユーザーの個人情報を含む機密性の高いプロジェクトに取り組んでいます。その作業を社内で行うために、Google Cloud Platform上にプロジェクトを立ち上げました。外部のコンサルタントが、プロジェクトのためにGoogle Cloud Dataflowパイプラインで複雑な変換のコーディングを支援することになりました。
ユーザーのプライバシーをどのように維持すべきでしょうか？
1. 
2. プロジェクトのコンサルタントに Cloud Dataflow Developer ロールを付与する
3. 
4. 
<details><div>
    答え：2
説明
Google Cloudのプラクティスとして、ロールベースの権限管理が必要です。
これによって、外部コンサルタントがアクセスするべきでないサービスへのアクセスコントロールが可能です。
外部コンサルタントはDataflowパイプラインの開発を支援するため、Viewerではなく、Developerロールが必要です。
このロールは、Dataflow ジョブを実行、操作するために必要な権限を付与する。
したがって、正解は「プロジェクトのコンサルタントに Cloud Dataflow Developer ロールを付与する」です。
参照：
https://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment
</div></details>

### Q. 問題28: 未回答
分析のために BigQuery でデータセットを使用します。あなたは第三者の企業に同じデータセットへのアクセスを提供したいと考えています。また、データ共有のコストを抑え、データの最新性を確保する必要があります。
どのソリューションを選ぶべきでしょうか？
1. データへのアクセスを制御するために BigQuery テーブルにオーソライズドビュー を作成し、その view へのアクセスをサードパーティ企業に提供する
2. 
3. 
4. 
<details><div>
    答え：1
説明
BigQueryの機能を用いて第三者に対するアクセスコントロールを実現する必要があります。
BigQuery は、ペタバイト級のアナリティクス データ ウェアハウスであり、巨大容量のデータに対して SQL クエリをリアルタイムに近い速度で実行できます。
データセットに表示アクセス権を設定する場合、BigQuery ではオーソライズドビューを作成します。
オーソライズドビューを使用すると、元のテーブルへのアクセス権がないユーザーでも、クエリの結果を特定のユーザーやグループと共有できます。
ビューの SQL クエリを使用して、ユーザーがクエリを実行できる列（フィールド）を制限することもできます。このチュートリアルでは、オーソライズドビューを作成します。
したがって、正解は「データへのアクセスを制御するために BigQuery テーブルにオーソライズドビュー を作成し、その view へのアクセスをサードパーティ企業に提供する」です。
参照：
https://cloud.google.com/bigquery/docs/share-access-views
</div></details>

### Q. 問題29: 未回答
あなたは、過去のデータをCloud Storageにアップロードする必要があります。セキュリティルールでは、外部のIPからオンプレミスのリソースへのアクセスを許可していません。また、最初のアップロードを行なった後には、既存のオンプレミスアプリケーションから毎日新しいデータが追加されていきます。
要件を達成するためにするべきことは何ですか？
1. オンプレミスのサーバーからgsutil rsyncを実行する
2. 
3. 
4. 
<details><div>
    答え：1
説明
セキュリティ要件を満たしつつ、Cloud Storageにアップロードする方法を選択する必要があります。
gsutil ツールは、一般的なエンタープライズ規模のネットワークを介してプライベート データセンターから Google Cloud へ、小規模から中規模（1 TB 未満）の転送を行う場合の標準ツールです。
ローカル システムと Cloud Storage 間のデータのコピーなど、Cloud Storage インスタンスの管理に必要な基本機能をすべて備えた、信頼性に優れたツールです。
そのほかの Data Transfer Serviceとは異なり、オンプレミスのサーバーにツールをインストールするだけで利用できるため、外部IPアドレスからのアクセスが不要です。
データが毎日更新され続ける場合の、Cloud Storage バケットへのリアルタイムの増分同期は、gsutilのrsyncが最適です。
gsutil rsyncコマンドは、同期先のオブジェクトの内容を同期元の内容と同じにする際に使用されます。
差分が生じているオブジェクトはコピー/削除が実行されるため、オンプレミスのデータの転送を自動で行うことが可能です。
したがって、正解は「オンプレミスのサーバーからgsutil rsyncを実行する」です。
参照：
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data
https://cloud.google.com/storage/docs/gsutil/commands/rsync
</div></details>

### Q. 問題30: 未回答
時系列のトランザクションデータをコピーするデータパイプラインを作成し、データサイエンスチームがBigQueryからクエリを実行して分析できるようにする必要があります。
1時間ごとに、何千ものトランザクションが新しいステータスで更新されます。最初のデータセットのサイズは1.5PBで、1日に3TBずつ増えていきます。データは高度に構造化されており、データサイエンスチームはこのデータに基づいて機械学習モデルを構築することになります。あなたは、データサイエンスチームのために、パフォーマンスとユーザビリティを最大化したいと考えています。
どの戦略を採用すべきでしょうか？（2つ選択）
1. 可能な限りデータを非正規化する
2. 
3. 
4. ステータスの更新を、UPDATEではなくBigQuery APPENDにするデータパイプラインを開発する
<details><div>
    答え：1,4
説明
膨大なデータが日々追加される場合のBigQueryのテーブルと更新方法を設計する必要があります。
高度に構造化された膨大なデータを高いパフォーマンスでクエリする際は、非正規化をすることが有効です。
これによって、クエリを行うレコードの範囲を絞ることができるため、クエリ時間の短縮が可能になります。
また、大量のデータを日々更新する場合は、BigQuery UPDATEよりもAPPENDの方がパフォーマンスに優れています。
したがって、正解は以下の通りです。
- 可能な限りデータを非正規化する
- ステータスの更新を、UPDATEではなくBigQuery APPENDにするデータパイプラインを開発する
参照：
https://cloud.google.com/solutions/bigquery-data-warehouse#denormalizing_data
https://cloud.google.com/solutions/bigquery-data-warehouse#handling_change
</div></details>

### Q. 問題31: 未回答
あなたは、自社のETLパイプラインをApache Hadoopクラスター上で実行できるようにする責任があります。パイプラインは、いくつかのチェックポイントと分割パイプラインを必要とします。
あなたはどの方法でパイプラインを書くべきでしょうか？
1. Pigを使ったPigLatin
2. 
3. 
4. 
<details><div>
    答え：1
説明
チェックポイントと分割パイプラインがある処理に適した方法を選択する必要があります。
Pigは大規模なデータセットを分析するためのプラットフォームです。
Hadoop上に構築されており、プログラミングの容易さ、最適化の機会、拡張性を提供する。
Pig Latinはリレーショナルデータフロー言語であり、Pigのコアなコンポーネントの1つです。
データパイプラインの構築の際には、いくつかの理由からSQLよりもPig Latinの方がより優れた選択になります。
- Pig Latinでは、パイプラインの開発者が、パイプラインのどこにデータをチェックポイントするか決めることができます。
- Pig Latinでは、オプティマイザに頼らず、特定の演算子の実装を直接選択することができます。
- Pig Latinはパイプラインの分割をサポートする。
- Pig Latinでは、開発者がデータパイプラインのほとんどどこにでも独自のコードを挿入することができます。
Pig Latinを用いることで、柔軟なパイプラインを構築することが可能になります。
したがって、正解は「Pigを使ったPigLatin」です。
参照：
http://maheshwaranm.blogspot.com/2013/07/comparing-pig-latin-and-sql-for.html
</div></details>

### Q. 問題32: 未回答
Cloud Dataprocクラスター上でスケジュール通りに実行される複数のSparkジョブがあります。いくつかのジョブは順番に実行され、いくつかのジョブは同時に実行されています。あなたは、このプロセスを自動化する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. Cloud ComposerでDirected Acyclic Graphを作成する
4. 
<details><div>
    答え：3
説明
パイプラインの実行を自動化するマネージドサービスを選択する必要があります。
Cloud ComposerはApache Airflow で構築された、フルマネージドのワークフロー オーケストレーション サービスです。
DAG（Direct Acyclic Graph）と呼ばれるデータ形式でジョブ同士の関係性を定義し、実行します。
Cloud Composer はフルマネージド サービスであり、Apache Airflow は互換性に優れているため、リソースのプロビジョニングに気をとられず、ワークフローの作成、スケジューリング、モニタリングに専念できます。
BigQuery、Dataflow、Dataproc、Datastore、Cloud Storage、Pub/Sub、AI Platform などの Google Cloud プロダクトとのエンドツーエンドの統合により、ユーザーはパイプラインを自由かつ完全にオーケストレートできます。
これによって、複数の依存関係のあるジョブを、少ないオーバーヘッドで実行することが可能になります。
したがって、正解は「Cloud ComposerでDirected Acyclic Graphを作成する」です。
参照：
https://cloud.google.com/composer/docs/how-to/using/writing-dags
https://cloud.google.com/composer/?hl=en
</div></details>

### Q. 問題33: 未回答
あなたは、スケジュール通りに実行しなければならない複数のバッチジョブを実装しています。これらのジョブには、特定の順序で実行しなければならない多くの相互に依存するステップがあります。ジョブの一部には、シェルスクリプトの実行、Hadoopジョブの実行、BigQueryでのクエリの実行が含まれています。ジョブの実行時間は数分から数時間を想定しています。手順が失敗した場合は、一定の回数を再試行する必要があります。
これらのジョブの実行を管理するために、どのサービスを使用しますか？
1. 
2. 
3. 
4. Cloud Composer
<details><div>
    答え：4
説明
ジョブの依存関係をコントロールしつつ実行を自動化するマネージドサービスを選択する必要があります。
Cloud ComposerはApache Airflow で構築された、フルマネージドのワークフロー オーケストレーション サービスです。
DAG（Direct Acyclic Graph）と呼ばれるデータ形式でジョブ同士の関係性を定義し、実行します。
Cloud Composer はフルマネージド サービスであり、Apache Airflow は互換性に優れているため、リソースのプロビジョニングに気をとられず、ワークフローの作成、スケジューリング、モニタリングに専念できます。
BigQuery、Dataflow、Dataproc、Datastore、Cloud Storage、Pub/Sub、AI Platform などの Google Cloud プロダクトとのエンドツーエンドの統合により、ユーザーはパイプラインを自由かつ完全にオーケストレートできます。
これによって、複数の依存関係のあるジョブを、少ないオーバーヘッドで実行することが可能になります。
したがって、正解は「Cloud Composer」です。
参照：
https://cloud.google.com/composer/docs/how-to/using/writing-dags
https://cloud.google.com/composer/?hl=en
</div></details>

### Q. 問題34: 未回答
Cloud Dataflowジョブでデータパイプラインを構築し、時系列メトリクスを集約してCloud Bigtableに書き出します。このデータは、組織全体で何千人ものユーザーが使用するダッシュボードに供給されます。同時接続ユーザーの増加に対応し、データの書き込みに必要な時間を短縮する必要があります。
あなたが取るべきアクションはどれですか？（2つ選択）
1. 
2. PipelineOptionsでmaxNumWorkersを設定して、Cloud Dataflowのワーカーの最大数を増やす
3. Cloud Bigtableクラスターのノード数を増やす
4. 
<details><div>
    答え：2,3
説明
接続ユーザーが増加すると、メトリクスの集約をする処理の負荷が増加します。
これはDataflow、Bigtableそれぞれに対しての負荷となるため、それぞれのコンピューティングリソースを増強するための対策が必要です。
Cloud Dataflowでは、ワーカーという単位でデータの処理を行います。
このワーカーは、スケールアップとスケールアウトに対応しているため、ワーカーインスタンスのサイズを大きくすること、ワーカー数を増やすことで、処理をスムーズに行うことができます。
ワーカー数を増やす際は、maxNumWorkersを設定することで実現することができます。
Bigtableでは、クラスタにノードを追加することで、スループットを高めることができます。
例えば3つのノードからなるクラスタにさらに3つのノードを追加することで、書き込みスループットは2倍になります。
したがって、正解は以下の通りです。
- PipelineOptionsでmaxNumWorkersを設定して、Cloud Dataflowのワーカーの最大数を増やす
- Cloud Bigtableクラスターのノード数を増やす
参照：
https://cloud.google.com/bigtable/docs/performance#performance-write-throughput
https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-pipeline-options
</div></details>

### Q. 問題35: 未回答
あなたはデータパイプラインにセキュリティのベストプラクティスを導入しています。現在は、プロジェクトオーナーとして手動でジョブを実行しています。これらのジョブを自動化するには、非公開情報を含むバッチファイルをGoogle Cloud Storageから毎晩取得し、Google Cloud Dataprocクラスター上のSpark Scalaジョブで処理し、その結果をGoogle BigQueryに取り込む必要があります。Dataprocクラスター上でSpark Scalaジョブで処理し、その結果をGoogle BigQueryにデポジットすることで、これらのジョブを自動化したいと考えています。
このワークロードを安全に実行するにはどうすればよいでしょうか。
1. 
2. 
3. バッチファイルの読み取りと BigQuery への書き込みが可能なサービスアカウントを使用する
4. 
<details><div>
    答え：3
説明
秘匿情報を扱うデータパイプラインの処理を、Google Cloudのベストプラクティスに則りセキュアに実行する必要があります。
今回遵守すべきGoogle Cloudのベストプラクティスは最小権限の原則とサービスアカウントの使用になります。
今回は、バッチファイルの読み取りとBigQueryへの書き込みのみできれば良いため、これらの権限が付与されたサービスアカウントを使用することが最適です。
したがって、正解は「バッチファイルの読み取りと BigQuery への書き込みが可能なサービスアカウントを使用する」です。
参照：
https://cloud.google.com/iam/docs/best-practices-for-using-and-managing-service-accounts
</div></details>

### Q. 問題36: 未回答
あなたの会社では、データレイクとしてマネージドHadoopシステムを構築したいと考えています。データ変換プロセスは、連続して実行される一連のHadoopジョブで構成されています。ストレージとコンピュートを分離するという設計を実現するために、Cloud Storageコネクタを使用して、すべての入力データ、出力データ、および中間データを保存することにしました。しかし、あるHadoopジョブの実行速度が、オンプレミスのベアメタルHadoop環境（100GBのRAMを搭載した8コアノード）と比較して、Cloud Dataprocでは非常に遅いことに気付きました。分析の結果、この特定のHadoopジョブはディスクI/Oが集中していることがわかりました。あなたはこの問題を解決したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. Hadoop クラスタに十分な永続的ディスク領域を割り当て、その特定の Hadoop ジョブの中間データをネイティブ HDFS に格納する
3. 
4. 
<details><div>
    答え：2
説明
Dataproc は、Apache Hadoop および Hadoop 分散ファイル システム（HDFS）と統合されています。
この際、ローカルHDFSストレージを用いることで、パフォーマンスを高めることができる場合があります。
ローカルHDFSストレージは、以下のような場合に有効なオプションです。
- ジョブで多くのメタデータ操作が必要な場合。例えば、数千のパーティションやディレクトリがあり、各ファイルのサイズが比較的小さい場合。
- HDFSのデータを頻繁に変更したり、ディレクトリの名前を変更したりする場合。クラウドストレージのオブジェクトは不変なので、ディレクトリ名の変更は、すべてのオブジェクトを新しいキーにコピーし、その後削除することになるため、高価な操作となります。
- HDFSのファイルに対してappend操作を多用する場合。
- 重いI/Oを伴うワークロードがある場合。例えば、次のようなパーティション化された書き込みを多く行っている場合。
-- spark.read().write.partitionBy(...).parquet("gs://")のようなパーティションによる書き込みが多い場合。
- レイテンシーに特に敏感なI/Oワークロードがある場合。たとえば、ストレージ操作ごとに1桁のミリ秒のレイテンシが必要な場合です。
したがって、正解は「Hadoop クラスタに十分な永続的ディスク領域を割り当て、その特定の Hadoop ジョブの中間データをネイティブ HDFS に格納する」です。
参照：
https://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance
https://cloud.google.com/solutions/migration/hadoop/hadoop-gcp-migration-jobs
</div></details>

### Q. 問題37: 未回答
あなたはCloud Dataprocのクラスターを管理しています。あなたは、クラスター上で進行中の作業を失うことなく、コストを最小限に抑えながらジョブを高速に実行する必要があります。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. プリエンプティブルなワーカーノードのクラスタサイズを大きくし、グレースフルデコミッショニングを使用するように設定する
<details><div>
    答え：4
説明
コスト最適化された方法でジョブを高速に実行する方法を選択する必要があります。
Dataproc クラスタを作成した後、クラスタ内のプライマリ ワーカーノードまたはセカンダリ ワーカーノードの数を増減することで（水平方向のスケーリング）、クラスタを調整（「スケール」）できます。
Dataproc クラスタは、クラスタでジョブを実行している場合も含めいつでもスケールできます。
既存のクラスタのマシンタイプは変更できません（垂直方向のスケーリング）。
垂直方向のスケーリングを行うには、サポートされているマシンタイプを使用してクラスタを作成してから、その新しいクラスタにジョブを移行します。
Dataprocのようなバッチの場合は、コンピューティングリソースが時間課金という特性を活かして、水平スケーリングを積極的に活用するべきです。
つまり、一つのコンピューティングリソースでワークロードを処理する場合も、複数のリソースでワークロードを処理する場合も原理的には変わらないという性質を利用します。
また、ワーカーノードには、プリエンプティブルなインスタンスを用いることで、コスト削減も実現できます。
したがって、正解は「プリエンプティブルなワーカーノードのクラスタサイズを大きくし、グレースフルデコミッショニングを使用するように設定する」です。
参照：
https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters
</div></details>

### Q. 問題38: 未回答
あなたの会社は、Google Cloud StorageとGoogle Compute Engine上のCassandraクラスタに保存されている非常に大きなデータセットに対して複雑な分析を行いたいと考えている新しいデータサイエンティストを雇いました。このサイエンティストは、主に機械学習プロジェクトのためのラベル付きデータセットを作成し、いくつかの可視化タスクを行いたいと考えています。彼女は、自分のラップトップがタスクを実行するのに十分なパワーを持っていないことを報告しました。実際にそのことによって彼女の開発速度が低下しています。あなたは、彼女のタスクの実行を支援したいと考えています。
要件を達成するためにするべきことは何ですか？
1. 
2. 
3. 
4. Google Cloud DatalabをGoogle Compute Engine上の仮想マシン（VM）にデプロイする
<details><div>
    答え：4
説明
ユーザーのローカル環境のコンピューティングリソースが限られている場合は、Google Cloud上のコンピューティングリソースを活用することが有効です。
クラウドデータラボは、Google Cloud Platform上でデータの探索、分析、変換、可視化、機械学習モデルの構築を行うために作成された強力な対話型ツールです。
Google Compute Engine上で動作し、複数のクラウドセンディに簡単に接続できるため、データサイエンスのタスクに集中することができます。
したがって、正解は「Google Cloud DatalabをGoogle Compute Engine上の仮想マシン（VM）にデプロイする」です。
参照：
https://cloud.google.com/datalab/
</div></details>

### Q. 問題39: 未回答
Apache Kafkaを中心に構築されたIoTパイプラインを運用しており、通常は1秒間に約5000件のメッセージを受信しています。Google Cloud Platformを使用して、1時間の移動平均が毎秒4000メッセージを下回ったらすぐにアラートを作成したいと考えています。
要件を達成するためにするべきことは何ですか？
1. Cloud DataflowでKafka IOを使ってデータのストリームを消費する。5分ごとに1時間のスライディング・タイム・ウィンドウを設定する。ウィンドウが閉じたときに平均値を計算し、平均値が4000メッセージ未満の場合はアラートを送信する
2. 
3. 
4. 
<details><div>
    答え：1
説明
Kafkaを用いたIoTパイプラインの処理として利用可能なサービスを選択する必要があります。
Dataflow と KafkaIO を使用してメッセージのストリーミングパイプラインを構築することが可能です。
この場合、Kafka は Google Cloud の外部にありますが、Dataflow を使用して Google Cloud の内部でメッセージを処理します。
この際Dataflowにネイティブに備わっているウィンドウ関数を使用することで、メッセージ数に応じたアクションを行うことが可能です。
ウィンドウ関数は、制限なしコレクションを論理的な要素、つまりウィンドウに分割します。
ウィンドウ関数は、個々の要素のタイムスタンプで制限なしコレクションをグループ化します。各ウィンドウには一定数の要素が入ります。
スライディングタイム ウィンドウは、データ ストリーム内の一定の時間間隔を表すことができます。
タンブリング ウィンドウは重なりませんが、ホッピング ウィンドウはスライディングするため、データ取得期間が重なることがあります。
たとえば、ホッピング ウィンドウが 30 秒ごとに開始し、1 分間のデータとウィンドウを持つ場合があります。
ホッピング ウィンドウの開始間隔はピリオドといいます。この例では、1 分間のウィンドウと 30 秒のピリオドが設定されています。
今回の場合は、スライディングウィンドウによって、平均値の計算と計算結果に基づくアラートを設定することが有効です。
したがって、正解は「Cloud DataflowでKafka IOを使ってデータのストリームを消費する。5分ごとに1時間のスライディング・タイム・ウィンドウを設定する。ウィンドウが閉じたときに平均値を計算し、平均値が4000メッセージ未満の場合はアラートを送信する」です。
参照：
https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp
</div></details>

### Q. 問題40: 未回答
あなたは世界的な海運会社で働いています。40TBのデータを使ってモデルを学習し、各リージョンのどの船がその日に配送遅延を起こす可能性が高いかを予測したいと考えています。このモデルは、複数のソースから収集した複数の属性に基づいています。GeoJSON形式の位置情報を含むテレメトリデータを各船舶から取得し、1時間ごとにロードします。このデータを元に、あるリージョンで遅延を起こす可能性のある船舶の数と種類を示すダッシュボードを用意したいと考えています。また、予測と地理空間処理のためのネイティブ機能を備えたストレージソリューションを使用したいと考えています。
どのストレージソリューションを使うべきでしょうか？
1. BigQuery
2. 
3. 
4. 
<details><div>
    答え：1
説明
地理情報を含むGeoJSON形式をサポートしておりかつ、可視化ツールとネイティブな連携が可能なストレージサービスを選択する必要があります。
BigQuery のようなデータ ウェアハウスでは、位置情報が非常によく使われます。
多くの重要なビジネス上の決定は、位置情報を中心に行われます。
たとえば、配送車両またはパッケージの緯度と経度を時間の経過とともに記録できます。
また、顧客トランザクションを記録し、店舗の位置情報を使用して別のテーブルにデータを結合することもできます。
この種の位置情報を使用して、パッケージがいつ到着するかの判断や、特定の店舗の位置のメーラーを受け取る顧客の判別ができます。
地理空間分析では、地理データ型と標準の SQL 地理関数を使用して、BigQuery で地理空間データを分析し、可視化できます。
したがって、正解は「BigQuery」です。
参照：
https://cloud.google.com/bigquery/docs/gis-intro
</div></details>

### Q. 問題41: 未回答
あなたはBigQuery、Cloud Dataflow、Cloud Dataprocでデータパイプラインを実行しています。ヘルスチェックや動作の監視を行い、障害が発生した場合はパイプラインを管理しているチームに通知する必要があります。また、複数のプロジェクトにまたがって機能することも必要です。プラットフォームの機能のうち、マネージドサービスを使用することを希望しています。
要件を達成するためにするべきことは何ですか？
1. Cloud Monitoringに情報をエクスポートし、Alertingポリシーを設定する
2. 
3. 
4. 
<details><div>
    答え：1
説明
Cloud Monitoringは、アプリケーションとインフラストラクチャのパフォーマンス、可用性、健全性を可視化できます。
Cloud Monitoring には、Google Cloud サービス用に自動ですぐに使用できる指標を集めたダッシュボードが用意されています。
また、ハイブリッドおよびマルチクラウド環境のモニタリングもサポートします。
指標、ダッシュボード、稼働時間モニタリング、アラートのすべてが 1 つのサービスに統合されているため、システム間のアクセスにかかる時間を短縮できます。
コンテキストでのオブザーバビリティにより、複数のプロジェクトにまたがって、Google Cloud リソースページ内で指標が利用可能になります。
したがって、正解は「Cloud Monitoringに情報をエクスポートし、Alertingポリシーを設定する」です。
参照：
https://cloud.google.com/monitoring/workspaces/#account-project
https://cloud.google.com/monitoring
</div></details>

### Q. 問題42: 未回答
天気情報アプリは、15分ごとにデータベースに問い合わせて現在の気温を取得します。フロントエンドはGoogle App Engineを使用しており、何百万人ものユーザーが利用しています。
データベースの障害に対応するために、フロントエンドをどのように設計すべきでしょうか？
1. 
2. 指数バックオフでクエリを再試行し、上限を15分とする
3. 
4. 
<details><div>
    答え：2
説明
データベースの障害が発生した際にリクエストを一定間隔で送り続けると、負荷増大による更なる障害につながる可能性があり、これを回避する必要があります。
指数バックオフを使用すると、アプリケーションでデータベースに接続できないときに、アプリケーションによって応答しない数の接続リクエストが送信されるのを防ぎます。
この再試行は、最初に接続するときや、プールから最初に接続を取得したときにのみ有効です。
トランザクションの途中でエラーが発生した場合、アプリケーションはトランザクションの最初から再試行する必要があります。
そのためプールの構成が適切でも、接続が失われるとアプリケーションにエラーが表示されることがあります。
したがって、正解は「指数バックオフでクエリを再試行し、上限を15分とする」です。
参照：
https://cloud.google.com/sql/docs/mysql/manage-connections
</div></details>

### Q. 問題43: 未回答
あなたの会社は、バッチベースとストリームベースの両方のイベントデータを受け取っています。Google Cloud Dataflowを使用して、予測可能な期間にデータを処理したいと考えています。
しかし、場合によってはデータの到着が遅れたり、順番が狂ったりすることがあることを認識しています。
遅れたり順番が狂ったりしたデータを処理するために、Cloud Dataflowのパイプラインをどのように設計すればよいでしょうか。
1. 
2. 
3. ウォーターマークとタイムスタンプを使用して、遅延したデータをキャプチャする
4. 
<details><div>
    答え：3
説明
Dataflowによって順序が担保されない場合、タイムスタンプデータとウォーターマークを使用してアプリケーション側で順序を保証する必要があります。
ウォーターマークとは、Dataflow でウィンドウのすべてのデータが必要になるしきい値を表する。
新しく受信したデータのタイムスタンプがウォーターマークより古い場合、データは遅延データとみなされます。
Dataflow では、次の理由からウォーターマークを追跡する。
- データを時間順や予測可能な間隔で受信できるとは限りません。
- データイベントは、生成時と同じ順序でパイプラインに提供されるとは限りません。
データソースによってウォーターマークが決まります。
Apache Beam SDK では、遅延データを許可できます。
Dataflow SQL は遅延データを処理しません。
したがって、正解は「ウォーターマークとタイムスタンプを使用して、遅延したデータをキャプチャする」です。
参照：
https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines
</div></details>

### Q. 問題44: 未回答
あなたは、ジョブジェネレーターとジョブランナーという2つの異なるタイプのアプリケーション間でデータを共有するために、新しいデータパイプラインを構築しています。このソリューションは、使用量の増加に合わせて拡張する必要があり、既存のアプリケーションのパフォーマンスに悪影響を与えることなく、新しいアプリケーションの追加に対応しなければなりません。
要件を達成するためにするべきことは何ですか？
1. 
2. Cloud Pub/Subトピックを使用してジョブを発行し、サブスクリプションを使用してジョブを実行する
3. 
4. 
<details><div>
    答え：2
説明
スケーラビリティを持ったデータパイプラインを構築する必要があります。
Pub/Sub は、パブリッシャー / サブスクライバーの略で、これを使用するとサービスが 100 ミリ秒程度のレイテンシで非同期に通信できます。
Pub/Sub は、データを取り込んで配布するためのストリーミング分析とデータ統合パイプラインに使用されます。
これは、サービスの統合を目的としたメッセージング指向のミドルウェア、または、タスクを同時に読み込むキューとして使用されます。
パブリッシャーは、イベントが処理される方法やタイミングとは無関係に、Pub/Sub サービスにイベントを送信します。
その後、Pub/Sub によって、イベントに反応する必要のあるすべてのサービスにイベントが配信されます。
サブスクライバーがデータを受信するのをパブリッシャーが待たなければならない RPC を介して通信するシステムと比べて、このような非同期統合のほうがシステム全体の柔軟性と堅牢性が向上します。
したがって、正解は「Cloud Pub/Subトピックを使用してジョブを発行し、サブスクリプションを使用してジョブを実行する」です。
参照：
https://cloud.google.com/pubsub/docs/overview
https://cloud.google.com/pubsub
</div></details>

### Q. 問題45: 未回答
あなたの会社では、多数のレイヤーを持つTensorFlowのニュートラルネットワークモデルを構築しました。このモデルは、学習データに対してはうまく適合します。しかし、新しいデータに対してテストすると、パフォーマンスが低下してしまいます。
この問題に対処するために、どのような方法を採用できますか？
1. 
2. 
3. ドロップアウト率の増加
4. 
<details><div>
    答え：3
説明
機械学習モデルを正しく評価するためには、学習データとは異なるデータを用いたテストでの性能を評価する必要があります。
テストでの性能のことを汎化性能といい、この性能が実問題での性能であると考えることができます。
トレーニングデータセットに対する性能とテストデータセットに対する性能に応じて、モデルの学習状況の問題は大きく２つに分類されます。
- 過剰適合（overfitting）
トレーニングデータセットに対する性能が良いが、テストデータセットに対する性能が低い状態です。
実際の問題にモデルを使用した際に、思ったような性能が得られない可能性があるため、モデルに以下のようなチューニングを加える必要があります。
-- 正則化の制約を増やす
-- ドロップアウトを増やす
-- 次元圧縮（特徴量の数を減らす）
-- （ニューラルネットワークの場合）レイヤーを減らして単純なモデルにする
-- 学習率を小さくする
- 適合不足（underfitting）
トレーニングデータセットに対する性能も、テストデータセットに対する性能も低い状態です。
モデルのチューニング等を行ってもこの状態が改善されない場合は、データセット自体を増やす必要があります。
データセットはマニュアルでのデータセットの追加や、データ拡張による追加が有効です。
また、特徴量が少なすぎることにより、現象を正しくとらえられていない可能性もあります。
この場合は、新たな特徴量を追加する、特徴量エンジニアリングによって特徴量を合成する、といった対処が必要です。
今回であれば、過剰適合を防止するために必要なアクションを取る必要があります。
したがって、正解は「ドロップアウト率の増加」です。
参照：
https://www.ydc.co.jp/column/mi/mieruka03.html
https://towardsdatascience.com/dealing-with-highly-dimensional-data-using-principal-component-analysis-pca-fea1ca817fe6
</div></details>

### Q. 問題46: 未回答
Node.jsで書かれたCloud Functionsが、Cloud Pub/Subからメッセージを引き出し、データをBigQueryに送信しています。Pub/Subトピックのメッセージ処理速度が予想よりも遥かにかかっていることが確認されましたが、Cloud Monitoring Log Viewerにはエラーが記録されていません。
この問題の原因として考えられるものはどれですか。（2つ選択）
1. 
2. 
3. サブスクライバのコードのエラー処理がランタイムエラーを適切に処理していない
4. 
5. サブスクライバのコードは、プルをしたメッセージを確認しない
<details><div>
    答え：3,5
説明
Cloud Pub/SubとCloud Functionsの連携によるデータ処理で発生するトラブルの解決が必要です。
Cloud Pub/Subで配信されるデータは、プルをするだけでなく、確認を明示的に行う必要があります。
この処理を行わない場合、再度メッセージがPub/Sub内に蓄積されてしまいます。
これによって、新しいデータを処理することができずに時間を浪費してしまいます。
また、Cloud Functionsのエラー処理を行うためには、
したがって、正解は以下の通りです。
- サブスクライバのコードのエラー処理がランタイムエラーを適切に処理していない
- サブスクライバのコードは、プルをしたメッセージを確認しない
参照：
https://cloud.google.com/functions/docs/bestpractices/retries
https://cloud.google.com/pubsub/docs/pull
</div></details>

### Q. 問題47: 未回答
あなたは、不動産物件のデータセットに基づいて、住宅価格を予測するモデルを学習しています。今回は、全結合層のみで構成されるのニューラルネットを学習する予定です。また、データセットには物件の緯度と経度が含まれていることがわかりました。不動産の専門家によると、物件の位置情報は価格に大きく影響するとのことなので、この物理的な依存性を組み込んだ1つの特徴量を作りたいと考えています。
どのような手法を用いることが最適ですか？
1. 
2. 
3. 緯度と経度の特徴量の掛け合わせを作成し、分レベルでバケット化し、最適化の際にL1正則化を使用する
4. 
<details><div>
    答え：3
説明
各特徴量の重要度が異なるということが予め分かっており、またそれらを組み合わせる必要があります。
複数の特徴量から1つの特徴量を生成するためには、それらの特徴量を一定のルールに基づいて掛け合わせることが必要です。
緯度と軽度の値をパラメータに取り、それらに対して四則演算を行うことで、一つの特徴量を作成することができます。
また、特徴量を増やした場合や、生成した特徴量が他に比べて重要であると分かっている場合はL1正則化を使用することが有効です。
L1正則化はL2正則化と異なり、重要でない特徴量の重みを0にするため過学習を効率よく防ぐことができます。
したがって、正解は「緯度と経度の特徴量の掛け合わせを作成し、分レベルでバケット化し、最適化の際にL1正則化を使用する」です。
参照：
https://www.coursera.org/lecture/feature-engineering-jp/te-zheng-kurosunogai-yao-Jvgym
https://qiita.com/k3nNy_51rcy/items/832eab321c602aa587c6
</div></details>

### Q. 問題48: 未回答
Google Cloudにデータパイプラインを導入するにあたり、20TBのテキストファイルのストレージを設計しています。入力データはCSV形式です。複数のユーザーが複数のエンジンでCloud Storageのデータを照会する場合、集約値の照会コストを最小化したいと考えています。
どのストレージサービスとスキーマデザインを使用すべきでしょうか？
1. 
2. 
3. ストレージにはCloud Storageを使用する。問い合わせ用にBigQueryのパーマネントテーブルとしてリンクする
4. 
<details><div>
    答え：3
説明
適切なストレージを選択しつつ、参照のためのコスト最適なテーブルを設計する必要があります。
Cloud StorageはCSVデータを低コストで大量に保存する際に最適なストレージです。
また、Cloud Storageに保存されたCSVファイルは、BigQueryによるクエリがネイティブにサポートされているので、データ照会を容易に行うことができます。
BigQueryのテーブル設計について注意が必要な点は、照会コストを最小にするという要件です。
一時テーブルを利用する場合は、BigQuery データセットの 1 つにテーブルを作成するわけではありません。
つまり、テーブルはデータセットに恒久的に保存されないので、クエリを行うたびにコストがかかります。
永続的なテーブルは、データセットに作成され、外部データソースにリンクされます。
テーブルが永続的であるため、データセットレベルのアクセス制御を使用して、基礎となる外部データソースへのアクセス権を持つ他の人とテーブルを共有し、低コストでテーブルを照会することが可能になります。
したがって、正解は「ストレージにはCloud Storageを使用する。問い合わせ用にBigQueryのパーマネントテーブルとしてリンクする」です。
参照：
https://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer
https://cloud.google.com/bigquery/external-data-cloud-storage
</div></details>

### Q. 問題49: 未回答
細胞組織に関するデータベースに格納されているデータを使用して、特定の細胞組織サンプルが将来悪性腫瘍になるかどうかの分類したいと考えています。細胞組織サンプルを分類するために、教師なしの異常検出法を評価しています。
この方法をサポートするデータ特性はなんですか？（2つ選択）
1. 正常なサンプルと比較して、変異の発生が非常に少ない
2. 
3. 
4. 将来の変異は、データベース内の変異したサンプルと類似した特徴を持つと予想される
<details><div>
    答え：1,4
説明
教師なし学習によって行う異常検知は、異常データが正常データの集団と比較して分離ができる（＝外れ値である）という前提に立っています。
これは言い換えると、異常データの発生確率は正常データの発生確率に比べて少なく、正常データと比較して既存の観測されている異常データと新たな異常データが近しい特徴を持つ、ということになります。
一方で、教師あり学習を行うことができるデータは、すでに正常/異常のラベルがついており、モデル作成可能な比率で正常/異常のデータが一定数存在しています。
したがって、正解は以下の通りです。
- 正常なサンプルと比較して、変異の発生が非常に少ない
- 将来の変異は、データベース内の変異したサンプルと類似した特徴を持つと予想される
参照：
https://products.sint.co.jp/aisia-ad/blog/possible-to-detect-anomalies-without-teacher-data
https://dev.classmethod.jp/articles/yoshim-sagemaker-rcf/#sec2
</div></details>

### Q. 問題50: 未回答
あなたは、ユーザーの注文する料理を予測する機械学習ベースの食品注文サービスのデータベーススキーマを設計しています。以下は、あなたが保存する必要のある情報の一部です。
- ユーザーのプロフィール。ユーザーが好きなもの、嫌いなもの
- ユーザーのアカウント情報。名前、住所、好みの食事時間
- 注文情報。いつ、どこから、どのお店に注文するか
データベースは、サービスのすべてのトランザクションデータを格納するために使用されます。あなたはデータスキーマを最適化したいと考えています。
どのGoogle Cloud Platformサービスを使うべきでしょうか？
1. 
2. 
3. 
4. Cloud Datastore
<details><div>
    答え：4
説明
ユーザープロファイルは、ユーザーの過去の活動や嗜好に基づいて、ユーザーの体験をカスタマイズするためのものです。
そのため、リレーショナルデータベースよりも柔軟なスキーマ構造を保持する必要があります。
Datastoreの柔軟なスキーマにより、アプリケーションの新機能をサポートするために新しいプロパティを追加するなど、時間の経過とともにユーザプロファイルの構造を変化させることができます。
スキーマの変更はダウンタイムなしで行われ、ユーザー数が増加してもパフォーマンスが低下することはありません。
したがって、正解は「Cloud Datastore」です。
参照：
https://cloud.google.com/architecture/building-scalable-web-apps-with-cloud-datastore
https://cloud.google.com/architecture/building-scalable-web-apps-with-cloud-datastore#integration-with-other-gcp-products</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>
