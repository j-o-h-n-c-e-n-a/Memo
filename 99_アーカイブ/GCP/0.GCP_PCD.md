



## 模擬テスト
### Q. 
クラスタ内部での保持が必要なさまざまなマイクロサービスを使用するアプリケーションを開発しているとします。特定の数のレプリカを使用して各マイクロサービスを構成できる必要があります。また、マイクロサービスをどのくらいの数のレプリカにスケーリングするかにかかわらず、一貫した方法で、特定のマイクロサービスを別のマイクロサービスからアドレス指定できる必要もあります。このソリューションは Google Kubernetes Engine で実装する予定です。どうすればよいですか。

1. 各マイクロサービスを Deployment としてデプロイする。Service を使用してクラスタで Deployment を公開する。Service の DNS 名を使用して、クラスタ内の他のマイクロサービスから Deployment をアドレス指定する。
2. 各マイクロサービスを Deployment としてデプロイする。Ingress を使用してクラスタで Deployment を公開する。Ingress の IP アドレスを使用して、クラスタ内の他のマイクロサービスから Deployment をアドレス指定する。
3. 各マイクロサービスを Pod としてデプロイする。Service を使用してクラスタで Pod を公開する。Service の DNS 名を使用して、クラスタ内の他のマイクロサービスから特定のマイクロサービスをアドレス指定する。
4. 各マイクロサービスを Pod としてデプロイする。Ingress を使用してクラスタで Pod を公開する。クラスタ内の他のマイクロサービスから Pod をアドレス指定するために Ingress の IP アドレスを使用する。
<details><div>
フィードバック
A が正解です。Service はクラスタ内部に DNS エントリを持っており、他のマイクロサービスはそれを使用して、Service のターゲットとなっている Deployment の各 Pod をアドレス指定できます。
B は不正解です。Ingress は外部または内部の HTTP(S) ロードバランサを使用して Service を公開し、直接 Deployment には適用されません。
C は不正解です。Pod はマイクロサービスの単一インスタンスであり、Deployment が複数のレプリカで構成される場合があります。
D は不正解です。B と C の両方の誤りが組み合わされています。
 
https://cloud.google.com/kubernetes-engine/docs/concepts/service
</div></details>

### Q. 
アプリケーションが Google Kubernetes Engine（GKE）クラスタのコンテナとして実行されています。Kubernetes API サーバーへの呼び出しによって Secret が公開されることを防ぐ、安全なアプローチを使用して、アプリケーションに Secret を追加する必要があります。どうすればよいですか。

1. Kubernetes Secret を作成し、その Secret を環境変数としてコンテナに渡す。
2. Cloud Key Management Service（KMS）鍵を使用して、GKE アプリケーションレイヤの Secret の暗号化をクラスタで有効化する。
3. Cloud KMS に Secret を格納する。Cloud KMS から Secret を読み取るために Google サービス アカウントを作成する。サービス アカウント キーを JSON 形式でエクスポートしてその JSON ファイルを Cloud KMS から Secret を読み取ることができる ConfigMap Volume としてコンテナにマウントする。
4. Secret を Secret Manager に格納する。Secret Manager から Secret を読み取るための Google サービス アカウントを作成する。コンテナを実行するための Kubernetes サービス アカウントを作成する。Workload Identity を使用して Google サービス アカウントとして認証する。
<details><div>
フィードバック
A は不正解です。Kubernetes Secret は文字列をエンコードするのみであるため、Secret を読み取ることができる誰もがそれをデコードできます。
B は不正解です。GKE アプリケーションレイヤで Secret を暗号化しても、Secret はやはり Kubernetes API サーバーからの base64 エンコードされた文字列として提示されます。
C は不正解です。Cloud KMS は実際の Secret ではなく暗号鍵の格納に使用されます。また、Google サービス アカウント キーを Volume として渡すとしていますが、Secret を読み取ることができる誰もがこれを読み取ることができます。
D は正解です。これにより、Secret を安全に格納するサービスと、Workload Identity で安全に Secret を取得するアプローチが提供されます。
 
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
 
https://cloud.google.com/kms/docs
 
https://kubernetes.io/docs/concepts/configuration/secret/
 
https://cloud.google.com/secret-manager/docs/overview
</div></details>

### Q. 
Istio を使用し、Google Kubernetes Engine（GKE）でマイクロサービス アプリケーションを管理しているとします。Istio AuthorizationPolicy、Kubernetes NetworkPolicy、相互 TLS（mTLS）を GKE クラスタに実装することで、マイクロサービス間の通信チャネルのセキュリティを確保しています。 2 つの Pod 間での特定の URL に対する HTTP リクエストが失敗することが判明しました。一方、Pod 間での他の URL に対するリクエストは成功します。

A.  Kubernetes NetworkPolicy リソースによって、Pod 間の HTTP トラフィックがブロックされている。
B.  HTTP リクエストを開始した Pod が、不適切な TCP ポートを介してターゲット Pod に接続しようとしている。
C.  クラスタの AuthorizationPolicy によって、アプリケーション内で特定のパスの HTTP リクエストがブロックされている。
D.  クラスタで mTLS が制約なしモードに設定されているものの、Pod のサイドカー プロキシによって、暗号化されていないトラフィックが書式なしテキスト形式で送信されている。
<details><div>
フィードバック
A は不正解です。Kubernetes NetworkPolicy のリソースでは、選択されたパスではなく、Pod のグループ間の HTTP トラフィックをブロックできます。(https://kubernetes.io/docs/concepts/services-networking/network-policies/).
B は不正解です。クライアントの Pod がサーバーと通信するために使用するポートが誤っていると、Pod のリクエストはすべての URL パスで失敗します。
C は正解です。Istio 認可ポリシーによって、Pod 間で特定の URL パスについて HTTP メソッドをブロックできます（https://istio.io/latest/docs/tasks/security/authorization/authz-http/）。
D は不正解です。Istio を使用した mTLS 構成によって HTTP リクエストの失敗は発生しません。PERMISSIVE モード（デフォルト構成）では、サービスは平文と mTLS 暗号化トラフィックの両方を受け入れることができます（https://istio.io/latest/docs/tasks/security/authentication/mtls-migration/）。
 
https://istio.io/latest/docs/tasks/security/authorization/authz-http/
 
https://kubernetes.io/docs/concepts/services-networking/network-policies/
 
https://istio.io/latest/docs/tasks/security/authentication/mtls-migration/
</div></details>

### Q. 
Google Kubernetes Engine（GKE）で実行するマイクロサービス ベースのアプリケーションを開発しているとします。一部のサービスは、さまざまな Google Cloud API にアクセスする必要があります。Google 推奨のベスト プラクティスに沿ってクラスタでこれらのサービスの認証を設定するには、どうすればよいですか（2 つ選択してください）。

A. GKE ノードに関連付けられたサービス アカウントを使用する。
B. gcloud コマンドライン ツールを経由して、クラスタで Workload Identity を有効にする。
C. Secret Manager から Google サービス アカウント キーにアクセスする。
D. Google サービス アカウント キーを Secret Manager に格納する。
E. gcloud で roles/iam.workloadIdentityUser を使用して、Google サービス アカウントと Kubernetes サービス アカウントをバインドする。
<details><div>
フィードバック
A は不正解です。機能する場合もありますが、すべてのサービスが同じサービス アカウントを使用するため、権限の区別や詳細なロギングを実現できません。
B と E はともに、GKE と Google サービス アカウントを接続するため、GKE は Google サービス アカウントでサービスを認証できます。
C は不正解です。この方法は可能であるものの、Workload Identity では推奨されません。サービス アカウントのキーのローテーションが必須であるためです。
D は不正解です。この方法は可能であるものの、Workload Identity では推奨されません。サービス アカウントのキーのローテーションが必須であるためです。
 
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
 
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable_on_existing_cluster
</div></details>

### Q. 
あなたの会社のプロダクト チームでは、Google Kubernetes Engine（GKE）クラスタで稼働しているステートレスな分散型サービスを自動でスケーリングしてほしいという依頼をお客様から受けています。この機能を 2 週間以内にリリースするために、最小限の変更で実現できるソリューションを求めています。どうすればよいですか。
A.  垂直 Pod 自動スケーリングをデプロイして、CPU の負荷に基づいてスケーリングする。
B.  垂直 Pod 自動スケーリングをデプロイして、カスタム指標に基づいてスケーリングする。
C.  水平 Pod 自動スケーリングをデプロイして、CPU の負荷に基づいてスケーリングする。
D.  水平 Pod 自動スケーリングをデプロイして、カスタム指標に基づいてスケーリングする。
<details><div>
フィードバック
A. 不正解: この方法は、分散アプリケーションでは機能しません。
B. 不正解: この方法は機能しますが、Cloud Monitoring のインテグレーションと、場合によってはアプリケーションの変更が必要になります。また、分散アプリケーションでは機能しません。
C. 正解: 最小限のコード変更で要件を満たすことができます。
D. 不正解: この方法は機能しますが、Cloud Monitoring のインテグレーションと、場合によってはアプリケーションの変更が必要になります。
 
https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling
 
https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler
 
https://cloud.google.com/kubernetes-engine/docs/tutorials/custom-metrics-autoscaling
</div></details>

### Q. 
自社用の e コマース プラットフォームをチームで開発しています。ユーザーはウェブサイトにログインしてショッピング カートに商品を追加します。アクティブでない状態が 30 分続くと、ユーザーは自動的にログアウトします。ユーザーが再びログインした場合は、そのユーザーのショッピング カートを使用できるようにします。Google が推奨するベスト プラクティスに沿ってユーザーのセッションとショッピング カートの情報を保存するには、どのようにしますか。
A. セッションとショッピング カートの情報をローカルメモリに保存し、Google HTTP ロードバランサで Cookie ベースのセッション アフィニティを有効にする。
B. Cloud Storage のオブジェクトにショッピング カートの情報を保存し、オブジェクト名をセッション ID にする。
C. セッションとショッピング カートの情報を BigQuery に保存する。
D. Memorystore for Redis にセッション情報を保存し、ショッピング カート情報を Firestore に保存する。
<details><div>
フィードバック
A は不正解です。ローカルメモリはプロセスが終了するときに失われるため、カート情報が失われます。
B は不正解です。セッション情報のために Cloud Storage バケットにアクセスすることは、低速でコストが高くなります。これは Google Cloud のベスト プラクティスではありません。
C は不正解です。BigQuery ではカートとセッションに対する頻繁な更新を処理できません。
D は正解です。Memorystore はセッション情報を保存するソリューションとして標準的かつ高速であり、Firestore はショッピング カートのような小規模な構造化データに最適です。ユーザーは必要に応じて新しいセッションのショッピング カートにマッピングされます。
 
https://cloud.google.com/memorystore
 
https://cloud.google.com/datastore/docs
</div></details>

### Q. 
API 呼び出し元の認証、割り当ての適用、指標のレポートを行う公開 API を作成する必要があります。このアーキテクチャを実現するには、どのツールを使用する必要がありますか。
Google Cloud のアーキテクチャを表す図。「コンピュータ」から「Cloud Load Balancing」に矢印が伸び、さらに「Cloud Load Balancing」から空のボックスに、そして空のボックスから「PHP アプリケーション」に矢印が伸びています。
Google Cloud のアーキテクチャを表す図。「コンピュータ」から「Cloud Load Balancing」に矢印が伸び、さらに「Cloud Load Balancing」から空のボックスに、そして空のボックスから「PHP アプリケーション」に矢印が伸びています。
A. Cloud Run
B. Cloud Endpoints
C. Identity-Aware Proxy
D. HTTP(S) 負荷分散用 GKE Ingress
<details><div>
フィードバック
A は不正解です。Cloud Run にはこれらの機能はありません。
B は正解です。これら 3 つの機能（認証、割り当て / レート制限、指標）はすべて、Cloud Endpoints のコア機能です。
C は不正解です。IAP は認証機能のみを提供します。
D は不正解です。GKE Ingress にはこれらの機能はありません。
 
https://cloud.google.com/endpoints/docs/openapi/quotas-configure
 
https://cloud.google.com/endpoints/docs/openapi/monitoring-your-api
</div></details>

### Q. 
あなたは最近、ログデータを Cloud Storage バケットに毎日転送するウェブ アプリケーションを開発しました。認証されたユーザーが 2 週間ごとにログを調べて、その間に重大なイベントが発生していないか確認します。さらに、外部の監査者が年に 1 回ログを調べます。データは少なくとも 7 年間保存する必要があります。提案するストレージ ソリューションで、これらの要件を満たしつつコストを最小限に抑えるには、どうすればよいですか。2 つ選択してください。
A. バケットロック機能を使用してデータの保持ポリシーを設定する
B. 14 日以上経過したオブジェクトのストレージ クラスを Coldline に設定するジョブをスケジュールする。
C. Coldline Storage バケットにアクセスする必要があるユーザー向けに JSON ウェブトークン（JWT）を作成する。
D. 14 日以上経過したオブジェクトのストレージ クラスを Coldline に設定するライフサイクル管理ポリシーを作成する。
E. 14 日以上経過したオブジェクトのストレージ クラスを Nearline に設定するライフサイクル管理ポリシーを作成する。
<details><div>
フィードバック
A は正解です。バケットロック機能を使用すると、必要な保持ポリシーを適用できます。
B は不正解です。ストレージ クラスはライフサイクル管理ポリシーで設定する必要があります。
C は不正解です。JWT は認証に使用できますが、オブジェクトのライフサイクルの設定には使用でき
ません。
D は正解です。ライフサイクル管理ポリシーによってストレージ クラスが変更され、費用が最小限に
抑えられます。
E は不正解です。データへのアクセスが年 1 回のみであれば、Archive Storage または Coldline Storage のほうが Nearline Storage よりも経済的です。
 
https://cloud.google.com/storage/docs/bucket-lock
 
https://cloud.google.com/storage/docs/lifecycle
</div></details>

### Q. 
Cloud Storage バケットにオブジェクトを格納してアクセスするアプリケーションを開発しています。規制要件を遵守するために、すべてのオブジェクトについて、最初に作成されてから 7 年以上アクセスできるようにする必要があります。3 年以上前に作成されたオブジェクトは、アクセス頻度が非常に低くなります（1 年に 1 回未満）。ストレージのコストを最適化しつつ、オブジェクトのストレージを構成する必要があります。どうすればよいですか（2 つ選択してください）。
A. 期間を 7 年にしてバケットに保持ポリシーを設定する。
B. オブジェクト名の接頭辞に作成時刻を含め、IAM Conditions を使用して、オブジェクトの作成日から 7 年間、オブジェクトに対して読み取りアクセスのみができるようにする。
C. オブジェクトのバージョニングを有効にして、作成から 7 年間、オブジェクトが意図せず削除されるのを防止する。
D. バケットでオブジェクトのライフサイクル ポリシーを作成し、3 年が経過したオブジェクトを Standard Storage から Archive Storage に移行する。
E. バケット内の各オブジェクトの経過期間をチェックし、3 年が経過したオブジェクトを Archive Storage クラスの 2 つ目のバケットに移行する Cloud Function を実装する。Cloud Scheduler を使用して、Cloud Function を毎日トリガーする。
<details><div>
フィードバック
A は正解です。Cloud Storage には保持ライフサイクル ルールを構成するオプションがあります。
B は不正解です。データの保持要件を実装する方法としてはおすすめしません。
C は不正解です。オブジェクトの作成後 7 年の間にオブジェクトが削除されないことが保証されません。
D は正解です。ストレージ ライフサイクル ポリシーを実装する場合、Standard から Archive ティアに移行する方法が最も簡単であり、推奨されます。
E は不正解です。2 つのストレージ ティアにオブジェクトを格納するために、2 つのバケットは必要ありません。
</div></details>

### Q. 
Google Cloud で実行されるウェブ アプリケーションを開発しています。ほとんどの日はトラフィックがなく、それ以外の日は大きいスパイクが発生するため、着信するユーザー トラフィックのレートは予測できないと想定しています。アプリケーションが自動的にスケールアップ、スケールダウンするようにするとともに、アプリケーションの実行に伴う費用を最小化する必要があります。どうすればよいですか。
A. Firestore をデータベースとしてアプリケーションを構築する。そのアプリケーションをCloud Run にデプロイする。
B. Firestore をデータベースとしてアプリケーションを構築する。そのアプリケーションをGoogle Kubernetes Engine Standard クラスタにデプロイする。
C. Cloud SQL をデータベースとしてアプリケーションを構築する。そのアプリケーションを Google Kubernetes Engine Autopilot クラスタにデプロイする。
D. Firestore をデータベースとしてアプリケーションを構築する。そのアプリケーションを Compute Engine マネージド インスタンス グループにデプロイし、自動スケーリングを適用する。
<details><div>
フィードバック
A は正解です。Cloud Run はゼロへのスケーリングをサポートしています。さらに、Firestore の費用はストレージのみです。そのため、トラフィックがない場合は運用の費用はゼロです。
B は不正解です。GKE はゼロへのスケーリングを行いません。
C は不正解です。GKE はゼロへのスケーリングを行いません。さらに、Cloud SQL の実行に関連する費用が発生します。
D は不正解です。Compute Engine はゼロへのスケーリングを行わないインスタンスを管理します。少なくとも 1 つのインスタンスが実行されている必要があります。
 
https://cloud.google.com/serverless-options
 
https://cloud.google.com/appengine/docs/the-appengine-environments
 
https://cloud.google.com/run
</div></details>

### Q. 
自社の開発チームが、コンテナ イメージでさまざまなオープンソースのオペレーティング システムを使用することを必要としています。イメージがパブリッシュされるとき、共通脆弱性識別子（CVE）のスキャンが必要となります。このスキャン処理がソフトウェア開発のアジリティに影響することがあってはなりません。可能な場所ではマネージド サービスを使用することを希望しています。どうすればよいですか。

A. Container Analysis API を有効にして、Artifact Registry でイメージの脆弱性スキャンを実施する。
B. コード チェックインでトリガーされ、コードの CVE をスキャンする Cloud Run サービスを作成する。
C. 商用サポートされていないベースイメージを開発環境で使用することを禁止する。
D. Cloud Monitoring を使用して Cloud Build の出力を確認し、脆弱性のあるバージョンが使用されているかどうかを判断する。
<details><div>
フィードバック
A は正解です。Container Analysis API により、Debian、Ubuntu、Alpine、Red Hat Enterprise Edition、National Vulnerability Database の各ソースの CVE のイメージ スキャンが可能になります。
B は不正解です。この方法は機能する場合もありますが、マネージド サービスの条件を満たしていません。
C は不正解です。この方法は機能しますが、アジャイル ソフトウェア開発の文化に反しており、進捗を妨げる可能性があります。
D は不正解です。Cloud Monitoring ではログをスキャンできても、脆弱性に関する分析情報は得られません。
 
https://cloud.google.com/container-registry/docs/get-image-vulnerabilities#viewing_vulnerabilities_and_other_occurrences
 
https://cloud.google.com/container-registry/docs/container-analysis
</div></details>

### Q. 
新しいコンテナ イメージの構築を自動化するために、Cloud Build を使用して継続的インテグレーション パイプラインを構成しています。このパイプラインでは、ソースコードからアプリケーションを
構築し、単体テストと統合テストを別々のステップで実行した後、コンテナを Artifact Registry に 
push します。コンテナ ビルド ファイルは次のとおりです。
説明のない画像
A. Cloud Build を実行する仮想マシン（VM）の CPU 割り当てを増やす。
B. VPC 内の Compute Engine VM に Container Registry をデプロイし、これを使用して最後のイメージを保存する。
C. ビルド構成ファイルで --cache-from 引数を使用して、以降のビルドのコンテナ イメージをキャッシュに保存する。
D. ビルドファイル内のベースイメージを ubuntu:latest に変更し、パッケージ マネージャー ユーティリティを使用して Python 3.7 をインストールする。
E. Cloud Storage にアプリケーション ソースコードを保存し、gsutil を使用してソースコードをダウンロードするようにパイプラインを構成する。
<details><div>
フィードバック
A は正解です。高 CPU 仮想マシンタイプにすると短時間でビルドできるようになります。
B は不正解です。VM に Container Registry をデプロイしてもビルドは高速になりません。
C は正解です。以降のステップで、テストとレジストリへの push のために同じコンテナが使用されます。
D は不正解です。Ubuntu コンテナ イメージは python:3.7-alpine イメージよりもはるかに大きくなります。
E は不正解です、Cloud Storage にアプリケーション ソースコードを保存しても、アプリケーションをビルドする時間は短縮されません。
 
https://cloud.google.com/cloud-build/docs/speeding-up-builds
 
https://cloud.google.com/build/docs/optimize-builds/increase-vcpu-for-builds
</div></details>

### Q. 
開発したアプリケーションを Cloud Run でホストしようとしています。このアプリケーションはログレコードをテキストとしてローカル ファイルに書き込みます。ログが Cloud Logging に書き込まれるようにし、さらに、保守する必要があるコードの量を最小化する必要があります。どうすればよいですか。
A. コードに Cloud Logging ライブラリをインポートして、それをログの書き込みに使用する。
B. プログラミング言語のロガーを使用して、標準出力（stdout）ストリームと標準エラー（stderr）ストリームにログを書き込む。
C. ログファイルを www.mycompany.com/logs に公開する。ブラウザを使用してファイルを手動でダウンロードして Cloud Storage にアップロードする。
D. cron を使用して、ログファイルを 1 日に 1 回 Cloud Storage にコピーするジョブをスケジュールする。
<details><div>
フィードバック
A. 不正解です。ライブラリを使用することは技術的には可能ですが、保守するコードが大量になります。
B. 正解です。Cloud Run は stdout と stderr 経由で Cloud Logging に直接的に統合されます。
C. 不正解です。このタスクは手動で実行することが必要になります。さらに、これは最も安全なアプローチではありません。
D. 不正解です。データを Cloud Storage バケットにコピーすることはできますが、これを行ってもログデータを Cloud Logging 内に格納するという目標には役立ちません。
 
https://cloud.google.com/run/docs/logging#container-logs
 
https://cloud.google.com/logging/docs/reference/libraries
</div></details>

### Q. 
Knative で実行されているオンプレミスのコンテナを Google Cloud に移行します。この移行がアプリケーションのデプロイ戦略に影響を与えないようにしつつ、フルマネージド サービスを使用できるようにするには、次のうちどの Google Cloud サービスを使用してコンテナをデプロイすればよいですか。
A. Cloud Run
B. Compute Engine
C. Google Kubernetes Engine（GKE）
D. App Engine フレキシブル環境
<details><div>
フィードバック
A. 正解: Cloud Run は Knative サーバーレス フレームワークを利用しているため、Knative をサポートしている任意のクラスタでワークロードを柔軟に実行できます。
B. 不正解: Compute Engine を使用すると、Google Kubernetes Engine と Knative のインストールとメンテナンスを手動で行うことになるため、そのオーバーヘッドによってデプロイ戦略が変化します。
C. 不正解: GKE を使用してもワーカーノードを管理する必要があり、このためこれはフルマネージド サービスではありません。さらに、アプリケーションのデプロイ戦略に重大な変更を加えることが必要になります。
D. 不正解: App Engine フレキシブル環境ではコンテナを実行できるものの、アプリケーションのデプロイ戦略に重大な変更を加えることが必要になります。
 
https://cloud.google.com/serverless-options/
 
https://cloud.google.com/appengine/docs/the-appengine-environments
 
https://cloud.google.com/knative/
 
https://cloud.google.com/blog/products/containers-kubernetes/when-to-use-google-kubernetes-engine-vs-cloud-run-for-containers
</div></details>

### Q. 
プロジェクト A とプロジェクト B という 2 つの Google Cloud プロジェクトがあります。プロジェクト A で、プロジェクト B の Cloud Storage バケットに出力を保存する Cloud Functions の関数を作成する必要があります。最小権限の原則に従う場合、どうすればよいですか。
A. 1. ロジェクト B に Google サービス アカウントを作成する。2. プロジェクト B にあるストレージ バケットで、このサービス アカウントに roles/storage.objectCreator ロールを割り当てる。3. プロジェクト B のサービス アカウントを使用して、プロジェクト A に Cloud Functions の関数をデプロイする。
B. 1. プロジェクト A に Google サービス アカウントを作成する。2. プロジェクト B にあるストレージ バケットで、このサービス アカウントに roles/storage.objectCreator ロールを割り当てる。3. プロジェクト A のサービス アカウントを使用して Cloud Functions の関数をデプロイする。
C. 1. プロジェクト A でデフォルトの App Engine サービス アカウント（PROJECT_ID@appspot.gserviceaccount.com）を決める。2. プロジェクト B にあるストレージ バケットで、デフォルトの App Engine サービス アカウントに roles/storage.objectCreator ロールを割り当てる。3. プロジェクト A でデフォルトの App Engine サービス アカウントを使用して Cloud Functions の関数をデプロイする。
D. 1. プロジェクト B でデフォルトの App Engine サービス アカウント（PROJECT_ID@appspot.gserviceaccount.com）を決める。2. プロジェクト B にあるストレージ バケットで、デフォルトの App Engine サービス アカウントに roles/storage.objectCreator ロールを割り当てる。3. プロジェクト A でデフォルトの App Engine サービス アカウントを使用して Cloud Functions の関数をデプロイする。
<details><div>
フィードバック
A は不正解です。異なる Google Cloud プロジェクトにあるサービス アカウントで Cloud Functions の関数を実行することはできません。
B は正解です。この方法は最小権限の原則に沿っています。また、Cloud Functions の関数については、関数が実行されるプロジェクトでサービス アカウントを作成する必要があります。
C は不正解です。この方法は最小権限の原則に沿っていません。また、誤って App Engine アプリケーションにストレージ バケットへの書き込み権限を付与する可能性があります。
D は不正解です。この方法は最小権限の原則に沿っていません。また、サービス アカウントは、Cloud Functions の関数が実行されるプロジェクト内にある必要があります。
 
https://cloud.google.com/functions/docs/securing/function-identity#per-function_identity
 
https://cloud.google.com/storage/docs/access-control/iam-roles
 
https://cloud.google.com/iam/docs/using-iam-securely#least_privilege
 
https://cloud.google.com/iam/docs/understanding-service-accounts#granting_minimum_permissions_to_service_accounts
 
https://cloud.google.com/functions/docs/securing/function-identity#runtime_service_account
</div></details>

### Q. 
Google Kubernetes Engine インスタンスに Cloud SQL と通信するアプリケーションをデプロイします。Cloud SQL Auth Proxy を使用して、アプリケーションのインスタンスに関連付けられたサービス アカウントでデータベースと通信できるようにします。サービス アカウントに割り当てられるロールについては、最小権限の原則に沿っている必要があります。どうすればよいですか。
A. プロジェクト編集者のロールを割り当てる。
B. プロジェクト オーナーのロールを割り当てる。
C. Cloud SQL クライアントのロールを割り当てる。
D. Cloud SQL 編集者のロールを割り当てる。
<details><div>
フィードバック
A は不正解です。このサービス アカウントに必要なのは Cloud SQL に接続する権限だけであり、必要最小限のアクセス権のみを付与することが、Google が推奨するベスト プラクティスとなります。
B は不正解です。このサービス アカウントに必要なのは Cloud SQL に接続する権限だけであり、必要最小限のアクセス権のみを付与することが、Google が推奨するベスト プラクティスとなります。
C は正解です。Cloud SQL クライアントのロールでは、Cloud SQL への接続に必要な最小限のアクセス権が付与されます。
D は不正解です。このサービス アカウントに必要なのは Cloud SQL に接続する権限だけであり、必要最小限のアクセス権のみを付与することが、Google が推奨するベスト プラクティスとなります。
 
https://cloud.google.com/iam/docs/understanding-roles
 
https://cloud.google.com/iam/docs/overview
 
https://cloud.google.com/sql/docs/mysql/connect-instance-kubernetes#set_up_a_service_account
 
https://cloud.google.com/sql/docs/postgres/connect-instance-kubernetes#set_up_a_service_account
</div></details>

### Q. 
最近開発したアプリケーションで、パブリック IP アドレスのない Compute Engine インスタンスから Cloud Storage API を呼び出す必要があります。どうすればよいですか。
A.  キャリア ピアリングを使用する
B.  VPC ネットワーク ピアリングを使用する
C.  共有 VPC ネットワークを使用する
D.  限定公開の Google アクセスを使用する
<details><div>
フィードバック
A は不正解です。キャリア ピアリングは、サービス プロバイダ経由で Google Workspace などの Google アプリケーションにアクセスし、エンタープライズ クラスのネットワーク サービスによってインフラストラクチャを Google に接続するためのものです。
B は不正解です。VPC ネットワーク ピアリングは、異なる VPC ネットワーク内のワークロードがプライベート（RFC 1918）空間で通信できるように、VPC ネットワークをピアリングするためのものです。トラフィックは Google のネットワーク内に留まり、公共のインターネットを経由することはありません。
C は不正解です。共有 VPC は、組織内の複数のプロジェクトから共通の VPC ネットワークにリソースを接続し、そのネットワークの内部 IP を使用して安全で効率的な相互通信を行うためのものです。
D は正解です。限定公開の Google アクセスは各サブネットワークで使用できます。これを有効にすると、外部 IP アドレスがなくても、サブネットワーク内のインスタンスからパブリック Google API エンドポイントと通信できます。
 
https://cloud.google.com/compute/docs/private-google-access/private-google-access
 
https://cloud.google.com/vpc/docs/shared-vpc
 
https://cloud.google.com/vpc/docs/vpc-peering
 
https://cloud.google.com/i
</div></details>

### Q. 
ローカルテストでは問題がなく、Compute Engine インスタンスにデプロイするとパフォーマンスが大幅に低下するアプリケーションがあります。変更は最小限にして問題を診断する必要があります。どうすればよいですか。
A. Cloud サポートにチケットを提出し、アプリケーションがローカルでは高速に動作することを説明する。
B. Cloud デバッガのスナップショットを使用して、特定時点でのアプリケーションの実行を確認する。
C. Cloud Profiler を使用して、アプリケーションのどの関数に最も時間がかかっているのかを判断する。
D. アプリケーションにロギング コマンドを追加し、Cloud Logging を使用して、レイテンシの問題がどこで発生しているのかを確認する。
<details><div>
フィードバック
A は不正解です。自分のマシンでは動作したのに Google Cloud では動作しないと主張しても有用性はありません。
B は不正解です。デバッガのスナップショットでは、ある時点のアプリケーションの状況を確認できるのみです。
C は正解です。関数ごとのレイテンシと、レイテンシの履歴情報が提供されます。
D は不正解です。有効ではありますが、必要な作業量が多くなり、明確で最適な選択肢ではありません。
 
https://cloud.google.com/profiler/docs/
</div></details>

### Q. 
Cloud Run と Memorystore for Redis を使用する e コマースのウェブ アプリケーションを開発してい
ます。このアプリにユーザーがログインすると、ユーザーの情報（セッション、名前、アドレス、好みなど）がキャッシュされ、購入手続きの際にすぐに取得できるよう保存されます。ブラウザでアプリケーションをテストすると、502 Bad Gateway のエラーが発生します。そこで、アプリケーションが Memorystore に接続されていないと判断しました。このエラーの原因は何ですか。
A. Memorystore for Redis インスタンスがパブリック IP アドレスなしでデプロイされた。
B. サーバーレス VPC アクセス コネクタを Cloud Run サービスとは異なるリージョンに構成した。
C. DevOps チームによるインフラストラクチャの更新中に、Cloud Run と Memorystore の間の接続を許可するファイアウォール ルールが削除された。
D. Cloud Run サービスとは異なるリージョンの、異なるサブネットのサーバーレス VPC アクセス コネクタを使用するようにアプリケーションを構成した。
<details><div>
フィードバック
A は不正解です。Cloud Run はサーバーレス VPC コネクタ経由で Memorystore に接続します。
プライベート ネットワークで接続するため、パブリック アドレスは必要ありません。
B は正解です。すべてのコンポーネントが同じリージョンにある必要があります。
C は不正解です。Cloud Run と Memorystore の接続に必要なのはサーバーレス VPN コネクタのみです。
D は不正解です。サーバーレス VPC コネクタは、VPC とは関連しない重複しないサブネットを使用して構成されます。
 
https://cloud.google.com/memorystore/docs/redis/redis-overview#connecting
 
https://cloud.google.com/memorystore/docs/redis/connect-redis-instance-cloud-run
 
https://cloud.google.com/vpc/docs/configure-serverless-vpc-access#creating_a_connector
</div></details>

### Q. 
最近、所属組織で、以前のアプリケーションのプラットフォームを再構築して Google Kubernetes Engine に移行するイニシアチブが開始されました。モノリシック アプリケーションをマイクロサービスに分解する必要があります。構成ファイルは共有ファイル システムに格納され、複数のインスタンスが、構成ファイルに対する読み取りと書き込みの権限を持っています。この移行を管理するために必要な作業を最小限に抑え、アプリケーション コードの書き直しを避ける必要があります。どうすればよいですか。
A. 新しい Cloud Storage バケットを作成し、そのバケットをコンテナの FUSE を介してマウントする。
B. 新しい永続ディスクを作成し、その Volume を共有 PersistentVolume としてマウントする。
C. 新しい Filestore インスタンスを作成し、その Volume を nfs PersistentVolume としてマウントする。
D. 構成ファイルのコンテンツを格納するための新しい ConfigMap と volumeMount を作成する。
<details><div>
フィードバック
A は不正解です。Cloud Storage FUSE は同時実行とファイルのロックをサポートしていません。
B は不正解です。永続ディスク PersistentVolume は読み取り / 書き込み（複数回）ではありません。読み取り / 書き込み（1 回）か、読み取り（複数回）にのみできます。
C は正解です。Google Kubernetes Engine でのファイルシステム アクセスで使用できる、マネージド読み取り / 書き込み（複数回）ストレージ オプションとして、これのみがサポートされます。
D は不正解です。ConfigMap に Pod から書き込むことはできません。
 
https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
 
https://cloud.google.com/filestore/docs/accessing-fileshares
 
https://cloud.google.com/storage/docs/gcs-fuse
</div></details>

### Q. 
開発中の Cloud Run サービスのロギングを構成しています。コンテナ インスタンスは、構造化ログを標準出力（stdout）ストリームと標準エラー（stderr）ストリームに書き込みます。自動的に作成されたリクエストログをコンテナログに相互に関連付けたいと考えています。どうすればよいですか。

A. Cloud Trace にトレースを送信するためにアプリケーションを計測可能にする。
B. リクエストごとに一意の ID をランダムに生成して、Snapshot Debugger を使用してログポイントを追加する。
C. ヘッダー値を X-Cloud-Trace-Context として、ログ ステートメントに logging.googleapis.com/trace フィールドを追加する。
D. リクエストごとに一意の ID をランダムに生成して、ログ ステートメントに logging.googleapis.com/labels フィールドを追加する。
<details><div>
フィードバック
A は不正解です。Cloud Trace は、レイテンシ データを調べるために使用します。
B は不正解です。最大 24 時間アクティブなままとなる一時的なロギングが追加されます。
C は正解です。ログ エクスプローラに「親子」関係が作成されます。
D は不正解です。自動生成されたリクエストログにコンテナログは関連付けられません。logging.googleapis.com/labels は主にユーザーが生成した構造化されたログのためのもので、コンテナログ内には関連付けできる対応する要素がありません。
 
https://cloud.google.com/run/docs/logging#correlate-logs
 
https://cloud.google.com/logging/docs/structured-logging
</div></details>

### Q. 
認証サービスからの監査イベントの取り込みを再設計して、トラフィックの大幅な増加に対処できるようにする必要があります。現在、監査サービスと認証サービスは同じ Compute Engine 仮想マシン（VM）で実行されています。各サービスを Compute Engine VM インスタンスの独自のプールに分割し、Pub/Sub を使用して認証サービスから監査サービスにイベントを送信することを計画しています。 

システムが確実に大量のメッセージを処理し、効率的にスケーリングできるようにするには、Pub/Sub トピックとサブスクリプションをどのように設計しますか。

A. 1 つの Pub/Sub トピックを作成する。1 つの pull サブスクリプションを作成する。
B. 1 つの Pub/Sub トピックを作成する。監査サービス インスタンスごとに 1 つの pull サブスクリプションを作成する。
C.  1 つの Pub/Sub トピックを作成する。1 つの push サブスクリプションを作成する。
D. 認証サービス インスタンスごとに 1 つの Pub/Sub トピックを作成する。トピックごとに 1 つのpull サブスクリプションを作成する。
E. 認証サービス インスタンスごとに 1 つの Pub/Sub トピックを作成する。トピックごとに 1 つの push サブスクリプションを作成する。
<details><div>
フィードバック
A は正解です。これは最も柔軟にスケーリングできる方法で、認証サービスと監査サービスを負荷に応じて独立してサイズ設定できます。
B は不正解です。サブスクリプションごとにコピーが作成され、メッセージが重複します。
C は不正解です。システムをスケーリングできるものの、push サブスクリプションは、大量のメッセージの処理には適していません。
D は不正解です。システムをスケーリングできるものの、それぞれの監査サービスがすべてのサブスクリプションをリッスンすることになります。
E は不正解です。システムをスケーリングできるものの、それぞれの監査サービスがすべてのサブスクリプションをリッスンすることが必要になります。さらに、push サブスクリプションは、大量のメッセージの処理には適していません。
 
https://cloud.google.com/pubsub/docs/subscriber#push-subscription
</div></details>

### Q. 
オンプレミス データセンターを Google Cloud に移行する最終段階にいるとします。最終期限が迫っていますが、サーバーで実行しているウェブ API が廃止予定であることがわかりました。この API のモダナイズと Google Cloud への移行を両立させるソリューションを提案する必要があります。

モダナイズ後のウェブ API は、次の要件を満たしている必要があります。

説明のない画像
A. コードをモダナイズして App Engine フレキシブル環境にデプロイする。
B. コードをモダナイズして Cloud Run にデプロイする。
C. モダナイズしたアプリケーションを n1-standard-1 Compute Engine インスタンスにデプロイする。
D. Google Kubernetes Engine でのコンテナとして実行されるようにアプリケーションを作り直すことを開発チームに依頼する。
<details><div>
フィードバック
A は不正解です。このアプローチはすべての要件を満たしますが、費用が最小ではありません。App Engine フレキシブルでは、常に 1 つ以上のインスタンスがオンラインになります。
B は正解です。Cloud Run はすべての要件を満たします。従量課金制で、0 インスタンスから自動スケーリングするように設計され、秒単位でデプロイできます。
C は不正解です。解決策にはなりますが、要件をすべて満たすわけではありません。単一インスタンスは自動的にスケーリングしません。さらに、すべての要件を満たすには、追加の作業と継続的なサーバー管理や設定などが必要になります。
D は不正解です。要件をすべて満たしますが、費用や作業が最小ではありません。
 
https://cloud.google.com/appengine/docs/flexible
 
https://cloud.google.com/appengine/docs/the-appengine-environments
 
https://cloud.google.com/run/docs/overview/what-is-cloud-run
</div></details>

### Q. 
あなたは、Cloud クライアント ライブラリに対応していない新しいプログラミング言語でアプリケーションを開発しています。アプリケーションでは、REST API 呼び出しを作成して Google Cloud サービスを呼び出します。アプリケーションは関連するサービス アカウントを使用して Cloud Run で実行されます。
A. API キーをアプリケーションに含め、その値を Authorization ヘッダーに渡す。
B. メタデータ サーバーからアクセス トークンを取得し、その値を Authorization ヘッダーに渡す。
C. GOOGLE_APPLICATION_CREDENTIALS 環境変数の値として、サービス アカウントの API キーを使用する。
D. gcloud auth application-default print-access-token の値を、スタートアップ時に GOOGLE_APPLICATION_CREDENTIALS 環境変数にパスが設定されたファイルに保存する。
<details><div>
フィードバック
A は不正解です。API キーは認可トークンとは異なり、HTTP リクエストの Authorization ヘッダーで渡された場合は考慮されません。
B は正解です。メタデータ サーバーから取得して HTTP リクエストの Authorization ヘッダーで Google Cloud に渡したアクセス トークンは、サービス アカウントとして正常に認証に利用できます。
C は不正解です。GOOGLE_APPLICATION_CREDENTIALS には、API キーの値ではなく、アクセス トークンを含むファイルの名前を設定する必要があります。
D は不正解です。GOOGLE_APPLICATION_CREDENTIALS は、アクセス トークンではなく、API キーを含むファイルを参照するために使用されます。
 
https://developers.google.com/identity/protocols/oauth2?#4.-send-the-access-token-to-an-api.
 
https://cloud.google.com/run/docs/container-contract#metadata-server
 
https://cloud.google.com/run/docs/securing/service-identity
</div></details>

### Q. 
Google Cloud ランタイムで実行されるように設計されたウェブ アプリケーションを作成しています。
このアプリケーションによって、ユーザーのアカウント ドメインがどれであるかに関係なく、そのユーザーのドライブにファイルが書き込まれるようにします。Google Drive API への認証を行うようにアプリケーションを構成する必要があります。どうすればよいですか。

A. ドメイン全体の権限が委任された OAuth クライアント ID を使用する。
B. ドメイン全体の権限が委任されたサービス アカウントを使用する。
C. OAuth クライアント ID と https://www.googleapis.com/auth/drive.file スコープを使用して、 各ユーザーのアクセス トークンを取得する。
D. サービス アカウントと https://www.googleapis.com/auth/drive.file スコープを使用して、署名付き JSON Web Token（JWT）を生成する。
<details><div>
フィードバック
A は不正解です。OAuth クライアント ID にはドメイン全体の権限を付与できません。
B は不正解です。機能するユーザーが限定されます（委任されたドメイン全体の権限を付与した元のドメインのユーザーのみ）。
C は正解です。OAuth 2.0 ウェブ認可はすべてのユーザーに対して機能する一方、ドメイン委任は単一のドメインに対してのみ機能します。
D は不正解です。ユーザーのドライブには書き込まれません。また、署名付き JWT を使用して Drive API に対して認証することはできません。
 
https://developers.google.com/identity/protocols/OAuth2
</div></details>

### Q. 
チームで、さまざまなソースのニュース記事を集約するアプリケーションを管理しているとします。モニタリング ダッシュボードには、一般公開のリアルタイム レポートが表示されます。このダッシュボードはウェブ アプリケーションとして Compute Engine インスタンス上で実行されます。リアルタイム レポートには、外部の関係者やアナリストが認証を行わずに安全なチャネルを介してアクセスできる必要があります。そうした安全なチャネルを構成するには、どうすればよいですか。
A. インスタンスのサービス アカウント キーを使用してトラフィックを暗号化する。
B. Cloud Scheduler を使用して 1 時間ごとに Cloud Build をトリガーし、レポートからのエクスポートを作成する。Cloud Storage の公開バケットにレポートを保存する。
C. HTTP(S) ロードバランサをモニタリング ダッシュボードの前に追加する。Identity-Aware Proxy を構成して通信チャネルを保護する。
D. HTTP(S) ロードバランサをモニタリング ダッシュボードの前に追加する。ロードバランサで Google マネージド SSL 証明書を設定し、トラフィックを暗号化する。
<details><div>
フィードバック
A は不正解です。サービス アカウントを使用して HTTPS トラフィックを暗号化することはできません。
B は不正解です。定期的なエクスポートは、リアルタイムの要件を満たしません。
C は不正解です。IAP は通信チャネルを保護するのではなくユーザーを認証します。技術的には、チャネルは Cloud Load Balancing によってすでに保護されていますが、適切な証明書が使用されません。
D は正解です。外部の HTTPS エンドポイントが提供され、Google マネージド サービスと有効な SSL 証明書が使用されます。
 
https://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs
</div></details>

### Q. 
あなたのチームは、世界中のユーザーが一般的なトピックについて投票できるモバイル ウェブ アプリケーションを開発しました。各トピックにつき、それぞれ 30 分の投票期間中に非常に多くの投票が行われることが予想されます。30 分ごとに各トピックの票を集計する必要があります。また、将来の分析とレポートのために票を保存する必要もあります。どうすればよいですか。
A. Memorystore に票を保存した後、Cloud Functions を使用して集計値を BigQuery に挿入し、結果を Google データポータルで表示する。
B. Pub/Sub に票をパブリッシュした後、Dataflow パイプラインを使用して集計値と票を BigQuery に挿入し、結果を Google データポータルで表示する。
C. Pub/Sub に票をパブリッシュした後、Cloud Functions を使用して集計値と票を Cloud Storage に挿入し、結果を Google データポータルで表示する。
D. Firebase でモバイル ユーザーを認証し、票を直接 Firestore にパブリッシュする。その後、票を CSV ファイルにエクスポートしてから、レポートのためにスプレッドシートにインポートする。
<details><div>
フィードバック
A. 不正解です。Memorystore は永続的なデータストアではないため、投票が失われる可能性があります。
B. 正解です。Pub/Sub では、1 秒あたり数百万件のレコードを取り込むことができ、メッセージの配信も保証されています。Dataflow は、トピック別にグループ化されたウィンドウ内の票を集計できます。分析には BigQuery を使用することが推奨されます。
C. 不正解です。Cloud Functions でウィンドウ処理とグループ化を行うには、中間データストアに状態を保存する必要があります。また、元データを blob ストアに保存すると、データポータルでの分析が非効率になります。
D. 不正解です。Google スプレッドシートで実行できるグループ化および分析オペレーションには制限があります。
 
https://cloud.google.com/pubsub/docs/publisher
 
https://cloud.google.com/memorystore/
 
https://cloud.google.com/bigquery/docs/visualize-data-studio
 
https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#windows
 
https://cloud.google.com/functions
</div></details>

### Q. 
あなたのチームが管理しているウェブ アプリケーションは、現在、ユーザー セッションの情報をオンプレミスの PostgreSQL データベースに保存しています。このデータベースをオンプレ
ミスに残して、ウェブ アプリケーションを Google Cloud に移行することになりました。移行
後に高いレイテンシが検出されたため、クイックテストを何件か実施してデータポイントを収
集し、パフォーマンスを改善する費用対効果の高い解決策をテクニカル リーダーに提案する必要があります。
説明のない画像
A. データベースを Cloud SQL に移行し、データを Cloud Storage にインポートする。
B. データベースを BigQuery に移行し、データをネイティブ モードの Firestore にインポートする。
C. データベースを Cloud SQL に移行し、データをネイティブ モードの Firestore にインポートする。
D. Google Cloud で Compute Engine 仮想マシン インスタンスを作成して、PostgreSQL と MongoDB のサーバー ソフトウェアをインストールし、データベースを PostgreSQL に、データを MongoDB にそれぞれ移行する。
<details><div>
フィードバック
A は不正解です。Cloud Storage は、ユーザー セッション情報の保存に効果的な NoSQL データベースではありません。
B は不正解です。BigQuery は、データ ウェアハウスです。挿入された行を更新 / 削除する機能が限定的であるため、セッションの進行状況に応じて変化するユーザー セッション データには不適切です。
C は正解です。Cloud SQL はマネージド PostgreSQL データベースで、移行時にスキーマの変更は不要です。ネイティブ モードの Firestore は、ユーザー セッション情報の保存用途に推奨されており、こうしたテストに最適です。
D は不正解です。マネージド サービスが使用されておらず、データベース サーバーの管理が必要になるため、テストにかかる時間が増加します。
 
https://cloud.google.com/bigtable/docs/overview
 
https://cloud.google.com/bigquery/docs/loading-data-cloud-firestore
 
https://cloud.google.com/sql/docs/postgres/import-export/importing
 
https://cloud.google.com/solutions/migrating-postgresql-to-gcp
</div></details>

### Q. 
あなたは、Node.js で Cloud Functions の関数を記述して、ソースコードを Git リポジトリに格納しました。ソースコードに commit された変更が自動的にテストされるようにしたいと考えています。そこで、ソースコードを一意の名前の Cloud Functions の関数に push し、関数をテストで呼び出した後に、削除してクリーンアップする Cloud Build 構成を記述しました。ところが、テストに失敗すると関数が削除されないことがわかりました。どうすればよいですか。
A. テストの実行前に Cloud Functions の関数を削除するようにステップの順序を変更する。
B. Cloud Build ステップに waitFor オプションを追加し、Cloud Functions 関数のテストステップを必要な直前のステップとして指定して削除する。
C. Cloud Build ステップで Cloud Functions 関数の結果をファイルに書き込み、0 を返すようにする。そのファイルに予想どおりの結果が含まれるかどうかを確認し、期待される結果でない場合は失敗とするステップを、Cloud Functions 関数の削除の後に追加する*。
D. Cloud Build ステップで結果を「result」という環境変数に設定し、0 を返すようにする。その環境変数に予想どおりの結果が含まれるかどうかを確認する最後のステップを、Cloud Functions 関数の削除の後に追加する。
<details><div>
フィードバック
A は不正解です。Cloud Functions 関数を削除すると、テストできません。

B は不正解です。Cloud Build の waitFor はステップを同期するために使用します。あるステップで waitFor 内の直前のステップが特定され、直前のステップが失敗した場合、Cloud Build 全体が機能しなくなり、関数は削除されません。

C は正解です。Cloud Build 内にはステップ間で共有される永続ファイル システムがあります。正しい方法は次のとおりです。
1. Cloud Functions 関数をデプロイする。
2. Cloud Functions 関数の呼び出し結果をファイルに保存する。
3. Cloud Functions 関数を削除する。
4. ファイルの内容をテストする。
ステップ 2 が失敗なく行われるようになるので、ステップ 3 が実行され、ステップ 4 でビルド全体の結果が判明します。

D は不正解です。環境変数は、ステップが実行されるコンテナ内にローカルに存在します。ステップが完了すると、コンテナは破棄されて環境変数も一緒になくなります。したがって、その環境変数は以後のステップでは使用できず、個別のステップ間の連携に利用することはできなくなります。
</div></details>

### Q. 
あなたは、Google Kubernetes Engine（GKE）クラスタにウェブ アプリケーションをデプロイしました。Cloud Monitoring の指標を調べたところ、クラスタの CPU 負荷が 1 日を通じて変動し続けていることがわかりました。コストを最小限に抑えつつパフォーマンスを最大にするために、Pod とノードの数を自動調整するにはどうすればよいですか。
A. マネージド インスタンス グループ（MIG）を変更して自動スケーリングを有効にし、CPU 負荷に基づいてノードの最大数と最小数を構成する。
B. GKE クラスタでクラスタ オートスケーラーを有効にし、CPU 負荷に基づいてワークロードを自動スケーリングするよう Horizontal Pod Autoscaler（HPA）を構成する。
C. GKE クラスタでクラスタ オートスケーラーを有効にし、カスタム指標に基づいてワークロードを自動スケーリングするよう HPA を構成する。
D. MIG を変更して自動スケーリングを有効にし、CPU 負荷に基づいてノードの最大数と最小数を構成する。また、CPU 負荷に基づいてワークロードをスケールするよう Vertical Pod Autoscaler（VPA）を構成する。
<details><div>
フィードバック
A は不正解です。GKE で作成したインスタンス グループで Compute Engine の自動スケーリング機能を使用するべきではありません。
B が正解です。Kubernetes Deployment を自動スケーリングするには、この方法がおすすめです。
C は不正解です。CPU 指標はデフォルトで有効になっており、カスタム指標は不要です。
D は不正解です。GKE で作成したインスタンス グループで Compute Engine の自動スケーリング機能を使用するべきではありません。
 
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
 
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler
 
https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps
</div></details>

### Q. 
あなたは、Cloud Run で Java アプリケーションを実行しています。アプリケーションのエラー メッセージが Error Reporting コンソールに表示されません。どうすればよいですか。
A. その Java アプリケーションに Cloud Monitoring クライアント ライブラリがバンドルされて
B. アプリケーション ログが正しい Regional Storage バケットに書き込まれていることを確認する。
C. アプリケーション エラーが stderr に書き込まれることを確認する。
D. System.out.println を使用して例外をログに記録する。
<details><div>
フィードバック
A は不正解です。Cloud Run でのエラーレポートに外部ライブラリは必要ありません。
B は不正解です。Error Reporting はリージョン ログバケットに格納されているログや、他のプロジェクトにルーティングされたログの分析は行いません。
C は正解です。Error Reporting で認識されるには、エラーは Cloud Run で stderr に書き込まれる必要があります。
D は不正解です。system.out.println は stderr ではなく stdout に出力します。
 
https://cloud.google.com/error-reporting/docs/troubleshooting
 
https://cloud.google.com/run/docs/error-reporting
</div></details>

### Q. 
あなたはアプリケーションのパフォーマンスを分析しています。クラスタ内にある Cloud Bigtable の特定のテーブルが他のテーブルより使用率が高く、エンドユーザーに対してアプリケーションのパフォーマンスの一貫性がないことが確認されました。一部のテーブルでは似た名前の行キーのデータが多くなっており、それらのテーブルは使用率が高く、他のテーブルは使用率が低いことがわかりました。また、ユーザーの郵便番号が行キーの最初の要素になっており、その郵便番号から作成されたプロファイルによるアプリケーション使用率が高いこともわかりました。行キーの生成方法を可読形式に変更して、Cloud Bigtable の要求がクラスタ内で均等に分散されるようにするには、どうすればよいですか。
A. シーケンシャルに生成される整数値を使用する。
B. 可読形式の複数の属性を連結して使用する。
C. 行の内容の MD5 ハッシュのサブセットを使用する。
D. ミリ秒単位で表された UNIX エポック形式のタイムスタンプを使用する。
<details><div>
フィードバック
A は不正解です。元の値の分散状況によっては、シーケンシャルに生成される整数値によってホットスポット化をもたらす可能性があります。
B は正解です。十分な数の属性を区切って使用することで、十分な分散を実現できます。
C は不正解です。現在は、この方法は推奨されていません。この方法では、問題のトラブルシューティングが困難になります。
D は不正解です。タイムスタンプは行キーの候補として適切ではありません。複数の更新が短い時間に連続して発生し、ミリ秒レベルで同じ時刻に実行されると、誤って ID の競合が発生します。
 
https://cloud.google.com/bigtable/docs/schema-design#types_of_row_keys
 
https://cloud.google.com/bigtable/docs/schema-design-time-series#ensure_that_your_row_key_avoids_hotspotting
</div></details>

### Q. 
あなたの会社で提供しているマルチプレーヤー ゲームは米国で人気があります。そこで今回、提供範囲を他のリージョンにも拡大することになりました。また、ユーザー同士がポイントを交換できる新機能をリリースする予定です。この機能は全世界のユーザーが利用できるようにします。会社で現在使用している MySQL バックエンドでは、ゲームをホストしている Compute Engine インスタンスが間もなく上限に達します。そのため、対象のリージョン全体での整合性と高可用性を実現できる他のデータベースに移行することを検討しています。どのデータベースを選択すればよいですか。
A. BigQuery
B. Cloud SQL
C. Cloud Spanner
D. Cloud Bigtable
<details><div>
フィードバック
A は不正解です。BigQuery をトランザクション データベースとして使用することはできません。
B は不正解です。Cloud SQL では複数のリージョンでの高可用性を実現できません。
C は正解です。Cloud Spanner のみがグローバルでの整合性と可用性を実現できます。
D は不正解です。Cloud Bigtable ではグローバルでの可用性を実現できません。
 
https://cloud.google.com/spanner
</div></details>

### Q. 
あなたの会社では、分析を行うユースケースを増やすことを計画しています。新しいユースケースのあるデータ分析では、SQL を使用してほぼリアルタイムでイベントを分析する必要があります。対象のデータは急速な増加が見込まれており、できる限りマネージド サービスを使用したいと考えています。どうすればよいですか。
A. まず大規模な Compute Engine インスタンス上に Kafka インスタンスを作成する。その後、ソースから Kafka パイプラインにイベントをストリーミングし、Dataflow を利用してこれらのイベントを Cloud Storage に取り込む。
B. まず Pub/Sub のトピックとサブスクリプションを作成する。その後、ソースからイベントを Pub/Sub のトピックにストリーミングし、Dataflow を利用してこれらのイベントを Cloud Storage に取り込む。
C. まず Pub/Sub のトピックとサブスクリプションを作成する。その後、ソースからイベントを Pub/Sub のトピックにストリーミングし、Dataflow を利用してこれらのイベントを BigQuery に取り込む。
D. まず Pub/Sub のトピックとサブスクリプションを作成する。その後、ソースからイベントを Pub/Sub のトピックにストリーミングし、Dataflow を利用してこれらのイベントを  Datastore モードの Firestore に取り込む。
<details><div>
フィードバック
A は不正解です。これはフルマネージド ソリューションではありません。
B は不正解です。Cloud Storage は、個々のイベントを挿入したり、SQL で分析したりする場合には適していません。
C は正解です。この説明に含まれる 3 つのプロダクトすべてが大規模にスケーリング可能です。さらに、BigQuery にデータを書き込むことで、すぐに SQL で分析できるようになります。
D は不正解です。Datastore モードの Firestore は、個別のイベントを挿入したり、分析したりする場合には適していません。
 
https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery
 
https://cloud.google.com/architecture/reference-patterns/overview#general_analytics
</div></details>

### Q. 
Cloud Run にデプロイされているアプリケーションが、大量のトラフィックを受信しています。アプリケーションに変更をデプロイすると、全ユーザーに悪影響が及ぶことが懸念されます。費用の問題から全環境での負荷テストは避けながら、できるだけ早く新機能をデプロイする必要があります。適切なアプローチはどれですか。

A. 本番環境のアプリケーションに対して週次の負荷テストをスケジュールする。
B. ローカルの開発環境を使用して、Google Cloud の外部で負荷テストを行う。
C. ユーザーに新機能へのアクセスを許可する前に、新バージョンとしてデプロイしてスモークテストを行う。その後、全ユーザーが新機能にアクセスできるようにする。
D. トラフィック分割機能を使用して一部のユーザーに新機能をテストしてもらい、徐々にトラフィック分割を調整しつつ、最終的に全ユーザーが新機能にアクセスできるようにする。
<details><div>
フィードバック
A は不正解です。パターン化された A/B テストの実施方法があるため、定期的に個別の負荷テストを行う必要性は小さくなります。
B は不正解です。本番環境が正確にシミュレートされません。
C は不正解です。説明にあるとおり、スモークテストでは新機能をすぐに全ユーザーに公開することになります。
D は正解です。トラフィック分割により、全ユーザーに影響を与えることなく実際のユーザーによるテストを行うことができ、負荷テストの費用を削減できます。
 
https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#split-traffic
</div></details>

### Q. 
あなたの組織のウェブサイトは Compute Engine にデプロイされています。マーケティング チームでは、異なる 3 つのデザインのウェブサイトでコンバージョン率をテストしたいと考えています。アプリケーションのコードを変更することはできません。どうすればよいですか。
A. ウェブサイトを Cloud Run にデプロイし、トラフィック分割を使用する。
B. ウェブサイトを 3 つの個別のリビジョンとして Cloud Run にデプロイする。
C. ウェブサイトを 3 つの個別の機能として Cloud Functions にデプロイする。
D. ウェブサイトを Cloud Functions にデプロイし、カスタムコードを実装して異なるデザインを表示する。
<details><div>
フィードバック
A は正解です。この方法により、トラフィックを単一のドメインにルーティングし、割合に基づいてトラフィックを分割できます。
B は不正解です。ドメイン名はリビジョンに基づいて変更されます。
C と D は不正解です。Cloud Functions はウェブサイトのデプロイには使用できません。
 
https://cloud.google.com/appengine/docs/standard/python/splitting-traffic
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
フィードバック
A は不正解です。この場合、メッセージの半分を BigQuery に書き込み、残りの半分を Cloud Storage に書き込みます。これでは要件を満たしません。
B は正解です。各 Cloud Run サービスは、書き込むメッセージを個別に取得します。また、再試行、失敗を個別に処理できます。
C は不正解です。1 つのシステムに対するメッセージ処理が失敗すると、BigQuery または Cloud Storage に重複して書き込む原因となる可能性があります。
D は不正解です。メッセージ送信の料金が重複します。また、異なる 2 つのトピックにメッセージをそれぞれ送信する必要があります。これではクライアント側で費用が増し、追加のトピックを管理する必要性および複雑度も増加します。
 
https://cloud.google.com/run/docs/triggering/pubsub-push
 
https://cloud.google.com/pubsub/docs/admin
</div></details>

### Q. 
あなたのチームでは Cloud Run を使用してすべての Pub/Sub メッセージを Cloud Storage オブジェクトと BigQuery テーブルの両方に書き込んでいます。運用上のオーバーヘッドを最小限に抑えるには、どのアーキテクチャを実装すればよいですか。


Cloud Logging で、重要な監査アクティビティの情報を取得しています。Cloud Logging から情報を読み取り、ほぼリアルタイムでログの分析を行う必要があります。複数のプロセスでログデータに対して異なる種類の分析を行うにはどうすればよいですか。
A. Cloud Logging API からログを直接読み取るアプリを作成する。
B. Pub/Sub に対して Cloud Logging シンクを設定し、Pub/Sub トピックからログを読み取る。
C. Cloud Logging API から直接読み取る Cloud Functions のエンドポイントを作成し、ログを Dataproc にコピーする。
D. Cloud Storage に対して Cloud Logging シンクを設定し、Cloud Storage バケットからログを読み取る。
<details><div>
フィードバック
A は不正解です。この API には読み取りに関する制限があり、複数の読み取り処理を行うソリューションには適していません。(https://cloud.google.com/logging/quotas)
B は正解です。このソリューションはほぼリアルタイムです。（https://cloud.google.com/logging/docs/export/using_exported_logs#pubsub-availability）
C は不正解です。これは効率的なソリューションではありません。（https://cloud.google.com/dataproc/docs/concepts/jobs/life-of-a-job）
D は不正解です。このソリューションはリアルタイムではありません。（https://cloud.google.com/logging/docs/export/using_exported_logs#gcs-availability）
</div></details>

### Q. 
あなたは API エンドポイントを記述しています。このエンドポイントでは、ウェブ アプリケーションからの注文を処理し、Datastore モードの Firestore のコレクションにデータを保存します。アプリケーションのテスト中に、次の事象に気がつきました。Datastore API から HTTP 5xx サーバーエラーが返されると、アプリケーションはこのエラーを検知し HTTP 200 OK のレスポンス コードをクライアントに返しますが、Datastore にはデータが保存されません。書き込みリクエストが失敗した場合に、API エンドポイントの利用者に通知するようにするにはどうすればよいですか。
A. HTTP 204 No Content のレスポンスを返す。
B. HTTP 406 Not Acceptable のレスポンスを返す。
C. HTTP 500 Internal Server Error のレスポンスを返す。
D. Firestore が HTTP 2xx のレスポンスを返すまで、指数バックオフを使用して Datastore API を再試行する。
<details><div>
フィードバック
A は不正解です。2xx のコードは、サーバーからの成功のレスポンスを表しています。
B は不正解です。406 は、リクエストの Accept ヘッダーに、サーバーで受理できない内容が含まれている場合に送信されます。
C は正解です。500 番台のレスポンスの場合は、API 呼び出しが失敗したことをクライアントがはっきり認識できるため、クライアントは個別に再送信できます。
D は不正解です。Datastore API が 500 番台以外のレスポンスで応答するタイミングは決まっていません。
 
https://www.restapitutorial.com/httpstatuscodes.html
</div></details>

### Q. 
1. 
2. 
3. 
4. 
<details><div>
    答え：
</div></details>

