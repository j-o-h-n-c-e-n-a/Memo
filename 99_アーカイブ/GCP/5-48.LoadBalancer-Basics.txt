Load balancer higher liability and auto scaling.

So we are going to get into all of this in this lecture.

So when we look at the load balancer traditionally what do we mean by that.

So what it does it takes the user request most of the time it's a web.

So if you hit to any website like Google dot com or any other Web site right.

So it takes the request and distribute that request to the backend services who is serving your request.

Maybe it's application server web server or any other kind of a you know API backend so all of those

servers.

So those are really sites in the backend so it takes the request and distribute that into the back end

services.

Traditionally we used to have the load balancer which was like hardware devices with software configuration

in it but it is no more required your house software defined load balancer nowadays and in public cloud

it is purely a software defined load balancers so in the context of we will flat platform.

There are multiple types of load balancer which you can use it and very widely used.

Load balancer resistor to be as load balancer hardest to G.P.S. load balancer along with that there

are SSL proxy or DCP proxy load balance of switch seats are which is treated as a global load balancer

and there are some regional load balancing under our internal load balancers but we are going to get

into it.

So in a nutshell in Google Cloud Platform load balancer is as Dean software defined network part and

it is not a hardware device.

So it can scale according to the number of requests which you hit it.

So it is a software defined component benefits scale your applications support heavy traffic it detects

automatically and remove unhealthy what your machine instances in the backend it allows their traffic

to the closest virtual machine and that is intelligent routing.

Those are very few benefits out of it but we are going to get into details of all the benefits in a

high level.

If you look at the basics of load balancer what it does it takes you know the request from the Internet

it has got a global forwarding rule target proxy configurations which is like you are back in services

you are a map and the backend services which you can configure it to rout your target proxy all the

traffic to the backend services and then you can configure health check to check whether a particular

backend instances are healthy or not.

So does that a part of load balancer some part of that is you can configured that into instance group

which we already saw in the compute service but this is in natural I'm not going in details of each

and everything because it is not a part of foundation we will get into details of it and the network

certifications.

So if you look at the load balancer how it works in Google Cloud Platform in that context.

Right.

So consider that are the customers who are in East there are customers in Central and there are customers

in western zone or Western Time Zone.

If you consider this as a US then USD to US Central and U.S. West our if you consider this as like East

means Asia Central means Europe Western means us then you get treated accordingly.

But this is like East to do on central zone and West Zone customers they are hitting your DNS service

and DNS services like that's where you take your own company dot com into actual IP address and that

IP address is your lord balance of IP address what it does is it takes the request and routing to your

back in instances and that back an instance could become pretty engine.

Cloud storage or any other compute services and there are computer services which are distributed are

spread across East Central and West Dawn's okay whether it is Asia are e part of cost you know U.S.

you treated accordingly.

So what happens when there is a request comes from the central customer right.

What it happens is it goes through DNS and then DNS sends their traffic to load balancer load balances

sees that as because it is set on top of the network it is a global entity it sees the request is coming

from the central users you know that's how it will try to locate the instance near to the customer so

it will allow the traffic to the central instance what happens if there is a customer.

The request comes from US East.

It will take the request.

It will understand that the request is from East and it will forward the traffic to the back end instance

which is which is in East.

Likewise in the West.

It will go to the west after some time you realize that you know there is a load on the West ensigns

because that's where that's where they start their day and then that particle load continuously going

increasing.

So what it will happen is it if at all there are one particular instance running to serve the traffic.

That particular instance in West gets bombarded with request.

So what it will do it.

It will kick in additional auto scaling group and it will try to start additional request additional

backend services into the resume.

But in the meantime as because it is it sometimes it takes time it will allow the traffic to some traffic

I will say to the central zone which is next hop to your next to your I know the vests on.

Okay so some of the requests will without it to central and if Central is also busy it will without

it to east.

But that is based on the latency and all other parameters.

Once additional service or additional backend instances are created what it will do is it will create

connection and it will start distributing traffic to the additional instances as well here.

So now there is additional insight you know instances also started getting the traffic.

In the meantime you realize that there are the backend services because of some reason either the zone

outage or somebody is in order.

You know what your mission got corrupted.

Your services suddenly stopped in the central zone.

So what it will do is it will intelligently root out even the central traffic to it at least all of

the waste depends on the load which and those backend services are carrying.

And this is again intelligent routing.

Okay.

And this is important differentiation which we need to understand so it will distribute our traffic

intelligently to the backend services near to the user.

But at the same time considering the load on the backend services if there are sufficient capacity available

and if it can be sold from other locations in reduced latency and this is intelligent routing so that

is overall as a load balancer as a basic concept guys who have any questions let me know otherwise you

can move to the next lecture here.

He'll check in load balancer.

ロードバランサーの高い責任と自動スケーリング。

したがって、この講義ではこれらすべてについて説明します。

したがって、ロードバランサーを従来のように見ると、それはどういう意味ですか。

そのため、ほとんどの場合、それはWebであるため、ユーザーリクエストを処理します。

したがって、Googleドットコムや他のWebサイトのようなWebサイトにアクセスした場合は正しいです。

そのため、リクエストを受け取り、リクエストを処理しているバックエンドサービスにそのリクエストを配信します。

たぶんそれはアプリケーションサーバーのウェブサーバーか、あなたが知っている他の種類のAPIバックエンドなので、それらはすべて

サーバー。

これらは本当にバックエンドのサイトなので、リクエストを受け取ってバックエンドに配布します

サービス。

従来、ソフトウェア構成のハードウェアデバイスのようなロードバランサーを使用していました。

その中で、それはもはやあなたの家のソフトウェア定義されたロードバランサーは、最近ではパブリッククラウドで必要ではありません

これは純粋にソフトウェアで定義されたロードバランサーであるため、プラットフォームをフラットにします。

使用できるロードバランサーには複数のタイプがあり、非常に広く使用されています。

ロードバランサーの抵抗器は、G.P.S。ロードバランサーと一緒に

スイッチシートのSSLプロキシまたはDCPプロキシロードバランスは、グローバルロードバランサーとして扱われます

内部のロードバランサーの下にいくつかの地域のロードバランシングがありますが、

それに。

したがって、Google Cloud Platformロードバランサーの要点は、Dean Softwareが定義したネットワークパーツであり、

ハードウェアデバイスではありません。

そのため、ヒットしたリクエストの数に応じてスケーリングできます。

したがって、ソフトウェア定義コンポーネントの利点であり、アプリケーションが検出する大量のトラフィックをサポートするスケールです

バックエンドのマシンインスタンスがトラフィックを許可している不健康なものを自動的に削除します

最も近い仮想マシンに接続します。これがインテリジェントルーティングです。

これらの利点はほとんどありませんが、すべての利点の詳細については、

高いレベル。

ロードバランサーの基本を見ると、それが何をするかがわかります。インターネットからのリクエストを知ることができます。

あなたがサービスに戻っているようなグローバル転送ルールのターゲットプロキシ設定を持っています

あなたはマップであり、ターゲットプロキシをすべてルーティングするように設定できるバックエンドサービスです

バックエンドサービスへのトラフィック。その後、ヘルスチェックを設定して、特定の

バックエンドインスタンスが正常かどうか。

そのため、ロードバランサーの一部はインスタンスグループに構成できます。

これは既にコンピューティングサービスで見たものですが、これは当然のことですが、それぞれの詳細については説明しません

すべてが基盤の一部ではないため、すべての詳細とネットワークについて説明します

認定。

そのため、ロードバランサーをそのコンテキストでGoogle Cloud Platformでどのように機能させるかを見ると、

正しい。

東にいる顧客は中央にいる顧客であり、顧客があることを考慮してください

西部時間帯または西部時間帯。

これを米国と見なす場合、米国中部および米国西部への米ドルは、これを東部と見なす場合

は、アジア中部がヨーロッパを意味することを意味します。

しかし、これは、東部があなたのDNSサービスにアクセスしている中央ゾーンと西ゾーンの顧客に対して行うようなものです。

そして、そのようなDNSサービスは、あなた自身の会社のドットコムを実際のIPアドレスに持ち込む場所であり、

IPアドレスは、IPアドレスの主なバランスです。IPアドレスは、リクエストを受け取り、

インスタンスに戻ると、インスタンスがきれいなエンジンになる可能性があります。

クラウドストレージまたはその他のコンピューティングサービスと、配布されるコンピューターサービスがあります

イーストセントラルとウェストドーンに広がるのは、それがアジアであるかどうかは大丈夫です。

あなたはそれに応じて扱いました。

そのため、リクエストがあったときに何が起こるかは、中央の顧客からのものです。

発生するのは、DNSを通過し、DNSがトラフィックをロードバランサーのロードバランスに送信することです

ネットワークの最上部に設定されているため、グローバルエンティティであるため、リクエストが来ていることがわかります

あなたが知っている中央ユーザーから、それは顧客の近くにインスタンスを見つけようとする方法です

顧客がいる場合に何が起こるかは、中央インスタンスへのトラフィックを許可します。

米国東部からのリクエストです。

要求を受け取ります。

要求は東からのものであり、トラフィックをバックエンドインスタンスに転送することを理解します。

どちらが東にありますか。

西洋でも同様です。

西側に負荷があることがわかっていることに気づいた後、西側に行きます。

それは彼らが一日を始める場所だからです

増加しています。

そのため、トラフィックを処理するために実行されている特定のインスタンスが1つでもある場合に発生します。

西のその特定のインスタンスは、リクエストで攻撃されます。

それで何をするのでしょう。

追加の自動スケーリンググループを開始し、追加のリクエストを開始しようとします。

履歴書にバックエンドサービス。

しかし、一方で、それは時々時間がかかるので、あるトラフィックへのトラフィックを許可します

私はベストを知っているあなたの隣にあなたの隣にある中央ゾーンに言います。

わかりましたので、リクエストの一部はセントラルに送信されず、セントラルもビジーの場合、

東へ。

しかし、それは待ち時間と他のすべてのパラメーターに基づいています。

追加のサービスまたは追加のバックエンドインスタンスが作成されると、それが行うことは作成されます

ここで追加のインスタンスへのトラフィックの分散が開始されます。

そのため、インスタンスがトラフィックを取得し始めたことを知っている追加の洞察があります。

それまでの間、何らかの理由でゾーンのバックエンドサービスがあることに気付きます。

停止または誰かが正常です。

あなたはあなたの使命が壊れたものを知っています。

サービスが中央ゾーンで突然停止しました。

そのため、中央トラフィックでさえも少なくともすべてをインテリジェントにルートアウトします

無駄は、バックエンドサービスが運ぶ負荷に依存します。

そしてこれもまたインテリジェントなルーティングです。

はい。

そして、これはトラフィックを分散させるために理解する必要がある重要な差別化です

ユーザーの近くのバックエンドサービスにインテリジェントに。

ただし、十分な容量が利用可能な場合は、同時にバックエンドサービスの負荷を考慮してください

遅延を抑えて他の場所から販売できる場合、これはインテリジェントルーティングであるため、

全体的なロードバランサーとして、基本的な概念として質問があります。

ここで次の講義に移動できます。

ロードバランサーをチェックインします。
