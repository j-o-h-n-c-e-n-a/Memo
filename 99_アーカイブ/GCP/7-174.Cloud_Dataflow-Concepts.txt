We are we are actually going to learn some of the concepts which we used in last demo so that it is

data flow concepts.

Ah but you've been on concepts you drop that you can name it.

Okay.

So typically if you look at what is data flow as a PI you know they deploy is a set of processes which

work in a convention.

It takes an input and transformed it into the output and you can apply some transformation logic in

between you have some collections which can hold the data while it is getting transposed.

So typically high level concepts are the pipe so the pipe is complete.

This pipeline the P.C. collections are the collections you can think of data structures which holds

the data.

Transformations is their transformation engine you can apply multiple transformation on the data and

then you can create the next level of data collection and then ultimately I will transform these are

the connector to read the input and then create output beam driver program.

Typically if we look at the beam program it is create apply the transformation and then actually the

beam program did design consultations.

Where is your input data stored so how you read the data.

What does your data look like.

What do you want to do with the data.

Just like transforming.

How does your data output look like and where it should go.

And that is your the things.

So these are like high level concepts.

It is it takes in a you know input and then run transformation logic on top of it and then create an

output that is the whole but you beam are the data flow.

There are different contexts which you can create it out of data flow and these are like architectural

you can take thoughts around it what you can do is we can read the database and then create a piece

collection of data.

Table rows and you can apply to different transformation and create two different output collections.

You can take the data from the database tables.

You can take those rules in a collection and then apply different transformation logic and you can create

the transformation output which is to be you can take the data from the database tables order rows from

the file you can apply to different transformations and create two different outcomes and then you can

flatten the two outcomes together into one collection and then you can sink that into B query or some

other locations and you can read the data.

You can do that it was files and you can merge so take those P collections and then merge the joint

data into one so these are all you know looks really nice.

Graphic interface which is really good but if you look at actual creation of a job which is no one right.

This one job creation is not a problem or you can create use the template is not a problem but when

you're creating actual program if we are told you open the actual program it is going to be like say

I'm going and taking it to the being that are posted afterwards and you can go in and look at this particular

document.

If you want to write the actual transformation logic and see it but just getting into an overall you

know how you can write the and the pipelines it is all going to look like this one.

Okay.

Create a pipeline configure pipeline setting up the input creating the transformation logic.

So all of those are like program it is not configurable at such in using the UI.

So when you're creating a program data flow program or a budget program or pipeline it is plain simple

the Java or Python coding and then you can take that programs and put that into Google Cloud Platform

and run those jobs and that is so done on works it is like everything is managed for you.

You can go ahead and so I'll give you an example here.

So you can choose the different template you can see where you want to run it but at the same time what

you can do is you can.

Let me just give you the name.

You can go ahead and do additional parameters so you can say maximum work or node if at all the you

know the amount of data is very huge.

You can specify the zone where it should decide if at all you are very sensitive about its own execution

service I can't emulate the it is required to access some of the DCP resources and you want to provide

access using the service account then sort of secondary can provide it here.

Now you can define the machine Dave in one standard one will be used by default but if it all the The

Lord is really huge you can use the big machine types additional experiments if you want to do a network

if at all you want to choose different network altogether but by default it is going to use default

network and the subnet work if you want to use it.

So all of those mix it together as a managed service inside Google Cloud Platform because all the other

components are not about you being components it is data flow components the way you want to provision

the infrastructure the way you want to use a budget beam.

Jobs are the framework inside Google Cloud Platform that's it guys is a Apogee being our cloud data

flow let's go and look at some of the differences between data flow and data because data proc is also

used for retail jobs.

When should we use data flow versus you know that data product let's call and see some differences.


前回のデモで使用した概念のいくつかを実際に学習して、

データフローの概念。

ああ、あなたはあなたがそれを命名することができるというあなたが落とす概念の上にいました。

はい。

したがって、通常、PIとしてデータフローとは何かを見ると、展開する一連のプロセスであることがわかります。

コンベンションで働く。

入力を受け取り、それを出力に変換し、いくつかの変換ロジックを適用できます

間には、転置されている間、データを保持できるコレクションがいくつかあります。

したがって、一般的に高レベルの概念はパイプであるため、パイプは完全です。

このパイプラインはP.C.コレクションは、保持するデータ構造について考えることができるコレクションです

データ。

変換は、データに複数の変換を適用できる変換エンジンです。

次のレベルのデータ収集を作成し、最終的にこれらを変換します

入力を読み取り、出力ビームドライバプログラムを作成するコネクタ。

通常、beamプログラムを見ると、変換を適用してから実際に作成します

ビームプログラムは設計相談を行いました。

入力データはどこに保存されるので、データをどのように読み取るか。

データはどのように見えますか。

データをどうしますか。

変身のように。

データ出力はどのように見え、どこに行くべきか。

そして、それはあなたのものです。

したがって、これらは高レベルの概念に似ています。

それは、あなたが知っている入力を取り込んで、その上で変換ロジックを実行してから、

出力は全体ですが、ビームはデータフローです。

データフローから作成できるさまざまなコンテキストがあり、これらはアーキテクチャのようなものです

あなたができることは、データベースを読み取って作品を作成することです。

データの収集。

テーブルの行と異なる変換に適用し、2つの異なる出力コレクションを作成できます。

データベーステーブルからデータを取得できます。

これらのルールをコレクションに入れてから、異なる変換ロジックを適用して、作成することができます

データベーステーブルからデータを取得できる変換出力

異なる変換に適用し、2つの異なる結果を作成できるファイル

2つの結果を1つのコレクションにまとめると、それをBクエリまたはいくつかのコレクションにシンクできます

他の場所とデータを読み取ることができます。

あなたはそれがファイルであったことを行うことができ、マージすることができますので、それらのPコレクションを取り、ジョイントをマージします

データを1つにまとめることで、これらがすべてわかりやすくなります。

本当に優れたグラフィックインターフェースですが、実際のジョブの作成を見ると、誰も正しくありません。

この1つのジョブの作成は問題ではありません。テンプレートを使用して作成しても問題ありませんが、

実際のプログラムを開くように言われたら、あなたは実際のプログラムを作成しています

後で投稿された存在にそれを取りに行きます。

資料。

実際の変換ロジックを作成して表示したいが、全体の概要を把握したい場合

あなたがどのように書くことができるかを知っており、パイプラインはすべてこのようになります。

はい。

パイプラインを作成し、パイプラインを構成して、変換ロジックを作成する入力を設定します。

そのため、これらはすべてプログラムのようなものであり、UIを使用してそのような設定を行うことはできません。

したがって、プログラムデータフロープログラム、予算プログラム、またはパイプラインを作成するときは簡単です

JavaまたはPythonコーディングを行い、そのプログラムをGoogle Cloud Platformに配置できます

それらのジョブを実行しますが、それは作品で行われ、すべてがあなたのために管理されているようなものです。

先に進むことができますので、ここで例を示します。

したがって、別のテンプレートを選択して、それを実行する場所を確認できますが、同時に

できることはできることです。

名前を教えてください。

先に進み、追加のパラメーターを実行して、最大の作業またはノードを言うことができます

データの量が非常に大きいことを知っています。

独自の実行に非常に敏感であるかどうかを判断するゾーンを指定できます

いくつかのDCPリソースにアクセスするために必要であり、提供したいサービスをエミュレートできません

サービスアカウントを使用してアクセスし、セカンダリの並べ替えをここで提供できます。

これで、デフォルトで使用される1つの標準的なマシンでマシンDaveを定義できますが、

あなたはネットワークをしたいなら、あなたは大きなマシンタイプの追加実験を使用できる本当に巨大です

まったく別のネットワークを選択したいが、デフォルトではデフォルトを使用する場合

使用したい場合、ネットワークとサブネットが機能します。

そのため、これらすべてがGoogle Cloud Platform内でマネージドサービスとしてそれを組み合わせています

コンポーネントは、あなたがコンポーネントであるということではなく、あなたがプロビジョニングしたいデータフローコンポーネントです

インフラストラクチャを予算の用途に合わせて使用​​します。

ジョブはGoogle Cloud Platform内のフレームワークであり、クラウドデータであるApogeeです

データプロシージャもあるので、データフローとデータの違いをいくつか見てみましょう。

小売ジョブに使用されます。

データフローとデータフローのどちらを使用すべきかは、データ製品を呼び出していくつかの違いを確認することです。