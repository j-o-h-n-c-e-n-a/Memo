## 問題集

1. ニューラルネットワークは高い表現力を持つ反面、過学習をしやすいという性質を持つため、それを改善させる方法が多数考案されている。例えば、学習の際に一部のノードを無効化する[ア]、一部の層の出力を正規化する[イ]、データの水増しをしてデータの不足を補う[ウ]、パラメータのノルムにペナルティを課す[エ]などがそれに当たる。

2. 学習率の値は学習の進み方に大きな影響を与える。例えば、学習率が過度に[ア]とコスト関数の高い局所的最適解から抜け出せなくなることがある。また、大域的最適解に向かって収束している場合でも、学習率が[イ]と、収束は速いがコスト関数の最終的な値が高く、逆に[ウ]と収束は遅いが最終的にはより最適解に近いパラメータになるため、コスト関数は小さな値に収束する。

3. 生成モデルの一つであり、生成ネットワークと識別ネットワークの２つのネットワークを対抗させるように学習させることで、得られる生成モデルの名称として最も適切なものを１つ選べ。

4. データが少量しかないなどの理由で、対象のタスクを学習させることが困難なときに、関連する別のタスクで学習し、その学習済みの特徴やパラメータなどを利用することで効率的に対象のタスクを学習することがある。これを[ア]という。

5. ディープラーニングは、ニューラルネットワークを多層化したものであり、観測データから本質的な情報である[ア]を自動的に抽出できる点が特徴である。また、従来の機械学習手法と比べると、[学習が必要なパラメータ数が多い、計算量が多い、より複雑な関数を近似できる]という性質も持っている。

6. 機械学習においては過学習を避けるために、訓練誤差ではなく汎化誤差を可能なかぎり小さくする手法である[正則化]を用いることが多い。また複数のモデルの予測結果の平均を利用する[アンサンブル学習]がある。他にもディープニューラルネットワークに対しては、ランダムに一定の割合のノードを削除して学習を行う[ドロップアウト]が有効とされている。[正則化]手法にはいくつかのパラメータをスパースにする[Lasso]などがある。

7. ディープニューラルネットワークの普及に貢献した一つの要素に、[勾配消失問題]を克服する手法が提案されたことがある。[勾配消失問題]は誤差逆伝播法において、[イ]ことによって生じるとされている。[勾配消失問題]に対処するための一つの方法として、あらかじめ良い重みの初期値を計算する[事前学習]や、活性化関数に[ReLU]を利用するなどがある。

8. ディープニューラルネットワークの普及に貢献した一つの要素に、[ア]を克服する手法が提案されたことがある。[ア]は誤差逆伝播法において、[イ]ことによって生じるとされている。[ア]に対処するための方法として、あらかじめ良い重みの初期値を計算する[ウ]や、活性化関数に[エ]を利用するなどがある。

9. 大規模なディープニューラルネットワークの学習では学習するべきパラメータ数が膨大となるため、処理の高速化が必要となる。2012年に提案された分散並列技術である[DistBelief]や画像処理に特化したプロセッサの[GPU]は大規模なニューラルネットワークの学習を実現するために利用されてきた。また、大規模なニューラルネットワークの学習が困難となる原因の一つとして、ある層の入力がそれより下層の学習が進むにつれて変化する[内部共変量シフト]がある。[内部共変量シフト]を防ぐために出力値の分布の偏りを抑制する[バッチ正規化]が2015年に提案されている。

10. 畳み込みニューラルネットワークは、畳み込み層とプーリング層を積み上げた構成をしている。画像データを用いた場合、畳み込み層では、出力画像のサイズを調整するために元の画像の周りを固定の値で埋める[パッティング]行う。プーリング層では畳み込み層の出力を圧縮するプーリングを行う、[誤差プーリング]などの手法がある。

11. ニューラルネットワークには[バイアス]などの多くのハイパーパラメータが存在し、これらの値が精度に大きな影響を与える。ハイパーパラメータのチューニング方法としては、パラメータの候補値を指定し、それらの組み合わせを調べる[グリッドサーチ]などがある。また、近年は、ハイパーパラメータを含め最適化問題とする[ベイズ最適化]が効率的なチューニング方法として注目をあびている。

12. 自己符号化器はニューラルネットワークによる[教師なし学習]の代表的な応用であり、出力が入力に近づくようにニューラルネットを学習させる。主に[次元削減]のために利用されることが多く、活性化関数に恒等写像を用いた場合の3層の自己符号化器は[主成分分析]と同様の結果を返す。自己符号化器を多層化すると、ディープニューラルネット同様に勾配消失問題が生じるため、複雑な内部表現を得ることは困難であった。この問題に対して2006年頃に[ヒントん]らは、単層の自己符号化器に分割し入力層から繰り返し学習させる[層ごとの貪欲法]を積層自己符号化器に適用することで、汎用的な自己符号化器の利用を可能とした。また、自己符号化器の代表的な応用例として[仮想計測]がある。

13. RNNは[ア]系列データの処理に長けているニューラルネットワークである。RNNは、[内部にループ構造を持つため]勾配消失問題が起きやすいという特徴を持っていたが、RNNの一種であるLSTMでは[ウ]を含むLSTM Blockを組み込むことで、長期間の系列情報に対しても勾配消失せずに学習を行うことができた。

14. ディープニューラルネットワークの学習の目的は[出力関数]を最小化することであり、この最適化のために勾配降下法が利用される。しかし、勾配降下法にはパラメータの勾配を数値的に求めると[計算量が膨大となってしまう]問題があり、このような問題を避けるために誤差逆伝播法が利用される。またディープラーニングには過学習の問題もある。過学習とは[訓練誤差]は小さいにも関わらず、[汎化誤差]が小さくならないことであり、これらの問題を克服するために様々な手法の開発が進められている。

15. 勾配降下法においてパラメータの更新量を決める[学習率]の決定は重要である。例えば[学習率]が小さすぎると[収束が遅くなる]などの課題が生じるため、[スタッキング]などの様々な[学習率]調整手法が提案されている。

16. CNNで行われる畳み込み演算の計算処理について考える。5×5のサイズの画像に対して、3×3のフィルタをパディング1、ストライド1で適当した場合の出力の図のサイズを答えよ。

17. ディープニューラルネットワークのパラメータ最適化手法として[確定的勾配降下法]などの勾配降下法が適用される。しかし、勾配降下法には[大域的最適解への収束]などの問題があり、これらの問題に対処するために、学習率をパラメータに適応させることで自動的に学習率を調整することができる[Adagrad]や勾配の平均と分散をオンラインで推定し利用する[Adam]が利用されてきた。

18. 機械学習の分野において有名な二つの定理について扱う。[醜いアヒルの子の定理]は、認知できる全ての客観的な特徴に基づくと全ての対象は同程度に類似している、つまり特徴を選択しなければ表現の類似度に基づく分類は不可能であることを示している。[ノーフリーランチ定理]は、全てのタスクに対して常に他よりすぐれている万能的なアルゴリズムは存在しないことを示している。

19. 深層学習のモデルは、確定的モデルと確率的モデルに分類することができる。これらのモデルの例として、確定的モデルに[ア]や確率的モデルに[イ]がある。

20. 深層学習を含めて機械学習において精度の高い学習をするためには、観測データの適切な前処理が必須である。異なるスケールの特徴量を同時に扱えるようにするために、平均を0に分散を1に規格化する[標準化]や、特徴量の線形結合からデータ内の分散が大きくなるような特徴量を得る[主成分分析]などは広く利用されている。また、画像処理の分野においては、減算正規化と除算正規化の処理を行う[局所コントラスト正規化]などが前処理として利用され、[OpenCV]などの画像処理に特化したライブラリで行うことができる。また、自然言語処理の分野においては、文章に単語が含まれているかどうかを考えてテキストデータを数値化する[bag-og-words]や文章に含まれる単語の重要度を特徴量とする[TF-IDF]などがある。

22. 生成モデルとは、訓練データからそのデータの特徴を学習し、類似したデータを生成することができるモデルである。ディープニューラルネットの生成モデルの例として、自己符号化器の潜在変数に確率分布を導入した[ア]や、訓練データと生成器が生成したデータを識別器で判別させることによって学習を進める[イ]がある。

23. [adversarial example]は深層学習における重要な課題の一つであり、学習済みのディープニューラルネットモデルを欺くように人工的に作られたサンプルのことである。サンプルに対して微小な摂動を加えることで、作為的にモデルの誤認識を引き起こすことができる。

24. 強化学習では、行動を学習する[エージェント]と[エージェント]が行動を加える対象である[環境]を考え、行動に応じて[環境]は[エージェント]に状態と[報酬]を返す。行動と状態/[報酬]の獲得を繰り返し、最も多くの[報酬]をもらえるような方策を得ることが強化学習の目的である。

25. 強化学習において、行動価値関数の関数近似に畳み込みニューラルネットワークを用いた手法が[ア]である。

26. 確率的勾配法は深層学習において最もよく知られる最適化アルゴリズムであり、いくつかの改善を加えたものが広く使われている。例えば、以前に適用した勾配の方向を現在のパラメータ更新にも影響させる[モメンタム]という手法や、勾配を２乗した値を蓄積し、すでに大きく更新されたパラメータほど更新量[学習率]を小さくする[AdaGrad]や、[AdaGrad]における一度更新量が飽和した重みはもう更新されないという欠点を、指数移動平均を蓄積することにより解決した[RMSprop]などがある。

27. ディープラーニングの技術を利用したシステムを開発する際、複雑な処理が比較的簡潔に記述できることから、既存のフレームワークを利用することも多い。ディープラーニングのフレームワークは複数あり、Google社提供の[ア]や[ア]のラッパーとして機能する[イ]、国内企業であるPreferredNetworksで開発された[ウ]などがある。また、[エ]は[ウ]と同じDefine-by-Run方式を採用している。

28. ニューラルネットワークの学習には独自の問題が生じる。層を深くするほど、入力層に近い層で学習が行われにくくなる[ア]問題が起こったり、パラメータがつくる空間が高次元になり、その空間内の局所最適解や[イ]にトラップされることが多くなる。

29. 自己符号化器[Autoencoder]は、出力が入力と同じものに近づくことを目指して学習する。[ア]のアルゴリズムであり、[イ]が可能になる。このときの[ウ]が入力の特徴を抽出した表現となる。

30. 機械学習において、重み更新に関わる単位として、[ア]と[イ]がある。[ア]は、重みが更新された回数であり、[イ]は訓練データを何回繰り返し学習したかを表す単位である。また一回の[ア]に用いるサンプル数は[ウ]と呼ばれる。

31. 活性化関数とは、ニューロンの出力に何らかの非線形な変数を加える関数である。単純パーセプトロンの出力層では[ア]が用いられ、ニューラルネットワークの中間層では、はじめ[イ]などの正規化の機能を持つ関数が好まれた。しかし現在では、誤差逆伝播で勾配が消失しやすいという問題から、中間層では勾配消失問題の影響を抑えられ、かつ簡単な[ウ]などが用いられている。また、出力層では出力の総和が１になるため確率的な解釈が可能になる[エ]がよく用いられる。

32. 大きなニューラルネットワークなどの入出力をより小さなネットワークに学習させる技術として、[蒸留]がある。[蒸留]とは、すでに学習されているモデル[教師モデル]を利用して、より小さくシンプルなモデル[生徒モデル]を学習させる手法である。こうすることにより、生徒モデルを単独で学習させる場合よりも[過学習を緩和する]ことができる。

33. 画像データに対しては、前処理を施すことが多い。カラー画像を白黒画像に変換して計算量を削減する[グレースケール]や、細かいノイズの影響を除去する[平滑化]、画素ごとの明るさをスケーリングする[ヒストグラム平均]などがこれに含まれる。

34. 畳み込みニューラルネットワークの[ア]のパラメータ数は[イ]と比較して極めて少ない。これは[ウ]によって[エ]ため、パラメータ数が減り、計算量が少なくなるためである。

35. 現在、人工知能研究は抽象概念や知識理解に辿り着くために大きく分けて３つの路線を辿っている。この３ つの路線は、とりわけある企業や大学によって研究が進められている。
・言語データによるRNNや映像データからの概念・知識理解を目指す(ア)路線
・実世界を対象に研究を進め、知識理解を目指す(イ)路線
・オンライン空間上でできることをターゲットにして、知識理解を目指す(ウ)路線 

36. [ア]はデータに潜む空間的構造をモデル化する。[イ]は時間的構造をモデル化する。

37. 人間の脳における学習の枠組みに基づいた３つの学習が機械学習には存在する。１つ目は小脳の働きを模 倣した[ア]である。これは学習者に対して、教師が間違いを指摘し、学習者が正しい解を得ることである。２つ 目は大脳皮質の働きを模倣した[イ]である。代表的な手法として主成分分析などの次元圧縮手法がある。３つ目 は大脳基底核の働きを模倣した[ウ]である。学習者は正解値でなく、行動した結果に基づいた報酬が与えられる。この報酬をなるべく大きくするように学習者が行動していく。

38. 狭い意味でのディープラーニングとは層の数が深いニューラルネットワークを用いた機械学習である。複数の層を持つ階層的ニューラルネットワークは、1980年代には(ア)という方法がすでに提案されていたが、現在ほど多くの層を持った学習をすることはできなかった。その理由として２つの理由が挙げられる。１つ目は、出力層における誤差を入力層に向けて伝播させる間に、誤差情報が徐々に拡散し、入力層に近い層では勾配の値が小さくなって学習がうまく進まないという問題が発生したからだ。このことを(イ)という。２つ目は、層の数が多いニューラルネットワークの学習の目的関数は多くの(ウ)を持ち、適切な結合の重みの初期値の設定が難しかった。

39. 現在の教師あり学習は、与えられたデータがどの分類に当てはまるのかを識別する(ア)と、様々な関連性のある過去の数値から未知の数値を予測する(イ)という２つに分類される。(ア)を用いることで、(ウ)のようなことができる。また(イ)を用いることで、(エ)のようなことができる。

40. 表現学習とは、ディープラーニングを抽象化した概念で、画像、音、自然言語などの要素を、予測問題として解くことで分散表現[ベクトル]を得て、各々の要素を抽象化する手法である。こうした特徴表現は、通常は 人間の知識によって定義されるが、それによって機械学習の性能が大きく異なってしまう。こうした知的な情報処理を可能にしたのがディープラーニングである。ディープラーニングは観測データの説明要因を捉え、人間の知識では気がつくことができない共通点を捉えることができるが、この共通点のことをよい表現という。ヨシュ ア・ベンジオ氏は良い表現に共通する、世界に関する多くの一般的な事前知識として、いくつかを提唱している。よい表現として当てはまらないものを選択肢から1つ選べ。

41. 良い表現として、ディープラーニングのアプローチは(ア)、 (イ)、(ウ)に着目している。このことから、(ア)、(イ)、(ウ)の事前知識を適切に活用できるなら、表現学習は必 ずしも層の数が多いニューラルネットワークの形をしていなくてもよいことになる。

42. 教師なし学習の中で有名なのは、未知の集合を、いくつかの集まりに分類させる(ア)という学習方法と、正常な行為がどのようなものかを学習し、それと大きく異なるものを識別する(イ)がある。(ア)は特に(ウ)というア ルゴリズムを使用して顧客の分類分けによるDM配信やレコメンドを行う。(イ)は(エ)というアルゴリズムを基に、セキュリティシステムなどに使用されている。

43. 画像の認識では、主に入力から出力に向かう結合のみを持つ階層的なニューラルネットワーク、特に画像などの信号に内在する局所的な特徴が集まって、より大域的な特徴を構成するという構造を反映した、(ア)がよく用いられる。一方、自然言語テキストや動画に代表される構造を持った系列情報を扱うために(イ)が用いられている。特にケプラー大学のゼップ・ホフレイター氏の提案した(ウ)は必要な文脈情報の長さを適応的に制御することで、時間を遡る誤差逆伝播の可能性向上させ、画像からの説明文の生成や機械翻訳など、多くの課題に適用されている。実際、2016年秋に、GoogleはGoogle翻訳に(ウ)を取り入れてアップデートし、非常に高精度な翻訳を提供することが可能になった。

44. 2012に開催された一般物体認識のコンテスト「ILSVRC」(ImageNet Large Scale Visual Recognition Challenge)において、深い構造を持つCNNが、従来手法の分類性能を大幅に上回って以来、ディープラーニングが画像認識に盛んに用いられるようになった。ディープラーニングの画像認識への応用先として正しい組み合わせを選択肢から1つ選べ。

45. クラス分類の領域では、CNNという沢山の層を重ねて、深い階層構造した手法によって研究が進めらていて、従来の手法よりも精度の高い認識や分類が可能となった。しかし、沢山の層を重ねた結果、学習に用いられるパラメータの数が膨大となり、学習が上手く進まないという問題が生じていた。その問題を解決するために提案されたのが(ア)である。(ア)は、入力層から出力層まで伝播する値と入力層の値を足し合わせたモデルで、この方法によって、入力層まで、勾配値がきちんと伝わり、今では1000層といったかなり深い構造でも学習が可能となった。実際、2015年のILSVRCで(ア)は人間の成績を上回る成果をあげている

46. 物体検出とは(ア)である。一方物体セグメンテーションとは(イ)である。

47. 画像キャプションとは、ある画像からそこに写っているものの説明を生成する、画像処理と自然言語処理の融合分野である。キャプションは、対象となる画像を(ア)に入力し、そこから得られた特徴を(イ)に入力することで生成することが可能である。

48. 画像生成とは、何もない状態、もしくはある入力値に応じて目標の画像を生成する技術である。今最も利用されている画像生成手法は、GANという生成敵対ネットワークである。特に、あるランダムな数値の入力値をもとに画像生成を行うDC[ア]やある文章から画像を生成するAttention[ア]などが有名である。このネットワー クは(イ)と(ウ)から構成されており、(イ)は(エ)を騙すような画像を出力し、(ウ)は(イ)から出力された画像と本物の画像とを分類するようにそれぞれ学習する。このように学習することで、(イ)は適切な画像を出力することが可能となる。 

49. 従来は、現在のディープラーニングのように入力から出力までの処理を一括で行うことができない情報を扱うことがあった。そうした場合、まず用意したデータをある手法を用いて加工し、それが入力値となり、別の手法を用いて処理を行いといった、ステップバイステップの学習が必要だった。しかし、ディープラーニングの登場によって、処理を複数回に分けて行う必要がなくなったこのような、深層学習において重要な方法論のことを(ア)と呼ぶ。

50. 1990年代の音声認識は(ア)による、音自体を判別するための音響モデルと、(イ)による語と語のつながりを判別する言語モデルの両方でできている。しかし、ディープラーニングの登場、とりわけ(ウ)の登場により、音響特徴量から音素、文字列、更には単語列に直接変換するEnd to Endモデルというアプローチを取ることが可能になり、人的に前処理を行わなくても解析することが可能となった。

51. ディープラーニングはソフトウェアフレームワークを利用して実装するのが一般的である。多層のニューラルネットワークモデルを定義し、データを用いて学習・予測を実行するのがフレームワークの役割だが、重要なのはネットワークの記述方法とその柔軟性である。ネットワークには大きく分けて２つの記述方法がある。１つ目は(ア)による記述方法である。これらの記述方法を採用しているソフトウェアには(イ)があげられる。この方法を用いることによって、モデルの定義がテキストで設定でき、簡単に学習を開始開始させることが出来るというメリットがある。一方で、ループ構造をもつようなRNNなど、複雑なモデルを扱う際には、モデルの定義を記述することは難しくなる傾向にある。２つ目は(ウ)による記述方法である。代表的なフレームワークとして(エ)があげられる。一度書き方を覚えてしまば、複雑なモデルでも比較的簡単に記述することが出来るが、モデルは、それぞれのフレームワーク固有のソースコードで出来上がるため、モデルが使用しているソフトウェアに依存してしまうという問題がある。

52. 線形モデルとは、(ア)を含む項の線形結合で、(ア)を含んだ数式の出力値は(イ)と呼ばれる。この線形結合で、特に(ア)も(イ)も一次元のデータの場合は、y = b0 + b1 * xと表される。こういったモデルを単回帰モデルと呼んだりもする。この数式に置いて、各項の係数(例えばb0, b1)を(ウ)と呼び、このモデルを用いてテストデータを学習し、測定した実データを推定する。注意点として、(イ)が連続の値を取り扱う場合(エ)と呼ばれるが、離散の値を取り扱われる場合は(オ)と呼ばれ、それぞれ名称が異なる。ただ、実際のデータを扱うときに、(ア)が１次元であることはほとんどなく、２次元以上になることが一般的である。このような場合、(ア)の次元数分だけ、係数パラメータを増やして、モデルを拡張する必要がある。このように(ア)が２つ以上の場合を(カ)モデルと呼び、各項の係数パラメーターを(キ)という。またモデルによって出力された値と実際の測定値の誤差を(ク)という。この(ク)を用いて係数パラメータを推定する代表的なアルゴリズムに最小二乗法と最尤推定法がある。

53. ディープラーニングの学習の目的は、損失関数の値をできるだけ小さくするパラメータを見つけることで ある。このような問題を解くことを(ア)という。このパラメータを見つけるアルゴリズムとして有名なのは(イ) である。ただ、(イ)は対象の関数の形がある分布や方向に依存すると非効率な経路でパラメータを探索してしまい、学習に時間がかかってしまうというデメリットがある。そこで、現在では(イ)の欠点を改善するために(ウ) などのアルゴリズムが使用されている。

54. 機械が試行錯誤することで、取るべき最善の行動を決定する問題を扱うことができる学習方法を(ア)という。(ア)はボードゲームや自動運転、またロボットの歩行動作などに活用されている。代表的なアルゴリズムに (イ)があげられる。(ア)の課題として、主に(ウ)や(エ)などが挙げられる。理論的には無限に学習するが、実世界では全てが限られている。ロボットの場合、無限の試行を繰り返すことができず、損耗し、実験の続行が困難になる。そこで人間側がタスクを上手く切り分けてやさしいタスクからの学習をすることが期待される。また(エ)に関して、例として、２体のロボット同士で学習を開始させようとすると、お互いに初期状態であるタスクについての何も知識がない状態だと、学習過程の不安定化が見られる。現在はこれに対応するために逆強化学習やディープラーニングの技術を適用した(オ)などが適用され始めている。

55. 昨今、ディープラーニングを活用した音声認識技術、音声生成技術の向上に伴い、スマートスピーカーが普及しつつある。下記の選択肢のうち、スマートスピーカーの音声アシスタントソフトウェアの名称とその提供元の組み合わせとして正しいものを選択肢から１つ選べ。

56. AIの研究開発が進むにつれて、実世界への社会実装で最も期待されている分野の１つに自動走行車の開発が挙げられる。現在AIを用いた自動走行車には、その自動運転導入の程度に応じてレベルづけがなされており、各社でどのように最終的なゴールであるレベル５の完全自動走行に近づくかが議論されている。
各レベルにおける自動運転の概要について説明した文章のうち、自動運転レベル３に対応しているものを選択肢より一つ選べ。

57. 自動運転レベル５に至るには、２つのアプローチが存在している。１つは自動運転レベル１から徐々に運転自動化の範囲を広げていくアプローチ、もう１つは直接レベル３以上の自動運転を目指そうとするものである。この時、前者のレベル１から徐々に運転自動化を目指すアプローチを採っているプレイヤーは (ア) などである。他方で、後者の直接レベル３以上の運転自動化を目指すアプローチを採っているプレイヤーは (イ) である。また後者のアプローチを採る企業として著名なのは、Google 傘下の (ウ) 社である。

58. AI 技術の進展により一般に普及する可能性が急激に高まってきたのが、小型無人機 (以下ドローン) である。他方でこれまでに存在しなかった新しいプロダクトで、人々がこれまで意識することのなかった空域などの問題が生じてきた。例えば、ドローンを飛ばす空域は、飛ばすのに許可が必要な空域がある。またドローンはその利用方法に応じて、承認が必要となることがある。

59. ディープラーニングの活用を進めていく必要性の高まりに対して、日本国内においてはそうした先端IT技術に精通した人材不足が懸念されている。
例えば、経済産業省が定めた先端IT人材がどのような人材需給状況にあるかの推定によると、2020年には需給ギャップが広がり人材の不足は (ア) に及ぶと言われている。
こうした人材不足を解消するべく、様々な方法でAIに理解のある人材育成が試みられている。そのような試みの一つとして、MOOCs は期待を寄せられている。著名な例としては、AI研究の第一人者で、2014年から2017年にかけて Baidu の AI 研究所所長を務めた (イ) が創業した Coursera などは入門から上級まで様々なレベルの AI 講義が開かれており、多くの受講者を惹きつけるに至っている。

60. ディープラーニングでの学習を効率的に行うにあたって、共有データセットの整備が徐々に進められている。しかしながら、現在広く普及しているものには、いくつかの問題点が指摘されている。
第一は、 (ア) の問題である。現在は公正な利用がなされているとされているが、企業が共有データセットを利用して学習したモデルを自社のプロダクトに転用して売り上げを上げようとした場合に問題はないのかという議論が巻き起こっている。
他の問題として、これは日本にとっての問題であるが、多くのデータセットが (イ) であることが挙げられる。これにより、日本固有の食べ物を認識しようとすると、それが全く別の国の食べ物としてのみ認識されるという不具合が生じるに至っている。

61. 高度なAIモデルを作成していく為には、こうした共有のデータセットの拡充を進めると共に、学習モデルの共有を進め、こうした公開共有されたモデルを基にして独自のデータセットを適用して調整をしながら新たに学習をさせる (ア) が実用上鍵となるのではないかと言われている。

62. 全ての欠損値が完全に生じている場合には、様々な手法を使ってこれに対処することができる。１つは欠損があるサンプルをそのまま削除してしまう (ア) である。これは欠損に偏りがあった場合には、データ全体の傾向を大きく変えてしまうことになるので使用する際には欠損に特定の偏りがないかを確認して使用することが肝要である。
他の事例としては、欠損しているある特徴量と相関が強い他の特徴量が存在している場合は、(イ) という方法もある。

63. 機械学習による分析を行う際、カテゴリーデータをそのまま扱うのは非常に難しい。このため、これを数値に変換して扱いやすくすることが一般的である。
ドリンクのサイズS, M, Lなどの順序を持つ文字列のカテゴリーデータの場合、それぞれの値に対応する数値を辞書型データで用意し、これを数値に変化する方法 (ア) を利用して変換を行うことがある。
また順序を持たない名義特徴量のカテゴリーデータについては、各変数に対応したダミー変数を新たに作り出す (イ) が有用である。

64. 既存の学習済みニューラルネットワークモデルを活用する手法に[ア]と[イ]がある。[ア]では、学習済みモデルに対して新たに別の課題を学習させることで、少量のデータセットかつ少ない計算量で高い性能のモデルを得ることができる。また、[イ]は、学習済みの大規模モデルの入力と出力を小規模なモデルの教師データとして利用することで、少ない計算資源で従来のモデルと同程度の性能を実現することが可能となる。

65. 畳み込みニューラルネットワークの手法について扱う。
(ア)は1998年に提案された、現在広く使われているCNNの元となるモデルであり、初めて多層CNNに誤差逆伝播法を適用した手法である。2012年に提案された(イ)は、画像認識のコンペティションILSVRCで他手法に圧倒的な差をつけて優勝し、画像認識におけるディープラーニング活用の火付け役となった。しかし、一般にCNNは層を深くすると、パラメータ数が膨大となり学習が困難になってしまう傾向があった。層が深くなってもうまく学習を行うことができるモデルとして、ILSVRC2015において多くの部門でトップの成績を収めた[ウ]がある。[ウ]は出力を入力と入力からの差分の和で表現したニューラルネットワークモデルである。

66. ニューラルネットワークで用いられる活性化関数について扱う。出力層の活性化関数には、回帰では[ア]が、多クラス分類では(イ)が一般的に利用されてきた。また中間層の活性化関数として、従来は[ウ]などが一般的に利用されてきた。しかし、これらの活性化関数を利用すると勾配消失問題が起きやすいという問題があったため、近年は、入力が0を超えていれば入力をそのまま出力に渡し、0未満であれば出力を0とする[エ]や複数の線形関数の中での最大値を利用する[オ]などが利用されている。

67. 正則化とは、機械学習の学習において汎化誤差をできるだけ小さくするための手法の総称である。ディープニューラルネットワークの学習で一般に用いられる正則化の手法に[ア]があり、誤差関数に重みのL2ノルムを加えることで重みの発散を抑えることができる。また、L2ノルムの代わりにL1ノルムを用いるL1正則化は、[イ]の一種であり、重要でないパラメータを0に近づけることができる。 L1正則化を回帰に利用した場合、[ウ]と呼ばれる。

68. 深層学習の実験に用いられるデータセットについて扱う。[ア]はアメリカの国立標準技術研究所によって提供されている手書き数字のデータベースである。また、スタンフォード大学がインターネット上から画像を集めて分類したデータセットである[イ]は、約1400万枚の自然画像を有しており、画像認識の様々なタスクに利用される。

69. 機械学習の手法は学習の枠組みに応じて主に3つに分類することができる。
[ア]は入力とそれに対する出力のペアの集合を学習用データとする手法で、[イ]などが[ア]に含まれる。[ウ]は入力の集合だけから学習を行う手法であり、[エ]などが[ウ]に含まれる。最後に[オ]は、最終結果または連続した行動の結果に対して報酬を与え、報酬ができるだけ大きくなるような行動を探索する手法である。

70. 機械学習の種類を大きく分類すると教師あり学習、教師なし学習、強化学習がある。ニューラルネットワークにもそれらに対応するものがある。例えば、教師あり学習には[ア]、教師なし学習には[イ]、強化学習には[ウ]などがある。

71. 畳み込みニューラルネットワークに特有の構造として、畳み込み層とプーリング層がある。これらは画像から特徴量を抽出するために用いられる。逆に特徴量[特徴マップ]から画像を生成する際には、それらと逆の操作を行う。代表的な構造として、畳込み層の逆操作である[ア]やプーリングの逆操作である[イ]がある。これらの構造を用いるタスクの例として[ウ]がある。

72. ニューラルネットワークの学習は、損失関数[コスト関数]の最適化により行われる。そして、その損失関数は学習の目的に応じて決定する。よく使われる損失関数として、回帰問題には[ア]、分類問題には[イ]がある。また分布を直接学習する際には[ウ]が用いられることもある。さらに、損失関数にパラメータの二乗ノルムを加えると[エ]となる。

73. ニューラルネットワークには様々なモデルがあり、タスクによって適切な選択をする必要がある。例えば、画像を扱う際には[ア]、自然言語処理などの系列データには[イ]がよく使われる。他にも次元削減には[ウ]、画像生成には[エ]などが用いられる。

74. ニューラルネットワークには様々なモデルがあり、タスクによって適切な選択をする必要がある。例えば、画像を扱う際には[ア]、自然言語処理などの系列データには[イ]がよく使われる。他にも次元削減には[ウ]、画像生成には[エ]などが用いられる。

75. ニューラルネットワークの学習には勾配降下法が用いられる。勾配降下法の手順を適切な順番に並べ替えたとき、１番目になるのはどれか。
A．重みとバイアスを初期化する。
B．誤差を減らすように重み[バイアス]を修正する。
C．最適な重みやバイアスになるなるまで繰り返す。
D．ネットワークの出力と正解ラベルとの誤差を計算する。
E．データ[ミニバッチ]をネットワークに入力し出力を得る。

76. ニューラルネットワークの学習には勾配降下法が用いられる。勾配降下法の手順を適切な順番に並べ替えたとき、２番目になるのはどれか。
A．重みとバイアスを初期化する。
B．誤差を減らすように重み[バイアス]を修正する。
C．最適な重みやバイアスになるなるまで繰り返す。
D．ネットワークの出力と正解ラベルとの誤差を計算する。
E．データ[ミニバッチ]をネットワークに入力し出力を得る。

77. ニューラルネットワークの学習には勾配降下法が用いられる。勾配降下法の手順を適切な順番に並べ替えたとき、３番目になるのはどれか。
A．重みとバイアスを初期化する。
B．誤差を減らすように重み[バイアス]を修正する。
C．最適な重みやバイアスになるなるまで繰り返す。
D．ネットワークの出力と正解ラベルとの誤差を計算する。
E．データ[ミニバッチ]をネットワークに入力し出力を得る。

78. ニューラルネットワークの学習には勾配降下法が用いられる。勾配降下法の手順を適切な順番に並べ替えたとき、４番目になるのはどれか。
A．重みとバイアスを初期化する。
B．誤差を減らすように重み[バイアス]を修正する。
C．最適な重みやバイアスになるなるまで繰り返す。
D．ネットワークの出力と正解ラベルとの誤差を計算する。
E．データ[ミニバッチ]をネットワークに入力し出力を得る。

79. ニューラルネットワークの学習には勾配降下法が用いられる。勾配降下法の手順を適切な順番に並べ替えたとき、５番目になるのはどれか。
A．重みとバイアスを初期化する。
B．誤差を減らすように重み[バイアス]を修正する。
C．最適な重みやバイアスになるなるまで繰り返す。
D．ネットワークの出力と正解ラベルとの誤差を計算する。
E．データ[ミニバッチ]をネットワークに入力し出力を得る。

80. 画像認識のモデルとしてResNetがある。これは求めたい関数と入力との差である[ア]を学習するようにしたことで深いネットワークの学習を容易にした。

## 特徴マップの問題
* CNNによって、幅：1500、高さ：1500ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 736
* CNNによって幅x高さ=250x250ピクセルの画像を入力とし12x12ピクセルのフィルタを用いて、ストライド2で、大きさ1で0パディングして畳み込み層で学習を行った結果得られる特徴マップのサイズは次のうちどれか？ 121*121
* CNNによって、幅：900、高さ：900ピクセルの画像を入力とし、幅：3、高さ：3ピクセルのフィルタを用いて、ストライド：3で、大きさ：3の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？302
* CNNによって、幅：400、高さ：400ピクセルの画像を入力とし、幅：70、高さ：70ピクセルのフィルタを用いて、ストライド：1で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 333
* CNNによって、幅：777、高さ：777ピクセルの画像を入力とし、幅：7、高さ：7ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 387
* CNNによって、幅：114、高さ：114ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 43
* CNNによって、幅：918、高さ：918ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 445
* CNNによって幅x高さ=300x400ピクセルの画像を入力とし24x24ピクセルのフィルタを用いて、ストライド2で、大きさ2で0パディングして畳み込み層で学習を行った結果得られる特徴マップのサイズは次のうちどれか？ 141*191
* CNNによって、幅：500、高さ：500ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 236
* CNNによって、幅：200、高さ：200ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 86
* CNNによって、幅：40、高さ：40ピクセルの画像を入力とし、幅：4、高さ：4ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 20
* CNNによって、幅：100、高さ：100ピクセルの画像を入力とし、幅：10、高さ：10ピクセルのフィルタを用いて、ストライド：1で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 93
* CNNによって、幅：100、高さ：100ピクセルの画像を入力とし、幅：4、高さ：4ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 50
* CNNによって、幅：13、高さ：13ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 8
* CNNによって、幅：450、高さ：450ピクセルの画像を入力とし、幅：20、高さ：20ピクセルのフィルタを用いて、ストライド：1で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 433
* CNNによって、幅：418、高さ：418ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 195
* CNNによって、幅：112、高さ：112ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 42
* CNNによって幅x高さ=400x400ピクセルの画像を入力とし24x24ピクセルのフィルタを用いて、ストライド3で、大きさ1で0パディングして畳み込み層で学習を行った結果得られる特徴マップのサイズは次のうちどれか？ 127
* CNNによって、幅：300、高さ：300ピクセルの画像を入力とし、幅：50、高さ：50ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 127
* CNNによって、幅：1600、高さ：1600ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 786
* CNNによって幅x高さ=300x300ピクセルの画像を入力とし12x12ピクセルのフィルタを用いて、ストライド1で、大きさ1で0パディングして畳み込み層で学習を行った結果得られる特徴マップのサイズは次のうちどれか？ 291*291
* CNNによって、幅：200、高さ：200ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 86
* CNNによって、幅：333、高さ：333ピクセルの画像を入力とし、幅：5、高さ：5ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 166
* CNNによって、幅：300、高さ：300ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：1で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 271
* CNNによって、幅：800、高さ：800ピクセルの画像を入力とし、幅：5、高さ：5ピクセルのフィルタを用いて、ストライド：5で、大きさ：5の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？162
* CNNによって、幅：1100、高さ：1100ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 536
* CNNによって、幅：20、高さ：20ピクセルの画像を入力とし、幅：5、高さ：5ピクセルのフィルタを用いて、ストライド：1で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 18
* CNNによって、幅：110、高さ：110ピクセルの画像を入力とし、幅：4、高さ：4ピクセルのフィルタを用いて、ストライド：1で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 109
* CNNによって、幅：80、高さ：80ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 26
* CNNによって、幅：700、高さ：700ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：3の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 338
* CNNによって、幅：50、高さ：50ピクセルの画像を入力とし、幅：4、高さ：4ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 25
* y=ax + by + czという式をzについて偏微分した答えを次から選べ。[c]
* CNNによって、幅：555、高さ：555ピクセルの画像を入力とし、幅：5、高さ：5ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 277
* CNNによって、幅：500、高さ：500ピクセルの画像を入力とし、幅：10、高さ：10ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 247
* f(x)=ax+by+czという関数がある。この関数をyについて偏微分せよ。[b]
* CNNによって、幅：95、高さ：95ピクセルの画像を入力とし、幅：5、高さ：5ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 47
* CNNによって、幅：916、高さ：916ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？444
* CNNによって、幅：300、高さ：300ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：3で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 91
* CNNによって、幅：200、高さ：200ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 86
* CNNによって、幅：900、高さ：900ピクセルの画像を入力とし、幅：4、高さ：4ピクセルのフィルタを用いて、ストライド：4で、大きさ：4の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 227
* CNNによって幅x高さ=1200x1200ピクセルの画像を入力とし240x240ピクセルのフィルタを用いて、ストライド1で、大きさ10で0パディングして畳み込み層で学習を行った結果得られる特徴マップのサイズは次のうちどれか？ 981*981
* CNNによって、幅：1800、高さ：1800ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 886
* CNNによって、幅：400、高さ：400ピクセルの画像を入力とし、幅：32、高さ：32ピクセルのフィルタを用いて、ストライド：2で、大きさ：1の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか？ 186
* CNNによって、幅：100、高さ：100ピクセルの画像を入力とし、幅：16、高さ：16ピクセルのフィルタを用いて、ストライド：2で、大きさ：2の0パディングを行った結果、特徴マップのサイズは次のうちどれと等しくなるか45
* CNNによって幅x高さ=764x900ピクセルの画像を入力とし32x32ピクセルのフィルタを用いて、ストライド2で、大きさ1で0パディングして畳み込み層で学習を行った結果得られる特徴マップのサイズは次のうちどれか？ 368*436
* 
* 

